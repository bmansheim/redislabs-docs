[{"uri":"/ri/release-notes/v.2.16.0/","uriRel":"/ri/release-notes/v.2.16.0/","title":"RedisInsight v2.16.0, December 2022","tags":[],"keywords":[],"description":"RedisInsight v2.16.0","content":"2.16.0 (December 2022) This is the General Availability (GA) release of RedisInsight 2.16.\nHighlights Bulk import database connections from a file Navigation enhancements for the Tree view Pre-populated host, port, and database alias in the form when adding a new Redis database Details Features and improvements\n#1492, #1497, #1500, #1502 Migrate your database connections from other Redis GUIs, including RESP.app, with the new feature to bulk import database connections from a file. #1506 Pre-populated host (127.0.0.1), port (6379, or 26379 for Sentinel connection type), and database alias in the form when adding a new Redis database #1473 Browser view is renamed List view to avoid confusion with the Browser tool #1464 Navigation enhancements for the Tree view, covering cases when filters are applied, the list of keys is refreshed or the view is switched to the Tree view #1481, #1482, #1489 Indication of new database connections that have been manually added, auto-discovered or imported, but not opened yet #1499 Display values of JSON keys when JSON.DEBUG MEMORY is not available Bugs\n#1514 Scan the database even when the DBSIZE returns 0 ","categories":[]},{"uri":"/ri/release-notes/v.2.14.0/","uriRel":"/ri/release-notes/v.2.14.0/","title":"RedisInsight v2.14.0, November 2022","tags":[],"keywords":[],"description":"RedisInsight v2.14.0","content":"2.14.0 (November 2022) This is the General Availability (GA) release of RedisInsight 2.14.\nHighlights Support for search capabilities in Browser: Create secondary index via dedicated form, run queries and full-text search in Browser or Tree views Ability to resize the column width of key values when displaying hashes, lists, and sorted sets Command processing time displayed as part of the result in Workbench Details Features and improvements\n#1345, #1346, #1376 Added support for search capabilities in Browser tool. Create secondary index of your data using a dedicated form. Conveniently run your queries and full-text search against the preselected index and display results in Browser or Tree views. #1385 Resize the column width of key values when displaying hashes, lists, and sorted sets #1354 Do not scroll to the end of results when double-clicking a command output in CLI #1347 Display command processing time as part of the result in Workbench (time taken to process the command by both RedisInsight backend and Redis) #1351 Display the namespaces section in the Database analysis report when no namespaces were found ","categories":[]},{"uri":"/ri/release-notes/v.2.18.0/","uriRel":"/ri/release-notes/v.2.18.0/","title":"RedisInsight v2.18.0, January 2023","tags":[],"keywords":[],"description":"RedisInsight v2.18.0","content":"2.18.0 (January 2023) This is the General Availability (GA) release of RedisInsight 2.18.\nHighlights Support for SSH tunnel to connect to your Redis database Ability to switch between database indexes while connected to your database Recommendations on how to optimize the usage of your database Details Features and improvements\n#1567, #1576, #1577 Connect to your Redis database via SSH tunnel using a password or private key in PEM format. #1540, #1608 Switch between database indexes while connected to your database in Browser, Workbench, and Database Analysis. #1457, #1465, #1590 Run Database Analysis to generate recommendations on how to save memory and optimize the usage of your database. These recommendations are based on industry standards and Redis best practices. Upvote or downvote recommendations in terms of their usefulness. #1598 Check and highlight the JSON syntax using new Monaco Editor. #1583 Click a pencil icon to make changes to database aliases. #1579 Increase the database password length limitation to 10,000. ","categories":[]},{"uri":"/ri/release-notes/v2.12.0/","uriRel":"/ri/release-notes/v2.12.0/","title":"RedisInsight v2.12.0, October 2022","tags":[],"keywords":[],"description":"RedisInsight v2.12.0","content":"2.12.0 (October 2022) This is the General Availability (GA) release of RedisInsight 2.12.\nHighlights Database Analysis: Get insights and optimize the usage and performance of your Redis or Redis Stack based on the overview of the memory and data type distribution, big or complicated keys, and namespaces used Faster initial loading of the list of keys in Browser and Tree views Performance optimizations for large results in Workbench Details Features and improvements\n#1207, #1222, #1295, #1159, #1231, #1155 Get insights and optimize the usage and performance of your Redis or Redis Stack with Database Analysis. Navigate to Analysis Tools and scan up to 10,000 keys in the database to see the summary of memory allocation and the number of keys per Redis data type, memory likely to be freed over time, top 15 key namespaces, and the biggest keys found. You can extrapolate results based on the total number of keys or see the exact results for the number of keys scanned. #1280 Speed up the initial load of the key list in Browser and Tree views #1285 Support for infinite floating point numbers in sorted sets #1290 Performance optimizations to process large results of Redis commands in Workbench Bugs\n#1293 Fixed Workbench visualizations in Redis Stack ","categories":[]},{"uri":"/rs/installing-upgrading/get-started-redis-enterprise-software/","uriRel":"/rs/installing-upgrading/get-started-redis-enterprise-software/","title":"Get started with Redis Enterprise Software","tags":[],"keywords":[],"description":"","content":"This guide helps you install Redis Enterprise Software on a Linux host to test its capabilities.\nWhen finished, you\u0026rsquo;ll have a simple cluster with a single node:\nStep 1: Install Redis Enterprise Software Step 2: Set up a Redis Enterprise Software cluster Step 3: Create a new Redis database Step 4: Connect to your Redis database Note: This quick start is designed for local testing only. For production environments, the install and setup guide walks through deployment options appropriate for a production environment. Quick start guides are also available to help you:\nRun Redis Software using a Docker container, which lets you skip the installation process Set up a Redis on Flash cluster to optimize memory resources Set up an Active-Active cluster to enable high availability Benchmark Redis Enterprise Software performance. Step 1: Install Redis Enterprise Software To install Redis Enterprise Software:\nDownload the installation files from the Redis Enterprise Download Center and copy the download package to machine with a Linux-based OS. To untar the image:\ntar vxf \u0026lt;downloaded tar file name\u0026gt; Note: You are required to create a free login to access the download center. Once the tar command completes, run the install.sh script in the current directory.\nsudo ./install.sh -y Port availability If port 53 is in use, the installation fails. This is known to happen in default Ubuntu 18.04 installations where systemd-resolved (DNS server) is running. To workaround this issue, change the system configuration to make this port available before installing Redis Enterprise Software.\nHere\u0026rsquo;s one way to do so:\nRun: sudo vi /etc/systemd/resolved.conf Add DNSStubListener=no as the last line in the file and save the file. Run: sudo mv /etc/resolv.conf /etc/resolv.conf.orig Run: sudo ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf Run: sudo service systemd-resolved restart Step 2: Set up a cluster To set up your machine as a Redis Enterprise software cluster:\nIn the web browser on the host machine, go to https://localhost:8443 to see the Redis Enterprise Software admin console.\nNote: If your browser displays a certificate error, you can safely proceed. If the server does not show the login screen, try again after a few minutes. Choose Setup to begin configuring the node.\nIn the Node Configuration settings, enter a cluster FQDN such as `cluster.local\u0026rsquo; and then select Next.\nIf you have a license key, enter it and then select Next.\nIf you do not have a license key, a trial version is installed.\nEnter an email and password for the administrator account.\nThese credentials are also used for connections to the REST API.\nSelect OK to acknowledge the replacement of the HTTPS TLS certificate on the node. If you receive a browser warning, you can proceed safely.\nStep 3: Create a database Select \u0026ldquo;redis database\u0026rdquo; and the \u0026ldquo;single region\u0026rdquo; deployment, and click Next.\nEnter a database name such as database1 and then select Show Advanced Options.\nIn Endpoint port number, enter 12000.\nSelect the Activate button to create your database.\nYou now have a Redis database!\nStep 4: Connect to your database After you create the Redis database, you are ready to store data in your database. See test connectivity page for a tutorial on connecting to your database.\nSupported web browsers To use the Redis Software admin console, you need a modern browser with JavaScript enabled.\nThe following browsers have been tested with the current version of the admin console:\nMicrosoft Windows, version 10 or later.\nGoogle Chrome, version 48 and later Microsoft Edge, version 20 and later Mozilla Firefox, version 44 and and later Opera, version 35 and later. Apple macOS:\nGoogle Chrome, version 48 and later Mozilla Firefox, version 44 and and later Opera, version 35 and later. Linux:\nGoogle Chrome, version 49 and later Mozilla Firefox, version 44 and and later Opera, version 35 and later. Next steps Now you have a Redis Enterprise cluster ready to go. You can connect to it with a redis client to start loading it with data or you can use the memtier_benchmark Quick Start to check the cluster performance.\n","categories":["RS"]},{"uri":"/modules/modules-quickstart/","uriRel":"/modules/modules-quickstart/","title":"Modules quick start","tags":[],"keywords":[],"description":"","content":"To quickly set up a Redis database with modules, you can sign up for a free Redis Enterprise Cloud subscription and create a Redis Stack database.\nRedis Stack databases include the following modules:\nRediSearch RedisJSON RedisGraph RedisTimeSeries RedisBloom Alternatively, you can use one of these methods to set up a Redis database with modules:\nRedis Enterprise Software Redis Enterprise Software in a Docker container Other platforms for Redis Enterprise Software Set up a Redis Cloud database To set up a Redis Cloud database with modules enabled, follow these steps:\nCreate a new Redis Cloud subscription.\nCreate a Redis Stack database.\nConnect to the database.\nFor more details, see the Redis Enterprise Cloud quick start.\nCreate a subscription To create a new subscription:\nSign in to the Redis Enterprise Cloud admin console or create a new account.\nSelect the New subscription button:\nConfigure your subscription:\nSelect Fixed plans. For the cloud vendor, select Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure. Select a region to deploy the subscription to. From the dataset size list, select the Free tier (30MB). Enter a name for the subscription. Select the Create subscription button:\nCreate a Redis Stack database After you create a subscription, follow these steps to create a Redis Stack database:\nSelect the New database button:\nIn General settings, enter a Database name.\nFor database Type, select Redis Stack.\nSelect the Activate database button:\nConnect to the database After creating the database, you can view its Configuration settings. You will need the following information to connect to your new database:\nPublic endpoint: The host address of the database Redis password/Default user password: The password used to authenticate with the database With this information, you can connect to your database with the redis-cli command-line tool, an application, or RedisInsight.\nTry modules To try out a module, follow its corresponding quick start guide:\nRediSearch RedisJSON RedisGraph RedisTimeSeries RedisBloom ","categories":["Modules"]},{"uri":"/ri/release-notes/v2.10.0/","uriRel":"/ri/release-notes/v2.10.0/","title":"RedisInsight v2.10.0, September 2022","tags":[],"keywords":[],"description":"RedisInsight v2.10.0","content":"2.10.0 (September 2022) This is the General Availability (GA) release of RedisInsight 2.10.\nHighlights Formatters: Additional support for values of keys with Protobuf, Binary, PHP unserialize (view and edit serialized PHP values as JSON), and Java serialized objects, save formatters selected when viewing other keys New overview for cluster databases displays memory and key allocation as well as database information per shards Configure Workbench to persist the Editor after commands have been run and group the results Complete an optional user survey Details Features and improvements\n#1159, #1160, #1068, #1071, #1095, #1097, #1098 A dedicated Analysis Tools page displays memory and key allocation in cluster databases as well as database information per shards #1017, #1025, #1029, #1059, #1092 Added support for additional data formats in Browser/Tree view, including Protobuf, Binary, Pickle, PHP unserialize (view and edit serialized PHP values as JSON), and Java serialized objects #1130 Save formatters selected when viewing other keys #1177 Add a validation when edited value is not valid in the selected format in Browser/Tree view #1048 Configure Workbench to persist the Editor after commands have been run #1119 Pipeline mode configuration for Workbench moved to Settings \u0026gt; Workbench #1149 Save Workbench space by grouping results #1162 Complete an optional user survey #1037 Added tooltip to display long fields in Redis Streams #1202 Removed format validations from the admin username in the Redis Enterprise Cluster autodiscovery process Bugs\n#1180 Fix to display full values for truncated TTL in munutes #1197 Workbench is now available even when encryption failed #1176 Save the refresh value in Browser/Tree view #1101 Fixed an issue when key names are not displayed ","categories":[]},{"uri":"/rs/installing-upgrading/get-started-docker/","uriRel":"/rs/installing-upgrading/get-started-docker/","title":"Get started with Redis Enterprise Software using Docker","tags":[],"keywords":[],"description":"","content":" Warning - Docker containers are currently only supported for development and testing environments, not for production. For testing purposes, you can run Redis Enterprise Software (RS) on Docker containers on Linux, Windows, or MacOS. The Redis Enterprise Software container represents a node in an RS Cluster. When deploying RS using Docker, there are a couple of common topologies:\nTopology #1: The simplest topology is to run a single-node RS Cluster with a single container in a single host machine. This is best for local development or functional testing. Obviously, single-node clusters come with limited functionality in a few ways. For instance, in a single-node topology, Redis Enterprise Software can\u0026rsquo;t replicate to replica shards or provide any protection for failures. Simply follow the instruction in the Getting Started pages for Windows, macOS and Linux to build your development environment.\nTopology #2: You may also run multi-node RS Cluster with multiple RS containers deployed to a single host machine. This topology is similar to the Topology #1 except that you run a multi-node cluster to develop and test against. The result is a system that is scale-minimized but similar to your production Redis Enterprise Software deployment. It is important to note that this topology isn\u0026rsquo;t ideal for performance-sensitive systems. In this topology, containers may interfere with each other under load. In addition, even though the RS cluster provides replication to protect against failures the cluster cannot protect you against the failure of the single host (because all nodes reside on the same physical host). With all this, Topology #2 (or other hybrid deployment methods in which you put multiple RS nodes in containers on the same physical host) is not recommended if you are looking for predictable performance or high availability.\nTopology #3: You may also run multi-node RS Cluster with multiple RS containers each deployed to its own host machine. This topology minimizes interference between RS containers, so it performs more predictably than Topology #2.\nTo get started with a single Redis Enterprise Software container:\nStep 1: Install Docker Engine for your operating system Step 2: Run the RS Docker container Step 3: Set up a cluster Step 4: Create a new database Step 5: Connect to your database Step 1: Install Docker Engine Note: Docker containers are currently only supported for development and testing environments, not for production. Go to the Docker installation page for your operating system for detailed instructions about installing Docker Engine:\nLinux MacOS Windows Step 2: Run the container To pull and start the Redis Enterprise Software Docker container, run this docker run command in the terminal or command-line for your operating system.\nNote: On Windows, make sure Docker is configured to run Linux-based containers. docker run -d --cap-add sys_resource --name rp -p 8443:8443 -p 9443:9443 -p 12000:12000 redislabs/redis The Docker container with RS runs on your localhost with port 8443 open for HTTPS connections, 9443 for REST API connections, and port 12000 open for redis client connections. You can publish other ports with -p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; or use the --network host option to open all ports to the host network.\nStep 3: Set up a cluster In the web browser on the host machine, go to https://localhost:8443 to see the Redis Enterprise Software admin console.\nNote: If your browser displays a certificate error, you can safely proceed. If the server does not show the login screen, try again after a few minutes. Choose Setup to begin configuring the node.\nIn the Node Configuration settings, enter a cluster FQDN such as `cluster.local\u0026rsquo; and then select Next.\nIf you have a license key, enter it and then select Next.\nIf you do not have a license key, a trial version is installed.\nEnter an email and password for the administrator account.\nThese credentials are also used for connections to the REST API.\nSelect OK to acknowledge the replacement of the HTTPS TLS certificate on the node. If you receive a browser warning, you can proceed safely.\nStep 4: Create a database Select \u0026ldquo;redis database\u0026rdquo; and the \u0026ldquo;single region\u0026rdquo; deployment, and click Next.\nEnter a database name such as database1.\nClick Show advanced options and, in the Endpoint port number, enter 12000 for the port number.\nIf port 12000 is not available, enter any available port number between 10000 to 19999 and connect to the database with that port number.\nClick Activate to create your database\nNote: If you cannot activate the database because of a memory limitation, make sure that Docker has enough memory allocated in the Docker Settings. The database configuration is shown. When you see a green check mark, the database is activated and ready for you to use.\nYou now have a Redis database!\nStep 5: Connect to your database After you create the Redis database, you are ready to store data in your database. You can test connectivity to your database with:\nredis-cli - the built-in command-line tool A Hello World application using Python Connecting using redis-cli redis-cli is a simple command-line tool to interact with Redis database.\nUse \u0026ldquo;docker exec\u0026rdquo; to switch your context into the Redis Enterprise Software container\ndocker exec -it rp bash Run redis-cli, located in the /opt/redislabs/bin directory, to connect to the database port number, and store and retrieve a key in database1.\n$ /opt/redislabs/bin/redis-cli -p 12000 127.0.0.1:16653\u0026gt; set key1 123 OK 127.0.0.1:16653\u0026gt; get key1 \u0026#34;123\u0026#34; Connecting using Hello World application in Python A simple python application running on the host machine, not the container, can also connect to database1.\nNote: The following section assumes you already have Python and redis-py (python library for connecting to Redis) configured on the host machine running the container. You can find the instructions to configure redis-py on the github page for redis-py. Create a new file called redis_test.py with this contents:\nimport redis r = redis.StrictRedis(host=\u0026#39;localhost\u0026#39;, port=12000, db=0) print (\u0026#34;set key1 123\u0026#34;) print (r.set(\u0026#39;key1\u0026#39;, \u0026#39;123\u0026#39;)) print (\u0026#34;get key1\u0026#34;) print(r.get(\u0026#39;key1\u0026#39;)) Run the redis_test.py application to store and retrieve a key:\npython.exe redis_test.py If the connection is successful, the output of the application looks like this:\nset key1 123 True get key1 123 Next steps Now you have a Redis Enterprise cluster ready to go. You can connect to it with a redis client to start loading it with data or you can use the memtier_benchmark Quick Start to check the cluster performance.\n","categories":["RS"]},{"uri":"/ri/release-notes/v2.8.0/","uriRel":"/ri/release-notes/v2.8.0/","title":"RedisInsight v2.8.0, August 2022","tags":[],"keywords":[],"description":"RedisInsight v2.8.0","content":"2.8.0 (August 2022) This is the General Availability (GA) release of RedisInsight 2.8.0\nHeadlines Formatters: See formatted key values in JSON, MessagePack, Hex, Unicode, or ASCII in Browser/Tree view for an improved experience of working with different data formats. Clone existing database connection or connection details: Clone the database connection you previously added to have the same one, but with a different database index, username, or password. Raw mode in Workbench: Added support for the raw mode in Workbench results Details Features and improvements\n#978, #959, #984, #996, #992, #1030 Added support for different data formats in Browser/Tree view, including JSON, MessagePack, Hex, and ASCII. If selected, formatter is applied to the entire key value and is available for any data type supported in Browser/Tree view. If any non-printable characters are detected, data editing is available only in Workbench and CLI to avoid data loss. #955 Quickly clone a database connection to have the same one, but with a different database index, username, or password. Open a database connection in the edit mode, request to clone it, and make the changes needed. The original database connection remains the same. #1012 Added support for the raw mode (\u0026ndash;raw) in Workbench. Enable it to see the results of commands executed in the raw mode. #987 DBSIZE command is no longer required to connect to a database. #971 Updated icon for the RediSearch Light module. #1011 Enhanced navigation in the Command Helper allowing you to return to previous actions. Bugs fixed\n#1009 Fixed an error on automatic discovery of Sentinel databases. #978 Work with non-printable characters. To avoid data loss, when non-printable characters are detected, key and value are editable in Workbench and CLI only. ","categories":[]},{"uri":"/ri/release-notes/v1.13.0/","uriRel":"/ri/release-notes/v1.13.0/","title":"RedisInsight v1.13, Aug 2022","tags":[],"keywords":[],"description":"RedisInsight v1.13.0","content":"1.13.1 (November 2022) This is the maintenance release of RedisInsight 1.13 (v1.13.1).\nFixes: Core: Fixed container vulnerabilities. Prevented healthcheck API from overloading RedisInsight DB. Earlier, a separate session was created for each healthcheck hit, which overloaded the database with too many session tokens. Now, healtcheck API doesn\u0026rsquo;t create any session tokens. Get Sentinel host using IP field. Memory Analysis: Added support for hashlistpack, zsetlistpack, quicklist2 and streamlistpack2, encoding types. 1.13.0 (August 2022) This is the General Availability Release of RedisInsight 1.13 (v1.13.0).\nHeadlines Subpath Proxy Support Details Core Subpath Proxy support: RedisInsight can now be proxied behind a subpath Added trusted origins environment variable to set trusted origins Fixed major container vulnerabilities Added proxy notification that displays when such an environment is found RediSearch Fixed index information Profiler Added support for IPv6 clients Memory Analyzer Fixed Lua recommendation ","categories":[]},{"uri":"/modules/install/","uriRel":"/modules/install/","title":"Install and upgrade modules","tags":[],"keywords":[],"description":"","content":"Several modules come packaged with Redis Enterprise. However, if you want to use additional modules or upgrade a module to a more recent version, you need to:\nInstall a module package on the cluster. Enable a module for a new database or upgrade a module in an existing database. For custom modules not packaged and certified by Redis, package them with the RAMP utility before installation on a Redis Enterprise cluster.\n","categories":["Modules"]},{"uri":"/ri/release-notes/v.2.6.0/","uriRel":"/ri/release-notes/v.2.6.0/","title":"RedisInsight v2.6.0, July 2022","tags":[],"keywords":[],"description":"RedisInsight v2.6.0","content":"2.6.0 (July 2022) This is the General Availability (GA) release of RedisInsight 2.6.0\nHeadlines: Bulk actions: Delete the keys in bulk based on the filters set in Browser or Tree view Multiline support for key values in Browser and Tree View: Click the key value to see it in full Pipeline support in Workbench: Batch Redis commands in Workbench to optimize round-trip times In-app notifications: Receive messages about important changes, updates, or announcements inside the application. Notifications are always available in the Notification center, and can be displayed with or without preview. Details Features and improvements:\n#890, #883, #875 Delete keys in bulk from your Redis database in Browser and Tree view based on filters you set by key name or data type. #878 Multiline support for key values in Browser and Tree View: Select the truncated value to expand the row and see the full value, select again to collapse it. #837, #838 Added pipeline support for commands run in Workbench to optimize round-trip times. Default number of commands sent in a pipeline is 5, and is configurable in Settings \u0026gt; Advanced. #862, #840 Added in-app notifications to inform you about any important changes, updates, or announcements. Notifications are always available in the Notification center, and can be displayed with or without preview. #830 To more easily explore and work with stream data, always display stream entry ID and controls to remove the Stream entry regardless of the number of fields. #928 Remember the sorting on the list of databases. Bugs fixed:\n#932 Refresh the JSON value in Browser/Tree view. ","categories":[]},{"uri":"/ri/release-notes/v.2.4.0/","uriRel":"/ri/release-notes/v.2.4.0/","title":"RedisInsight v2.4.0, June 2022","tags":[],"keywords":[],"description":"RedisInsight v2.4.0","content":"2.4.0 (June 2022) This is the General Availability (GA) release of RedisInsight 2.4.0\nHeadlines: Pub/Sub: Added support for Redis pub/sub enabling subscription to channels and posting messages to channels. Consumer groups: Added support for streams consumer groups enabling provision of different subsets of messages from the same stream to many clients for inspection and processing. Database search: Search the list of databases added to RedisInsight to quickly find the required database. Details Features and improvements:\n#760, #737, #773 Added support for Redis pub/sub enabling subscription to channels and posting messages to channels. Currently does not support sharded channels. #717, #683, #684, #688, #720, Added support for streams consumer groups to manage different groups and consumers for the same stream, explicit acknowledgment of processed items, ability to inspect the pending items, claiming of unprocessed messages, and coherent history visibility for each single client. #754 New All Relationship toggle for RedisGraph visualizations in Workbench. Enable it to see all relationships between your nodes. #788 Quickly search the list of databases added to RedisInsight per database alias, host:port, or the last connection to find the database needed. #788 Overview displays the number of keys per the logical database connected if this number is not equal to the total number in the database. Bugs Fixed:\n#774 Fixed cases when not all parameters are received in Overview. #810 Display several streams values with the same timestamp. ","categories":[]},{"uri":"/ri/installing/install-redis-desktop/","uriRel":"/ri/installing/install-redis-desktop/","title":"Install the RedisInsight Desktop Client","tags":[],"keywords":[],"description":"","content":"The RedisInsight desktop client allows you to download and use the RedisInsight UI locally. The desktop client is supported on Windows, MacOS, and Ubuntu operating systems and works with all variants of Redis.\nSystem Requirements Requirement Ubuntu MacOS Windows Operating System Ubuntu 18.04 LTS or later MacOS 10.13 or later Windows 10 Memory (RAM) 8GB 8GB 8GB Processor 64 bit 64 bit 32 bit or 64 bit Note: For unsupported operating systems, you can still install RedisInsight. However, it may show unexpected behavior. We are happy to receive any feedback at redisinsight@redis.com. Note: Disk space: If you are using online memory analysis, you will want to have enough space to store the RDB file for your Redis database. This is usually 10-50% of the Redis instance’s memory usage. Install RedisInsight on Ubuntu Download RedisInsight. Open a terminal and navigate to the folder containing the downloaded file. Make your downloaded file into an executable. chmod +x redisinsight-linux64-\u0026lt;version\u0026gt; Start RedisInsight. ./redisinsight-linux64-\u0026lt;version\u0026gt; To access your RedisInsight UI, open a web browser and navigate to http://127.0.0.1:8001. Install RedisInsight on MacOS Warning - RedisInsight V1 is supported on Mac hardware with Intel chips, and does not have specific support for the Apple M1 (ARM) chip. You can download a special build of RedisInsight V2 for the Apple M1 (ARM) chip. Download RedisInsight. Run the installer. Note: MacOS 10.14.x users occasionally encounter errors during installation. If you encounter a problem installing RedisInsight, please contact us at redisinsight@redis.com and perform the following troubleshooting steps:\nMove the package to the Desktop and left-click the file while hold the Control key. Click \u0026ldquo;Open\u0026rdquo; to proceed past the warning message. After the web server starts, open http://127.0.0.1:8001 and add a Redis database connection. Install RedisInsight on Windows Note: RedisInsight should install and run on a fresh Windows system. There is no need to install any .NET framework. Download RedisInsight. Run the installer. After the web server starts, open http://127.0.0.1:8001 and add a Redis database connection. Next Steps Add a Redis database ","categories":["RI"]},{"uri":"/rc/databases/active-active-redis/","uriRel":"/rc/databases/active-active-redis/","title":"Active-Active Redis","tags":[],"keywords":[],"description":"Overview of the Active-Active feature for Redis Cloud.","content":"Active-Active databases store data across multiple regions and availability zones. This improves scalability, performance, and availability, especially when compared to standalone databases. To create Active-Active databases, you need a Flexible (or Annual) Redis Enterprise Cloud subscription that enables Active-Active Redis and defines the regions for each copy of your databases. This is defined when you create a new subscription.\nActive-Active databases are distributed across multiple regions (geo-distribution). This improves performance by reducing latency for nearby users and improves availability by protecting against data loss in case of network or resource failure.\nActive-Active databases allow read and write operations in each copy. Each copy eventually reflects changes made in other copies (eventual consistency). Conflict-free data types (CRDTs) synchronize read and write operations between copies. CRDTs ensure consistency and resolve conflicts.\nActive-Active geo-distributed replication Active-Active databases use both clustering and replication to strengthen your database. Active-Active Redis has additional features like geo-distribution, multiple active proxies, conflict resolution, and automatic failover to provide you with a more durable and scalable database.\nMulti-zone Geo-distributed replication maintains copies of both primary and replica shards in multiple clusters. These clusters can be spread across multiple availability zones. Active-Active Redis uses zone awareness to spread your primary and replica shards across zones, which helps protect against data loss from regional outages.\nMultiple active proxies Active-Active databases use a multi-primary architecture, which lets you read and write to a primary shard in any of your participating clusters. Having multiple active proxies allows users to connect to the cluster closest to them, reducing latency.\nConflict resolution Active-Active databases use special data types called conflict-free data types (CRDT). These automatically resolve conflicts that occur when writes are made to different clusters at the same time.\nAutomatic failover After a failure at any level (process, node, or zone), Active-Active databases automatically promote replica shards to replace failed primaries, copy data to new replica shards, and migrate shards to new nodes as needed. This reduces downtime and makes the most of your computing resources, even in the event of a failure.\n","categories":["RC"]},{"uri":"/kubernetes/re-databases/db-controller/","uriRel":"/kubernetes/re-databases/db-controller/","title":"Manage Redis Enterprise databases for Kubernetes","tags":[],"keywords":[],"description":"This section describes how the database controller provides the ability to create, manage, and use databases via a database custom resource.","content":"Redis Enterprise database (REDB) lifecycle A Redis Enterprise database (REDB) is created with a custom resource file. The custom resource defines the size, name, and other specifications for the REDB. The database is created when you apply the custom resource file.\nThe database controller in Redis Enterprise for Kubernetes:\nDiscovers the custom resource Makes sure the REDB is created in the same namespace as the Redis Enterprise cluster (REC) Maintains consistency between the custom resource and the REDB The database controller recognizes the new custom resource and validates the specification. If valid, the controller combines the values specified in the custom resource with default values to create a full specification. It then uses this full specification to create the database on the specified Redis Enterprise cluster (REC).\nOnce the database is created, it is exposed with the same service mechanisms by the service rigger for the Redis Enterprise cluster. If the database custom resource is deleted, the database and its services are deleted from the cluster.\nCreate a database Your Redis Enterprise database custom resource must be of the kind: RedisEnterpriseDatabase and have values for name and memorySize. All other values are optional and will be defaults if not specified.\nCreate a file (in this example mydb.yaml) that contains your database custom resource.\napiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: mydb spec: memorySize: 1GB To create a REDB in a different namespace from your REC, you need to specify the cluster with redisEnterpriseCluster in the spec section of your RedisEnterpriseDatabase custom resource.\nredisEnterpriseCluster: name: rec Apply the file in the namespace you want your database to be in.\nkubectl apply -f mydb.yaml Check the status of your database.\nkubectl get redb mydb -o jsonpath=\u0026#34;{.status.status}\u0026#34; When the status is active, the database is ready to use.\nModify a database The custom resource defines the properties of the database. To change the database, you can edit your original specification and apply the change or use kubectl edit.\nTo modify the database:\nEdit the definition:\nkubectl edit redb mydb Change the specification (only properties in spec section) and save the changes.\nFor more details, see RedisEnterpriseDatabaseSpec or Options for Redis Enterprise databases.\nMonitor the status to see when the changes take effect:\nkubectl get redb mydb -o jsonpath=\u0026#34;{.status.status}\u0026#34; When the status is active, the database is ready for use.\nDelete a database The database exists as long as the custom resource exists. If you delete the custom resource, the database controller deletes the database. The database controller removes the database and its services from the cluster.\nTo delete a database, run:\nkubectl delete redb mydb Connect to a database After the database controller creates a database, the services for accessing the database are created in the same namespace. By default there are two services, one \u0026lsquo;ClusterIP\u0026rsquo; service and one \u0026lsquo;headless\u0026rsquo; service.\nConnection information for the database is stored in a Kubernetes secret maintained by the database controller. This secret contains:\nThe database port (port) A comma separated list of service names (service_names) The database password for authenticating (password) The name of that secret is stored in the database custom resource.\nNote: The steps below are only for connecting to your database from within your K8s cluster. To access your database from outside the K8s cluster, you need to configure ingress or use OpenShift routes. Retrieve the secret name.\nkubectl get redb mydb -o jsonpath=\u0026#34;{.spec.databaseSecretName}\u0026#34; The database secret name usually takes the form of redb-\u0026lt;database_name\u0026gt;, so in our example it will be redb-mydb.\nRetrieve and decode the password.\nkubectl get secret redb-mydb -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 --decode Retrieve and decode the port number.\nkubectl get secret redb-mydb -o jsonpath=\u0026#34;{.data.port}\u0026#34; | base64 --decode Retrieve and decode the service_names.\nkubectl get secret redb-mydb -o jsonpath=\u0026#34;{.data.service_names}\u0026#34; | base64 --decode You\u0026rsquo;ll need to pick just one service listed here to use for connecting.\nFrom a pod within your cluster, use redis-cli to connect to your database.\nredis-cli -h \u0026lt;service_name\u0026gt; -p \u0026lt;port\u0026gt; Enter the password you retrieved from the secret.\nauth \u0026lt;password\u0026gt; You are now connected to your database!\n","categories":["Platforms"]},{"uri":"/ri/release-notes/v2.2.0/","uriRel":"/ri/release-notes/v2.2.0/","title":"RedisInsight v2.2.0, May 2022","tags":[],"keywords":[],"description":"RedisInsight v2.2.0","content":"2.2.0 (May 2022) This is the General Availability (GA) release of RedisInsight 2.2.0\nHeadlines: SlowLog: New tool based on results of the Slowlog command to analyze slow operations in Redis instances Streams: Added support for Redis Streams Open the list of keys or key details in full screen Automatically refresh the list of keys and key values with a timer Details Features and improvements:\n#621 , #645 , #649 Added SlowLog, a tool that displays the list of logs captured by the Slowlog command to analyze all commands that exceed a specified runtime, which helps in troubleshooting performance issues. Specify both the runtime and the maximum length of SlowLog (which are server configurations) to configure the list of commands logged and set the auto-refresh interval to automatically update the list of commands displayed. #597 , #598, #601 , #603 , #608 , #613 , #614 , #632 Support for Redis Streams, including creation and deletion of Streams, addition and deletion of entries, and filtration of entries per timestamp. Consumer groups will be added in a future release. #643 List of keys or key details are supported in full screen mode. To open the key list in full screen, close the key details. To open key details in full screen, use the new Full Screen control in key details section. #633 Automatically refresh the list of keys and key values with a timer. To do so, enable the Auto Refresh mode by clicking the control next to the Refresh button and set the refresh rate. #634 Removed the max value limitation in the Database Index field of the form for adding a new database. Bugs Fixed:\n#656 Binary key names will not trigger errors in databases with enabled OSS Cluster API. Data type, TTL, and size of such keys are displayed in the list of keys in all Redis instances. Key details are currently not available. ","categories":[]},{"uri":"/ri/release-notes/v1.12.0/","uriRel":"/ri/release-notes/v1.12.0/","title":"RedisInsight v1.12, May 2022","tags":[],"keywords":[],"description":"RedisInsight v1.12.0","content":"1.12.1 (July 2022) This is the maintenance release of RedisInsight 1.12 (v1.12.1)!\nCritical Bug Fix: Core: When you add or remove a Redis Enterprise Software or Redis Enterprise Cloud database in RedisInsight v1 that has the RediSearch module loaded, all hashes within that database are deleted. Fixes: Core: Added curl command to container. Fixed container vulnerabilities (CVE-2022-1292, CVE-2022-2068). RediSearch: Fixed index info to report correct information for RediSearch v2. Profiling: Added support for profiling module commands. Added support for viewing information of clients that use IPv6 addresses. RedisGraph: Added support for RO_QUERY only mode. RedisGraph now responds to the RO_QUERY command. 1.12.0 (May 2022) This is the General Availability Release of RedisInsight 1.12 (v1.12.0)!\nHeadlines: Authenticate database users: Ask for database username and password Support for GRAPH.RO_QUERY command in RedisGraph tool. Support for variable CPU in RedisAI tool. Full Details Core Authenticate database users: Ask for database username and password If enabled, each time a user attempts to open a database previously added to RedisInsight, a form to enter username and password is displayed. This form displays also if a user is idle for a configurable amount of time. Fix major container vulnerabilities. Decrease Docker image size by discarding unnecessary contents. Streams Fix slowdown and crash while loading large streams data. Use UTC time for stream id timestamp. Graph Allow scanning for more keys. Add support for GRAPH.RO_QUERY command. Browser Fix Delete key dialog box that displays when no key is selected. RedisAI Add support for variable CPU number. ","categories":[]},{"uri":"/modules/modules-lifecycle/","uriRel":"/modules/modules-lifecycle/","title":"Module lifecycle","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software follows the Redis Enterprise lifecycle. (For complete details, see the Redis Enterprise Software subscription agreement.)\nRedis modules also follow a release lifecycle and schedule. Here, you\u0026rsquo;ll find the \u0026ldquo;end-of-life\u0026rdquo; dates for each module and release.\nModule release numbering Redis modules use a three-place numbering scheme to identify released versions.\nThe format is “Major1.Major2.Minor”.\nMajor sections of the version number represent fundamental changes to functionality and feature capabilities. The Major1 and Major2 part of the version number are incremented according to the size and scale of the changes in each release.\nThe Minor section of the version number represents quality improvements and fixes to existing capabilities. The minor release number is increased when release quality improves.\nModule end-of-life schedule End-of-Life for a given Major version is 18 months after the formal release of that version or 12 months after the release of the next subsequent (following) version, whichever comes last.\nRediSearch Release Date End-of-Life (EOL) 2.4 - March 2022 - 2.2 - November 2021 May 31, 2023 2.0 – September 2020 November 30, 2022 1.6 – January 2020 March 31, 2022 1.4 – August 2018 March 31, 2021 1.2 – June 2018 December 31, 2019 1.1 – April 2018 October 31, 2019 1.0 – November 2017 May 31, 2019 RedisJSON Release Date End-of-Life (EOL) 2.0 – November 2021 - 1.0 – November 2017 November 30, 2022 RedisGraph Release Date End-of-Life (EOL) 2.8 – February 2022 - 2.4 – March 2021 February 28, 2023 2.2 – November 2020 May 31, 2022 2.0 – January 2020 November 30, 2021 1.2 – April 2019 December 31, 2020 1.0 – November 2018 May 31, 2020 RedisTimeSeries Release Date End-of-Life (EOL) 1.6 - January 2022 - 1.4 – September 2020 January 31, 2023 1.2 – January 2020 September 30, 2021 1.0 – June 2019 December 31, 2020 RedisBloom Release Date End-of-Life (EOL) 2.2 – December 2019 - 2.0 – June 2019 December 31, 2020 1.1 – February 2019 August 31, 2020 1.0 – September 2017 March 31, 2019 RedisGears Release Date End-of-Life (EOL) 1.2 – February 2022 - 1.0 – May 2020 February 28, 2023 ","categories":["Modules"]},{"uri":"/ri/release-notes/v2.0/","uriRel":"/ri/release-notes/v2.0/","title":"RedisInsight v2.0, Nov 2021","tags":[],"keywords":[],"description":"RedisInsight v2.0.2","content":"2.0.6 (April 2022) This is the General Availability (GA) release of RedisInsight 2.0.6\nHeadlines: SNI support - added SNI support to indicate a hostname in the TLS handshake Save Profiler logs into a file - now you can save and download Profiler logs into a .TXT file Customize delimiters in Tree view - added support for custom delimiters in Tree view Support for node grouping and pulsing in RedisGraph visualizations in Workbench Details Features and improvements:\n#548, #542 Added SNI support - use the \u0026ldquo;Add Database Manually\u0026rdquo; form to see the new \u0026ldquo;Use SNI\u0026rdquo; option under the TLS section to specify the server name and connect to your Redis Database #521 Added an option to save Profiler logs. Enable saving before starting the Profiler to save the logs into a .TXT file and download it to analyze them outside of the application #496 Now you can specify your own delimiters to work with namespaces in the Tree view, default delimiter is colon (\u0026rsquo;:') #473 Added a link to GitHub repository of RedisInsight to quickly find and access it - you can see the icon below the \u0026ldquo;Settings\u0026rdquo; #586 Added support for node grouping and pulsing in the visualisations for RedisGraph in Workbench #455 Limited the movement of the special editor with Cypher highlights to Workbench Editor area (to work with it, just type in RedisGraph commands in Workbench) #462 Provided additional information about database indexes in the form to add a database using host and port #489 Reworked user experience with filters per key type and key name in Browser #535 Added highlights of timestamps and improved text wrapping in Profiler Bug fixes: #581 Fixed the issue with displaying keys in multi-shard databases #576 Fixed encoding in Workbench 2.0.5 (March 2022, GA) This is the General Availability (GA) release of RedisInsight 2.0.\nHeadlines Tree view - A new view of the keys in Browser, which automatically groups keys scanned in your database into folders based on key namespaces. Now you can navigate through and analyze your list of keys quicker by opening only folders with namespaces you want. Support for Apple M1 (arm64) - You can download it here. Added auto-discovery of local databases - RedisInsight will automatically find and add your local databases when you open the application for the first time. A dedicated Editor for Cypher syntax - Workbench supports autocomplete and highlighting of Cypher syntax for RedisGraph queries. Details You can switch to the Tree view in Browser to see all the keys grouped into folders according to their namespaces. Note that we use the colon (:) as a default separator, and it is not customizable yet. Added support for Apple M1 (arm64). Added a mechanism to auto-discover local databases based on the following parameters: The mechanism only triggers when you open the application for the first time. The database has standalone connection type. The database uses the default username and requires no password or TLS certificates. Added new built-in guides in Workbench for additional capabilities. Added tutorials in Workbench for Redis Stack databases that describe common use cases for Redis capabilities. Added a new dedicated Editor to Workbench with support for Cypher syntax autocomplete and highlighting. Use the \u0026ldquo;Shift+Space\u0026rdquo; shortcut inside of the quotes for your query to open the dedicated Editor. Show modules uploaded to databases in the list of databases. Added support for returning to the previous command in Workbench Editor. Use arrow up when your cursor is at the beginning of the first row to return to the previous command. Note: there is no support for the reverse direction yet, so use it with caution. If you installed RedisInsight-preview before, this folder will still exist at the following path:\nFor MacOs: /.redisinsight-preview For Windows: C:/Users/{Username}/.redisinsight-preview For Linux: /.redisinsight-preview 2.0.4 (February 2022) This is the maintenance release of RedisInsight Preview 2.0 (v2.0.4)!\nHeadlines Fixes to the issues found Profiler Added RedisInsight Profiler, which uses the MONITOR command to analyze every command sent to the redis instance in real-time. Workbench: Added support for RedisGears and RedisBloom on the intelligent Redis command auto-complete. Keep command results previously received in the Workbench. Support for repeating commands. CLI: Added support for RedisGears and RedisBloom on the intelligent Redis command auto-complete. Support for repeating commands. Command Helper: Added information about RedisGears and RedisBloom Redis commands. Details Profiler Added RedisInsight Profiler, which uses the MONITOR command to analyze every command sent to the redis instance in real-time. Note: Running the MONITOR command is dangerous to the performance of your production server, so run it reasonably and remember to stop the Profiler. Workbench: Added support for RedisGears and RedisBloom on the intelligent Redis command auto-complete, so the list of similar commands and their arguments are displayed when you start typing any RedisGears or RedisBloom commands. Keep command results (up to 1MB) previously received in the Workbench, so they are available even after you restart the application. Connect Workbench to the database index selected when adding a database. To repeat any command in Workbench, just enter any integer and then a Redis command with arguments. CLI: Added support for RedisGears and RedisBloom on the intelligent Redis command auto-complete, so hints with arguments are displayed when you enter any RedisGears or RedisBloom commands CLI is by default connected to the database index selected when adding a database. Added displaying of the database index connected. To repeat any command in CLI, just enter any integer and then a Redis command with arguments. Command Helper: Added information about RedisGears and RedisBloom Redis commands. Core: Fixed an issue with displaying parameter values in the Overview when no information is received for these parameters. 2.0.3 (December 2021) This is the maintenance release of RedisInsight Preview 2.0 (v2.0.3).\nHeadlines Workbench: Added indications of commands New hints with the list of command arguments Reworked navigation for the built-in guides Help Center: Added a page with list of supported keyboard shortcuts Core: Uncoupled Command Helper from CLI Renamed ZSET to Sorted Set Details Browser: Changed the format of TTL in the list of keys CLI: Fixed a bug with FT.CREATE command that rendered the window blank Workbench: Fixed a bug to avoid executing the Redis command one more time when the view of results is changed Added a new information message when there are no results to display Added indications of commands (currently, not clickable) in Editor area to point out the lines where commands start Added new hints in Editor to display the list of command arguments with the following keyboard shortcuts: Ctrl+Shift+Space for Windows and Linux ⌘ ⇧ Space for Mac Added support for remembering the state (expanded or collapsed) for left side panel in Workbench Reworked navigation for the built-in guides Changed icons for default and custom plugins Command Helper: Changed titles of command groups to make them consistent with redis.io Help Center: Added a page with supported keyboard shortcuts Core: Reworked logic to open CLI and Command Helper, added an option to open Command Helper without a need to open CLI Changed fonts and colors across the application to enhance readability Renamed ZSET to Sorted Set Added description of RedisGears and RedisBloom commands to hints in CLI, Command Helper, and Workbench Added support for automatic updates to the list of commands and their description in CLI, Command Helper, and Workbench 2.0.2 (November 2021) This is the public preview release of RedisInsight 2.0 (v2.0.2).\nRedisInsight 2.0 is a complete product rewrite based on a new tech stack. This version contains a number of must-have and most-used capabilities from previous releases, plus a number of differentiators and delights.\nRedisInsight-preview 2.0 can be installed along with the current GA (1.11) version of RedisInsight with no issues.\nHeadlines Developed using a new tech stack based on Electron, Elastic UI, and Monaco Editor Introducing Workbench - advanced command line interface with intelligent command auto-complete and complex data visualizations Ability to write and render your own data visualizations within Workbench Built-in click-through Redis guides available Support for Light and Dark themes Enhanced user experience with Browser Details Core: Enhanced user experience with the list of databases: View, sort and edit databases added Multiple deletion of databases Ability to connect to Redis Standalone, Redis Cluster and Redis Sentinel Auto discovery of databases managed by Redis Enterprise, Redis Cloud (Flexible), and Redis Sentinel Support for Redis OSS Cluster API Support for TLS connection Works with Microsoft Azure (official support upcoming) Workbench: Advanced command-line interface that lets you run commands against your Redis server Workbench editor allows comments, multi-line formatting and multi-command execution Intelligent Redis command auto-complete and syntax highlighting with support for RediSearch, RedisJSON, RedisGraph, RedisTimeSeries, RedisGears, RedisAI, RedisBloom Allows rendering custom data visualization per Redis command using externally developed plugins Browser: Browse, filter and visualize key-value Redis data structures Visual cues per data type Quick view of size and ttl in the main browser view Ability to filter by pattern and/or data type Ability to change the number of keys to scan through during filtering CRUD support for Lists, Hashes, Strings, Sets, Sorted Sets Search within the data structure (except for Strings) CRUD support for RedisJSON Database overview: A number of metrics always on display within the database workspace Metrics updated every 5 second CPU, number of keys, commands/sec, network input, network output, total memory, number of connected clients Enabled modules per Redis server listed CLI: Command-line interface with enhanced type-ahead command help Embedded command helper where you can filter and search for Redis commands RediSearch: Tabular visualizations within Workbench of RediSearch index queries and aggregations (support for FT.INFO, FT.SEARCH and FT.AGGREGATE) Custom plugins: Ability to build your own visualization plugins to be rendered within Workbench Documentation on how to develop custom plugins and a reference example are provided Built-in guides: Built-in click-through guides for Redis capabilities Added a guide on Document Capabilities within Redis User interface (UI): Light/dark themes available Colour palette adjusted to the highest level of Web content accessibility guidelines Data encryption: Optional ability to encrypt sensitive data such as connection certificates and passwords ","categories":[]},{"uri":"/modules/enterprise-capabilities/","uriRel":"/modules/enterprise-capabilities/","title":"Enterprise feature compatibility for Redis modules","tags":[],"keywords":[],"description":"Describes the Redis Enterprise features supported by each Redis module.","content":"This article describes Redis Enterprise feature compatibility for Redis modules. Version numbers indicate the minimum module version required for feature support. Footnotes provide additional information as needed.\nRedis Enterprise module support The following table shows which modules are supported by Redis Enterprise Software and Redis Enterprise Cloud.\nModule Redis EnterpriseSoftware Redis EnterpriseCloud RediSearch ✅ Yes ✅ Yes RedisJSON ✅ Yes ✅ Yes RedisGraph ✅ Yes ✅ Yes RedisTimeSeries ✅ Yes ✅ Yes RedisBloom ✅ Yes ✅ Yes RedisGears ✅ Yes ❌ No RedisAI ✅ Yes ❌ No Module feature support The following tables show feature support for each Redis module.\nVersion numbers indicate when the feature was first supported. If you\u0026rsquo;re using an earlier version than what\u0026rsquo;s shown in the table, the feature is not supported.\nFor details about individual modules, see the corresponding documentation.\nFeature name/capability RediSearch RedisJSON RedisGraph Active-Active (CRDB)1 Yes (v2.0) Yes (v2.2) No Backup/Restore Yes (v1.4) Yes (v1.0) Yes (v1.0) Clustering Yes (v1.6)2 Yes (v1.0) Yes (v2.2.3)3 Custom hashing policy Yes (v2.0) Yes (v1.0) Yes (v1.0) Eviction expiration Yes (v2.0) Yes (v1.0) Yes (v2.8.10) Failover/migration Yes (v1.4) Yes (v1.0) Yes (v1.0) Internode encryption Yes (v2.0.11) Yes (v1.0.8) Yes (v2.4) Module datatypes Yes Yes Yes Persistence (AOF) Yes (v1.4) Yes (v1.0) Yes (v2.0) Persistence (snapshot) Yes (v1.6) Yes (v1.0) Yes (v1.0) Redis on Flash (RoF)1 Yes (v2.0) Yes (v1.0) No Replica Of Yes (v1.6)4 Yes (v1.0) Yes (v2.2) Reshard/rebalance Yes (v2.0) Yes (v1.0) No Feature name/capability RedisTimeSeries RedisBloom RedisGears RedisAI Active-Active (CRDB)1 No No Yes (v1.0) No Backup/Restore Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Clustering Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Custom hashing policy Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Eviction expiration Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Failover/migration Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Internode encryption Yes (v1.4.9) Yes (v2.2.6) Yes (v1.2) Yes (v1.2) Module datatypes Yes Yes Yes Yes Persistence (AOF) Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Persistence (snapshot) Yes (v1.2) Yes (v2.0) Yes (v1.0) Yes (v1.0) Redis on Flash (RoF)1 No Yes (vTBD) Yes (vTBD) No Replica Of Yes (v1.2) Yes (v2.0) No Yes (v1.0) Reshard/rebalance Yes (v1.2) Yes (v2.0) Yes (v1.0) No Feature descriptions The following table briefly describes each feature shown in the earlier tables.\nFeature name/capability Description Active-Active (CRDB) Compatible with Active-Active (CRDB) databases Backup/Restore Supports import and export features Clustering Compatible with sharded databases and shard migration Custom hashing policy Compatible with databases using custom hashing policies Eviction expiration Allows data to be evicted when the database reaches memory limits Failover/migration Compatible with primary/replica failover and the migration of shards between nodes within the cluster Internode encryption Compatible with encryption on the data plane Persistence (AOF) Compatible with databases using AoF persistence Persistence (snapshot) Compatible with databases using snapshot persistence Redis on Flash (RoF) Compatible with Redis on Flash (RoF) Replica Of Compatible with Active-Passive replication Reshard/rebalance Compatible with database scaling for clustered databases, which redistributes data between the new shards. Footnotes You currently cannot combine Redis on Flash and Active-Active with modules in Redis Cloud.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou cannot use RediSearch with the OSS Cluster API.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe RedisGraph module supports clustering; however, individual graphs contained in a key reside in a single shard, which can affect pricing. To learn more, contact support.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn version 1.6, RediSearch supported Replica Of only between databases with the same number of shards. This limitation was fixed in v2.0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","categories":["Modules"]},{"uri":"/modules/redis-stack/","uriRel":"/modules/redis-stack/","title":"Redis Stack and Redis Enterprise","tags":[],"keywords":[],"description":"Describes Redis Stack.","content":"Redis Stack simplifies installation and deployment of multiple modules with open source Redis databases.\nRedis Enterprise Cloud and Redis Enterprise Software support all capabilities of Redis Stack.\nFor Fixed or Free subscriptions, Redis Enterprise Cloud supports Redis Stack through the Create database workflow.\nWhen you create a new database in a Fixed or Free Redis Enterprise Cloud subscription, you can set the Type property to Redis Stack.\nThis automatically adds the following modules to the new database:\nRediSearch RedisJSON RedisGraph RedisTimeSeries RedisBloom Flexible or Annual Redis Cloud subscriptions and Redis Enterprise Software also support all capabilities of Redis Stack.\nWhen creating new databases for Redis Enterprise Software or for Flexible/Annual Redis Enterprise Cloud subscriptions, you select the specific modules that support your specific scenario.\nEach module is available to the database; combine them to meet your needs.\n(At this time, RedisGraph cannot be combined with other modules in sharded databases.)\nTo learn more, see:\nThe Redis Stack docs on redis.io. The Create database topic for Redis Enterprise Cloud. ","categories":["Modules"]},{"uri":"/ri/using-redisinsight/add-instance/","uriRel":"/ri/using-redisinsight/add-instance/","title":"Add a Redis database","tags":[],"keywords":[],"description":"","content":"Before using any of the tools to work with your database, you must first add the database so RedisInsight can connect to it.\nEach of these database configurations requires specific steps to add them to RedisInsight:\nStandalone Redis Redis cluster Redis Sentinel Redis with TLS authentication Elasticache Note: Currently, RedisInsight supports Redis versions 4 and newer. Add a standalone Redis database This is the simplest setup of a Redis database with just a single Redis server.\nTo add a standalone Redis database to RedisInsight:\nWhen you open RedisInsight, click Add Redis Database.\nSelect Add Database.\nEnter the details for your database:\nHost: The hostname of your Redis database, for example redis.acme.com. If your Redis server is running on your local machine, you can enter either 127.0.0.1 or localhost. You can also paste a Connection URL. When pasted, the database details are automatically filled.\nThe following Connection URLs are supported:\nredis://[[user]:[pass]]@host:port rediss://[[user]:[pass]]@host:port host:port Port: The port your Redis database is available on. The default port for Redis is 6379.\nName: A name for your Redis database. It does not have to match the name of the database in the Redis cluster.\nUsername: The username, if your database is ACL enabled, otherwise leave this field empty. (If you plan to share RedisInsight across multiple users within the same project, you can can enforce reentry of database usernames and passwords. For more information, see Authenticate database users.\nPassword: The password, if any, for your Redis database. If your database doesn\u0026rsquo;t require a password, leave this field empty.\nUse TLS: If your Redis database uses TLS to connect with clients, select this option.\nAfter you enter the details for your database, click Add Redis Database to add the database to RedisInsight.\nYour newly added database is shown in the home screen.\nAdd a Redis cluster database To add a Redis Cluster database:\nFollow steps 1 - 2 in the Add a Standalone Redis Database section above.\nEnter the host, port and other details of your Redis Cluster database and click Add Redis Database.\nSelect the \u0026ldquo;seed nodes\u0026rdquo; of the cluster and click Add Redis Database.\nFor Redis cluster databases, a set of \u0026ldquo;seed nodes\u0026rdquo; or \u0026ldquo;startup nodes\u0026rdquo; are provided to clients. To make sure that RedisInsight has the same view of the cluster as your application, select the same seed nodes that your application or any other client uses to connect to this Redis database. If you don\u0026rsquo;t know which node to select or if there are no other clients for the database yet, select all of the nodes.\nAdd a Redis Sentinel database Redis Sentinel is often used for High Availability deployments of Redis. It provides automatic master-to-replica failover. When you use Sentinel, clients connect to the Sentinel instance to get the current topology of the databases. The client then connects to the current master. If the connection fails, the client queries the Sentinel instance again to find the current master.\nIn this configuration, RedisInsight can connect to your database through the Sentinel instance. Then if a failover happens, RedisInsight changes the database connection just like any other client.\nTo add a Redis database with a Redis Sentinel configuration:\nWhen you open RedisInsight, click Add Redis Database.\nSelect Add Database.\nEnter the host, port and other details of your Redis Sentinel instance and click Add Redis Database.\nSelect the database to add.\nA single set of Sentinel instances can monitor and manage the failover of multiple databases. If your database requires a password, enter the password and click Add Selected Database.\nAdd a Redis database that uses TLS Some configurations of Redis use SSL/TLS for network communication.\nTo add a TLS-enabled Redis database:\nWhen you open RedisInsight, click Add Redis Database.\nSelect Add Database.\nEnter the host, port and username (if your database is ACL enabled), password (if any) of your database.\nSelect Use TLS.\nIf the server needs to be authenticated, pass a CA Certificate.\nIf the certificate returned by the server needs to be verified, select Verify TLS Certificate. If your database requires TLS client authentication to do mutual authentication:\nSelect Requires TLS Client Authentication. Some implementations of Redis, such as Redis Enterprise, support TLS Mutual Authentication so that the Redis server also verifies the identity of the client.\nProvide the client certificate and private key to RedisInsight.\nIf you already have a certificate-key pair in RedisInsight, select it from the Client Certificate list. If you do not, create a certificate-key pair: Select Add New Certificate. Enter a name for your client certificate. Enter the text of the TLS certificate and private key in the next two fields. Click Add Redis Database to add your database.\nConnecting to ElastiCache ElastiCache Redis caches cannot be accessed from outside the VPC, as they don\u0026rsquo;t have public IP addresses assigned to them.\nIf you want to work with ElastiCache Redis caches with RedisInsight, you can:\nIf you are not using Redis Cluster, set up an SSH Tunnel between RedisInsight and your ElastiCache instance.\nTo create an SSH tunnel, run:\nssh -f -N -L \u0026lt;local_port\u0026gt;:\u0026lt;elasticache_endpoint\u0026gt; -i \u0026lt;path_to_pem_file\u0026gt; \u0026lt;ec2_endpoint\u0026gt; Go to Add Instance in RedisInsight and add an instance with:\nhost - \u0026lt;localhost\u0026gt; port - \u0026lt;local_port\u0026gt; name - \u0026lt;your_instance_name\u0026gt; Install RedisInsight on an EC2 instance that is in the same VPC and has access to the ElastiCache Redis cache.\nThis option gives the best performance and works with Redis Cluster.\nSet up a VPN to your AWS VPC using AWS VPN.\nYou can then access the ElastiCache Redis cache using the private endpoint.\n","categories":["RI"]},{"uri":"/rs/security/access-control/","uriRel":"/rs/security/access-control/","title":"Access control","tags":[],"keywords":[],"description":"An overview of access control in Redis Enterprise Software.","content":"Role-based access control allows you to scale your Redis deployments while minimizing the overhead involved in managing a cluster with many databases, multiple users, and various access control lists. With RBAC, you can create a role once and then deploy it across multiple databases in the cluster.\nYou can configure roles with standard or custom templates for database permissions that are based on the Redis ACL syntax. Redis Enterprise allows you to restrict database operations by command, command category, key pattern, and pub/sub channel. Keys are typically restricted based on a namespace using a glob-style wildcard.\nThe role CacheReader demonstrated below has been given the ACL rule +get ~cached:*. Users with this role can access a key prefixed with cached: and the GET command only. This lets them access the key cached:foo with the command GET but does not give them access to the SET command. This role cannot access the key foo because it is not prefixed with cached:.\nTo learn more, see the Redis ACL rules documentation.\n","categories":["RS"]},{"uri":"/ri/using-redisinsight/auto-discover-databases/","uriRel":"/ri/using-redisinsight/auto-discover-databases/","title":"Automatically Discovering Databases","tags":[],"keywords":[],"description":"","content":"RedisInsight lets you automatically add Redis Enterprise Software and Redis Enterprise Cloud databases.\nNote: For Redis Cloud, auto-discovery is supported only for Flexible or Annual subscriptions. Auto-discovery for Redis Software To automatically discover and add Redit Software databases to RedisInsight:\nIn RedisInsight, select ADD REDIS DATABASE.\nSelect Automatically Discover Databases.\nSelect Redis Enterprise.\nEnter the connection details and then select DISCOVER DATABASES.\nFrom the list of databases, choose the databases that you want to add and then select ADD SELECTED DATABASES.\nAll of the databases that were successfully added are dislayed on the screen. To see the databases in the Databases page, click VIEW DATABASES.\nAuto-discovery for Redis Cloud databases To automatically discover and add Redis Cloud databases to RedisInsight:\nIn RedisInsight, select ADD REDIS DATABASE.\nSelect Automatically Discover Databases.\nSelect Redis Enterprise Cloud.\nEnter the Account Key and the Secret key associated with your Redis Cloud subscription and then select SUBMIT.\nVerify the details for the account:\nIf the account details match your account, select MY SUBSCRIPTIONS to list the subscriptions in your account. If the account details do not match your account, select NOT MY ACCOUNT and then verify the credentials. Choose the subscriptions that contain databases to be added to RedisInsight and then select SHOW DATABASES.\nFrom the list of active databases in the selected subscriptions, choose the databases to be added to RedisInsight and then select REGISTER DATABASES.\nWhen sucessfully added, databases are dislayed on the screen. If all databases are added successfully, the message All selected databases added successfully is displays.\nTo see the automatically discovered databases in the Databases page, select VIEW DATABASES.\n","categories":["RI"]},{"uri":"/rs/references/compatibility/commands/cluster/","uriRel":"/rs/references/compatibility/commands/cluster/","title":"Cluster management commands compatibility","tags":[],"keywords":[],"description":"Cluster management commands compatible with Redis Enterprise.","content":"Clustering in Redis Enterprise Software and Redis Enterprise Cloud differs from the open source Redis cluster and works with all standard Redis clients.\nRedis Enterprise blocks most cluster commands. If you try to use a blocked cluster command, it returns an error.\nCommand Redis\nEnterprise Redis\nCloud Notes ASKING ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER ADDSLOTS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER BUMPEPOCH ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER COUNT-FAILURE-REPORTS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER COUNTKEYSINSLOT ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER DELSLOTS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER FAILOVER ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER FLUSHSLOTS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER FORGET ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER GETKEYSINSLOT ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER HELP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supported with the OSS cluster API. CLUSTER INFO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supported with the OSS cluster API. CLUSTER KEYSLOT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supported with the OSS cluster API. CLUSTER MEET ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER MYID ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER NODES ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supported with the OSS cluster API. CLUSTER REPLICAS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER REPLICATE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER RESET ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER SAVECONFIG ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER SET-CONFIG-EPOCH ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER SETSLOT ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLUSTER SLAVES ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Deprecated as of Redis v5.0.0. CLUSTER SLOTS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supported with the OSS cluster API. READONLY ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active READWRITE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ","categories":["RS"]},{"uri":"/rs/clusters/configure/cluster-settings/","uriRel":"/rs/clusters/configure/cluster-settings/","title":"Cluster settings","tags":[],"keywords":[],"description":"You can view and set various cluster settings such as cluster name, email service, time zone, and license.","content":"You can view and set various cluster settings such as cluster name, email service, time zone, and license in the Settings \u0026gt; General page.\nEntering a cluster key After purchasing a cluster key and if your account has the \u0026ldquo;Admin\u0026rdquo; role, you can enter the key in the Cluster Key field, either during initial\ncluster creation or at any time afterward. The key defines various cluster settings, such as the maximum number of shards you can have in the cluster. For more detailed information see Cluster License Keys.\nViewing the maximum number of allowed shards The maximum number of allowed shards, which is determined by the Cluster Key, appears in the Max number of shards field.\nViewing the cluster name The cluster name appears in the Cluster name field. This gives a common name that your team or Redis support can refer to. It is especially helpful if you have multiple clusters.\nSetting your time zone You can set your time zone in the Timezone field. This is recommended in order to make sure that the date, time fields, and log entries are shown in your preferred time zone.\nConfiguring email server settings To enable receiving alerts by email, fill in the details for your email server in the email server settings section and select the requested connection security method: TLS/SSL, STARTTLS, or None. Upon completing to fill-in all details, it is advisable to verify the specified settings by clicking Test Mail.\n","categories":["RS"]},{"uri":"/rs/databases/durability-ha/clustering/","uriRel":"/rs/databases/durability-ha/clustering/","title":"Database clustering","tags":[],"keywords":[],"description":"Clustering to allow customers to spread the load of a Redis process over multiple cores and the RAM of multiple servers.","content":"Open source Redis is a single-threaded process to provide speed and simplicity. A single Redis process is bound by the CPU core that it is running on and available memory on the server.\nRedis Enterprise Software (RS) supports database clustering to allow customers to spread the load of a Redis process over multiple cores and the RAM of multiple servers. A database cluster is a set of Redis processes where each process manages a subset of the database keyspace.\nIn an RS cluster, the keyspace is partitioned into database shards. At any moment a shard resides on a single node and is managed by that node. Each node in a Redis database cluster can manage multiple shards. The key space in the shards is divided into hash slots. The slot of a key is determined by a hash of the key name or part of the key name.\nDatabase clustering is transparent to the Redis client that connects to the database. The Redis client accesses the database through a single endpoint that automatically routes all operations to the relevant shards. You can connect an application to a single Redis process or a clustered database without any difference in the application logic.\nTerminology In clustering, these terms are commonly used:\nTag or Hash Tag - A part of the key that is used in the hash calculation. Slot or Hash Slot - The result of the hash calculation. Shard - Redis process that is part of the Redis clustered database. When to use clustering (sharding) Clustering is an efficient way of scaling Redis that should be used when:\nThe dataset is large enough to benefit from using the RAM resources of more than one node. When a dataset is more than 25 GB (50 GB for RoF), we recommend that you enable clustering to create multiple shards of the database and spread the data requests across nodes. The operations performed against the database are CPU-intensive, resulting in performance degradation. By having multiple CPU cores manage the database\u0026rsquo;s shards, the load of operations is distributed among them. Number of shards When enabling database clustering you can set the number of database shards. The minimum number of shards per database is 2 and the maximum depends on the subscription you purchased.\nOnce database clustering has been enabled and the number of shards has been set, you cannot disable database clustering or reduce the number of shards. You can only increase the number of shards by a multiple of the current number of shards. For example, if the current number of shards was 3, you can increase to 6, 9, 12 and so on.\nSupported hashing policies Standard hashing policy When using the standard hashing policy, a clustered database behaves just like a standard, open-source, Redis cluster:\nKeys with a hash tag: a key\u0026rsquo;s hash tag is any substring between \u0026lsquo;{\u0026rsquo; and \u0026lsquo;}\u0026rsquo; in the key\u0026rsquo;s name. That means that when a key\u0026rsquo;s name includes the pattern \u0026lsquo;{\u0026hellip;}\u0026rsquo;, the hash tag is used as input for the hashing function. For example, the following key names have the same hash tag and would, therefore, be mapped to the same slot: foo{bar}, {bar}baz, foo{bar}baz. Keys without a hash tag: when a key does not contain the \u0026lsquo;{\u0026hellip;}\u0026rsquo; pattern, the entire key\u0026rsquo;s name is used for hashing. You can use the \u0026lsquo;{\u0026hellip;}\u0026rsquo; pattern to direct related keys to the same hash slot so that multi-key operations can be executed on these keys. On the other hand, not using a hash tag in the key\u0026rsquo;s name results in a (statistically) even distribution of keys across the keyspace\u0026rsquo;s shards. If your application does not perform multi-key operations, you do not need to construct key names with hash tags.\nCustom hashing policy A clustered database can be configured to a custom hashing policy. A custom hashing policy is required when different keys need to be kept together on the same shard to allow multi-key operations. The custom hashing policy is provided through a set of Perl Compatible Regular Expressions (PCRE) rules that describe the dataset\u0026rsquo;s key name patterns.\nTo configure a custom hashing policy, enter the regular expression (RegEx) rules that identify the substring in the key\u0026rsquo;s name - hash tag \u0026ndash; on which hashing is done. The hashing tag is denoted in the RegEx by the use of the `tag` named subpattern. Different keys that have the same hash tag is stored and managed in the same slot.\nOnce you enable the custom hashing policy, the following default RegEx rules are implemented. Update these rules to fit your specific logic:\nRegEx Rule Description .*{(?\u0026lt;tag\u0026gt;.*)}.* Hashing is done on the substring between the curly braces. (?\u0026lt;tag\u0026gt;.*) The entire key\u0026rsquo;s name is used for hashing. You can modify existing rules, add new ones, delete rules or change their order to suit your application\u0026rsquo;s requirements.\nCustom hashing policy notes and limitations You can define up to 32 RegEx rules, each up to 256 characters. RegEx rules are evaluated in their order and the first rule matched is used. Therefore, strive to place common key name patterns at the beginning of the rule list. Key names that do not match any of the RegEx rules trigger an error. The \u0026lsquo;.*(?\u0026lt;tag\u0026gt;)\u0026rsquo; RegEx rule forces keys into a single slot because the hash key are always empty. Therefore, when used, this should be the last, catch-all rule. The following flag is enabled in the regular expression parser: PCRE_ANCHORED: the pattern is constrained to match only at the start of the string being searched. Changing the hashing policy The hashing policy of a clustered database can be changed. However, most hashing policy changes trigger the deletion (i.e., FLUSHDB) of the data before they can be applied.\nExamples of such changes include:\nChanging the hashing policy from standard to custom or conversely, custom to standard. Changing the order of custom hashing policy rules. Adding new rules in the custom hashing policy. Deleting rules from the custom hashing policy. Note: The recommended workaround for updates that are not enabled, or require flushing the database, is to back up the database and import the data to a newly configured database. Multi-key operations Operations on multiple keys in a clustered database are supported with the following limitations:\nMulti-key commands: Redis offers several commands that accept multiple keys as arguments. In a clustered database, most multi-key commands are not allowed across slots. The following multi-key commands are allowed across slots: DEL, MSET, MGET, EXISTS, UNLINK, TOUCH\nIn CRBDs, multi-key commands can only be run on keys that are in the same slot.\nCommands that affect all keys or keys that match a specified pattern are allowed in a clustered database, for example: FLUSHDB, FLUSHALL, KEYS\nNote: When using these commands in a sharded setup, the command is distributed across multiple shards and the responses from all shards are combined into a single response. Geo commands: For the GEORADIUS and GEORADIUSBYMEMBER commands, the STORE and STOREDIST options can only be used when all affected keys reside in the same slot.\nTransactions: All operations within a WATCH / MULTI / EXEC block should be performed on keys that are mapped to the same slot.\nLua scripts: All keys used by a Lua script must be mapped to the same slot and must be provided as arguments to the EVAL / EVALSHA commands (as per the Redis specification). Using keys in a Lua script that were not provided as arguments might violate the sharding concept but do not result in the proper violation error being returned.\nRenaming/Copy keys: The use of the RENAME / RENAMENX / COPY commands is allowed only when the key\u0026rsquo;s original and new values are mapped to the same slot.\n","categories":["RS"]},{"uri":"/kubernetes/re-clusters/connect-to-admin-console/","uriRel":"/kubernetes/re-clusters/connect-to-admin-console/","title":"Connect to the admin console","tags":[],"keywords":[],"description":"Connect to the Redis Enterprise admin console to manage your Redis Enterprise cluster.","content":"The username and password for the Redis Enterprise Software admin console are stored in a Kubernetes secret. After retrieving your credentials, you can use port forwarding to connect to the admin console.\nNote: There are several methods for accessing the admin console. Port forwarding is the simplest, but not the most efficient method for long-term use. You could also use a load balancer service or ingress. Switch to the namespace with your Redis Enterprise cluster (REC).\nkubectl config set-context --current --namespace=\u0026lt;namespace-of-rec\u0026gt; Find your cluster name from your list of secrets.\nkubectl get secret In this example, the cluster name is rec.\nExtract and decode your credentials from the secret.\nkubectl get secret \u0026lt;cluster-name\u0026gt; -o jsonpath=\u0026#39;{.data.username}\u0026#39; | base64 --decode kubectl get secret \u0026lt;cluster-name\u0026gt; -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 --decode Find the port for the REC UI service in the spec:ports section of the service definition file.\nkubectl get service/\u0026lt;cluster-name\u0026gt;-ui -o yaml Note: The default port is 8443. Use kubectl port-forward to forward your local port to the service port.\nkubectl port-forward service/\u0026lt;cluster-name\u0026gt;-ui \u0026lt;local-port\u0026gt;:\u0026lt;service-port\u0026gt; View the admin console from a web browser on your local machine at https://localhost:8443.\n","categories":["Platforms"]},{"uri":"/rs/references/compatibility/commands/connection/","uriRel":"/rs/references/compatibility/commands/connection/","title":"Connection management commands compatibility","tags":[],"keywords":[],"description":"Connection management commands compatibility.","content":"The following tables show which open source Redis connection management commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes AUTH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT CACHING ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT GETNAME ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT GETREDIR ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT ID ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Because Redis Enterprise clustering allows multiple active proxies, CLIENT ID cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT INFO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT KILL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT LIST ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT PAUSE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT REPLY ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT SETNAME ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT TRACKING ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT TRACKINGINFO ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CLIENT UNBLOCK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CLIENT UNPAUSE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ECHO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HELLO ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active PING ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active QUIT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RESET ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active SELECT ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Redis Enterprise does not support shared databases due to potential negative performance impacts and blocks any related commands. ","categories":["RS"]},{"uri":"/rs/databases/create/","uriRel":"/rs/databases/create/","title":"Create a Redis Enterprise Software database","tags":[],"keywords":[],"description":"Create a database with Redis Enterprise Software.","content":"Redis Enterprise Software lets you create databases and distribute them across a cluster of nodes. These databases can use Redis Enterprise features like:\nRedis on Flash High availability Data persistence Redis modules Note: For databases with Active-Active replication for geo-distributed locations, create an Active-Active database. Create a database To create a new database:\nIn your web browser, open the admin console of the cluster that you want to connect to in order to create the database .\nBy default, the address is: https://\u0026lt;RS_address\u0026gt;:8443\nIn databases, click .\nIf you do not have any databases on the node, you are prompted to create a database.\nClick Next to create a single-region, in-memory database.\nIf your cluster supports Redis on Flash (RoF), in Runs on you can select Flash so that your database uses Flash memory.\nEnter the mandatory details of the new database :\nName - The database name requirements are:\nMaximum of 63 characters Only letter, number or hyphen (-) characters Starts with a letter; ends with a letter or digit. Note: The database name is not case-sensitive Memory limit - The database memory limits includes all database replicas and shards, including replica shards in database replication and database shards in database clustering. If the total size of the database in the cluster reaches the memory limit, then the data eviction policy for the database is enforced.\nNote: If you create a Redis on Flash or a Memcached Flash database, you also have to set the RAM-to-Flash ratio for this database. Minimum RAM portion is 10%, and maximum RAM portion is 50%. Configure the database options that you want for the database:\nReplication - We recommend that you use intra-cluster replication to create replica shards for each database for high-availablity of your data.\nIf the cluster is configured to support rack-zone awareness, you can also enable rack-zone awareness for the database.\nRedis Modules - When you create a new in-memory database, you can enable multiple Redis modules to the database. For RoF databases, you can add modules that support RoF.\nNote: Modules can be add to database only when creating a new database. You can't add a module to an existing database. To add a module to the database:\nIn the Redis Modules field, click . Select the module that you want to add. If you want the module to use a custom configuration, click Add configuration and enter the optional custom configuration. Click . Data persistence - To protect against loss of data stored in RAM, you can enable data persistence and select to store a copy of the data on disk with snapshots or Append Only File.\nDefault database access - When you configure a password for your database, all connections to the database must authenticate with the AUTH command. If you also configure an access control list, connections can specify other users for authentication, and requests are allowed according to the Redis ACLs specified for that user.\nNote that creating a database without further ACLs (see below) contains a default user with full access to the database which in turn requires the definition a password for security reasons.\nNote: If you are creating a Memcached database, enter a username and password for SASL Authentication. Configure the database advanced options that you want for the database:\nAccess Control List - You can specify the user roles that have access to the database and the Redis ACLs that apply to those connections.\nTo define an access control list:\nIn the Access control list section of the database configuration, click . Select the role that you want to have access to the database. Select the ACL that you want the role to have in the database. Click Save to save the ACL. Click Update to save the changes to the database. Endpoint port number - You can define the port number that clients use to connect to the database, or a port is randomly selected.\nNote: You cannot change the port number after the database is created. Database clustering - You can either:\nEnable database clustering and select the number of shards that you want to have in the database. When database clustering is enabled, databases are subject to limitations on Multi-key commands. You can increase the number of shards in the database at any time.\nYou can accept the standard hashing policy or define a custom hashing policy to define where keys are located in the clustered database.\nClear the Database clustering option to use only one shard so that you can use Multi-key commands without the limitations.\nOSS Cluster API - Redis OSS Cluster API reduces access times and latency with near-linear scalability. The Redis OSS Cluster API provides a simple mechanism for Redis clients to know the cluster topology.\nClients must first connect to the master node to get the cluster topology, and then they connect directly to the Redis proxy on each node that hosts a master shard.\nNote: You must use a client that supports the OSS cluster API to connect to a database that has the OSS cluster API enabled. Data eviction policy - By default, when the total size of the database reaches its memory limit the database evicts keys according to the least recently used keys out of all keys with an \u0026ldquo;expire\u0026rdquo; field set in order to make room for new keys. You can select a different data eviction policy.\nReplica Of - You can make this database a repository for keys from other databases.\nTLS - You can require TLS encryption and authentication for all communications, TLS encryption and authentication for Replica Of communication only, and TLS authentication for clients.\nPeriodic backup - You can configure periodic backups of the database, including the interval and backup location parameters.\nAlerts - You can select alerts to show in the database status and configure their thresholds. You can also select to send the alerts by email to relevant users.\nClick Activate.\nIf you did not specify a port number for the database, note the port number shown in the Endpoint field of the database configuration.\n1. [Test your connectivity](/rs/databases/connect/test-client-connectivity/). ","categories":["RS"]},{"uri":"/rc/api/examples/manage-subscriptions/","uriRel":"/rc/api/examples/manage-subscriptions/","title":"Create and manage subscriptions","tags":[],"keywords":[],"description":"This article describes how to create and manage a subscription using `cURL` commands.","content":"The Redis Enterprise Cloud REST API lets you create and manage a subscription.\nCreate a subscription Use POST /v1/subscriptions to create a subscription.\nPOST \u0026#34;https://[host]/v1/subscriptions\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;Basic subscription example\u0026#34;, \u0026#34;paymentMethodId\u0026#34;: \u0026lt;payment_id\u0026gt;, \u0026#34;cloudProviders\u0026#34;: [ { \u0026#34;cloudAccountId\u0026#34;: \u0026lt;account_id\u0026gt;, \u0026#34;regions\u0026#34;: [ { \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;networking\u0026#34;: { \u0026#34;deploymentCIDR\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; } } ] } ], \u0026#34;databases\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Redis-database-example\u0026#34;, \u0026#34;memoryLimitInGb\u0026#34;: 1.1 } ] } Modify the following parameters in the sample JSON document to create a subscription on your own account:\npaymentMethodId - Specify a payment method connected to your account.\nUse GET /payment-methods to find a payment method ID.\nYou don\u0026rsquo;t need to pass this field in your API request if you subscribed to Redis Enterprise Cloud through GCP Marketplace.\ncloudAccountId - Set a cloud account ID connected to your account.\nTo list cloud account IDs, use GET /cloud-accounts. To use internal resources, set it to \u0026quot;cloudAccountId\u0026quot;: 1.\nIf you subscribed to Redis Enterprise Cloud through GCP Marketplace, use 1 for this field.\nThe request JSON body contains two primary segments: subscription specification and databases specification. When you create a subscription, you must specify one or more databases in the \u0026ldquo;databases\u0026rdquo; array.\nYou can include the contents of the JSON document in the POST /subscriptions operation in the Swagger UI. See Swagger user interface for more details.\nNote: The Swagger UI generates default JSON examples for POST and PUT operations. You can reference these examples and modify them to fit your specific needs and account settings. The examples will fail if used as-is. The response body contains the taskId for the task that creates the subscription. You can use GET /v1/tasks/\u0026lt;taskId\u0026gt; to track the task\u0026rsquo;s status.\nUpdate a subscription Use PUT /v1/subscriptions/\u0026lt;subscriptionId\u0026gt; to update a subscription.\nPUT \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscriptionId\u0026gt;\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;new-subscription-name\u0026#34;, \u0026#34;paymentMethodId\u0026#34;: \u0026lt;payment_id\u0026gt; } You can only change the following settings with this endpoint:\nname - Specify a new name for your subscription.\npaymentMethodId - Specify a different payment method connected to your account.\nUse GET /payment-methods to find a payment method ID.\nThe response body contains the taskId for the task that updates the subscription. You can use GET /v1/tasks/\u0026lt;taskId\u0026gt; to track the task\u0026rsquo;s status.\nDelete a subscription Use DELETE /v1/subscriptions/\u0026lt;subscriptionId\u0026gt; to delete a subscription.\nDELETE \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscriptionId\u0026gt;\u0026#34; The response body contains the taskId for the task that deletes the subscription. You can use GET /v1/tasks/\u0026lt;taskId\u0026gt; to track the task\u0026rsquo;s status.\n","categories":["RC"]},{"uri":"/rs/security/certificates/create-certificates/","uriRel":"/rs/security/certificates/create-certificates/","title":"Create certificates","tags":[],"keywords":[],"description":"Create self-signed certificates to install on a Redis Enterprise cluster.","content":"Follow these instructions to create your own certificates to install on your Redis Enterprise cluster. Note that you can install a separate certificate per cluster component.\nCreate a private key:\nopenssl genrsa -out \u0026lt;key-file-name\u0026gt;.pem 2048 Create a certificate signing request:\nopenssl req -new -key \u0026lt;key-file-name\u0026gt;.pem -out \u0026lt;key-file-name\u0026gt;.csr Note: You will be prompted for a Country Name, State or Province Name, Locality Name, Organization Name, Organizational Unit, and Common Name.\nYou will need to check with your security team or certificate authority for the right values for your organization.\nThe database\u0026rsquo;s fully qualified domain name (FQDN) is typically used as the common name for the certificate.\nSign the private key using your certificate authority.\nHow to obtain a CA signed certificate is different for each organization and CA vendor. Consult your security team or certificate authority for the appropriate way to sign a certificate.\nUpload the certificate to the cluster.\nTo upload the new certificate and replace the current certificate with the rladmin command-line utility, run the cluster certificate set command:\nrladmin cluster certificate set \u0026lt;cert-name\u0026gt; \\ certificate_file \u0026lt;cert-file-name\u0026gt;.pem \\ key_file \u0026lt;key-file-name\u0026gt;.pem Replace the following variables with your own values:\n\u0026lt;cert-name\u0026gt; - The name of the certificate to update. See the certificates table for the list of valid certificate names. \u0026lt;cert-file-name\u0026gt; - The certificate filename \u0026lt;key-file-name\u0026gt; - The key filename ","categories":["RS"]},{"uri":"/rc/databases/create-database/","uriRel":"/rc/databases/create-database/","title":"Create a database","tags":[],"keywords":[],"description":"","content":"To create a database in your Redis Enterprise Cloud subscription:\nSign in to the Redis Cloud admin portal. (Create an account if you don\u0026rsquo;t already have one.)\nIf you have more than one subscription, select the target subscription from the list. This displays the Databases tab for the selected subscription.\nSelect the New database button.\nThis displays the New database screen, which varies according to your subscription plan.\nThe New database screen is divided into sections, each dedicated to a specific category of settings. Note that not every section or setting is available to every subscription plan.\nWhen you\u0026rsquo;ve configured your new database, use the Activate database button to create and activate it.\nGeneral section The General section defines basic properties about your database.\nThe available settings vary according to your subscription plan:\nSetting name Description Subscription Read-only description of your subscription plan, including cloud provider and region Active-Active Redis Checked when the subscription supports Active-Active databases (coming soon; Flexible or Annual subscriptions only) Redis on Flash Checked when the subscription supports Redis on Flash (Flexible or Annual subscriptions only) Database name A name for your database (required) Database port Automatically or manually assigns a database port (range: 10000-19999) (Flexible or Annual subscriptions only) Type Controls optional capabilities, such as modules or protocol. Supported values include Redis Stack (available only for Fixed and Free), Redis (default for Flexible and Annual subscriptions), and Memcached Modules Extend core Redis functionality using modules. Redis Enterprise Cloud supports selected modules; for details, see Redis Enterprise module support Database port All subscriptions automatically assign a database port by default.\nFlexible (and Annual) subscriptions let you choose between two options:\nAuto assign automatically assigns a port number during database creation. Manually assign lets you enter a custom port number between 10000 and 19999. You cannot assign a port that is reserved or already in use. Modules Modules extend Redis database functionality by adding new data types and options.\nAvailable options depend on your subscription and your database Type.\nFixed (and Free) module options Fixed and Free subscriptions support Redis Stack, which enables the most frequently used modules.\nWhen the database Type is set to Redis Stack, the modules section of the database details page displays the modules included with the database and their versions.\nRedis Enterprise Cloud is updated on a regular basis, which includes the modules supported by the service. Module versions displayed by the admin console may vary from those shown above. For the latest details of any module, see Redis modules.\nRedis Stack is available only for Fixed and Free subscriptions.\nFlexible and Annual module options Flexible and Annual subscriptions let you choose modules for each database.\nYou can select more than one module for a database, though there are limits:\nThe following modules can be combined in Flexible and Annual subscriptions:\nRediSearch 2 RedisJSON RedisTimeSeries RedisBloom RedisGraph cannot be combined with other modules\nWhen you select RedisJSON, RediSearch 2 is automatically added because the modules complement each other.\nYou can remove RediSearch 2 if you prefer.\nYou don\u0026rsquo;t have to combine modules. To remove a selected module, either clear the checkbox in the menu or select the module\u0026rsquo;s Delete icon.\nTo learn more, see Redis Stack and Redis modules.\nScalability section The Scalability section lets you manage the maximum size, throughput, and hashing policy for a database.\nThe Scalability section is available only for Flexible and Annual plans.\nSetting name Description Memory size Maximum size (in GB) for your database Throughput Defines throughput in terms of maximum operations per second for the database RediSearch databases use the number of shards to determine throughput. To determine how many shards you need for your RediSearch database, use the RediSearch sizing calculator. Hashing policy Defines the hashing policy OSS Cluster API Enables the OSS Cluster API for a databaseWhen this option is enabled, you cannot define a custom hashing policy To learn more about these settings and when to use them, see Database clustering.\nMemory size Memory size represents the maximum amount of memory for the database, which includes data values, keys, module data, and overhead for specific features. High availability features, such as replication and Active-Active, dramatically increase memory consumption.\nHere are some general guidelines:\nMemory size represents an upper limit. You cannot store more data than the memory size. Depending on your other selections, available memory for data may be much less than expected.\nReplication doubles memory consumption; that is, 512MB of data requires at least 1GB of memory size when replication is enabled.\nActive-Active replication also doubles memory consumption. The effect is cumulative; that is, if you enable Active-Active and replication, the memory size impact can be as large as four times (4x) the original data size. (This is significantly reduced when Redis on Flash is enabled.)\nModules also consume memory.\nMemory limits in Redis Enterprise Cloud are subject to the same considerations as Redis Enterprise Software; to learn more, see Database memory limits\nDurability section The Durability section helps you keep your database (and your data) available when problems occur.\nSetting name Description High availability Replicates your data across multiple nodes, as allowed by your subscription plan Data persistence Defines whether (and how) data is saved to disk; available options depend on your plan type Data eviction policy Defines what happens when your database reaches its memory size limit Remote backup (paid Fixed, Flexible, or Annual subscriptions only) When enabled, identifies a location and interval for data backups. Active-Passive Redis (Flexible or Annual subscriptions only) When enabled, identifies a path to the linked database. Security section The Security section helps you control access to your database.\nSetting name Description Default user When enabled, permits access using a simple password Redis password Password assigned to the database when created CIDR allow list (paid Fixed, Flexible, or Annual subscriptions only) Range of IP addresses/security groups allowed to access the database Transport layer security (TLS) (Flexible or Annual subscriptions only) Enables transport security layer(TLS) encryption for database access. Alerts section The Alerts section defines notification emails sent to your account and the conditions that trigger them.\nThe available alerts vary according to the subscription type.\nSetting name Description Dataset size has reached When enabled, sends an an email when the database reaches the defined memory size (Free, Flexible, or Annual plans only) Latency is higher than When enabled, sends an an email when the latency exceeds the defined memory size (paid Fixed plans only) Number of connections When enabled, sends an email when the connections exceeds the defined limit. (Free and Fixed plans only) Replica Of - database unable to sync with source When enabled, sends email when the replica database cannot sync with the primary (source) database (Flexible or Annual plans only) Replica Of - sync lag is higher than When enabled, sends email when the sync lag exceeds the defined threshold (Flexible or Annual plans only) Throughput is higher than When enabled, sends an email when the operations per second exceed the defined threshold (paid Fixed, Flexible, or Annual plans only) Throughput is lower than When enabled, sends an email when the operations per second falls below the defined threshold (paid Fixed, Flexible, or Annual plans only) Total size of datasets under this plan reached When enabled, sends an an email when the database reaches the defined memory size (paid Fixed plans only) ","categories":["RC"]},{"uri":"/rs/databases/import-export/replica-of/create/","uriRel":"/rs/databases/import-export/replica-of/create/","title":"Create a database with Replica Of","tags":[],"keywords":[],"description":"Create Replica Of database","content":"Replica databases copy data from source databases (previously known as master), which enable read-only connections from apps and clients located in different geographic locations.\nTo create a replica connection, you define a database as a replica of a source database. Replica Of databases (also known as Active-Passive databases) synchronize in the background.\nSources databases can be:\nLocated in the same Redis Enterprise Software cluster Located in a different Redis Enterprise cluster Hosted by a different deployment, e.g. Redis Enterprise Cloud Open source Redis (OSS) databases Your apps can connect to the source database to read and write data; they can also use any replica for read-only access.\nReplica Of can model a variety of data relationships, including:\nOne-to-many relationships, where multiple replicas copy a single source database. Many-to-one relationships, where a single replica collects data from multiple source databases. When you change the replica status of a database by adding, removing, or changing sources, the replica database is synchronized to the new sources.\nConfigure Replica Of To configure a destination database as a Replica Of:\nOpen the database settings:\nFor a new database, create the database with its settings.\nFor an existing database:\nGo to databases. Select the database and then select the Configuration tab. Select the Edit button. Select Replica Of to display the Add button.\nSelect the Add button to display the source database prompt.\nEnter the URL of the source database endpoint.\nThe order of the multiple Replica Of sources has no material impact on replication.\nFor a source database in the same Redis Enterprise cluster - Enter the URL of the source database in the following format: \u0026lt;database name\u0026gt;: redis://admin:\u0026lt;password\u0026gt;@\u0026lt;endpoint\u0026gt;:\u0026lt;port\u0026gt; You can select the database that you want to use as the source.\nFor a source database in a different cluster:\nSign in to the admin console of the cluster hosting the source database.\nIn Databases, select the source database and then select the Configuration tab.\nUnder Endpoint, select Get Replica Of source URL.\nSelect Copy to Clipboard to copy the URL of the source endpoint to your Clipboard.\nTo change the internal password, select Regenerate Password.\nIf you regenerate the password, replication to existing destinations fails until their credentials are updated with the new password.\nIn the destination database, paste the URL of the source endpoint to the Replica Of edit box.\nUse the Save button to save your changes.\nFor source databases on different clusters, you can compress replication data to save bandwidth.\nFor a source database on an open source Redis (OSS) cluster - Enter the URL of the source endpoint in one of the following formats:\nFor databases with passwords:\nredis://:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt; Where the password is the Redis password represented with URL encoding escape characters.\nFor databases without passwords:\nredis://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt; For best results when using the Multicast DNS (mDNS) protocol to resolve the fully-qualified domain name (FQDN) of the cluster, verify that your client connections meet the client mDNS prerequisites.\nConfigure TLS on replica database When you enable TLS for Replica Of, the Replica Of synchronization traffic uses TLS certificates to authenticate the communication between the source and destination clusters.\nTo encrypt the Replica Of synchronization traffic, you must also configure encryption for the source database.\nTo enable TLS for Replica Of in the destination database:\nSelect the Enable TLS Authentication button.\nFrom the admin console of the cluster hosting the source database, select the Settings menu and then go to the General tab. Copy the full text of the proxy certificate to the Clipboard.\nEnter the copied certificate text as the Source Cluster Certificate for the destination database:\nSelect Continue to save the certificate, save the Replica Of endpoint, and then select Update to save your changes.\nEncrypt source database traffic To encrypt Replica Of synchronization traffic, you must also configure encryption for the replica database (the destination).\nEncrypt source synchronization traffic To enable TLS for Replica Of communication only on the source database:\nIn databases, either:\nCreate a new database. Select a database to configure and then select Edit. Enable TLS.\nSelect the communication that you want to secure:\nFor a new database - Require TLS for Replica Of communications only is selected by default.\nFor an existing database that is configured to Require TLS for all communications - Select Require TLS for Replica Of communications only.\nBy default, client authentication is enforced. This means you must enter the syncer certificates of the clusters hosting the replicas (the destination databases).\nTo enter the syncer certificates:\nCopy the syncer certificates for each cluster with a destination database:\nSign in to the cluster. Go to Settings. In the syncer certificates box, copy the full text of the certificate to the Clipboard. Select the Add button to open the certificate dialog.\nEnter the copied certificate text into the text box below the Enforce client authentication checkbox.\nUse the Save button to save the certificates.\nYou can also clear Enforce client authentication so that all clusters or clients can connect to your database without authentication.\nTo encrypt Replica Of synchronization traffic, you must also configure encryption for the replica database (the destination).\nEncrypt all source communication To enable TLS for Replica Of and client communication on the source database:\nFrom the Databases menu of the admin console, either:\nCreate a new database.\nSelect an existing database and then select the Edit button.\nEnable TLS and select Require TLS for all communications.\nBy default, client authentication is enforced so you must enter the syncer certificates of the clusters that host the destination databases.\nYou also need to add the certificates of the clients that connect to the database.\nTo enter the syncer and client certificates:\nCopy the entire text of the syncer and client certificates.\nFor each cluster with a destination database:\nSign in to the cluster. Go to Settings. In the syncer certificates box, copy the full text of the certificate to the Clipboard. Use the Add button to open the certificate dialog.\nEnter the copied certificate text into the text box below the Enforce client authentication checkbox.\nUse the Save button to save your changes.\nYou can also clear the Enforce client authentication checkbox to allow client connections without authentication.\n","categories":["RS"]},{"uri":"/rs/references/compatibility/commands/data-types/","uriRel":"/rs/references/compatibility/commands/data-types/","title":"Data type commands compatibility","tags":[],"keywords":[],"description":"Data type commands compatibility (bitmaps, geospatial indices, hashes, HyperLogLogs, lists, sets, sorted sets, streams, strings).","content":"The following tables show which open source Redis data type commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nBitmap commands Command Redis\nEnterprise Redis\nCloud Notes BITCOUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BITFIELD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BITFIELD_RO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BITOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BITPOS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GETBIT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SETBIT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Geospatial indices commands Command Redis\nEnterprise Redis\nCloud Notes GEOADD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GEODIST ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GEOHASH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GEOPOS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GEORADIUS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. GEORADIUS_RO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. GEORADIUSBYMEMBER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. GEORADIUSBYMEMBER_RO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. GEOSEARCH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GEOSEARCHSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Hash commands Command Redis\nEnterprise Redis\nCloud Notes HDEL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HEXISTS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HGET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HGETALL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HINCRBY ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HINCRBYFLOAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HKEYS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HLEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HMGET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HMSET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v4.0.0. HRANDFIELD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HSCAN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HSET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HSETNX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HSTRLEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HVALS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active HyperLogLog commands Command Redis\nEnterprise Redis\nCloud Notes PFADD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PFCOUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PFDEBUG ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active PFMERGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PFSELFTEST ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active List commands Command Redis\nEnterprise Redis\nCloud Notes BLMOVE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BLPOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BRPOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BRPOPLPUSH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. LINDEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LINSERT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LLEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LMOVE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LPOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LPOS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LPUSH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LPUSHX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LREM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LSET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active LTRIM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RPOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RPOPLPUSH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. RPUSH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RPUSHX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Set commands Command Redis\nEnterprise Redis\nCloud Notes SADD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SCARD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SDIFF ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SDIFFSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SINTER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SINTERSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SISMEMBER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SMEMBERS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SMISMEMBER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SMOVE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SPOP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SRANDMEMBER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SREM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SSCAN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SUNION ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SUNIONSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Sorted set commands Command Redis\nEnterprise Redis\nCloud Notes BZPOPMAX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active BZPOPMIN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZADD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZCARD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZCOUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZDIFF ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZDIFFSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZINCRBY ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZINTER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZINTERSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZLEXCOUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZMSCORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZPOPMAX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZPOPMIN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZRANDMEMBER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZRANGEBYLEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. ZRANGEBYSCORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. ZRANGESTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZRANK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZREM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZREMRANGEBYLEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZREMRANGEBYRANK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZREMRANGEBYSCORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZREVRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. ZREVRANGEBYLEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. ZREVRANGEBYSCORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. ZREVRANK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZSCAN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZSCORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZUNION ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ZUNIONSTORE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Stream commands Command Redis\nEnterprise Redis\nCloud Notes XACK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XADD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XAUTOCLAIM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XCLAIM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XDEL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XGROUP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XINFO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XLEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XPENDING ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XREAD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XREADGROUP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XREVRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XSETID ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active XTRIM ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active String commands Command Redis\nEnterprise Redis\nCloud Notes APPEND ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active DECR ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active DECRBY ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GETDEL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GETEX ✅ Standard\n✅ Active-Active* ✅ Standard\n✅ Active-Active* *Not supported for HyperLogLog. GETRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active GETSET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Deprecated as of Redis v6.2.0. INCR ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active INCRBY ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active INCRBYFLOAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MGET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MSET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MSETNX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PSETEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SETEX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SETNX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SETRANGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active STRALGO ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active STRLEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SUBSTR ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Deprecated as of Redis v2.0.0. ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/develop-for-aa/","uriRel":"/rs/databases/active-active/develop/develop-for-aa/","title":"Develop applications with Active-Active databases","tags":[],"keywords":[],"description":"Overview of how developing applications differs for Active-Active databases from standalone Redis databases.","content":"Developing geo-distributed, multi-master applications can be difficult. Application developers may have to understand a large number of race conditions between updates to various sites, network, and cluster failures that could reorder the events and change the outcome of the updates performed across geo-distributed writes.\nActive-Active databases (formerly known as CRDB) are geo-distributed databases that span multiple Redis Enterprise Software (RS) clusters. Active-Active databases depend on multi-master replication (MMR) and Conflict-free Replicated Data Types (CRDTs) to power a simple development experience for geo-distributed applications. Active-Active databases allow developers to use existing Redis data types and commands, but understand the developers intent and automatically handle conflicting concurrent writes to the same key across multiple geographies. For example, developers can simply use the INCR or INCRBY method in Redis in all instances of the geo-distributed application, and Active-Active databases handle the additive nature of INCR to reflect the correct final value. The following example displays a sequence of events over time : t1 to t9. This Active-Active database has two member Active-Active databases : member CRDB1 and member CRDB2. The local operations executing in each member Active-Active database is listed under the member Active-Active database name. The \u0026ldquo;Sync\u0026rdquo; even represent the moment where synchronization catches up to distribute all local member Active-Active database updates to other participating clusters and other member Active-Active databases.\nTime Member CRDB1 Member CRDB2 t1 INCRBY key1 7 t2 INCRBY key1 3 t3 GET key17 GET key13 t4 — Sync — — Sync — t5 GET key110 GET key110 t6 DECRBY key1 3 t7 INCRBY key1 6 t8 — Sync — — Sync — t9 GET key113 GET key113 Databases provide various approaches to address some of these concerns:\nActive-Passive Geo-distributed deployments: With active-passive distributions, all writes go to an active cluster. Redis Enterprise provides a \u0026ldquo;Replica Of\u0026rdquo; capability that provides a similar approach. This can be employed when the workload is heavily balanced towards read and few writes. However, WAN performance and availability is quite flaky and traveling large distances for writes take away from application performance and availability. Two-phase Commit (2PC): This approach is designed around a protocol that commits a transaction across multiple transaction managers. Two-phase commit provides a consistent transactional write across regions but fails transactions unless all participating transaction managers are \u0026ldquo;available\u0026rdquo; at the time of the transaction. The number of messages exchanged and its cross-regional availability requirement make two-phase commit unsuitable for even moderate throughputs and cross-geo writes that go over WANs. Sync update with Quorum-based writes: This approach synchronously coordinates a write across majority number of replicas across clusters spanning multiple regions. However, just like two-phase commit, number of messages exchanged and its cross-regional availability requirement make geo-distributed quorum writes unsuitable for moderate throughputs and cross geo writes that go over WANs. Last-Writer-Wins (LWW) Conflict Resolution: Some systems provide simplistic conflict resolution for all types of writes where the system clocks are used to determine the winner across conflicting writes. LWW is lightweight and can be suitable for simpler data. However, LWW can be destructive to updates that are not necessarily conflicting. For example adding a new element to a set across two geographies concurrently would result in only one of these new elements appearing in the final result with LWW. MVCC (multi-version concurrency control): MVCC systems maintain multiple versions of data and may expose ways for applications to resolve conflicts. Even though MVCC system can provide a flexible way to resolve conflicting writes, it comes at a cost of great complexity in the development of a solution. Even though types and commands in Active-Active databases look identical to standard Redis types and commands, the underlying types in RS are enhanced to maintain more metadata to create the conflict-free data type experience. This section explains what you need to know about developing with Active-Active databases on Redis Enterprise Software.\nLua scripts Active-Active databases support Lua scripts, but unlike standard Redis, Lua scripts always execute in effects replication mode. There is currently no way to execute them in script-replication mode.\nEviction The default policy for Active-Active databases is noeviction mode. Redis Enterprise version 6.0.20 and later support all eviction policies for Active-Active databases, unless Redis on Flash is enabled. For details, see eviction for Active-Active databases.\nExpiration Expiration is supported with special multi-master semantics.\nIf a key\u0026rsquo;s expiration time is changed at the same time on different members of the Active-Active database, the longer extended time set via TTL on a key is preserved. As an example:\nIf this command was performed on key1 on cluster #1\n127.0.0.1:6379\u0026gt; EXPIRE key1 10 And if this command was performed on key1 on cluster #2\n127.0.0.1:6379\u0026gt; EXPIRE key1 50 The EXPIRE command setting the key to 50 would win.\nAnd if this command was performed on key1 on cluster #3:\n127.0.0.1:6379\u0026gt; PERSIST key1 It would win out of the three clusters hosting the Active-Active database as it sets the TTL on key1 to an infinite time.\nThe replica responsible for the \u0026ldquo;winning\u0026rdquo; expire value is also responsible to expire the key and propagate a DEL effect when this happens. A \u0026ldquo;losing\u0026rdquo; replica is from this point on not responsible for expiring the key, unless another EXPIRE command resets the TTL. Furthermore, a replica that is NOT the \u0026ldquo;owner\u0026rdquo; of the expired value:\nSilently ignores the key if a user attempts to access it in READ mode, e.g. treating it as if it was expired but not propagating a DEL.\nExpires it (sending a DEL) before making any modifications if a user attempts to access it in WRITE mode.\nNote: Expiration values are in the range of [0, 2^49] for Active-Active databases and [0, 2^64] for non Active-Active databases. Out-of-Memory (OOM) If a member Active-Active database is in an out of memory situation, that member is marked \u0026ldquo;inconsistent\u0026rdquo; by RS, the member stops responding to user traffic, and the syncer initiates full reconciliation with other peers in the Active-Active database.\nActive-Active Database Key Counts Keys are counted differently for Active-Active databases:\nDBSIZE (in shard-cli dbsize) reports key header instances that represent multiple potential values of a key before a replication conflict is resolved. expired_keys (in bdb-cli info) can be more than the keys count in DBSIZE (in shard-cli dbsize) because expires are not always removed when a key becomes a tombstone. A tombstone is a key that is logically deleted but still takes memory until it is collected by the garbage collector. The Expires average TTL (in bdb-cli info) is computed for local expires only. INFO The INFO command has an additional crdt section which provides advanced troubleshooting information (applicable to support etc.):\nSection Field Description CRDT Context crdt_config_version Currently active Active-Active database configuration version. crdt_slots Hash slots assigned and reported by this shard. crdt_replid Unique Replica/Shard IDs. crdt_clock Clock value of local vector clock. crdt_ovc Locally observed Active-Active database vector clock. Peers A list of currently connected Peer Replication peers. This is similar to the slaves list reported by Redis. Backlogs A list of Peer Replication backlogs currently maintained. Typically in a full mesh topology only a single backlog is used for all peers, as the requested Ids are identical. CRDT Stats crdt_sync_full Number of inbound full synchronization processes performed. crdt_sync_partial_ok Number of partial (backlog based) re-synchronization processes performed. crdt_sync_partial-err Number of partial re-synchronization processes failed due to exhausted backlog. crdt_merge_reqs Number of inbound merge requests processed. crdt_effect_reqs Number of inbound effect requests processed. crdt_ovc_filtered_effect_reqs Number of inbound effect requests filtered due to old vector clock. crdt_gc_pending Number of elements pending garbage collection. crdt_gc_attempted Number of attempts to garbage collect tombstones. crdt_gc_collected Number of tombstones garbaged collected successfully. crdt_gc_gvc_min The minimal globally observed vector clock, as computed locally from all received observed clocks. crdt_stale_released_with_merge Indicates last stale flag transition was a result of a complete full sync. CRDT Replicas A list of crdt_replica \u0026lt;uid\u0026gt; entries, each describes the known state of a remote instance with the following fields: config_version Last configuration version reported. shards Number of shards. slots Total number of hash slots. slot_coverage A flag indicating remote shards provide full coverage (i.e. all shards are alive). max_ops_lag Number of local operations not yet observed by the least updated remote shard min_ops_lag Number of local operations not yet observed by the most updated remote shard ","categories":["RS"]},{"uri":"/rc/api/get-started/enable-the-api/","uriRel":"/rc/api/get-started/enable-the-api/","title":"Enable the API","tags":[],"keywords":[],"description":"Use the Redis Cloud dashboard to enable the REST API.  (Requires a Flexible or Fixed account.)","content":"If you have a Flexible (or Annual) Redis Enterprise Cloud subscription, you can use a REST API to manage your subscription programmatically.\nNote: The Redis Cloud REST API is available only to Flexible or Annual subscriptions. It is not supported for Fixed or Free subscriptions. For security reasons, the Redis Cloud API is disabled by default.\nTo enable the API:\nSign in to your Redis Cloud subscription as an account owner.\nFrom the menu, choose Access Management.\nWhen the Access Management screen appears, select the API Keys tab.\nIf a Copy button appears to the right of the API account key, the API is enabled. This button copies the account key to the Clipboard.\nIf you see an Enable API button, select it to enable the API and generate your API account key.\nTo authenticate REST API calls, you need to combine the API account key with an API user key to make API calls.\nOnly account owners can see the access key in the account settings.\nWarning - Make sure that you keep your access keys secret. Anyone who sends an API request with a valid access key can make changes to your account. To manage your API keys or to limit IP addresses for user keys, see Manage API keys.\n","categories":["RC"]},{"uri":"/rs/databases/memory-performance/eviction-policy/","uriRel":"/rs/databases/memory-performance/eviction-policy/","title":"Eviction policy","tags":[],"keywords":[],"description":"The eviction policy determines what happens when a database reaches its memory limit.","content":"The eviction policy determines what happens when a database reaches its memory limit.\nTo make room for new data, older data is evicted (removed) according to the selected policy.\nTo prevent this from happening, make sure your database is large enough to hold all desired keys.\nEviction Policy Description noeviction New values aren\u0026rsquo;t saved when memory limit is reachedWhen a database uses replication, this applies to the primary database allkeys-lru Keeps most recently used keys; removes least recently used (LRU) keys allkeys-lfu Keeps frequently used keys; removes least frequently used (LFU) keys allkeys-random Randomly removes keys volatile-lru Removes least recently used keys with expire field set to true volatile-lfu Removes least frequently used keys with expire field set to true volatile-random Randomly removes keys with expire field set to true volatile-ttl Removes least frequently used keys with expire field set to true and the shortest remaining time-to-live (TTL) value Eviction policy defaults volatile-lru is the default eviction policy for most databases.\nThe default policy for Active-Active databases is noeviction policy.\nActive-Active database eviction The eviction policy mechanism for Active-Active databases kicks in earlier than for standalone databases because it requires propagation to all participating clusters. The eviction policy starts to evict keys when one of the Active-Active instances reaches 80% of its memory limit. If memory usage continues to rise while the keys are being evicted, the rate of eviction will increase to prevent reaching the Out-of-Memory state. As with standalone Redis Enterprise databases, Active-Active eviction is calculated per shard. To prevent over eviction, internal heuristics might prevent keys from being evicted when the shard reaches the 80% memory limit. In such cases, keys will get evicted only when shard memory reaches 100%.\nIn case of network issues between Active-Active instances, memory can be freed only when all instances are in sync. If there is no communication between participating clusters, it can result in eviction of all keys and the instance reaching an Out-of-Memory state.\nNote: Data eviction policies are not supported for Active-Active databases with Redis on Flash (RoF). Avoid data eviction To avoid data eviction, make sure your database is large enough to hold required values.\nFor larger databases, consider using Redis on Flash (RoF).\nRedis on Flash stores actively-used data (also known as hot data) in RAM and the remaining data in flash memory (SSD). This lets you retain more data while ensuring the fastest access to the most critical data.\n","categories":["RS"]},{"uri":"/rc/api/get-started/","uriRel":"/rc/api/get-started/","title":"Get started with the REST API","tags":[],"keywords":[],"description":"Describes how Redis Cloud REST API uses keys to authenticate and authorize access.","content":"To use the Redis Enterprise Cloud REST API, you need to:\nEnable the API Create an account key Create a user key Collect endpoint details Note: The Redis Cloud REST API is available only with Flexible or Annual subscriptions. It is not supported for Fixed or Free subscriptions. To use the keys to authenticate and authorize your request, include the keys with the request headers:\nKey name HTTP header name Description Account key x-api-key Account-level key assigned to all users of an account User key x-api-secret-key Personal key associated with a specific user and possibly limited to certain IP ranges Enable the API The API is disabled on all accounts by default. You must enable the API before you can use it.\nAccount key The account key identifies your specific account when you perform an API request. This is the account responsible for your subscription.\nNote: An account key is an account-level secret. Do not share this key with anyone not authorized to use the account. You create the account key once when enabling API access.\nIf you need to change or delete your account key, please contact support.\nUser key The user key is a personal key that belongs to a specific user having the owner role. User keys are assigned owners when they\u0026rsquo;re created. Keys cannot be assigned to users that aren\u0026rsquo;t owners. Keys can belong to only one owner; however, an owner may have multiple keys.\nYou can view keys or copy their values only during the creation process.\nNote: User keys are personal secrets. Do not share them. Individual owners can generate multiple user keys for themselves, for separate apps, or for other owners within the same account.\nUse key names to uniquely associate specific API requests to individual users or apps.\nDoing so lets you audit API requests using the system log, which tracks the key used to authenticate each request.\nAuthentication using API keys Every API request must use the account key and a user key to authenticate.\nThe keys are provided as HTTP request headers, shown earlier.\nAuthenticate a request An API request successfully authenticates when:\nThe account and user keys are valid and properly defined in the HTTP request headers.\nThe user key is associated with the same account as the account key.\nThe request originates from a valid source IP address, as defined in a CIDR allow list associated with the user key.\nThis requirement applies when you\u0026rsquo;ve defined a CIDR allow list for the secret key.\nMore info To learn more, see:\nManage API keys Use the API ","categories":["RC"]},{"uri":"/rs/databases/import-export/import-data/","uriRel":"/rs/databases/import-export/import-data/","title":"Import data into a database","tags":[],"keywords":[],"description":"You can import export or backup files of a specific Redis Enterprise Software database to restore data. You can either import from a single file or from multiple files, such as when you want to import from a backup of a clustered database.","content":"You can import, export, or backup files of a specific Redis Enterprise Software database to restore data. You can either import from a single file or from multiple files, such as when you want to import from a backup of a clustered database.\nWarning - Importing data erases all existing content in the database. Import data into a database To import data into a database:\nSign in to the admin console and then select Databases from the main menu. Select the target database from the list. From the Configuration tab, locate and then select the Import button. Acknowledge the warning to continue the operation. From the Import dialog, enter the details for the import data. These vary according to the storage location. To receive email notifications, place a checkmark in the Receive email notification on success/failure option. Select Import. Supported storage services You can import data from a variety of services, ranging from local servers to cloud services.\nEarlier versions of Redis Enterprise Software supported OpenStack Swift a storage location; however, that support ended 30 November 2020. As a result, that option is no longer available.\nHTTP server To import RDB files from an HTTP server, enter the path to the files. You must enter each path on a separate line.\nFTP server Before you specify to import from an FTP server, make sure that:\nThe Redis Enterprise cluster has network connectivity to the FTP server. The user that you specify in the FTP server location has read privileges. To import an RDB file from an FTP server, enter the FTP server location in the format:\nftp://user:password@host\u0026lt;:custom_port\u0026gt;/path/filename.rdb For example: ftp://username:password@10.1.1.1/home/backups/\u0026lt;filename\u0026gt;.rdb\nSFTP server Before you specify to import from an SFTP server, make sure that:\nThe Redis Enterprise cluster has network connectivity to the SFTP server. The user that you specify in the SFTP server location has read privileges. The RS server and SFTP server have the correct TLS certificates. You can select either: Use the cluster auto generated key - Go to settings and copy the Cluster SSH Public Key to the SFTP server. Use a custom key - Generate a TLS key pair for the SFTP server, copy the private key to the SSH Private Key box, and copy the public key to the SFTP server. To import from an SFTP server, enter the SFTP server location in the format:\nsftp://user:password@host\u0026lt;:custom_port\u0026gt;/path/filename.rdb For example: sftp://username:password@10.1.1.1/home/backups/\u0026lt;filename\u0026gt;.rdb\nAWS S3 Before you import from Amazon S3, make sure that you have:\nPath in the format: s3://bucketname/path/\u0026lt;filename\u0026gt;.rdb Access key ID Secret access key You can also connect to a storage service that uses the S3 protocol but is not hosted by Amazon AWS. The storage service must have a valid SSL certificate. To connect to an S3-compatible storage location, run: rladmin cluster config s3_url \u0026lt;url\u0026gt;\nLocal mount point Before you specify to import from a local mount point, make sure that:\nThe node has network connectivity to the destination server of the mount point. The redislabs:redislabs user has read privileges on the local mount point and on the destination server. You must mount the storage in the same path on all cluster nodes. You can also use local storage but you must copy the imported files manually to all nodes because the import source folders on the nodes are not synchronized. As of version 6.2.12, Redis Enterprise reads files directly from the mount point using a symbolic link (symlink) instead of copying them to a temporary directory on the node.\nTo specify to import from a local mount point on a node:\nCreate the mount point:\nConnect to the terminal of the RS server that the node is running on.\nMount the remote storage to a local mount point.\nFor example:\nsudo mount -t nfs 192.168.10.204:/DataVolume/Public /mnt/Public In the path for the backup location, enter the mount point.\nFor example: /mnt/Public/\u0026lt;filename\u0026gt;.rdb\nAzure Blob Storage Before you choose to import from Azure Blob Storage, make sure that you have:\nStorage location path in the format: /container_name/[path/]/\u0026lt;filename\u0026gt;.rdb Account name An authentication token, either an account key or an Azure shared access signature (SAS). Azure SAS support requires Redis Software version 6.0.20. To learn more about Azure SAS, see Grant limited access to Azure Storage resources using shared access signatures.\nGoogle Cloud Storage Before you choose to import from Google Cloud Storage, make sure that you have:\nStorage location path in the format: /bucket_name/[path/]/\u0026lt;filename\u0026gt;.rdb Client ID Client email Private key ID Private key Importing into an Active-Active database When importing data into an Active-Active database, there are two options:\nUse flushall to delete all data from the Active-Active database, then import the data into the database. Import data but merge it into the existing data or add new data from the import file. Because Active-Active databases have a numeric counter data type, when you merge the imported data into the existing data RS increments counters by the value that is in the imported data. The import through the Redis Enterprise admin console handles these data types for you.\nYou can import data into an Active-Active database from the admin console. When you import data into an Active-Active database, there is a special prompt.\n","categories":["RS"]},{"uri":"/kubernetes/re-databases/set-up-ingress-controller/","uriRel":"/kubernetes/re-databases/set-up-ingress-controller/","title":"Establish external routing with an ingress controller","tags":[],"keywords":[],"description":"Configure an ingress controller to access your Redis Enterprise databases from outside the Kubernetes cluster.","content":"Every time a Redis Enterprise database (REDB) is created with the Redis Enterprise operator, a service is created that allows requests to be routed to that database. Redis Enterprise supports three types of services for accessing databases: ClusterIP, headless, or LoadBalancer.\nBy default, REDB creates a ClusterIP type service, which exposes a cluster-internal IP and can only be accessed from within the K8s cluster. For requests to be routed to the REDB from outside the K8s cluster, you need an ingress controller.\nRedis Enterprise for Kubernetes supports two ingress controllers, HAProxy and NGINX.\nPrerequisites Redis Enterprise database (REDB) Create a Redis Enterprise database with \u0026ldquo;TLS for all communication\u0026rdquo; enabled and \u0026ldquo;client authentication\u0026rdquo; disabled.\nThe YAML to create this REDB must include tlsMode: enabled as shown in this example:\napiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: \u0026lt;your-db-name\u0026gt; spec: tlsMode: enabled Use a previously created database If you are using an existing REDB that was created with a YAML file, you cannot make edits to that database in the Redis Enterprise UI. All changes need to be made in the YAML file.\nIf you are using an existing database that is managed from the UI, see Enable TLS for client connections for more information on these security settings.\nIngress controller Install one of the supported ingress controllers:\nNGINX Ingress Controller Installation Guide HAProxy Ingress Getting Started Warning - You\u0026rsquo;ll need to make sure ssl-passthrough is enabled. It\u0026rsquo;s enabled by default for HAProxy, but disabled by default for NGINX. See the NGINX User Guide for details. Create ingress resource Retrieve the hostname of your ingress controller\u0026rsquo;s LoadBalancer service.\n$ kubectl get svc \u0026lt;haproxy-ingress | ingress-ngnix-controller\u0026gt; \\ -n \u0026lt;ingress-ctrl-namespace\u0026gt; Below is example output for an HAProxy ingress controller running on a K8s cluster hosted by AWS.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE haproxy-ingress LoadBalancer 10.43.62.53 a56e24df8c6173b79a63d5da54fd9cff-676486416.us-east-1.elb.amazonaws.com 80:30610/TCP,443:31597/TCP 21m Choose the hostname you will use to access your database (this value will be represented in this article with \u0026lt;my-db-hostname\u0026gt;).\nCreate a DNS entry that resolves your chosen database hostname to the IP address for the ingress controller\u0026rsquo;s LoadBalancer.\nCreate the ingress resource YAML file.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: rec-ingress annotations: \u0026lt;controller-specific-annotations-below\u0026gt; spec: rules: - host: \u0026lt;my-db-hostname\u0026gt; http: paths: - path: / pathType: ImplementationSpecific backend: service: name: \u0026lt;db-name\u0026gt; port: name: redis For HAProxy, insert the following into the annotations section:\nkubernetes.io/ingress.class: haproxy ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; For NGINX, insert the following into the annotations section:\nkubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; The ssl-passthrough annotation is required to allow access to the database. The specific format changes depending on your ingress controller and any additional customizations. See NGINX Configuration annotations and HAProxy Ingress Options for updated annotation formats.\nTest your external access To test your external access to the database, you need a client that supports TLS and SNI.\nTest your access with Openssl Get the default CA certificate from the redis-enterprise-node container on any of the Redis Enterprise pods.\n$ kubectl exec -it \u0026lt;pod-name\u0026gt; -c redis-enterprise-node \\ -- cat /etc/opt/redislabs/proxy_cert.pem Run the following openssl command, substituting your own values for \u0026lt;my-db-hostname\u0026gt;.\n$ openssl s_client \\ -connect \u0026lt;my-db-hostname\u0026gt;:443 \\ -crlf -CAfile ./proxy_cert.pem \\ -servername \u0026lt;my-db-hostname\u0026gt; If you are connected to the database, you will receive PONG back, as shown below:\n... Verify return code: 0 (ok) --- PING +PONG Test your access with Python You can use the code below to test your access with Python, substituting your own values for \u0026lt;my-db-hostname\u0026gt; and \u0026lt;file-path\u0026gt;.\nimport redis r = redis.StrictRedis(host=\u0026#39;\u0026lt;my-db-hostname\u0026gt;\u0026#39;, port=443, db=0, ssl=True, ssl_ca_certs=\u0026#39;/\u0026lt;file-path\u0026gt;/proxy_cert.pem\u0026#39;) print(r.info()) Your output should look something like this:\n$ /Users/example-user/Documents/Projects/test_client/venv3.7/bin/python \\ /Users/example-user/Documents/Projects/test_client/test_ssl.py { \u0026#39;redis_version\u0026#39;: \u0026#39;5.0.5\u0026#39;, \u0026#39;redis_git_sha1\u0026#39;: 0, \u0026#39;redis_git_dirty\u0026#39;: 0, \u0026#39;redis_build_id\u0026#39;: 0, \u0026#39;redis_mode\u0026#39;: \u0026#39;standalone\u0026#39;, \u0026#39;os\u0026#39;: \u0026#39;Linux 4.14.154-128.181.amzn2.x86_64 x86_64\u0026#39;, \u0026#39;arch_bits\u0026#39;: 64, \u0026#39;multiplexing_api\u0026#39;: \u0026#39;epoll\u0026#39;, \u0026#39;gcc_version\u0026#39;: \u0026#39;7.4.0\u0026#39;, \u0026#39;process_id\u0026#39;: 1, \u0026#39;run_id\u0026#39;: \u0026#39;3ce7721b096517057d28791aab555ed8ac02e1de\u0026#39;, \u0026#39;tcp_port\u0026#39;: 10811, \u0026#39;uptime_in_seconds\u0026#39;: 316467, \u0026#39;uptime_in_days\u0026#39;: 3, \u0026#39;hz\u0026#39;: 10, \u0026#39;lru_clock\u0026#39;: 0, \u0026#39;config_file\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;connected_clients\u0026#39;: 1, \u0026#39;client_longest_output_list\u0026#39;: 0, \u0026#39;client_biggest_input_buf\u0026#39;: 0, \u0026#39;blocked_clients\u0026#39;: 0, \u0026#39;used_memory\u0026#39;: 12680016, \u0026#39;used_memory_human\u0026#39;: \u0026#39;12.9M\u0026#39;, \u0026#39;used_memory_rss\u0026#39;: 12680016, \u0026#39;used_memory_peak\u0026#39;: 13452496, \u0026#39;used_memory_peak_human\u0026#39;: \u0026#39;12.82M\u0026#39;, \u0026#39;used_memory_lua\u0026#39;: 151552, \u0026#39;mem_fragmentation_ratio\u0026#39;: 1, \u0026#39;mem_allocator\u0026#39;: \u0026#39;jemalloc-5.1.0\u0026#39;, \u0026#39;loading\u0026#39;: 0, \u0026#39;rdb_changes_since_last_save\u0026#39;: 0, \u0026#39;rdb_bgsave_in_progress\u0026#39;: 0, \u0026#39;rdb_last_save_time\u0026#39;: 1577753916, \u0026#39;rdb_last_bgsave_status\u0026#39;: \u0026#39;ok\u0026#39;, \u0026#39;rdb_last_bgsave_time_sec\u0026#39;: 0, \u0026#39;rdb_current_bgsave_time_sec\u0026#39;: -1, \u0026#39;aof_enabled\u0026#39;: 0, \u0026#39;aof_rewrite_in_progress\u0026#39;: 0, \u0026#39;aof_rewrite_scheduled\u0026#39;: 0, \u0026#39;aof_last_rewrite_time_sec\u0026#39;: -1, \u0026#39;aof_current_rewrite_time_sec\u0026#39;: -1, \u0026#39;aof_last_bgrewrite_status\u0026#39;: \u0026#39;ok\u0026#39;, \u0026#39;aof_last_write_status\u0026#39;: \u0026#39;ok\u0026#39;, \u0026#39;total_connections_received\u0026#39;: 4, \u0026#39;total_commands_processed\u0026#39;: 6, \u0026#39;instantaneous_ops_per_sec\u0026#39;: 14, \u0026#39;total_net_input_bytes\u0026#39;: 0, \u0026#39;total_net_output_bytes\u0026#39;: 0, \u0026#39;instantaneous_input_kbps\u0026#39;: 0.0, \u0026#39;instantaneous_output_kbps\u0026#39;: 0.0, \u0026#39;rejected_connections\u0026#39;: 0, \u0026#39;sync_full\u0026#39;: 1, \u0026#39;sync_partial_ok\u0026#39;: 0, \u0026#39;sync_partial_err\u0026#39;: 0, \u0026#39;expired_keys\u0026#39;: 0, \u0026#39;evicted_keys\u0026#39;: 0, \u0026#39;keyspace_hits\u0026#39;: 0, \u0026#39;keyspace_misses\u0026#39;: 0, \u0026#39;pubsub_channels\u0026#39;: 0, \u0026#39;pubsub_patterns\u0026#39;: 0, \u0026#39;latest_fork_usec\u0026#39;: 0, \u0026#39;migrate_cached_sockets\u0026#39;: 0, \u0026#39;role\u0026#39;: \u0026#39;master\u0026#39;, \u0026#39;connected_slaves\u0026#39;: 1, \u0026#39;slave0\u0026#39;: { \u0026#39;ip\u0026#39;: \u0026#39;0.0.0.0\u0026#39;, \u0026#39;port\u0026#39;: 0, \u0026#39;state\u0026#39;: \u0026#39;online\u0026#39;, \u0026#39;offset\u0026#39;: 0, \u0026#39;lag\u0026#39;: 0 }, \u0026#39;master_repl_offset\u0026#39;: 0, \u0026#39;repl_backlog_active\u0026#39;: 0, \u0026#39;repl_backlog_size\u0026#39;: 1048576, \u0026#39;repl_backlog_first_byte_offset\u0026#39;: 0, \u0026#39;repl_backlog_histlen\u0026#39;: 0, \u0026#39;used_cpu_sys\u0026#39;: 0.0, \u0026#39;used_cpu_user\u0026#39;: 0.0, \u0026#39;used_cpu_sys_children\u0026#39;: 0.0, \u0026#39;used_cpu_user_children\u0026#39;: 0.0, \u0026#39;cluster_enabled\u0026#39;: 0 } Process finished with exit code 0 ","categories":["Platforms"]},{"uri":"/modules/install/add-module-to-cluster/","uriRel":"/modules/install/add-module-to-cluster/","title":"Install a module on a cluster","tags":[],"keywords":[],"description":"","content":"Redis Enterprise comes packaged with several modules. You can view the installed modules and their versions from settings \u0026gt; redis modules in the Redis Enterprise admin console.\nTo use other modules or upgrade an existing module to a more recent version, you need to install the new module package on your cluster.\nNote: Modules are not supported in Redis Enterprise Software on RHEL/CentOS 6.x. Get packaged modules To install or upgrade a module on a Redis Enterprise cluster, you need a module package.\nFor Redis Enterprise modules, download packages from the Redis download center. For custom-packaged modules, either download a custom-packaged module from the developer or package the module yourself. Add a module to a cluster Use one of the following methods to add a module to a Redis Enterprise cluster:\nREST API POST request to the /v1/modules endpoint\nRedis Enterprise admin console\nFor RedisGears, follow these installation instructions\nREST API method To add a module to the cluster using the REST API:\nDownload the module package from the download center.\nCopy the package to a node in the cluster.\nAdd the module to the cluster with a POST request to the /v1/modules endpoint:\nPOST https://[host][:port]/v1/modules {\u0026#34;module=@/tmp/redisearch.Linux-ubuntu16.04-x86_64.2.2.6.zip\u0026#34;} Here, the module parameter specifies the full path of the module package and must be submitted as form-data. In addition, the package must be available and accessible to the server processing the request.\nIf the module installation succeeds, the POST request returns a JSON object that represents the new module. If it fails, it may return a JSON object with an error_code and description with more details.\nAdmin console method To add a module to the cluster using the admin console:\nIn the Redis Enterprise admin console, select settings.\nFrom redis modules, select the Add module button:\nUse the file browser to select the packaged module.\nVerify Selected module shows the correct filename and select the Upload button:\nThe new module version should appear in the list of Redis modules:\nNote: If you don\u0026rsquo;t see the updated module version, refresh the page. Next steps Create a database and enable the new module. Upgrade a module to the new version. ","categories":["Modules"]},{"uri":"/rs/references/compatibility/commands/generic/","uriRel":"/rs/references/compatibility/commands/generic/","title":"Key commands compatibility","tags":[],"keywords":[],"description":"Generic key commands compatible with Redis Enterprise.","content":"The following table shows which open source Redis key (generic) commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes COPY ✅ Standard\n✅ Active-Active* ✅ Standard\n✅ Active-Active* *Not supported for stream consumer group info. DEL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active DUMP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active EXISTS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active EXPIRE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active EXPIREAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active KEYS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MIGRATE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MOVE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Redis Enterprise does not support shared databases due to potential negative performance impacts and blocks any related commands. OBJECT ENCODING ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active OBJECT FREQ ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active OBJECT IDLETIME ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active OBJECT REFCOUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PERSIST ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PEXPIRE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PEXPIREAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PTTL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RANDOMKEY ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RENAME ✅ Standard\n✅ Active-Active* ✅ Standard\n✅ Active-Active* *Not supported for stream consumer group info. RENAMENX ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active RESTORE ✅ Standard\n✅ Active-Active* ✅ Standard\n✅ Active-Active* *Only supported for module keys. SCAN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SORT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active TOUCH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active TTL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active TYPE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active UNLINK ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active WAIT ✅ Standard*\n❌ Active-Active ❌ Standard\n❌ Active-Active *Blocked by default, but you can change the CCS flag to enable it. ","categories":["RS"]},{"uri":"/kubernetes/deployment/quick-start/","uriRel":"/kubernetes/deployment/quick-start/","title":"Deploy Redis Enterprise Software for Kubernetes","tags":[],"keywords":[],"description":"How to install Redis Enterprise Software for Kubernetes.","content":"To deploy Redis Enterprise Software for Kubernetes and start your Redis Enterprise cluster (REC), you need to do the following:\nCreate a new namespace in your Kubernetes cluster. Download the operator bundle. Apply the operator bundle and verify it\u0026rsquo;s running. Create a Redis Enterprise cluster (REC). This guide works with most supported Kubernetes distributions. If you\u0026rsquo;re using OpenShift, see Redis Enterprise on OpenShift. For details on what is currently supported, see supported distributions.\nPrerequisites To deploy Redis Enterprise for Kubernetes, you\u0026rsquo;ll need:\na Kubernetes cluster in a supported distribution a minimum of three worker nodes a Kubernetes client (kubectl) access to DockerHub, RedHat Container Catalog, or a private repository that can hold the required images. Create a new namespace Important: Each namespace can only contain one Redis Enterprise cluster. Multiple RECs with different operator versions can coexist on the same Kubernetes cluster, as long as they are in separate namespaces.\nThroughout this guide, each command is applied to the namespace in which the Redis Enterprise cluster operates.\nCreate a new namespace\nkubectl create namespace \u0026lt;rec-namespace\u0026gt; Change the namespace context to make the newly created namespace default for future commands.\nkubectl config set-context --current --namespace=\u0026lt;rec-namespace\u0026gt; You can use an existing namespace as long as it does not contain any existing Redis Enterprise cluster resources. It\u0026rsquo;s best practice to create a new namespace to make sure there are no Redis Enterprise resources that could interfere with the deployment.\nInstall the operator Redis Enterprise for Kubernetes bundle is published as a container image. A list of required images is available in the release notes for each version.\nThe operator definition and reference materials are available on GitHub. The operator definitions are packaged as a single generic YAML file.\nNote: If you do not pull images from DockerHub or another public registry, you need to use a private container registry. Download the operator bundle Pull the latest version of the operator bundle:\nVERSION=`curl --silent https://api.github.com/repos/RedisLabs/redis-enterprise-k8s-docs/releases/latest | grep tag_name | awk -F\u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $4}\u0026#39;` If you need a different release, replace VERSION with a specific release tag.\nCheck version tags listed with the operator releases on GitHub or by using the GitHub API to ensure the version of the bundle is correct.\nDeploy the operator bundle Apply the operator bundle in your REC namespace:\nkubectl apply -f https://raw.githubusercontent.com/RedisLabs/redis-enterprise-k8s-docs/$VERSION/bundle.yaml You should see a result similar to this:\nrole.rbac.authorization.k8s.io/redis-enterprise-operator created serviceaccount/redis-enterprise-operator created rolebinding.rbac.authorization.k8s.io/redis-enterprise-operator created customresourcedefinition.apiextensions.k8s.io/redisenterpriseclusters.app.redislabs.com configured customresourcedefinition.apiextensions.k8s.io/redisenterprisedatabases.app.redislabs.com configured deployment.apps/redis-enterprise-operator created Verify the operator is running Check the operator deployment to verify it\u0026rsquo;s running in your namespace:\nkubectl get deployment redis-enterprise-operator You should see a result similar to this:\nNAME READY UP-TO-DATE AVAILABLE AGE redis-enterprise-operator 1/1 1 1 0m36s Create a Redis Enterprise cluster (REC) A Redis Enterprise cluster (REC) is created from a RedisEnterpriseCluster custom resource that contains cluster specifications.\nThe following example creates a minimal Redis Enterprise cluster. See the RedisEnterpriseCluster API reference for more information on the various options available.\nCreate a file (my-rec.yaml) that defines a Redis Enterprise cluster with three nodes:\ncat \u0026lt;\u0026lt;EOF \u0026gt; my-rec.yaml apiVersion: \u0026#34;app.redislabs.com/v1\u0026#34; kind: \u0026#34;RedisEnterpriseCluster\u0026#34; metadata: name: my-rec spec: nodes: 3 EOF This will request a cluster with three Redis Enterprise nodes using the default requests (i.e., 2 CPUs and 4GB of memory per node).\nTo test with a larger configuration, use the example below to add node resources to the spec section of your test cluster (my-rec.yaml).\nredisEnterpriseNodeResources: limits: cpu: 2000m memory: 16Gi requests: cpu: 2000m memory: 16Gi Note: Each cluster must have at least 3 nodes. Single-node RECs are not supported. See the Redis Enterprise hardware requirements for more information on sizing Redis Enterprise node resource requests.\nApply your custom resource file in the same namespace as my-rec.yaml.\nkubectl apply -f my-rec.yaml You should see a result similar to this:\nredisenterprisecluster.app.redislabs.com/my-rec created You can verify the creation of the cluster with:\nkubectl get rec You should see a result similar to this:\nNAME AGE my-rec 1m At this point, the operator will go through the process of creating various services and pod deployments.\nYou can track the progress by examining the StatefulSet associated with the cluster:\nkubectl rollout status sts/my-rec or by looking at the status of all of the resources in your namespace:\nkubectl get all Enable the admission controller The admission controller dynamically validates REDB resources configured by the operator. It is strongly recommended that you use the admission controller on your Redis Enterprise Cluster (REC). The admission controller only needs to be configured once per operator deployment.\nAs part of the REC creation process, the operator stores the admission controller certificate in a Kubernetes secret called admission-tls. You may have to wait a few minutes after creating your REC to see the secret has been created.\nVerify the secret has been created.\nkubectl get secret admission-tls The output will look similar to\nNAME TYPE DATA AGE admission-tls Opaque 2 2m43s Save the certificate to a local environment variable.\nCERT=`kubectl get secret admission-tls -o jsonpath=\u0026#39;{.data.cert}\u0026#39;` Create a patch file for the Kubernetes validating webhook.\nsed \u0026#39;s/NAMESPACE_OF_SERVICE_ACCOUNT/demo/g\u0026#39; admission/webhook.yaml | kubectl create -f - cat \u0026gt; modified-webhook.yaml \u0026lt;\u0026lt;EOF webhooks: - name: redb.admission.redislabs clientConfig: caBundle: $CERT admissionReviewVersions: [\u0026#34;v1beta1\u0026#34;] EOF Patch the webhook with the certificate.\nkubectl patch ValidatingWebhookConfiguration redb-admission --patch \u0026#34;$(cat modified-webhook.yaml)\u0026#34; Limit the webhook to the relevant namespaces The operator bundle includes a webhook file. The webhook will intercept requests from all namespaces unless you edit it to target a specific namespace. You can do this by adding the namespaceSelector section to the webhook spec to target a label on the namespace.\nMake sure the namespace has a unique namespace-name label.\napiVersion: v1 kind: Namespace metadata: labels: namespace-name: example-ns name: example-ns Patch the webhook to add the namespaceSelector section.\ncat \u0026gt; modified-webhook.yaml \u0026lt;\u0026lt;EOF webhooks: - name: redb.admission.redislabs namespaceSelector: matchLabels: namespace-name: staging EOF Apply the patch.\nkubectl patch ValidatingWebhookConfiguration redb-admission --patch \u0026#34;$(cat modified-webhook.yaml)\u0026#34; Verify the admission controller is working Verify the admission controller is installed correctly by applying an invalid resource. This should force the admission controller to correct it.\n$ kubectl apply -f - \u0026lt;\u0026lt; EOF apiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: redis-enterprise-database spec: evictionPolicy: illegal EOF You should see your request was denied by the admission webhook \u0026quot;redb.admission.redislabs\u0026quot;.\nError from server: error when creating \u0026#34;STDIN\u0026#34;: admission webhook \u0026#34;redb.admission.redislabs\u0026#34; denied the request: eviction_policy: u\u0026#39;illegal\u0026#39; is not one of [u\u0026#39;volatile-lru\u0026#39;, u\u0026#39;volatile-ttl\u0026#39;, u\u0026#39;volatile-random\u0026#39;, u\u0026#39;allkeys-lru\u0026#39;, u\u0026#39;allkeys-random\u0026#39;, u\u0026#39;noeviction\u0026#39;, u\u0026#39;volatile-lfu\u0026#39;, u\u0026#39;allkeys-lfu\u0026#39;] Create a Redis Enterprise Database (REDB) You can create multiple databases within the same namespace as your REC or in other namespaces.\nSee manage Redis Enterprise databases for Kubernetes to create a new REDB.\n","categories":["Platforms"]},{"uri":"/rs/references/client_references/client_nodejs/","uriRel":"/rs/references/client_references/client_nodejs/","title":"Redis with Node.js (node_redis)","tags":[],"keywords":[],"description":"The node_redis client allows you to use Redis with Node.js.","content":"To use Redis with Node.js, you need to install a Node.js Redis client. The following sections explain how to use node_redis, a community-recommended Redis client for Node.js.\nAnother community-recommended client for Node.js developers is ioredis. You can find additional Node.js clients for Redis in the Node.js section of the Redis Clients page.\nInstall node_redis See the node_redis README file for installation instructions.\nTo install node_redis, run:\nnpm install redis Connect to Redis There are several ways that you can connect to Redis, each with different security considerations.\nDefault password The following code creates a connection to Redis:\nconst redis = require(\u0026#39;redis\u0026#39;); const client = redis.createClient({ socket: { host: \u0026#39;\u0026lt;hostname\u0026gt;\u0026#39;, port: \u0026#39;\u0026lt;port\u0026gt;\u0026#39; }, password: \u0026#39;\u0026lt;password\u0026gt;\u0026#39; }); client.on(\u0026#39;error\u0026#39;, err =\u0026gt; { console.log(\u0026#39;Error \u0026#39; + err); }); Replace the values in the example with the values for your Redis instance:\n\u0026lt;hostname\u0026gt; - The name of the host your database runs on \u0026lt;port\u0026gt; - The port that the database is running on (default: 6379) \u0026lt;password\u0026gt; - The password you use to access Redis, if necessary. Note: Remember to always store passwords outside of your code, for example in environment variables. TLS The following example demonstrates how to make a connection to Redis using TLS:\nconst redis = require(\u0026#39;redis\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const client = redis.createClient({ host: \u0026#39;\u0026lt;hostname\u0026gt;\u0026#39;, port: \u0026#39;\u0026lt;port\u0026gt;\u0026#39;, tls: { key: fs.readFileSync(\u0026#39;path_to_keyfile\u0026#39;, encoding=\u0026#39;ascii\u0026#39;), cert: fs.readFileSync(\u0026#39;path_to_certfile\u0026#39;, encoding=\u0026#39;ascii\u0026#39;), ca: [ fs.readFileSync(\u0026#39;path_to_ca_certfile\u0026#39;, encoding=\u0026#39;ascii\u0026#39;) ] } }); Where you must provide:\n\u0026lt;hostname\u0026gt; - The name of the host your database runs on \u0026lt;port\u0026gt; - The port that the database is running on (default: 6379) ACL user and password Redis 6 introduced Access Control Lists (also known as ACLs). ACLs allow you to create named user accounts, each having its own password.\nThe node_redis client doesn\u0026rsquo;t currently support ACL commands or the AUTH command with a username and password. Therefore, you will need to disable the client\u0026rsquo;s built in auth function and use the generic send_command function, which allows you to send any arbitrary command to Redis, to authenticate.\nExample:\nconst redis = require(\u0026#39;redis\u0026#39;); const client = redis.createClient({ host: \u0026#39;127.0.0.1\u0026#39;, port: \u0026#39;\u0026lt;port\u0026gt;\u0026#39; }); // Disable client\u0026#39;s AUTH command. client[\u0026#39;auth\u0026#39;] = null; // send_command expects a command name and array of parameters. client.send_command(\u0026#39;AUTH\u0026#39;, [\u0026#39;\u0026lt;username\u0026gt;\u0026#39;, \u0026#39;\u0026lt;password\u0026gt;\u0026#39;]); Replace the \u0026lt;port\u0026gt;, \u0026lt;username\u0026gt;, and \u0026lt;password\u0026gt; with the values for the ACL user that you are connecting as.\nExample code for Redis commands After your application connects to Redis, you can read and write data.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\nclient.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, (err, reply) =\u0026gt; { if (err) throw err; console.log(reply); client.get(\u0026#39;foo\u0026#39;, (err, reply) =\u0026gt; { if (err) throw err; console.log(reply); }); }); Example output:\n$ node example_node_redis.js OK bar The node_redis client exposes a function named for each Redis command.\nThese functions take strings as arguments, the first of which is usually the Redis key to run the command against. You can also add an optional error first callback function after the other arguments.\nPromises and async/await To use promises and async/await with node_redis, wrap it using Bluebird\u0026rsquo;s promisifyAll, as shown here:\nconst redis = require(\u0026#39;redis\u0026#39;); const { promisifyAll } = require(\u0026#39;bluebird\u0026#39;); promisifyAll(redis); const runApplication = async () =\u0026gt; { // Connect to redis at 127.0.0.1 port 6379 no password. const client = redis.createClient(); await client.setAsync(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;); const fooValue = await client.getAsync(\u0026#39;foo\u0026#39;); console.log(fooValue); }; runApplication(); This creates an async equivalent of each function, adding Async as a suffix.\nAlternatively, you can promisify a subset of node_redis functions one at a time using native Node.js promises and util.promisify:\nconst redis = require(\u0026#39;redis\u0026#39;); const { promisify } = require(\u0026#39;util\u0026#39;); const runApplication = async () =\u0026gt; { // Connect to redis at 127.0.0.1 port 6379 no password. const client = redis.createClient(); const setAsync = promisify(client.set).bind(client); const getAsync = promisify(client.get).bind(client); await setAsync(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;); const fooValue = await getAsync(\u0026#39;foo\u0026#39;); console.log(fooValue); }; runApplication(); ","categories":["RS"]},{"uri":"/rc/security/database-security/passwords-users-roles/","uriRel":"/rc/security/database-security/passwords-users-roles/","title":"Passwords, users, and roles","tags":[],"keywords":[],"description":"","content":"All Redis Cloud databases require either password-based authentication or role-based access control. Role-based access control lets you define multiple users with fine-grained authorization features.\nPrerequisites To use role-based access control, your Redis Cloud database needs to support Redis version 6.0.0 or later.\nThe Redis version of a database is displayed in the General section of the Configuration tab of the database detail screen.\nPassword-based authentication Password-based authentication is a basic but essential Redis security feature. When you create a Redis Cloud database, your database is given a randomly generated password called the Default user password.\nThis appears in the Security section of the Configuration tab of the database details screen.\nUse the copy button to copy the password to the clipboard:\nYou\u0026rsquo;ll need to use this password whenever you connect to your database using a Redis client. For example, in the Redis CLI, you use the AUTH command to provide this password:\nAUTH 4kTtH2ddXfN2sFmXE6sowOLukxiaJhN8n See your Redis client\u0026rsquo;s documentation to learn how to provide your password when connecting.\nChange password To change the default user password for your database:\nFrom the database Configuration tab, select Edit database:\nUnder the Security section, enter the new password in the Default user password field.\nSelect Save database to update the password:\nRole-based access control Role-based access control (RBAC) lets you define roles with specific sets of permissions. You can then assign users to these roles to provide appropriate levels of access.\nRBAC effectively lets you implement the principle of least privilege. For example, you can provide read-only access to an application whose only job is to display Redis data. Similarly, you can prevent new developers from running dangerous administrative commands.\nSet up RBAC To set up RBAC, first navigate to the Data Access Control screen.\nThere are three tabs on this screen: Users, Roles, and Redis ACLs.\nIn the Redis ACLs tab, you define named permissions for specific Redis commands, keys, and pub/sub channels.\nIn the Roles tab, you create roles. Each role consists of a set of permissions for one or more Redis Cloud databases.\nFinally, in the Users tab, you create users and assign each user a role.\nOSS Redis ACLs vs. Redis Enterprise Cloud RBAC In open source Redis, you can create users and assign ACLs to them using the ACL command. However, open source Redis does not support generic roles.\nIn Redis Enterprise Cloud, you configure RBAC using the admin console. As a result, certain open source Redis ACL subcommands are not available in Redis Cloud.\nSpecifically, Redis Cloud databases block the following ACL subcommands: LOAD, SAVE, SETUSER, DELUSER, GENPASS, and LOG.\nRedis Cloud databases allow these ACL subcommands: LIST, USERS, GETUSER, CAT, WHOAMI, and HELP.\nIn open source Redis, you must explicitly provide access to the MULTI, EXEC, and DISCARD commands. In Redis Cloud, these commands, which are used in transactions, are always permitted. However, the commands run within the transaction block are subject to RBAC permissions.\nWhen you run multi-key commands on multi-slot keys, the return value is failure but the command runs on the keys that are allowed.\nDefine permissions To define permissions, go to the Redis ACLs tab of the Data Access Control screen.\nYou define these named permissions using the Redis ACL syntax. This syntax lets you concisely specify which commands, command categories, keys, and pub/sub channels to allow.\nThe Redis ACL syntax emphasizes brevity:\n+ includes commands or command categories - excludes commands or command categories @ indicates a command category ~ defines a permitted key pattern \u0026amp; allows access to a pub/sub channel Command ACL rules A command can be any Redis command.\nFor example, this Redis ACL rule indicates that the SET command is permitted:\n+set Command category ACL rules A command category is a predefined, named set of commands.\nFor example, the Redis commands that read data are available in the read command category. This Redis ACL rule permits access to all read commands:\n+@read To find out which commands are included in the read command category, run the following command with redis-cli:\nACL CAT read Key ACL rules There\u0026rsquo;s also a syntax for specifying which keys are accessible.\nThe following ACL rule allows access to all keys:\n~* Whereas, this ACL rule only allows access to keys prefixed with cache:\n~cache:* Pub/sub ACL rules Pub/sub ACL rules determine which pub/sub channels a user can access.\nFor Redis version 6.2, pub/sub is permissive and allows access to all channels by default.\nRestrict channel access To block access to all channels, use the following ACL rule:\nresetchannels If you want to limit access to specific channels, first include resetchannels. Then use \u0026amp; syntax to allow access to particular channels:\nresetchannels \u0026amp;channel1 \u0026amp;channel2 Allow all channels To make pub/sub explicitly permissive and allow users to access all channels, set the following rule:\nallchannels Predefined permissions Redis Cloud includes three, predefined permissions:\nFull-Access (+@all ~*) - All commands are allowed for all keys\nRead-Write (+@all -@dangerous ~*) - All commands except for the \u0026ldquo;dangerous\u0026rdquo; command category are allowed for all keys\nRead-Only (+@read ~*) - Only the \u0026ldquo;read\u0026rdquo; command category is allowed for all keys\nModule command permissions Note that you can define permissions for the Redis module commands of any modules that are loaded on the subscription; however, these permissions can only be used for databases that support those modules.\nTo define database access control, you can either:\nUse the predefined data access roles and add Redis ACLs to them for specific databases. Create new data access roles and select the management roles and Redis ACLs that apply to the roles for specific databases. Assign roles and Redis ACLs to a database in the access control list section of the database configuration. Configure permissions with Redis ACLs To configure a Redis ACL that you can assign to a data access role:\nGo to Data Access Control \u0026gt; ACLs and either:\nCreate a new Redis ACL:\nPoint to an existing ACL and select Edit:\nProvide a descriptive name for the Redis ACL.\nEnter ACL syntax to define the ACL rule or select Rule Builder for help building the ACL rule with correct syntax.\nTo create a Redis ACL rule with the Rule Builder:\nFor Redis commands / categories, enter a command or command category.\nSelect whether to include or exclude the command or category.\nFor Keys, enter the pattern for permitted keys.\nIn Pub/Sub channels, enter a channel pattern to restrict pub/sub so it only allows access to the specified channels.\nThe rule builder automatically adds resetchannels to the ACL rule when you save. This rule changes pub/sub access from permissive (allows access to all channels) to restrictive (blocks access to all channels).\nNote: Pub/Sub channels are only available in the Rule Builder for accounts that have Redis version 6.2 or later for all subscriptions. If your account contains any Redis 6.0 subscriptions, you can\u0026rsquo;t use pub/sub ACLs unless you contact support to upgrade the subscriptions to a later version. To add more commands, categories, keys, or pub/sub channels to the ACL rule, select Add:\nWhen you finish building the ACL rule, select Save rule:\nSelect the check mark to save your changes:\nAssign permissions to roles To assign Redis ACLs to a data access role:\nGo to Data Access Control \u0026gt; Roles and either:\nPoint to an existing role and select the Edit button:\nSelect the Add button to create a new role:\nIn the Associations section of the Edit role or Create new role screen, you can:\nPoint to an existing association and select the Edit button:\nSelect the Add button to create a new association:\nSelect one or more databases from the Databases list.\nTo set the role\u0026rsquo;s level of access to the selected databases, select a Redis ACL from the list.\nSelect the check mark to confirm the association:\nSelect Save role:\nUsers assigned the role can access the databases according to the role\u0026rsquo;s associated Redis ACLs.\nAssign roles to users To assign a role to a user:\nGo to Data Access Control \u0026gt; Users.\nPoint to the user and select the Edit button when it appears:\nSelect a Role from the list.\nSelect the check mark to assign the role to the user:\n","categories":["RC"]},{"uri":"/rs/references/compatibility/commands/pub-sub/","uriRel":"/rs/references/compatibility/commands/pub-sub/","title":"Pub/sub commands compatibility","tags":[],"keywords":[],"description":"Pub/sub commands compatibility.","content":"The following table shows which open source Redis pub/sub commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes PSUBSCRIBE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PUBLISH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PUBSUB CHANNELS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PUBSUB NUMPAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PUBSUB NUMSUB ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active PUNSUBSCRIBE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SUBSCRIBE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active UNSUBSCRIBE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/quickstart/","uriRel":"/modules/redisgears/jvm/quickstart/","title":"RedisGears JVM quick start","tags":[],"keywords":[],"description":"A quick start to learn how to use RedisGears with Java.","content":"Prerequisites For this quick start, you need:\nA Redis Enterprise cluster with the RedisGears module and JVM plugin installed and enabled on a database redis-cli with connectivity to a Redis database Tutorial Create a Maven project Create a new Maven project.\nAdd the following sections to the pom.xml file:\n\u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;snapshots-repo\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/content/repositories/snapshots\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.redislabs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gear_runtime\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.3-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Add example code for either batch processing or event processing to your project\u0026rsquo;s main function.\nBuild a JAR Use the Maven command-line tool or an IDE plugin to compile and package your code into a JAR file:\n$ mvn package Upload the JAR Upload your JAR file to a node in the Redis Enterprise cluster. You will need to use the destination filepath when you run your code.\nRun RedisGears Java code Use the RG.JEXECUTE command to run your code:\n$ redis-cli -x -h {host} -p {port} RG.JEXECUTE {package.MainClass} \u0026lt; {filepath}/{JAR name}.jar Note: When you use GearsBuilder.run(), RG.JEXECUTE runs your code immediately. However, if you use GearsBuilder.register(), RG.JEXECUTE only outputs an OK message if it registers successfully. Your registered code will run whenever certain database events occur. Example code You can use these code examples with your own Maven project to try out batch processing or event processing with the RedisGears JVM plugin.\nBatch processing If you use the GearsBuilder.run() function within your code, then the functions you add to the pipeline will run exactly once when you use RG.JEXECUTE with your JAR file.\nThe following example calculates the average rating of all restaurant reviews stored in your database.\nAdd data to the database Connect to your database with redis-cli:\n$ redis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; Add a few review hashes to the database with the HSET command:\n127.0.0.1:12000\u0026gt; HSET review:1 user \u0026#34;Alex L\u0026#34; message \u0026#34;My new favorite restaurant!\u0026#34; rating 5 (integer) 3 127.0.0.1:12000\u0026gt; HSET review:2 user \u0026#34;Anonymous user\u0026#34; message \u0026#34;Kind of overpriced\u0026#34; rating 2 (integer) 3 127.0.0.1:12000\u0026gt; HSET review:3 user \u0026#34;Francis J\u0026#34; message \u0026#34;They have a really unique menu.\u0026#34; rating 4 (integer) 3 127.0.0.1:12000\u0026gt; exit Example code import java.io.Serializable; import gears.GearsBuilder; import gears.readers.KeysReader; import gears.records.KeysReaderRecord; public class Reviews implements Serializable { private static final long serialVersionUID = 1L; int count; // Total number of reviews int ratingsSum; // Sum of all review ratings // Reviews constructor public Reviews(int count, int ratingsSum) { this.count = count; this.ratingsSum = ratingsSum; } public static void main(String args[]) { // Create the reader that will pass data to the pipe KeysReader reader = new KeysReader(); // Create the data pipe builder GearsBuilder\u0026lt;KeysReaderRecord\u0026gt; gb = GearsBuilder.CreateGearsBuilder(reader); gb.filter(r-\u0026gt;{ // Filter out any keys that are not reviews return r.getKey().startsWith(\u0026#34;review:\u0026#34;); }).map(r-\u0026gt;{ // Extract the rating field return r.getHashVal().get(\u0026#34;rating\u0026#34;); }) .accumulate(new Reviews(0, 0), (accumulator, record)-\u0026gt; { // Count the reviews and add up all of their ratings accumulator.count++; accumulator.ratingsSum += Integer.parseInt(record); return accumulator; }).map(r-\u0026gt;{ // Calculate the average rating return Double.valueOf(((double) r.ratingsSum) / r.count); }); // Run the data through the pipeline immediately gb.run(); } } Example output $ redis-cli -x -h {host} -p {port} \\ RG.JEXECUTE com.domain.packagename.Reviews \u0026lt; /tmp/rgjvmtest-0.0.1-SNAPSHOT.jar 1) 1) \u0026#34;3.6666666666666665\u0026#34; 2) (empty array) Event processing If you use the GearsBuilder.register() function in your code, then the functions you add to the pipeline will run every time a certain database event occurs.\nThe following example registers a pipeline of functions to automatically update the maximum age every time you add a new person hash to your database.\nExample code import gears.GearsBuilder; import gears.readers.KeysReader; import gears.records.KeysReaderRecord; public class App { public static void main(String args[]) { // Create the reader that will pass data to the pipe KeysReader reader = new KeysReader(); // Create the data pipe builder GearsBuilder\u0026lt;KeysReaderRecord\u0026gt; gb = GearsBuilder.CreateGearsBuilder(reader); // Only process keys that start with \u0026#34;person:\u0026#34; gb.filter(r-\u0026gt;{ return r.getKey().startsWith(\u0026#34;person:\u0026#34;); }); // Compare each person\u0026#39;s age to the current maximum age gb.foreach(r-\u0026gt;{ String newAgeStr = r.getHashVal().get(\u0026#34;age\u0026#34;); int newAge = Integer.parseInt(newAgeStr); // Get the current maximum age String maxAgeKey = \u0026#34;age:maximum\u0026#34;; String maxAgeStr = (String) GearsBuilder.execute(\u0026#34;GET\u0026#34;, maxAgeKey); int maxAge = 0; // Initialize to 0 if (maxAgeStr != null) { // Convert the maximum age to an integer maxAge = Integer.parseInt(maxAgeStr); } // Update the maximum age if a new age is higher if (newAge \u0026gt; maxAge) { GearsBuilder.execute(\u0026#34;SET\u0026#34;, maxAgeKey, newAgeStr); } }); // Store this pipeline of functions and // run them when a new person key is added gb.register(ExecutionMode.SYNC); // Note: ExecutionMode defaults to ASYNC // if you call register() without any arguments } } Example event processing After you register your code with the RG.JEXECUTE command, add some data to the database and check the value of age:maximum to verify that it runs correctly.\nConnect to your database with redis-cli:\n$ redis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; Add a hash that represents a person with HSET:\n127.0.0.1:12000\u0026gt; HSET person:1 name \u0026#34;Alex\u0026#34; age 24 (integer) 2 The current value of age:maximum should match Alex\u0026rsquo;s age:\n127.0.0.1:12000\u0026gt; GET age:maximum \u0026#34;24\u0026#34; Add another person with a higher age and then check that age:maximum updated automatically:\n127.0.0.1:12000\u0026gt; HSET person:2 name \u0026#34;Morgan\u0026#34; age 45 (integer) 2 127.0.0.1:12000\u0026gt; GET age:maximum \u0026#34;45\u0026#34; Add a person with a lower age and verify that age:maximum did not change:\n127.0.0.1:12000\u0026gt; HSET person:3 name \u0026#34;Lee\u0026#34; age 31 (integer) 2 127.0.0.1:12000\u0026gt; GET age:maximum \u0026#34;45\u0026#34; ","categories":["Modules"]},{"uri":"/modules/redisgears/python/quickstart/","uriRel":"/modules/redisgears/python/quickstart/","title":"RedisGears Python quick start","tags":[],"keywords":[],"description":"A quick start to learn how to use RedisGears with Python.","content":"For this tutorial, you need:\nEither: A Redis Enterprise cluster with the RedisGears module and Python plugin installed and enabled on a database An OSS Redis database with the RedisGears module redis-cli with connectivity to a Redis database RedisGears basics In this quick start guide, we\u0026rsquo;ll see how to use RedisGears to perform batch processing and event processing.\nWith RedisGears, batch processing means processing the data already stored in a Redis database. Event processing means processing changes to the Redis key space.\nThe examples below assume an empty Redis database.\nBatch processing Let\u0026rsquo;s start with the simplest example. From the redis-cli, run the following command:\nredis.cloud:6379\u0026gt; RG.PYEXECUTE \u0026#34;GearsBuilder().run()\u0026#34; 1) (empty array) 2) (empty array) This command doesn\u0026rsquo;t do much; it simply iterates over the keyspace. Let\u0026rsquo;s add a key and run it again:\nredis.cloud:6379\u0026gt; SET message \u0026#34;hello world\u0026#34; OK redis.cloud:6379\u0026gt; RG.PYEXECUTE \u0026#34;GearsBuilder().run()\u0026#34; 1) 1) \u0026#34;{\u0026#39;event\u0026#39;: None, \u0026#39;key\u0026#39;: \u0026#39;message\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;hello world\u0026#39;}\u0026#34; 2) (empty array) We\u0026rsquo;ve added a single string, and you can see that this gears function processes it, even though it does nothing with the data. Let\u0026rsquo;s actually do something with the data. So first, we\u0026rsquo;ll add a few more strings:\nredis.cloud::6379\u0026gt; SET message:2 \u0026#34;hello galaxy\u0026#34; OK redis.cloud:6379\u0026gt; SET message:3 \u0026#34;hello universe\u0026#34; OK We now have three strings in our database. Suppose we want to perform a unique word count on these strings. We can write a RedisGears function to do this just that. So open a file called wordcount.py, and add the following code:\ngb = GearsBuilder() gb.map(lambda x: x[\u0026#39;value\u0026#39;]) # map each key object to its string value gb.flatmap(lambda x: x.split()) # split each string into a list of words gb.countby() # run a count-unique on these words gb.run() There are two ways to load files into RedisGears. For production deployments, we recommend using the special gears-cli. However, for the purpose of this demonstration, the easiest way is to pass the filename through the redis-cli command, like so:\n$ redis-cli rg.pyexecute \u0026#34;`cat wordcount.py`\u0026#34; 1) 1) \u0026#34;{\u0026#39;key\u0026#39;: \u0026#39;world\u0026#39;, \u0026#39;value\u0026#39;: 1}\u0026#34; 2) \u0026#34;{\u0026#39;key\u0026#39;: \u0026#39;galaxy\u0026#39;, \u0026#39;value\u0026#39;: 1}\u0026#34; 3) \u0026#34;{\u0026#39;key\u0026#39;: \u0026#39;hello\u0026#39;, \u0026#39;value\u0026#39;: 3}\u0026#34; 4) \u0026#34;{\u0026#39;key\u0026#39;: \u0026#39;universe\u0026#39;, \u0026#39;value\u0026#39;: 1}\u0026#34; 2) (empty array) The results here show the number of occurences of each word in all of our strings. So, we\u0026rsquo;ve effectively processed the data in our Redis database all at once, in a batch.\nEvent processing You may have noticed that all of the RedisGears functions above end with a call to run(). This indicates that the function should be run immediately on the data in the Redis database. But what if you want to process data as it arrives in Redis? In that case, your functions will end with a call to register(), which will store the function and apply it as events occur in Redis.\nLet\u0026rsquo;s see how to register a function. First, suppose we\u0026rsquo;re writing hashes to our database that represent users. They take the following form:\nredis.cloud:6379\u0026gt; HSET person:3 name \u0026#34;Summer Smith\u0026#34; age 17 (integer) 2 redis.cloud:6379\u0026gt; HSET person:4 name \u0026#34;James Jameson\u0026#34; age 21 (integer) 2 Each hash has two fields, one containing a name and the other an age. Now, suppose we want to keep a record of the maximum age of all users. We can register a RedisGears function to do this. Open up a file called maxage.py, and add the following code:\ndef age(x): \u0026#39;\u0026#39;\u0026#39; Extracts the age from a person\u0026#39;s record \u0026#39;\u0026#39;\u0026#39; return int(x[\u0026#39;value\u0026#39;][\u0026#39;age\u0026#39;]) def compare_and_swap(x): \u0026#39;\u0026#39;\u0026#39; Checks and sets the current maximum \u0026#39;\u0026#39;\u0026#39; k = \u0026#39;age:maximum\u0026#39; v = execute(\u0026#39;GET\u0026#39;, k) # read key\u0026#39;s current value v = int(v) if v else 0 # initialize to 0 if None if x \u0026gt; v: # if a new maximum found execute(\u0026#39;SET\u0026#39;, k, x) # set key to new value # Event handling function registration gb = GearsBuilder() gb.map(age) # Extract the \u0026#39;age\u0026#39; field from each hash gb.foreach(compare_and_swap) # Compare the max age to the value stored at age:maximum gb.register(\u0026#39;person:*\u0026#39;) # Only process keys matching the pattern \u0026#39;person:*\u0026#39; You can see here that we define two methods: age() and compare_and_swap(). Even if you\u0026rsquo;re not familiar with Python, you should be able to see what the methods do.\nBelow the method definitions is the RedisGears data flow that we\u0026rsquo;re defining. Notice that at the end we call register() to register the function to listen for events.\nTo load this function into RedisGears, run the following:\n$ redis-cli RG.PYEXECUTE \u0026#34;`cat maxage.py`\u0026#34; Now start the redis-cli, and create a couple of hashes:\nredis.cloud:6379\u0026gt; HSET person:5 name \u0026#34;Marek Michalski\u0026#34; age 17 (integer) 2 redis.cloud:6379\u0026gt; HSET person:6 name \u0026#34;Noya Beit\u0026#34; age 21 (integer) 2 To see if the RedisGears function is working, check the value of age:maximum:\nredis.cloud:6379\u0026gt; GET age:maximum \u0026#34;21\u0026#34; Next steps You should now have a basic idea of how to run RedisGears functions for batch and event processing. But there\u0026rsquo;s a lot more to RedisGears than this. To better understand it, see the RedisGears tutorial. If you\u0026rsquo;re interested in write-behind caching, see our write-behind caching overview.\n","categories":["Modules"]},{"uri":"/rc/rc-quickstart/","uriRel":"/rc/rc-quickstart/","title":"Redis Cloud quick start","tags":[],"keywords":[],"description":"","content":"If you\u0026rsquo;re new to Redis Enterprise Cloud, this quick start helps you get up and running.\nYou\u0026rsquo;ll learn how to:\nCreate an account, a free subscription, and a database\nConnect to your database\nIf you already have an account, see Create a Fixed subscription to create a Free 30MB subscription. Free plans are a tier of fixed plans; this provides an easy upgrade path when you need it.\nIf you already have a subscription, see Manage subscriptions and Manage databases.\nCreate an account To create a new account with a free subscription and database:\nOn the Redis Cloud admin console, select Sign up.\nEnter your information in the form and select Get Started, or sign up with Google or Github.\nIn the activation email, select Activate account to go to the Redis Cloud admin console.\nSelect your preferred cloud vendor and region.\nSelect Let\u0026rsquo;s start free to create your subscription and database.\nNote: If you would rather customize your subscription and database, select Create a custom database to go to the Add subscription page. From there, you can create a fixed subscription or create a flexible subscription. You\u0026rsquo;re taken to the Overview tab for your new subscription.\nSelect the database name to view the Configuration tab for your new database.\nIn the upper corner, an icon shows the current status of the database. If the icon shows an orange clock, this means your database is still being created and its status is pending.\nOnce the database has been created, it becomes active and the status indicator switches to a green circle containing a checkmark.\nAdmin console operations are asynchronous; they operate in the background. You can continue to use the admin console for other tasks, but pending resources aren\u0026rsquo;t available until they\u0026rsquo;re active.\nWhen your new database becomes active, you\u0026rsquo;re ready to connect to it.\nConnect to a database At this point, you\u0026rsquo;re viewing the Configuration details for your new database.\nTo connect to your database, you need your username and password. For the default user, the username is default.\nThe Security section contains your Default user password. By default, this is masked. Select the eye icon to show or hide the password.\nOnce you have the username and password, select Connect to open the connection wizard.\nThe connection wizard provides the following database connection methods:\nredis-cli utility\nRedis client for your preferred programming language\nRedisInsight\nredis-cli (via Docker) The redis-cli utility is installed when you install Redis. It provides a command-line interface that lets you work with your database using core Redis commands.\nDocker provides a convenient way to run redis-cli without the full installation experience.\nRun the following commands to create a redis Docker container and connect to your database with redis-cli:\nDownload the redis Docker image:\n$ docker pull redis Start a container created from the image:\n$ docker run -d --name redis1 redis Connect to a bash prompt running in the container:\n$ docker exec -it redis1 bash In the connection wizard, under Redis CLI, select the Copy button to copy the redis-cli command.\nEnter the copied redis-cli command in the terminal and replace \u0026lt;username\u0026gt; and \u0026lt;password\u0026gt; with your username and password.\nAfter you connect to your database, try these basic Redis commands:\nxxx:yyy\u0026gt; ping PONG xxx:yyy\u0026gt; set hello world OK xxx:yyy\u0026gt; get hello \u0026#34;world\u0026#34; To try other Redis commands, see the commands reference for help.\nRedis client Different programming languages use different connection clients to interact with Redis databases, and each client has its own syntax and installation process. For help with a specific client, see the client\u0026rsquo;s documentation.\nThe connection wizard provides code snippets to connect to your database with the following programming languages:\nnode.js using node-redis .NET using StackExchange.Redis Python using redis-py Java using Jedis See the client list to view all Redis clients by language.\nCode example (Python) To connect to your database using the redis-py library for Python:\nInstall the Redis client if it is not already installed.\n$ sudo pip install redis In the connection wizard, under Redis Client, select Python from the Select your client menu.\nSelect Copy to copy the connection code for your database.\nAdd the copied code to your program and replace \u0026lt;username\u0026gt; and \u0026lt;password\u0026gt; with your username and password.\nimport redis r = redis.Redis( host=\u0026#39;\u0026lt;host\u0026gt;\u0026#39;, port=\u0026lt;port\u0026gt;, password=\u0026#39;\u0026lt;password\u0026gt;\u0026#39;) # Redis commands r.set(\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;) print(r.get(\u0026#39;hello\u0026#39;)) Run the program.\n$ python example_redis.py world RedisInsight RedisInsight is a free Redis GUI that is available for MacOS, Windows, and Linux.\nIn the connection wizard, under Redis Client, select your operating system from the Download RedisInsight menu.\nSelect Download to download RedisInsight.\nInstall RedisInsight.\nOpen RedisInsight and select Add Redis Database.\nIn the connection wizard, under RedisInsight Desktop, select Copy to copy the connection information.\nIn RedisInsight, enter the copied connection information into the Host field. RedisInsight automatically populates the rest of the fields needed to connect to the database as the default user.\nSelect Add Redis Database to connect to the database.\nSee the RedisInsight documentation for more information.\nMore info Manage databases Data persistence Secure your Redis Enterprise Cloud database Back-up Flexible databases Monitor Redis Enterprise Cloud performance. ","categories":["RC"]},{"uri":"/rc/","uriRel":"/rc/","title":"Redis Enterprise Cloud","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Cloud delivers a fully managed Redis database offering hosted on major public cloud services.\nWith Redis Enterprise Cloud, you get all of the features of Redis Enterprise, including:\nRedis and Redis Stack support Linear scalability Instant failover, backups, and recovery Predictable performance 24/7 monitoring and support Get started Use the Quick start to create a free subscription and create your first database.\nConnect with redis-cli Connect with Redis client Connect with RedisInsight Subscriptions Learn about the types of subscriptions.\nCreate a fixed subscription Create a flexible subscription Accounts \u0026amp; settings Manage Redis Cloud accounts and settings.\nBilling and payments Manage cloud integrations Databases Create and manage Redis databases in the cloud.\nCreate database View and edit databases Monitor performance Manage databases Redis commands \u0026amp; compatibility Security Manage secure connections to cloud databases.\nAccess management Cloud database security Multi-factor authentication Single sign-on REST API Use the REST API to manage the database.\nGet started with the REST API REST API reference \u0026amp; examples Related info Redis Enterprise Software Open source Redis (redis.io) Redis Stack \u0026amp; modules Glossary ","categories":["RC"]},{"uri":"/rs/","uriRel":"/rs/","title":"Redis Enterprise Software","tags":[],"keywords":[],"description":"","content":"Redis Enterprise is a self-managed, enterprise-grade version of Redis.\nWith Redis Enterprise, you get many enterprise-grade capabilities, including:\nLinear scalability High availability, backups, and recovery Predictable performance 24/7 support You can run Redis Enterprise Software in an on-premises data center or on your preferred cloud platform.\nGet started Build a small-scale cluster with the Redis Enterprise Software container image.\nGet started Docker Get started with Active-Active Install \u0026amp; setup Install \u0026amp; set up a Redis Enterprise Software cluster.\nNetworking Set up \u0026amp; configure a cluster Release notes Databases Create and manage a Redis database on a cluster.\nCreate a Redis Enterprise Software database Configure database Create Active-Active database Edit Active-Active database Security Manage secure connections to the cluster and databases.\nAccess control Users \u0026amp; roles Certificates TLS \u0026amp; Encryption Reference Use command-line utilities and the REST API to manage the cluster and databases.\nrladmin, crdb-cli, \u0026amp; other utilities REST API reference \u0026amp; examples Redis commands (redis.io) Related info Redis Enterprise Cloud Open source Redis (redis.io) Redis Stack Glossary ","categories":["RS"]},{"uri":"/rs/references/compatibility/commands/scripting/","uriRel":"/rs/references/compatibility/commands/scripting/","title":"Scripting commands compatibility","tags":[],"keywords":[],"description":"Scripting commands compatibility.","content":"The following table shows which open source Redis scripting commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nScripting commands Command Redis\nEnterprise Redis\nCloud Notes EVAL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active EVALSHA ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SCRIPT DEBUG ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active SCRIPT EXISTS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SCRIPT FLUSH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SCRIPT KILL ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SCRIPT LOAD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ","categories":["RS"]},{"uri":"/rs/references/compatibility/commands/server/","uriRel":"/rs/references/compatibility/commands/server/","title":"Server management commands compatibility","tags":[],"keywords":[],"description":"Server management commands compatibility.","content":"The following tables show which open source Redis server management commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nAccess control commands Several access control list (ACL) commands are not available in Redis Enterprise. Instead, you can manage access controls from the admin consoles for Redis Enterprise Software and Redis Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes ACL CAT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. ACL DELUSER ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL GENPASS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL GETUSER ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. ACL HELP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. ACL LIST ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. ACL LOAD ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL LOG ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL SAVE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL SETUSER ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ACL USERS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. ACL WHOAMI ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. Configuration commands Command Redis\nEnterprise Redis\nCloud Notes CONFIG GET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supports a subset of configuration settings. CONFIG RESETSTAT ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active CONFIG REWRITE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active CONFIG SET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Only supports a subset of configuration settings. General server commands Command Redis\nEnterprise Redis\nCloud Notes COMMAND ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active COMMAND COUNT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active COMMAND GETKEYS ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active COMMAND HELP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active COMMAND INFO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active DEBUG ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active FLUSHALL ✅ Standard\n❌ Active-Active* ✅ Standard\n❌ Active-Active *Can use the Active-Active flush API request. FLUSHDB ✅ Standard\n❌ Active-Active* ✅ Standard\n❌ Active-Active *Can use the Active-Active flush API request. LOLWUT ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SHUTDOWN ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active SWAPDB ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active TIME ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Module commands For Redis Enterprise Software, you can manage Redis modules from the admin console or with REST API requests.\nRedis Cloud manages modules for you and lets you enable modules when you create a database.\nCommand Redis\nEnterprise Redis\nCloud Notes MODULE HELP ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MODULE LIST ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MODULE LOAD ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MODULE UNLOAD ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Monitoring commands Although Redis Enterprise does not support certain monitoring commands, you can use the admin consoles to view Redis Enterprise Software metrics and logs or Redis Cloud metrics and logs.\nCommand Redis\nEnterprise Redis\nCloud Notes DBSIZE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active INFO ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. LATENCY DOCTOR ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LATENCY GRAPH ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LATENCY HELP ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LATENCY HISTORY ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LATENCY LATEST ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LATENCY RESET ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MEMORY DOCTOR ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MEMORY HELP ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MEMORY MALLOC-STATS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MEMORY PURGE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MEMORY STATS ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MEMORY USAGE ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MONITOR ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active SLOWLOG GET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. SLOWLOG LEN ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. SLOWLOG RESET ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Not supported for scripts. Persistence commands Data persistence and backup commands are not available in Redis Enterprise. Instead, you can manage data persistence and backups from the admin consoles for Redis Enterprise Software and Redis Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes BGREWRITEAOF ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active BGSAVE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active LASTSAVE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active SAVE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Replication commands Redis Enterprise automatically manages replication.\nCommand Redis\nEnterprise Redis\nCloud Notes FAILOVER ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active MIGRATE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active PSYNC ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active REPLCONF ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active REPLICAOF ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active RESTORE-ASKING ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ROLE ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active SLAVEOF ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active Deprecated as of Redis v5.0.0. SYNC ❌ Standard\n❌ Active-Active ❌ Standard\n❌ Active-Active ","categories":["RS"]},{"uri":"/rs/clusters/new-cluster-setup/","uriRel":"/rs/clusters/new-cluster-setup/","title":"Set up a new cluster","tags":[],"keywords":[],"description":"How to set up a new cluster using the management UI.","content":"A Redis Enterprise Software cluster typically consists of several nodes. For production deployments, we recommend an uneven number of nodes, with a minimum of three.\nNote: In a cluster that consists of only one node, some features and capabilities are not enabled, such as database replication that provides high availability. To set up a new cluster, you must first install the Redis Enterprise Software package and then set up the cluster as described below. After the cluster is created you can add multiple nodes to the cluster.\nTo create a cluster:\nIn a browser, navigate to https://\u0026lt;name or IP address of the machine with RS installed\u0026gt;:8443. For example, if you installed RS on a machine with IP address 10.0.1.34, then navigate to https://10.0.1.34:8443.\nNote: The RS management UI uses a self-signed SSL/TLS encryption. If the machine has both an internal IP address and an external IP address, use the external IP address to access the setup UI. Click Setup.\nIn the Node Configuration page that appears:\nEnter a path for Persistent storage, or leave the default path.\nEnter a path for Ephemeral storage, or leave the default path.\nIf you want to enable Redis on Flash, select the Enable flash storage support checkbox and enter the path to the Flash storage.\nIf your machine is configured to have multiple IP addresses, the section IP Addresses Usage is shown. Use the IP Addresses Usage section to assign a single IPv4 type address for internal traffic and multiple IPv4/IPv6 type addresses for external traffic.\nIn Cluster configuration, select Create new cluster.\nIn Cluster name (FQDN), enter a unique name for the cluster. Also, make sure that you look at the instructions for setting up DNS to make sure your cluster is reachable by name.\nChoose whether to Enable private \u0026amp; public endpoints support.\nChoose whether to Enable rack-zone awareness. Enabling rack-zone awareness requires setting the Rack-zone ID for the node.\nClick Next.\nIf you purchased a cluster key, use the Cluster authentication page to enter the key. Otherwise, you get the trial license by default. Read the product Terms and Conditions and click Next.\nClick OK to confirm that you are aware of the replacement of the HTTPS SSL/TLS certificate on the node, and proceed through the browser warning.\nIn the Set admin credentials fields, enter the credentials of the cluster administrator.\nClick Next.\nAfter a short wait, your cluster is created and you can log in to the RS admin console.\nYou can now access any of the management capabilities, including:\nCreating a new database Joining a new node to a cluster ","categories":["RS"]},{"uri":"/rc/security/shared-responsibility-model/","uriRel":"/rc/security/shared-responsibility-model/","title":"Redis Cloud shared responsibility model","tags":[],"keywords":[],"description":"","content":"The security of all Redis Enterprise Cloud deployments is a shared responsibility. Redis, the public cloud providers (Amazon Web Services [AWS], Google Cloud Platform [GCP], and Microsoft Azure), and our customers all help ensure the security of these deployments.\nRedis responsibility Redis Enterprise Cloud\u0026rsquo;s offerings are managed by Redis and deployed on AWS, Azure, and Google Cloud infrastructure.\nRedis is responsible for the software that runs Redis Enterprise Cloud. This includes the patching and maintenance of the operating systems that Redis is deployed on as well as the patching and maintenance of Redis Enterprise Cloud.\nCloud provider responsibility The public cloud provider hosting your Redis Enterprise Cloud databases is responsible for the physical security of their data centers and the security of the network, storage, servers, and virtualization that form the core infrastructure of your deployment.\nAmazon, Microsoft, and Google’s public clouds embrace a wide range of security best practices and compliance standards. Compliance information—including audits, attestations, and certifications about resources hosted—can be found in the following compliance pages:\nAWS Compliance GCP Compliance Azure Compliance Customer responsibility Customers are responsible for the security configurations in their Redis databases and the Redis Enterprise Cloud admin console. Customers must understand and implement the Redis Enterprise Cloud security features and best practices.\nCustomers are also responsible for the applications built on Redis and the data they store in Redis. Customers determine the cloud provider, region, and availability zone of their deployments.\nCustomers understand that Redis Enterprise Cloud Fixed plans (including Free) are deployed to multi-tenant infrastructure. Flexible and Annual plans are deployed to single-tenant infrastructure dedicated to one specific customer.\n","categories":["RC"]},{"uri":"/rs/databases/connect/supported-clients-browsers/","uriRel":"/rs/databases/connect/supported-clients-browsers/","title":"Supported connection clients","tags":[],"keywords":[],"description":"Info about Redis client libraries and supported clients when using the discovery service.","content":"You can connect to Redis Enterprise Software databases programmatically using client libraries.\nRedis client libraries To connect an application to a Redis database hosted by Redis Enterprise Software, use a client library appropriate for your programming language.\nYou can also use the redis-cli utility to connect to a database from the command line.\nFor examples of each approach, see Get started with Redis Enterprise Software.\nNote: You cannot use client libraries to configure Redis Enterprise Software. Instead, use:\nThe Redis Software admin console The REST API Command-line utilities, such as rladmin Discovery service We recommend the following clients when using a discovery service based on the Redis Sentinel API:\nredis-py (Python Redis client) Hiredis (C Redis client) Jedis (Java Redis client) node-redis (Node.js Redis client) If you need to use another client, you can use Sentinel Tunnel to discover the current Redis master with Sentinel and create a TCP tunnel between a local port on the client and the master.\n","categories":["RS"]},{"uri":"/rs/installing-upgrading/supported-platforms/","uriRel":"/rs/installing-upgrading/supported-platforms/","title":"Supported platforms","tags":[],"keywords":[],"description":"","content":" Redis Enterprise Software is supported on several operating systems, cloud environments, and virtual environments.\nSystem requirements Make sure your system meets these requirements:\nOnly 64-bit operating systems are supported. You must install Redis Enterprise Software directly on the host, not through system cloning. You must install on a clean host with no other applications running so that all RAM is allocated to the operating system and Redis Enterprise Software only. Linux distributions must be installed with at least \u0026ldquo;Minimal Install\u0026rdquo; configuration. Supported platforms Platform Versions/Information Ubuntu 16.04 (deprecated), 18.04\nServer version is recommended for production installations. Desktop version is only recommended for development deployments. Red Hat Enterprise Linux (RHEL) 7, CentOS 7 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9\nRequires OpenSSL 1.0.2 and firewall configuration RHEL 8, CentOS 8 8.0, 8.1, 8.2, 8.3, 8.4, 8.5, and 8.6 Oracle Linux 7, Oracle Linux 8 Based on the corresponding RHEL version Rocky Linux 8 Based on RHEL 8 Amazon Linux Version 1 Docker Docker images of Redis Enterprise Software are certified for development and testing only. Kubernetes See the Redis Enterprise for Kubernetes documentation Operating system limitations Be aware that Redis Enterprise Software relies on certain components that require support from the operating system. You cannot enable support for components, services, protocols, or versions that aren\u0026rsquo;t supported by the operating system running Redis Enterprise Software. In addition, updates to the operating system or to Redis Enterprise Software can impact component support.\nTo illustrate, version 6.2.8 of Redis Enterprise Software removed support for TLS 1.0 and TLS 1.1 on Red Hat Enterprise Linux 8 (RHEL 8) because that operating system does not enable support for these versions by default.\nIf you have trouble enabling specific components, features, or versions, verify that they\u0026rsquo;re supported by your operating system and that they\u0026rsquo;re configured correctly.\nUpgrade RHEL when using modules RHEL 7 clusters cannot be directly upgraded to RHEL 8 when hosting databases using modules. Due to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules. Instead, you need to create a new cluster on RHEL 8 and then migrate existing data from your RHEL 7 cluster. This does not apply to clusters that do not use modules.\nVMware Redis Enterprise is compatible with VMware, but make sure that you:\nConfigure your memory, CPU, network, and storage settings to allow for optimal Redis Enterprise performance. Pin each Redis Enterprise shard to a specific ESX/ESXi host by setting the appropriate affinity rules. If you must manually migrate a virtual machine to another host, follow the best practices for shard maintenance and contact support if you have questions. Disable VMware VMotion because Redis Enterprise is not compatible with VMotion. Don\u0026rsquo;t use VMware snapshots because Redis Enterprise cluster manages state dynamically, so a snapshot might not have the correct node and cluster state. ","categories":["RS"]},{"uri":"/rs/references/compatibility/commands/transactions/","uriRel":"/rs/references/compatibility/commands/transactions/","title":"Transaction commands compatibility","tags":[],"keywords":[],"description":"Transaction commands compatibility.","content":"The following table shows which open source Redis transaction commands are compatible with standard and Active-Active databases in Redis Enterprise Software and Redis Enterprise Cloud.\nCommand Redis\nEnterprise Redis\nCloud Notes DISCARD ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active EXEC ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active MULTI ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active UNWATCH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active WATCH ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ","categories":["RS"]},{"uri":"/kubernetes/architecture/","uriRel":"/kubernetes/architecture/","title":"Redis Enterprise for Kubernetes architecture","tags":[],"keywords":[],"description":"This section provides an overview of the architecture and considerations for Redis Enterprise for Kubernetes.","content":"Redis bases its Kubernetes architecture on several vital concepts.\nLayered architecture Kubernetes is an excellent orchestration tool, but it was not designed to deal with all the nuances associated with operating Redis Enterprise. Therefore, it can fail to react accurately to internal Redis Enterprise edge cases or failure conditions. Also, Kubernetes orchestration runs outside the Redis Cluster deployment and may fail to trigger failover events, for example, in split network scenarios.\nTo overcome these issues, Redis created a layered architecture approach that splits responsibilities between operations Kubernetes does well, procedures Redis Enterprise Cluster excels at, and the processes both can orchestrate together. The figure below illustrated this layered orchestration architecture:\nOperator based deployment Operator allows Redis to maintain a unified deployment solution across various Kubernetes environments, i.e., RedHat OpenShift, VMware Tanzu (Tanzu Kubernetes Grid, and Tanzu Kubernetes Grid Integrated Edition, formerly known as PKS), Google Kubernetes Engine (GKE), Azure Kubernetes Service (AKS), and vanilla (upstream) Kubernetes. Statefulset and anti-affinity guarantee that each Redis Enterprise node resides on a Pod that is hosted on a different VM or physical server. See this setup shown in the figure below:\nNetwork-attached persistent storage Kubernetes and cloud-native environments require that storage volumes be network-attached to the compute instances, to guarantee data durability. Otherwise, if using local storage, data may be lost in a Pod failure event. See the figure below:\nOn the left-hand side (marked #1), Redis Enterprise uses local ephemeral storage for durability. When a Pod fails, Kubernetes launches another Pod as a replacement, but this Pod comes up with empty local ephemeral storage, and the data from the original Pod is now lost.\nOn the right-hand side of the figure (marked #2), Redis Enterprise uses network-attached storage for data durability. In this case, when a Pod fails, Kubernetes launches another Pod and automatically connects it to the storage device used by the failed Pod. Redis Enterprise then instructs the Redis Enterprise database instance/s running on the newly created node to load the data from the network-attached storage, which guarantees a durable setup.\nRedis Enterprise is not only great as an in-memory database but also extremely efficient in the way it uses persistent storage, even when the user chooses to configure Redis Enterprise to write every change to the disk. Compared to a disk-based database that requires multiple interactions (in most cases) with a storage device for every read or write operation, Redis Enterprise uses a single IOPS, in most cases, for a write operation and zero IOPS for a read operation. As a result, significant performance improvements are seen in typical Kubernetes environments, as illustrated in the figures below:\nMultiple services on each pod Each Pod includes multiple Redis Enterprise instances (multiple services). We found that the traditional method of deploying a Redis Enterprise database over Kubernetes, in which each Pod includes only a single Redis Enterprise instance while preserving a dedicated CPU, is notably inefficient. Redis Enterprise is exceptionally fast and in many cases can use just a fraction of the CPU resources to deliver the requested throughput. Furthermore, when running a Redis Enterprise Cluster with multiple Redis Enterprise instances across multiple Pods, the Kubernetes network, with its multiple vSwitches, can quickly become the deployment’s bottleneck. Therefore, Redis took a different approach to managing Redis Enterprise over the Kubernetes environment. Deploying multiple Redis Enterprise database instances on a single Pod allows us to better utilize the hardware resources used by the Pod such as CPU, memory, and network while keeping the same level of isolation. See the figure below:\n","categories":["Platforms"]},{"uri":"/kubernetes/deployment/","uriRel":"/kubernetes/deployment/","title":"Deployment","tags":[],"keywords":[],"description":"This section lists the different ways to set up and run Redis Enterprise for Kubernetes. You can deploy on variety of Kubernetes distributions both on-prem and in the cloud via our Redis Enterprise operator for Kubernetes.","content":"This section lists the different ways to set up and run Redis Enterprise for Kubernetes. You can deploy on variety of Kubernetes distributions both on-prem and in the cloud via our Redis Enterprise operator for Kubernetes.\nOperator overview Redis Enterprise for Kubernetes uses custom resource definitions (CRDs) to create and manage Redis Enterprise clusters (REC) and Redis Enterprise databases (REDB).\nThe operator is a deployment that runs within a given namespace. These operator pods must run with sufficient privileges to create the Redis Enterprise cluster resources within that namespace.\nWhen the operator is installed, the following resources are created:\na service account under which the operator will run a set of roles to define the privileges necessary for the operator to perform its tasks a set of role bindings to authorize the service account for the correct roles (see above) the CRD for a Redis Enterprise cluster (REC) the CRD for a Redis Enterprise database (REDB) the operator itself (a deployment) The operator currently runs within a single namespace and is scoped to operate only on the Redis Enterprise cluster in that namespace.\nDeploy Redis Enterprise Software for Kubernetes How to install Redis Enterprise Software for Kubernetes.\nDeploy Redis Enterprise for Kubernetes with OpenShift A quick introduction to the steps necessary to get a Redis Enterprise cluster installed in your OpenShift Kubernetes cluster\nFlexible deployment options Redis Enterprise for Kubernetes allows you to deploy to multiple namespaces. This article describes flexible deployment options you can use to meet your specific needs.\nDeploy with kustomize How to use the kustomize tool with Redis Enterprise for Kubernetes\nUse a private registry for container images This section details how the Redis Enterprise Software and Kubernetes operator images can be configured to be pulled from a variety of sources. This page describes how to configure alternate private repositories for images, plus some techniques for handling public repositories with rate limiting.\n","categories":["Platforms"]},{"uri":"/kubernetes/deployment/openshift/","uriRel":"/kubernetes/deployment/openshift/","title":"Deploy Redis Enterprise for Kubernetes with OpenShift","tags":[],"keywords":[],"description":"A quick introduction to the steps necessary to get a Redis Enterprise cluster installed in your OpenShift Kubernetes cluster","content":"The deployment of Redis Enterprise clusters is managed with the Redis Enterprise operator that you deploy in the namespace for your project. To create a database that your application workloads can use:\nInstall the Redis Enterprise operator.\nCreate a Redis Enterprise CRD to describe your desired cluster.\nThe operator reads this cluster description and deploys the various components on your K8s cluster.\nOnce running, use the Redis Enterprise cluster to create a database.\nThe operator automatically exposes the new database as a K8s service.\nFor OpenShift via the OperatorHub To create a database on an OpenShift 4.x cluster via the OperatorHub you only need the OpenShift 4.x cluster installed with at least three nodes that each meet the minimum requirements for a development installation.\nFor OpenShift via the CLI To create a database on an OpenShift cluster via the CLI, you need:\nAn OpenShift cluster installed with at least three nodes that each meet the minimum requirements for a development installation. The kubectl package installed at version 1.9 or higher The OpenShift cli installed ","categories":["Platforms"]},{"uri":"/kubernetes/deployment/deployment-options/","uriRel":"/kubernetes/deployment/deployment-options/","title":"Flexible deployment options","tags":[],"keywords":[],"description":"Redis Enterprise for Kubernetes allows you to deploy to multiple namespaces. This article describes flexible deployment options you can use to meet your specific needs.","content":"You can deploy Redis Enterprise for Kubernetes in several different ways depending on your database needs.\nMultiple Redis Enterprise database resources (REDB) can be associated with single Redis Enterprise cluster resource (REC) even if they reside in different namespaces.\nNote: The Redis Enterprise cluster (REC) custom resource must reside in the same namespace as the Redis Enterprise operator. Single REC and single namespace (one-to-one) The standard and simplest deployment deploys your Redis Enterprise databases (REDB) in the same namespace as the Redis Enterprise cluster (REC). No additional configuration is required for this, since there is no communication required to cross namespaces. See Deploy Redis Enterprise for Kubernetes.\nSingle REC and multiple namespaces (one-to-many) Multiple Redis Enterprise databases (REDB) spread across multiple namespaces within the same K8s cluster can be associated with the same Redis Enterprise cluster (REC). See Manage databases in multiple namespaces for more information.\nMultiple RECs and multiple namespaces (many-to-many) A single Kubernetes cluster can contain multiple Redis Enterprise clusters (REC), as long as they reside in different namespaces. Each namespace can host only one REC and each operator can only manage one REC.\nYou have the flexibility to create databases in separate namespaces, or in the same namespace as the REC, or combine any of the supported deployment options above. This configuration is geared towards use cases that require multiple Redis Enterprise clusters with greater isolation or different cluster configurations.\nSee Manage databases in multiple namespaces for more information.\nUnsupported deployment patterns Cross-cluster operations Redis Enterprise for Kubernetes does not support operations that cross Kubernetes clusters. Redis Enterprise clusters (REC) work inside a single K8s cluster. Crossing clusters could result in functional and security issues.\nMultiple RECs in one namespace Redis Enterprise for Kubernetes does not support multiple Redis Enterprise clusters (REC) in the same namespace. Creating more than one REC in the same namespace will result in errors.\n","categories":["Platforms"]},{"uri":"/ri/using-redisinsight/api/","uriRel":"/ri/using-redisinsight/api/","title":"Adding Databases Programmatically","tags":[],"keywords":[],"description":"","content":"If you have a lot of Redis databases or you are using RedisInsight as part of some automated workflow, you might want to add databases programmatically.\nNow this is possible using our experimental REST API. Below is the documentation for the endpoints required to add databases.\nNote that this API should not be considered stable at this point and might change or break entirely in future releases. Do not rely on this API for production.\nAdd Redis database Used to add Redis databases to RedisInsight.\nURL : /api/instance/\nMethod : POST\nBody Type: JSON\nAuth required : NO\nParameters These are the required parameters for any type of database.\nParameter Type Description name string A nick name for the Redis database. Any string is valid connectionType string One of \u0026quot;STANDALONE\u0026quot;, \u0026quot;CLUSTER\u0026quot; or \u0026quot;SENTINEL\u0026quot;. For any Redis Enterprise database (even with database clustering enabled), use \u0026quot;STANDALONE\u0026quot; The remaining parameters depend on the connection type.\nStandalone database parameters Standalone databases are added using connectionType: \u0026quot;STANDALONE\u0026quot;.\nThe following additional parameters are required for standalone databases.\nParameter Type Description host string The hostname or IP address of your Redis database port number The port your Redis datanase is listening on. It should be an integer password string (optional) The password for your Redis database. tls object (optional) TLS parameters for the database Example\n{ \u0026#34;name\u0026#34;: \u0026#34;QA Redis DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;STANDALONE\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;redis.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379 } Redis cluster database parameters Redis Cluster databases are added using connectionType: \u0026quot;CLUSTER\u0026quot;.\nThe following additional parameters are required for Redis Cluster databases.\nParameter Type Description seedNodes array An array of objects describing the nodes of the cluster. At least one node should be specified. The objects must contain properties host (string) and port (integer) password string (optional) The password for your Redis datanase. tls object (optional) TLS parameters for the database Example\n{ \u0026#34;name\u0026#34;: \u0026#34;QA Redis Cluster DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;CLUSTER\u0026#34;, \u0026#34;seedNodes\u0026#34;: [ { \u0026#34;host\u0026#34;: \u0026#34;redis-cluster-node-1.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379 } ] } Sentinel-monitored database parameters Sentinel-monitored databases are added using connectionType: \u0026quot;SENTINEL\u0026quot;.\nThe following additional parameters are required for standalone databases.\nParameter Type Description sentinelHost string The hostname or IP address of one of the sentinel instances sentinelPort number The hostname or IP address of one of the sentinel instances sentinelPassword string (optional) The password for the sentinel instances sentinelMaster object Information about the monitored database that is to be added. sentinelMaster.serviceName string The name of the database to be added. This is the same name used in the sentinel monitor directive. sentinelMaster.authPass string (optional) The password, if any, for the monitored database. This can be different from the password of the sentinel instance itself. This is the same password as provided in the sentinel auth-pass directive. tls object (optional) TLS parameters for the database Example\n{ \u0026#34;name\u0026#34;: \u0026#34;QA Redis Sentinel DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;SENTINEL\u0026#34;, \u0026#34;sentinelHost\u0026#34;: \u0026#34;redis-sentinel.acme.com\u0026#34;, \u0026#34;sentinelPort\u0026#34;: 26379, \u0026#34;sentinelPassword\u0026#34;: \u0026#34;sentinel-pass\u0026#34;, \u0026#34;sentinelMaster\u0026#34;: { \u0026#34;serviceName\u0026#34;: \u0026#34;mymaster\u0026#34;, \u0026#34;authPass\u0026#34;: \u0026#34;opensesame\u0026#34;, } } TLS parameters TLS parameters can be used to specify how RedisInsight should connect to the Redis database over TLS.\nThe following parameters can be used:\nParameter Type Description useTls boolean Whether to use TLS to connect to the database or not clientAuth boolean Whether TLS client authentication is required by the database clientCertificateKeyPair object (optional) The details of the client certificate and private key used to connect to the Redis database. If client authentication is not required, this has to be provided verifyServerCert boolean (optional) Whether to verify server certificate caCert object (optional) The details of the CA certificate used to connect to the Redis database TLS client certificate and key The client certificate and key details can be provided in two forms:\nIf the certificate and key has already been used for a database before, or has been added separately, the ID of that certificate can be provided directly.\nParameter Type Description id number The ID of the client certificate/key pair Alternatively, to create a new client certificate/key pair for the database, a name must be provided along with the certificate and key strings.\nParameter Type Description new object The details of the new client certificate/key pair to be created new.name string The name of the new client cert/key pair new.cert string The client certificate string new.key string The client private key string Example\nUsing an existing client certificate/key pair for a new database.\n{ \u0026#34;name\u0026#34;: \u0026#34;Prod Redis Enterprise DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;STANDALONE\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;redis-ent.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379, \u0026#34;tls\u0026#34;: { \u0026#34;useTls\u0026#34;: true, \u0026#34;clientAuth\u0026#34;: true, \u0026#34;clientCertificateKeyPair\u0026#34;: { \u0026#34;id\u0026#34;: 6 } } } Creating a new client certificate/key pair while adding the database.\n{ \u0026#34;name\u0026#34;: \u0026#34;Prod Redis Enterprise DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;STANDALONE\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;redis-ent.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379, \u0026#34;tls\u0026#34;: { \u0026#34;useTls\u0026#34;: true, \u0026#34;clientAuth\u0026#34;: true, \u0026#34;clientCertificateKeyPair\u0026#34;: { \u0026#34;new\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Prod client certificate\u0026#34;, \u0026#34;cert\u0026#34;: \u0026#34;-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----...-----END PRIVATE KEY-----\u0026#34; } } } } TLS CA certificate The CA certificate details can be provided in two forms:\nIf the certificate has already been used for a database before, the name of that certificate can be provided directly.\nParameter Type Description name string The name of the CA certificate Alternatively, to create a new CA certificate for the database, a name must be provided along with the certificate string.\nParameter Type Description new object The details of the new CA certificate to be created new.name string The name of the new CA cert new.cert string The CA certificate string Example\nUsing an existing CA certificate for a new database.\n{ \u0026#34;name\u0026#34;: \u0026#34;Prod Redis Enterprise DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;STANDALONE\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;redis-ent.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379, \u0026#34;tls\u0026#34;: { \u0026#34;useTls\u0026#34;: true, \u0026#34;caCert\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-cert\u0026#34; } } } Creating a new CA certificate while adding the database.\n{ \u0026#34;name\u0026#34;: \u0026#34;Prod Redis Enterprise DB\u0026#34;, \u0026#34;connectionType\u0026#34;: \u0026#34;STANDALONE\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;redis-ent.acme.com\u0026#34;, \u0026#34;port\u0026#34;: 6379, \u0026#34;tls\u0026#34;: { \u0026#34;useTls\u0026#34;: true, \u0026#34;caCert\u0026#34;: { \u0026#34;new\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Prod CA certificate\u0026#34;, \u0026#34;cert\u0026#34;: \u0026#34;-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\u0026#34;, } } } } Success response Code : 201 Created\nAdd TLS certificate and key pair Used to add a new TLS certificate and private key pair to use to connect to a Redis database.\nURL : /api/tls-client-cert/\nMethod : POST\nBody Type: JSON\nAuth required : NO\nParameters Parameter Type Description name string The name of the new cert/key pair cert string The certificate string key string The private key string Success response The name and ID is returned in the response body. The ID can be used to reference this certificate/key pair when adding databases.\nCode : 201 Created\nBody :\n{ \u0026#34;certificate\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;VeriCert ACME Certificate\u0026#34; } } Get added TLS certificate and key pairs Used to retrieve a list of previously added TLS certificate and private key pair to use to connect to a Redis database.\nURL : /api/tls-client-cert/\nMethod : GET\nAuth required : NO\nSuccess response A list of objects, each containing the client certificate\u0026rsquo;s name and ID, is returned in the response body. The ID can be used to reference this certificate/key pair when adding databases.\nCode : 200 OK\nBody :\n{ \u0026#34;certificates\u0026#34;: [ { \u0026#34;id\u0026#34;: 6, \u0026#34;name\u0026#34;: \u0026#34;VeriCert ACME Certificate\u0026#34; } ] } ","categories":["RI"]},{"uri":"/rc/security/access-management/","uriRel":"/rc/security/access-management/","title":"Access management","tags":[],"keywords":[],"description":"Access management","content":"The Access management screen helps you manage:\nThe team of users allowed to access your subscription and its databases The API keys that authenticate application access to your account Single sign-on (SSO) with SAML Here, you learn how to manage your team\u0026rsquo;s users and to control their level of access.\nFor help managing API keys, see Manage API keys.\nManage team access The Team tab lets you manage the people allowed to access your account. Each authorized person is assigned to a role that specifies their privileges.\nThe list contains one entry summarizing the team settings for each user in your team. By default, the list includes the account owner.\nThe list includes several buttons and icons to help you manage the list:\nIcon Description The Add button lets you add members to your team The Edit button lets you edit the settings for the selected team member The Delete button lets you remove members from your team Filter icons let you display team members matching conditions you specify The Sort ascending and Sort descending icons display the list according to the selected order If you have a large team, you can use the controls in the list footer to navigate quickly through the list. These controls are deactivated for small teams.\nAdd user When you add a member to your team, the Add user dialog appears.\nUse this dialog to specify the following values:\nSetting Description First name First name of the user displayed in the admin console and in email messages Last name Last name of the user displayed in the admin console and in email messages Role The role identifies their subscription and account privileges. For details, see Team management roles. Email The address used for alerts and other email messages regarding the account Alert emails Enable to be notified when subscription databases cross certain thresholds, such as exceeding memory limits or latency requirements Operational emails Notifications about subscription and database changes, such as creating or deleting a database Billing emails Notifications when bills are issued, paid, and so on.Note: Limited to a single user. Multi-factor authentication Whether MFA is enabled for the member. This is deactivated when members have not enabled or confirmed MFA in their user profile settings. Use the Add user button to save your new team member details.\nEdit user To edit user team details, select the user from the list and then select the Edit button.\nWhen you do this, the Edit user dialog displays the details you can change.\nYou can change any detail except the team member\u0026rsquo;s email address.\nUse the Save user button to save your changes.\nDelete user To remove a member from your team, select them from the list and then select the Delete button. When you do this, a confirmation dialog appears.\nSelect the Delete user button to confirm removal. This action is permanent and cannot be undone.\nTeam management roles Each team member is assigned a role that identifies their privileges and limits their activities in the admin console.\nThe following roles are available:\nOwner - Can view, create, and edit any settings in the account\nEach subscription must have at least one account owner. Accounts can have multiple owners.\nOwners can also manage subscriptions, databases, and API keys.\nManager - Can view, create, and edit any setting in the subscription\nManagers can change subscription costs and change the payment methods associated with a subscription, but they cannot cannot add/remove available payment methods.\nMember - Can view, create, and edit databases in Fixed accounts\nMembers may not impact costs associated with Flexible accounts; this means they cannot create databases or edit databases in ways that impact subscription cost.\nViewer - Can view all databases and their configurations (including database secrets)\nThe following table shows each role\u0026rsquo;s ability to perform common tasks using the admin console:\nTask Owner Manager Member Viewer Access management ✅ Yes ❌ No ❌ No ❌ No Account settings ✅ Yes ❌ No ❌ No ❌ No Billing \u0026amp; payments ✅ Yes ❌ No ❌ No ❌ No Create subscription ✅ Yes ✅ Yes ❌ No ❌ No Create database (Flexible) ✅ Yes ✅ Yes ❌ No ❌ No Edit database (affects cost) ✅ Yes ✅ Yes ❌ No ❌ No Create database (Fixed) ✅ Yes ✅ Yes ✅ Yes ❌ No Edit database (no cost impact) ✅ Yes ✅ Yes ✅ Yes ❌ No View subscription ✅ Yes ✅ Yes ✅ Yes ✅ Yes View database ✅ Yes ✅ Yes ✅ Yes ✅ Yes ","categories":["RC"]},{"uri":"/rs/security/audit-events/","uriRel":"/rs/security/audit-events/","title":"Audit connection events","tags":[],"keywords":[],"description":"Describes how to audit connection events.","content":"Starting with version 6.2.18, Redis Enterprise Software lets you audit database connection and authentication events. This helps you track and troubleshoot connection activity.\nThe following events are tracked:\nDatabase connection attempts Authentication requests, including requests for new and existing connections Database disconnects When tracked events are triggered, notifications are sent via TCP to an address and port defined when auditing is enabled. Notifications appear in near real time and are intended to be consumed by an external listener, such as a TCP listener, third-party service, or related utility.\nFor development and testing environments, notifications can be saved to a local file; however, this is neither supported nor intended for production environments.\nFor performance reasons, auditing is not enabled by default. In addition, auditing occurs in the background (asynchronously) and is non-blocking by design. That is, the action that triggered the notification continues without regard to the status of the notification or the listening tool.\nEnable audit notifications Cluster audits To enable auditing for your cluster, use:\nrladmin\nrladmin cluster config auditing db_conns \\ audit_protocol \u0026lt;TCP|local\u0026gt; \\ audit_address \u0026lt;address\u0026gt; \\ audit_port \u0026lt;port\u0026gt; \\ audit_reconnect_interval \u0026lt;interval in seconds\u0026gt; \\ audit_reconnect_max_attempts \u0026lt;number of attempts\u0026gt; where:\naudit_protocol indicates the protocol used to process notifications. For production systems, TCP is the only value.\naudit_address defines the TCP/IP address where one can listen for notifications\naudit_port defines the port where one can listen for notifications\naudit_reconnect_interval defines the interval (in seconds) between attempts to reconnect to the listener. Default is 1 second.\naudit_reconnect_max_attempts defines the maximum number of attempts to reconnect. Default is 0. (infinite)\nDevelopment systems can set audit_protocol to local for testing and training purposes; however, this setting is not supported for production use.\nWhen audit_protocol is set to local, \u0026lt;address\u0026gt; should be set to a stream socket defined on the machine running Redis Enterprise and \u0026lt;port\u0026gt; should not be specified:\nrladmin cluster config auditing db_conns \\ audit_protocol local audit_address \u0026lt;output-file\u0026gt; The output file (and path) must be accessible by the user and group running Redis Enterprise Software.\nthe REST API\nPUT /v1/cluster/auditing/db_conns { \u0026#34;audit_address\u0026#34;: \u0026#34;\u0026lt;address\u0026gt;\u0026#34;, \u0026#34;audit_port\u0026#34;: \u0026lt;port\u0026gt;, \u0026#34;audit_protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;audit_reconnect_interval\u0026#34;: \u0026lt;interval\u0026gt;, \u0026#34;audit_reconnect_max_attempts\u0026#34;: \u0026lt;max attempts\u0026gt; } where \u0026lt;address\u0026gt; is a string containing the TCP/IP address, \u0026lt;port\u0026gt; is a numeric value representing the port, \u0026lt;interval\u0026gt; is a numeric value representing the interval in seconds, and \u0026lt;max attempts\u0026gt; is a numeric value representing the maximum number of attempts to execute.\nDatabase audits Once auditing is enabled for your cluster, you can audit individual databases. To do so, use:\nrladmin\nrladmin tune db db:\u0026lt;id|name\u0026gt; db_conns_auditing enabled where the value of the db: parameter is either the cluster ID of the database or the database name.\nTo deactivate auditing, set db_conns_auditing to disabled.\nUse rladmin info to retrieve additional details:\nrladmin info db \u0026lt;id|name\u0026gt; rladmin info cluster the REST API\nPUT /v1/bdbs/1 { \u0026#34;db_conns_auditing\u0026#34;: true } To deactivate auditing, set db_conns_auditing to false.\nYou must enable auditing for your cluster before auditing a database; otherwise, an error appears:\nError setting description: Unable to enable DB Connections Auditing before feature configurations are set. Error setting error_code: db_conns_auditing_config_missing\nTo resolve this error, enable the protocol for your cluster before attempting to audit a database.\nPolicy defaults for new databases To audit connections for new databases by default, use:\nrladmin\nrladmin tune cluster db_conns_auditing enabled To deactivate this policy, set db_conns_auditing to disabled.\nthe REST API\nPUT /v1/cluster/policy { \u0026#34;db_conns_auditing\u0026#34;: true } To deactivate this policy, set db_conns_auditing to false.\nNotification examples Audit event notifications are reported as JSON objects.\nNew connection This example reports a new connection for a database:\n{ \u0026#34;ts\u0026#34;:1655821384, \u0026#34;new_conn\u0026#34;: { \u0026#34;id\u0026#34;:2285001002 , \u0026#34;srcip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;srcp\u0026#34;:\u0026#34;39338\u0026#34;, \u0026#34;trgip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;trgp\u0026#34;:\u0026#34;12635\u0026#34;, \u0026#34;hname\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;bdb_name\u0026#34;:\u0026#34;DB1\u0026#34;, \u0026#34;bdb_uid\u0026#34;:\u0026#34;5\u0026#34; } } Authentication request Here is a sample authentication request for a database:\n{ \u0026#34;ts\u0026#34;:1655821384, \u0026#34;action\u0026#34;:\u0026#34;auth\u0026#34;, \u0026#34;id\u0026#34;:2285001002 , \u0026#34;srcip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;srcp\u0026#34;:\u0026#34;39338\u0026#34;, \u0026#34;trgip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;trgp\u0026#34;:\u0026#34;12635\u0026#34;, \u0026#34;hname\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;bdb_name\u0026#34;:\u0026#34;DB1\u0026#34;, \u0026#34;bdb_uid\u0026#34;:\u0026#34;5\u0026#34;, \u0026#34;status\u0026#34;:2, \u0026#34;username\u0026#34;:\u0026#34;user_one\u0026#34;, \u0026#34;identity\u0026#34;:\u0026#34;user:1\u0026#34;, \u0026#34;acl-rules\u0026#34;:\u0026#34;~* +@all\u0026#34; } Status reports success or failure. Values of 2 or 8 indicate success; other values indicate failure.\nDatabase disconnect Here\u0026rsquo;s what\u0026rsquo;s reported when a database connection is closed:\n{ \u0026#34;ts\u0026#34;:1655821384, \u0026#34;close_conn\u0026#34;: { \u0026#34;id\u0026#34;:2285001002, \u0026#34;srcip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;srcp\u0026#34;:\u0026#34;39338\u0026#34;, \u0026#34;trgip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;, \u0026#34;trgp\u0026#34;:\u0026#34;12635\u0026#34;, \u0026#34;hname\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;bdb_name\u0026#34;:\u0026#34;DB1\u0026#34;, \u0026#34;bdb_uid\u0026#34;:\u0026#34;5\u0026#34; } } Notification field reference The field value that appears immediately after the timestamp describes the action that triggered the notification. The following values may appear:\nnew_conn indicates a new external connection new_int_conn indicates a new internal connection close_conn occurs when a connection is closed \u0026quot;action\u0026quot;:\u0026quot;auth\u0026quot; indicates an authentication request and can refer to new authentication requests or authorization checks on existing connections In addition, the following fields may also appear in audit event notifications:\nField name Description acl-rules ACL rules associated with the connection, which includes a rule for the default user. bdb_name Destination database name - The name of the database being accessed. bdb_uid Destination database ID - The cluster ID of the database being accessed. hname Client hostname - The hostname of the client. Currently empty; reserved for future use. id Connection ID - Unique connection ID assigned by the proxy. identity Identity - A unique ID the proxy assigned to the user for the current connection. srcip Source IP address - Source TCP/IP address of the client accessing the Redis database. srcp Source port - Port associated with the source IP address accessing the Redis database. Combine the port with the address to uniquely identify the socket. status Status result code - An integer representing the result of an authentication request. trgip Target IP address - The IP address of the destination being accessed by the action. trgp Target port - The port of the destination being accessed by the action. Combine the port with the destination IP address to uniquely identify the database being accessed. ts Timestamp - The date and time of the event, in Coordinated Universal Time (UTC). Granularity is within one second. username Authentication username - Username associated with the connection; can include default for databases that allow default access. (Passwords are not recorded). Status result codes The status field reports the results of an authentication request as an integer. Here\u0026rsquo;s what different values mean:\nError value Error code Description 0 AUTHENTICATION_FAILED Invalid username and/or password. 1 AUTHENTICATION_FAILED_TOO_LONG Username or password are too long. 2 AUTHENTICATION_NOT_REQUIRED Client tried to authenticate, but authentication isn\u0026rsquo;t necessary. 3 AUTHENTICATION_DIRECTORY_PENDING Attempting to receive authentication info from the directory in async mode. 4 AUTHENTICATION_DIRECTORY_ERROR Authentication attempt failed because there was a directory connection error. 5 AUTHENTICATION_SYNCER_IN_PROGRESS Syncer SASL handshake. Return SASL response and wait for the next request. 6 AUTHENTICATION_SYNCER_FAILED Syncer SASL handshake. Returned SASL response and closed the connection. 7 AUTHENTICATION_SYNCER_OK Syncer authenticated. Returned SASL response. 8 AUTHENTICATION_OK Client successfully authenticated. ","categories":["RS"]},{"uri":"/kubernetes/re-clusters/create-aa-database/","uriRel":"/kubernetes/re-clusters/create-aa-database/","title":"Create Active-Active databases on Kubernetes","tags":[],"keywords":[],"description":"This section how to set up an Active-Active Redis Enterprise database on Kubernetes using the Redis Enterprise Software operator.","content":"On Kubernetes, Redis Enterprise Active-Active databases provide read and write access to the same dataset from different Kubernetes clusters. For more general information about Active-Active, see the Redis Enterprise Software docs.\nCreating an Active-Active database requires routing network access between two Redis Enterprise clusters residing in two different Kubernetes clusters. Without the proper access configured for each cluster, syncing between the databases instances will fail.\nThis process consists of:\nDocumenting values to be used in later steps. It\u0026rsquo;s important these values are correct and consistent. Editing the Redis Enterprise cluster (REC) spec file to include the ActiveActive section. This will be slightly different depending on the K8s distribution you are using. Creating the database with the crdb-cli command. These values must match up with values in the REC resource spec. Prerequisites Before creating Active-Active databases, you\u0026rsquo;ll need two or more working Kubernetes clusters that each have:\nRouting for external access with an ingress controller (OpenShift users can use routes) A working Redis Enterprise cluster (REC) with a unique name Enough memory resources available for the database (see hardware requirements) Document required parameters The most common mistake when setting up Active-Active databases is incorrect or inconsistent parameter values. The values listed in the resource file must match those used in the crdb-cli command.\nDatabase name \u0026lt;db-name\u0026gt;: Description: Combined with ingress suffix to create the Active-Active database hostname Format: string Example value: myaadb How you get it: you choose The database name requirements are: Maximum of 63 characters Only letter, number, or hyphen (-) characters Starts with a letter; ends with a letter or digit. Database name is not case-sensitive You\u0026rsquo;ll need the following information for each participating Redis Enterprise cluster (REC):\nNote: You\u0026rsquo;ll need to create DNS aliases resolve your API hostname \u0026lt;api-hostname\u0026gt;,\u0026lt;ingress-suffix\u0026gt;, \u0026lt;replication-hostname\u0026gt; to the IP address for the ingress controller’s LoadBalancer (or routes in Openshift) for each database. To avoid entering multiple DNS records, you can use a wildcard in your alias (such as *.ijk.redisdemo.com). REC hostname \u0026lt;rec-hostname\u0026gt;: Description: Hostname used to identify your Redis Enterprise cluster in the crdb-cli command. This MUST be different from other participating clusters. Format: \u0026lt;rec-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local Example value: rec01.ns01.svc.cluster.local How to get it: List all your Redis Enterprise clusters kubectl get rec API hostname \u0026lt;api-hostname\u0026gt;: Description: Hostname used to access the Redis Enterprise cluster API from outside the K8s cluster Format: string Example value: api.ijk.redisdemo.com Ingress suffix \u0026lt;ingress-suffix\u0026gt;: Description: Combined with database name to create the Active-Active database hostname Format: string Example value: -cluster.ijk.redisdemo.com REC admin credentials \u0026lt;username\u0026gt; \u0026lt;password\u0026gt;: Description: Admin username and password for the REC stored in a secret Format: string Example value: username: user@redisdemo.com, password: something How to get them: kubectl get secret \u0026lt;rec-name\u0026gt; \\ -o jsonpath=\u0026#39;{.data.username}\u0026#39; | base64 --decode kubectl get secret \u0026lt;rec-name\u0026gt; \\ -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 --decode Replication hostname \u0026lt;replication-hostname\u0026gt;: Description: Hostname used inside the ingress for the database Format: \u0026lt;db-name\u0026gt;\u0026lt;ingress-suffix\u0026gt; Example value: myaadb-cluster.ijk.redisdemo.com How to get it: Combine \u0026lt;db-name\u0026gt; and \u0026lt;ingress-suffix\u0026gt; values you documented above. Replication endpoint \u0026lt;replication-endpoint\u0026gt;: Description: Endpoint used externally to contact the database Format: \u0026lt;db-name\u0026gt;\u0026lt;ingress-suffix\u0026gt;:443 Example value: myaadb-cluster.ijk.redisdemo.com:443 How to get it:\u0026lt;replication-hostname\u0026gt;:443 Add activeActive section to the REC resource file From inside your K8s cluster, edit your Redis Enterprise cluster (REC) resource to add the following to the spec section. Do this for each participating cluster.\nThe operator uses the API hostname (\u0026lt;api-hostname\u0026gt;) to create an ingress to the Redis Enterprise cluster\u0026rsquo;s API; this only happens once per cluster. Every time a new Active-Active database instance is created on this cluster, the operator creates a new ingress route to the database with the ingress suffix (\u0026lt;ingress-suffix\u0026gt;). The hostname for each new database will be in the format \u0026lt;db-name\u0026gt;\u0026lt;ingress-suffix\u0026gt;.\nUsing ingress controller If your cluster uses an ingress controller, add the following to the spec section of your REC resource file. Nginx:\nactiveActive: apiIngressUrl: \u0026lt;api-hostname\u0026gt; dbIngressSuffix: \u0026lt;ingress-suffix\u0026gt; ingressAnnotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: HTTPS nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; method: ingress HAproxy:\nactiveActive: apiIngressUrl: \u0026lt;api-hostname\u0026gt; dbIngressSuffix: \u0026lt;ingress-suffix\u0026gt; ingressAnnotations: kubernetes.io/ingress.class: haproxy ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; method: ingress After the changes are saved and applied, you can verify a new ingress was created for the API.\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE rec01 api.abc.cde.redisdemo.com 225161f845b278-111450635.us.cloud.com 80 24h Verify you can access the API from outside the K8s cluster.\ncurl -k -L -i -u \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt; https://\u0026lt;api-hostname\u0026gt;/v1/cluster If the API call fails, create a DNS alias that resolves your API hostname (\u0026lt;api-hostname\u0026gt;) to the IP address for the ingress controller\u0026rsquo;s LoadBalancer.\nMake sure you have DNS aliases for each database that resolve your API hostname \u0026lt;api-hostname\u0026gt;,\u0026lt;ingress-suffix\u0026gt;, \u0026lt;replication-hostname\u0026gt; to the IP address of the ingress controller’s LoadBalancer. To avoid entering multiple DNS records, you can use a wildcard in your alias (such as *.ijk.redisdemo.com).\nIf using Istio Gateway and VirtualService No changes are required to the REC spec if you are using Istio in place of an ingress controller. The activeActive section added above creates ingress resources. The two custom resources used to configure Istio (Gateway and VirtualService) replace the need for ingress resources.\nWarning - These custom resources are not controlled by the operator and will need to be configured and maintained manually. For each cluster, verify the VirtualService resource has two - match: blocks in the tls section. The hostname under sniHosts: should match your \u0026lt;replication-hostname\u0026gt;.\nUsing OpenShift routes Make sure your Redis Enterprise cluster (REC) has a different name (\u0026lt;rec-name.namespace\u0026gt;) than any other participating clusters. If not, you\u0026rsquo;ll need to manually rename the REC or move it to a different namespace. You can check your new REC name with:\noc get rec -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; If the rec name was modified, reapply scc.yaml to the namespace to reestablish security privileges.\noc apply -f scc.yaml oc adm policy add-scc-to-group redis-enterprise-scc system:serviceaccounts:\u0026lt;namespace\u0026gt; Make sure you have DNS aliases for each database that resolve your API hostname \u0026lt;api-hostname\u0026gt;,\u0026lt;ingress-suffix\u0026gt;, \u0026lt;replication-hostname\u0026gt; to the route IP address. To avoid entering multiple DNS records, you can use a wildcard in your alias (such as *.ijk.redisdemo.com).\nIf your cluster uses OpenShift routes, add the following to the spec section of your Redis Enterprise cluster (REC) resource file.\nactiveActive: apiIngressUrl: \u0026lt;api-hostname\u0026gt; dbIngressSuffix: \u0026lt;ingress-suffix\u0026gt; method: openShiftRoute Make sure you have a DNS aliases that resolve to the routes IP for both the API hostname (\u0026lt;api-hostname\u0026gt;) and the replication hostname (\u0026lt;replication-hostname\u0026gt;) for each database. To avoid entering each database individually, you can use a wildcard in your alias (such as *.ijk.redisdemo.com).\nAfter the changes are saved and applied, you can see that a new route was created for the API.\n$ oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD rec01 api-openshift.apps.abc.redisdemo.com rec01 api passthrough None Create an Active-Active database with crdb-cli The crdb-cli command can be run from any Redis Enterprise pod hosted on any participating K8s cluster. You\u0026rsquo;ll need the values for the required parameters for each Redis Enterprise cluster.\ncrdb-cli crdb create \\ --name \u0026lt;db-name\u0026gt; \\ --memory-size \u0026lt;mem-size\u0026gt; \\ --encryption yes \\ --instance fqdn=\u0026lt;rec-hostname-01\u0026gt;,url=https://\u0026lt;api-hostname-01\u0026gt;,username=\u0026lt;username-01\u0026gt;,password=\u0026lt;password-01\u0026gt;,replication_endpoint=\u0026lt;replication-endpoint-01\u0026gt;,replication_tls_sni=\u0026lt;replication-hostname-01\u0026gt; \\ --instance fqdn=\u0026lt;rec-hostname-02\u0026gt;,url=https://\u0026lt;api-hostname-02\u0026gt;,username=\u0026lt;username-02\u0026gt;,password=\u0026lt;password-02\u0026gt;,replication_endpoint=\u0026lt;replication-endpoint-02\u0026gt;,replication_tls_sni=\u0026lt;replication-hostname-02\u0026gt; To create a database that syncs between more than two instances, add additional --instance arguments.\nSee the crdb-cli reference for more options.\nTest your database The easiest way to test your Active-Active database is to set a key-value pair in one database and retrieve it from the other.\nYou can connect to your databases with the instructions in Manage databases. Set a test key with SET foo bar in the first database. If your Active-Active deployment is working properly, when connected to your second database, GET foo should output bar.\n","categories":["Platforms"]},{"uri":"/rs/security/internode-encryption/","uriRel":"/rs/security/internode-encryption/","title":"Internode encryption","tags":[],"keywords":[],"description":"Describes internode which improves the security of data in transit.","content":"As of v6.2.4, Redis Enterprise Software supports internode encryption, which encrypts internal communication between nodes. This improves the security of data as it travels within a cluster.\nInternode encryption is enabled for the control plane, which manages the cluster and its databases.\nInternode encryption is supported for the data plane, which encrypts communication used to replicate shards between nodes and proxy communication with shards located on different nodes.\nThe following diagram shows how this works.\nData plane encryption is disabled by default for individual databases in order to optimize for performance. Encryption adds latency and overhead; the impact is measurable and varies according to the database, its field types, and the details of the underlying use case.\nYou can enable data plane encryption for a database by changing the database configuration settings. This lets you choose when to favor performance and when to encrypt data.\nPrerequisites Internode encryption requires certain prerequisites.\nYou need to:\nUpgrade all nodes in the cluster to v6.2.4 or later.\nOpen port 3342 for the TLS channel used for encrypted communication.\nEnable data plane encryption To enable internode encryption for a database (also called data plane encryption), you need to enable the appropriate setting for each database you wish to encrypt. To do so, you can:\nUse the admin console to enable the Internode encryption setting from the database configuration screen:\nUse the rladmin command-line utility to set the data_internode_encryption setting for the database:\nrladmin tune db \u0026lt;database_id\u0026gt; data_internode_encryption enable Use the Redis Enterprise Software REST API to set the data_internode_encryption setting for the database.\nput /v1/bdbs/${database_id} { “data_internode_encryption” : true } When you change the data internode encryption setting for a database, all active remote client connections are disconnected. This restarts the internal (DMC) proxy and disconnects all client connections.\nTo enable data plane encryption by default for new databases, use rladmin to tune the cluster:\nrladmin tune cluster data_internode_encryption enable Encryption ciphers and settings To encrypt internode communications, Redis Enterprise Software uses TLS 1.2 and the following Cipher suites:\nECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES128-GCM-SHA256 No configurable settings are exposed; internode encryption is used internally within a cluster and not exposed to any outside service.\nCertificate authority and rotation Starting with v6.2.4, internode communication is managed, in part, by two certificates: one for the control plane and one for the data plane. These certificates are signed by a private certificate authority (CA). The CA is not exposed outside of the cluster, so it cannot be accessed by external processes or services. In addition, each cluster generates a unique CA that is not used anywhere else.\nThe private CA is generated when a cluster is created or upgraded to 6.2.4.\nWhen nodes join the cluster, the cluster CA is used to generate certificates for the new node, one for each plane. Certificates signed by the private CA are not shared between clusters and they\u0026rsquo;re not exposed outside the cluster.\nAll certificates signed by the internal CA expire after ninety (90) days and automatically rotate every thirty (30) days. Alerts also monitor certificate expiration and trigger when certificate expiration falls below 45 days. If you receive such an alert, contact support.\nYou can use the Redis Enterprise Software REST API to rotate certificates manually:\nput /v1/cluster/certificates/rotate ","categories":["RS"]},{"uri":"/kubernetes/re-databases/routes/","uriRel":"/kubernetes/re-databases/routes/","title":"Use OpenShift routes for external database access","tags":[],"keywords":[],"description":"","content":"Every time the Redis Enterprise operator creates a Redis Enterprise database (REDB), it creates a service that allows requests to be routed to that database. Redis Enterprise supports three types of services for accessing databases: ClusterIP, headless, or LoadBalancer.\nBy default, REDB creates a ClusterIP type service, which exposes a cluster-internal IP and can only be accessed from within the cluster. OpenShift routes allow requests to be routed to the REDB from outside the cluster. For OpenShift deployments, routes provide a preferred alternative to an ingress.\nPrerequisites Before you can connect to your database from outside the cluster, you\u0026rsquo;ll need the root CA certificate of the DMC Proxy server to validate the server certificate.\nBy default, the DMC Proxy uses a self-signed certificate. You can retrieve it from the Redis Enterprise admin console and save it as a file (for example, named \u0026ldquo;ca.pem\u0026rdquo;) on the client machine.\nYour database also needs TLS encryption enabled.\nCreate OpenShift route Select the Networking/Routes section of the OpenShift web console.\nSelect Create route and fill out the following fields:\nName: Choose any name you want as the first part of your generated hostname Hostname: Leave blank Path: Leave as is (\u0026quot;/\u0026quot;) Service: Select the service for the database you want to access TLS Termination: Choose \u0026ldquo;passthrough\u0026rdquo; Insecure Traffic: Select \u0026ldquo;None\u0026rdquo; Select Create.\nFind the hostname for your new route. After route creation, it appears in the \u0026ldquo;Host\u0026rdquo; field.\nVerify you have a DNS entry to resolve the hostname for your new route to the cluster\u0026rsquo;s load balancer.\nAccess database Access the database from outside the cluster using redis-cli or openssl.\nTo connect with redis-cli:\nredis-cli -h \u0026lt;hostname\u0026gt; -p 443 --tls --cacert ./ca.pem --sni \u0026lt;hostname\u0026gt; Replace the \u0026lt;hostname\u0026gt; value with the hostname for your new route.\nTo connect with openssl:\nopenssl s_client -connect \u0026lt;hostname\u0026gt;:443 -crlf -CAfile ./ca.pem -servername \u0026lt;hostname\u0026gt; ","categories":["Platforms"]},{"uri":"/ri/using-redisinsight/api-get/","uriRel":"/ri/using-redisinsight/api-get/","title":"Adding Databases via GET URL","tags":[],"keywords":[],"description":"","content":"If you want to automate adding a Redis database without filling the database form, you might want to add database via GET URL.\nBelow is the documentation for the URL required to add databases.\nAdd Redis database Used to add Redis databases to RedisInsight.\nURL : /add/\nMethod : GET\nQuery Parameters These are the required query parameters for any type of database.\nParameter Type Required/Optional Description host string required Hostname of your Redis database. port number required Port of your Redis database. name string optional A nick name for the Redis database. Any string is valid username string optional Username of your Redis database. passsword string optional Password of your Redis database. tls boolean optional \u0026quot;true\u0026quot; if your Redis database has TLS enabled. verifyServer boolean optional \u0026quot;true\u0026quot; if the server certificate of you Redis database has to be verified. masterName string optional master name of the monitoring database masterUsername string optional master username of the monitoring database masterPassword string optional master password of the monitoring database redirect boolean optional \u0026quot;true\u0026quot; if you want to redirect to instance page after successful addition. Note: If you want to add the TLS certificates for your Redis database, you have to manually fill database form. Examples Standalone database /add/?name=standalone\u0026amp;host=localhost\u0026amp;port=6379 Cluster database /add/?name=cluster\u0026amp;host=172.19.0.3\u0026amp;port=7000 Sentinel database /add/?name=sentinel\u0026amp;host=localhost\u0026amp;port=26379 Standalone database with ACL /add/?name=redis-acl\u0026amp;host=172.19.0.2\u0026amp;port=6379\u0026amp;username=myuser\u0026amp;password=p1pp0 Standalone database with TLS /add/?name=redis-tls\u0026amp;host=172.19.0.2\u0026amp;port=6379\u0026amp;tls=true ","categories":["RI"]},{"uri":"/kubernetes/re-clusters/redis-on-flash/","uriRel":"/kubernetes/re-clusters/redis-on-flash/","title":"Use Redis on Flash on Kubernetes","tags":[],"keywords":[],"description":"Deploy a cluster with Redis on Flash on Kubernetes.","content":"Prerequisites Redis Enterprise Software for Kubernetes supports using Redis on Flash, which extends your node memory to use both RAM and flash storage. SSDs (solid state drives) can store infrequently used (warm) values while your keys and frequently used (hot) values are still stored in RAM. This improves performance and lowers costs for large datasets.\nNote: NVMe (non-volatile memory express) SSDs are strongly recommended to achieve the best performance. Before creating your Redis clusters or databases, these SSDs must be:\nlocally attached to worker nodes in your Kubernetes cluster formatted and mounted on the nodes that will run Redis Enterprise pods dedicated to RoF and not shared with other parts of the database, (e.g. durability, binaries) provisioned as local persistent volumes You can use a local volume provisioner to do this dynamically a StorageClass resource with a unique name For more information on node storage, see Node persistent and ephemeral storage.\nCreate a Redis Enterprise cluster To deploy a Redis Enterprise cluster (REC) with flash storage, you\u0026rsquo;ll need to specify the following in the redisOnFlashSpec section of your REC custom resource:\nenable Redis on Flash (enabled: true) flash storage driver (flashStorageEngine) The only supported value is rocksdb storage class name (storageClassName) minimal flash disk size (flashDiskSize) Here is an example of an REC custom resource with these attributes:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: \u0026#34;rec\u0026#34; spec: nodes: 3 redisOnFlashSpec: enabled: true flashStorageEngine: rocksdb storageClassName: local-scsi flashDiskSize: 100G Create a Redis Enterprise database By default, any new database will use RAM only. To create a Redis Enterprise database (REDB) that can use flash storage, specify the following in the redisEnterpriseCluster section of the REDB custom resource definition:\nisRof: true enables Redis on Flash rofRamSize defines the RAM capacity for the database Below is an example REDB custom resource:\napiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: rof-redb spec: redisEnterpriseCluster: name: rec isRof: true memorySize: 2GB rofRamSize: 0.5GB Note: This example defines both memorySize and rofRamSize. When using Redis on Flash, memorySize refers to the total combined memory size (RAM + flash) allocated for the database. rofRamSize specifies only the RAM capacity for the database. rofRamSize must be at least 10% of memorySize. ","categories":["Platforms"]},{"uri":"/ri/using-redisinsight/auth-database/","uriRel":"/ri/using-redisinsight/auth-database/","title":"Authenticate database users","tags":[],"keywords":[],"description":"","content":"You can enforce authentication of users who share your databases by running Redisinsight with variables RIAUTHPROMPT, RIAUTHTIMER, and RILOGLEVEL. For more information on variables, see Configure RedisInsight.\nBy setting the variables, enforce the prompt for username and password each time the database is opened and at a specific time interval while users work with the database. You can maintain multiple tabs with the same database without having to enter username and password in each one.\nFor Windows and Linux: docker run -p 8001:8001-e RIAUTHPROMPT=1 -e RIAUTHTIMER=5 redislabs/redisinsight # 5 minutes idle timer For Mac: docker run -p 8001:8001-e RIAUTHPROMPT=1 -e RIAUTHTIMER=5 redislabs/redisinsight # 5 minutes idle timer Where:\nRIAUTHPROMPT enables authentication prompt when opening instances and when the user is idle RIAUTHTIMER sets user idle timer value in minutes RILOGLEVEL logs to console/file Note: Do not store username and password in the browser. ","categories":["RI"]},{"uri":"/kubernetes/re-clusters/multi-namespace/","uriRel":"/kubernetes/re-clusters/multi-namespace/","title":"Manage databases in multiple namespaces","tags":[],"keywords":[],"description":"Redis Enterprise for Kubernetes allows you to deploy to multiple namespaces within your Kubernetes cluster. This article shows you how to configure your Redis Enterprise cluster to connect to databases in multiple namespaces","content":"Multiple Redis Enterprise database resources (REDBs) can be associated with a single Redis Enterprise cluster resource (REC) even if they reside in different namespaces.\nTo learn more about designing a multi-namespace Redis Enterprise cluster, see flexible deployment options.\nPrerequisites Before configuring a multi-namespace deployment, you must have a running Redis Enterprise cluster (REC). See more information in the deployment section.\nCreate role and role binding for managed namespaces Both the operator and the RedisEnterpriseCluster (REC) resource need access to each namespace the REC will manage. For each managed namespace, create a role.yaml and role_binding.yaml file within the managed namespace, as shown in the examples below.\nReplace \u0026lt;rec-namespace\u0026gt; with the namespace the REC resides in. Replace \u0026lt;service-account-name\u0026gt; with your own value (defaults to the REC name).\nrole.yaml example:\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: redb-role labels: app: redis-enterprise rules: - apiGroups: - app.redislabs.com resources: [\u0026#34;redisenterpriseclusters\u0026#34;, \u0026#34;redisenterpriseclusters/status\u0026#34;, \u0026#34;redisenterpriseclusters/finalizers\u0026#34;, \u0026#34;redisenterprisedatabases\u0026#34;, \u0026#34;redisenterprisedatabases/status\u0026#34;, \u0026#34;redisenterprisedatabases/finalizers\u0026#34;] verbs: [\u0026#34;delete\u0026#34;, \u0026#34;deletecollection\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;update\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;listallnamespaces\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;watchlist\u0026#34;, \u0026#34;watchlistallnamespaces\u0026#34;, \u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;replace\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;deletecollection\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] role_binding.yaml example:\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: redb-role labels: app: redis-enterprise subjects: - kind: ServiceAccount name: redis-enterprise-operator namespace: \u0026lt;rec-namespace\u0026gt; - kind: ServiceAccount name: redis-enterprise-admission namespace: \u0026lt;rec-namespace\u0026gt; - kind: ServiceAccount name: \u0026lt;service-account-name\u0026gt; namespace: \u0026lt;rec-namespace\u0026gt; roleRef: kind: Role name: redb-role apiGroup: rbac.authorization.k8s.io Apply the files:\nkubectl apply -f role.yaml kubectl apply -f role_binding.yaml Update Redis Enterprise operator ConfigMap Patch the operator-environment-config in the REC namespace with a new environment variable (REDB_NAMESPACES).\nkubectl patch ConfigMap/operator-environment-config \\ -n \u0026lt;rec-namespace\u0026gt; \\ --type merge \\ -p `{\u0026#34;data\u0026#34;:{\u0026#34;REDB_NAMESPACES\u0026#34;: \u0026#34;\u0026lt;comma,separated,list,of,namespaces,to,watch\u0026#34;}}` Warning - Only configure the operator to watch a namespace after the namespace is created and configured with the role/role_binding as explained above. If configured to watch a namespace without setting those permissions or a namespace that is not created yet, the operator will fail and not perform normal operations. ","categories":["Platforms"]},{"uri":"/kubernetes/re-clusters/upgrade-redis-cluster/","uriRel":"/kubernetes/re-clusters/upgrade-redis-cluster/","title":"Upgrade a Redis Enterprise cluster (REC) on Kubernetes","tags":[],"keywords":[],"description":"This task describes how to upgrade a Redis Enterprise cluster via the operator.","content":"Redis implements rolling updates for software upgrades in Kubernetes deployments. The upgrade process consists of two steps:\nUpgrade the Redis Enterprise operator Upgrade the Redis Enterprise cluster (REC) Warning - When upgrading existing Redis Enterprise clusters running on RHEL7-based images, make sure to select a RHEL7-based image for the new version. See release notes for more info. Upgrade the operator Download the bundle Make sure you pull the correct version of the bundle. You can find the version tags by checking the operator releases on GitHub or by using the GitHub API.\nYou can download the bundle for the latest release with the following curl command:\nVERSION=`curl --silent https://api.github.com/repos/RedisLabs/redis-enterprise-k8s-docs/releases/latest | grep tag_name | awk -F\u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $4}\u0026#39;` curl --silent -O https://raw.githubusercontent.com/RedisLabs/redis-enterprise-k8s-docs/$VERSION/bundle.yaml For OpenShift environments, the name of the bundle is openshift.bundle.yaml, and so the curl command to run is:\ncurl --silent -O https://raw.githubusercontent.com/RedisLabs/redis-enterprise-k8s-docs/$VERSION/openshift.bundle.yaml If you need a different release, replace VERSION in the above with a specific release tag.\nApply the bundle Apply the bundle to deploy the new operator binary. This will also apply any changes in the new release to custom resource definitions, roles, role binding, or operator service accounts.\nNote: If you are not pulling images from Docker Hub, update the operator image spec to point to your private repository. If you have made changes to the role, role binding, RBAC, or custom resource definition (CRD) in the previous version, merge them with the updated declarations in the new version files. Upgrade the bundle and operator with a single command, passing in the bundle YAML file:\nkubectl apply -f bundle.yaml If you are using OpenShift, run this instead:\nkubectl apply -f openshift.bundle.yaml After running this command, you should see a result similar to this:\nrole.rbac.authorization.k8s.io/redis-enterprise-operator configured serviceaccount/redis-enterprise-operator configured rolebinding.rbac.authorization.k8s.io/redis-enterprise-operator configured customresourcedefinition.apiextensions.k8s.io/redisenterpriseclusters.app.redislabs.com configured customresourcedefinition.apiextensions.k8s.io/redisenterprisedatabases.app.redislabs.com configured deployment.apps/redis-enterprise-operator configured Reapply other manual configurations When upgrading the operator, there are few configurations you\u0026rsquo;ll need to reapply.\nIf you have the admission controller enabled, you need to manually reapply the ValidatingWebhookConfiguration. See the Enable the admission controller step during deployment for more details.\nIf you are using OpenShift, you will also need to manually reapply the Security context constraints file (scc.yaml).\noc apply -f openshift/scc.yaml Verify the operator is running You can check your deployment to verify the operator is running in your namespace.\nkubectl get deployment/redis-enterprise-operator You should see a result similar to this:\nNAME READY UP-TO-DATE AVAILABLE AGE redis-enterprise-operator 1/1 1 1 0m36s Warning - We recommend upgrading the REC as soon as possible after updating the operator. After the operator upgrade completes, the operator suspends the management of the REC and its associated REDBs, until the REC upgrade completes. Upgrade the Redis Enterprise cluster (REC) The Redis Enterprise cluster (REC) can be updated automatically or manually. To trigger automatic upgrade of the REC after the operator upgrade completes, specify autoUpgradeRedisEnterprise: true in your REC spec. If you don\u0026rsquo;t have automatic upgrade enabled, follow the below steps for the manual upgrade.\nBefore beginning the upgrade of the Redis Enterprise cluster, check the K8s operator release notes to find the Redis Enterprise image tag. For example, in Redis Enterprise K8s operator release 6.0.12-5, the Images section shows the Redis Enterprise tag is 6.0.12-57.\nAfter the operator upgrade is complete, you can upgrade Redis Enterprise cluster (REC).\nEdit redisEnterpriseImageSpec in the REC spec Edit the REC custom resource YAML file.\nkubectl edit rec \u0026lt;your-rec.yaml\u0026gt; Replace the versionTag: declaration under redisEnterpriseImageSpec with the new version tag.\nspec: redisEnterpriseImageSpec: imagePullPolicy: IfNotPresent repository: redislabs/redis versionTag: \u0026lt;new-version-tag\u0026gt; Save the changes to apply.\nMonitor the upgrade You can view the state of the REC with kubectl get rec.\nDuring the upgrade, the state should be Upgrade. When the upgrade is complete and the cluster is ready to use, the state will change to Running. If the state is InvalidUpgrade, there is an error (usually relating to configuration) in the upgrade.\n$ kubectl get rec NAME NODES VERSION STATE SPEC STATUS LICENSE STATE SHARDS LIMIT LICENSE EXPIRATION DATE AGE rec 3 6.2.10-107 Upgrade Valid Valid 4 2022-07-16T13:59:00Z 92m To see the status of the current rolling upgrade, run:\nkubectl rollout status sts \u0026lt;REC_name\u0026gt; Upgrade databases After the cluster is upgraded, you can upgrade your databases. The process for upgrading databases is the same for both Kubernetes and non-Kubernetes deployments. For more details on how to upgrade a database, see the Upgrade an existing Redis Enterprise Software deployment documentation.\nNote that if your cluster redisUpgradePolicy or your database redisVersion are set to major, you won\u0026rsquo;t be able to upgrade those databases to minor versions. See Redis upgrade policy for more details.\nHow does the REC upgrade work? The Redis Enterprise cluster (REC) uses a rolling upgrade, meaning it upgrades pods one by one. Each pod is updated after the last one completes successfully. This helps keep your cluster available for use.\nTo upgrade, the cluster terminates each pod and deploys a new pod based on the new image. Before each pod goes down, the operator checks if the pod is a primary (master) for the cluster, and if it hosts any primary (master) shards. If so, a replica on a different pod is promoted to primary. Then when the pod is terminated, the API remains available through the newly promoted primary pod.\nAfter a pod is updated, the next pod is terminated and updated. After all of the pods are updated, the upgrade process is complete.\n","categories":["Platforms"]},{"uri":"/ri/using-redisinsight/proxy/","uriRel":"/ri/using-redisinsight/proxy/","title":"Proxy","tags":[],"keywords":[],"description":"","content":"Trusted origins By default, RedisInsight trusts only the origin to which RedisInsight binds to. If RedisInsight is run behind a proxy, the origin where the requests come from is not trusted if the proxy\u0026rsquo;s origin is not the origin where RedisInsight binds to. While RedisInsight does start and is reachable, no operations are allowed. Because the origin of requests/proxy is always known, the trusted origins must be manually set via the RITRUSTEDORIGINS environment variable.\nNote: Values in the RITRUSTEDORIGINS setting must include the scheme (e.g. http:// or https://) instead of only the hostname.\nAlso, values that started with a dot must also include an asterisk before the dot. For example, instead of .example.com, use https://*.example.com.\nExample Here\u0026rsquo;s a sample Nginx docker-compose file:\nversion: \u0026#34;3.7\u0026#34; services: redisinsight: image: redislabs/redisinsight:latest environment: - RITRUSTEDORIGINS=http://localhost:9000 # This is the proxy origin from browser networks: - redis-network nginx-basicauth: image: nginx ports: - \u0026#34;9000:9000\u0026#34; environment: - NGINX_PORT=9000 volumes: # Your nginx.conf - ./nginx-basic-auth.conf.template:/etc/nginx/templates/nginx-basic-auth.conf.template depends_on: - redisinsight networks: - redis-network networks: redis-network: driver: bridge Subpath proxy You can enable subpath proxy by setting RIPROXYENABLE environment variable. Once enabled, either RIPROXYPATH or RIPROXYPREFIX must be set to path of the proxy subpath. Use RIPROXYPATH to set a static proxy subpath and RIPROXYPREFIX for a dynamic proxy subpath.\nRIPROXYPATH static subpath When RIPROXYPATH is being set with a path, RedisInsight is accessible only on that subpath. The default routes are given the provided prefix subpath. There isn\u0026rsquo;t a way to add another proxy behind this proxy unless the same subpath is used for the new proxy.\nNote: Once the static subpath is set, RedisInsight is only reachable on the provided subpath, both the one that is directly reachable via the binded origin and from the proxy server.\nIf no value is provided for RIPROXYPATH, RedisInsight assumes dynamic subpath and uses RIPROXYPREFIX variable to extract the proxy prefix from HTTP Headers at runtime.\nExample version: \u0026#34;3.7\u0026#34; services: redisinsight: image: redislabs/redisinsight:latest environment: - RITRUSTEDORIGINS=http://localhost:9000 # Trust the proxy origin - RIPROXYENABLE=t # Enable Subpath Proxy - RIPROXYPATH=/apps/redisinsight/ # Set static proxy subpath ports: - \u0026#34;8001:8001\u0026#34; networks: - redis-network nginx-subpath-proxy: image: nginx volumes: # Your nginx config - ./nginx-subpath-proxy.conf.template:/etc/nginx/templates/nginx-subpath-proxy.conf.template ports: - \u0026#34;9000:9000\u0026#34; environment: - NGINX_PORT=9000 - NGINX_PROXY_PATH=/apps/redisinsight/ command: # Redisinsight will now be available only in `/apps/redisinsight/`. - bash - -c - | printf \u0026#34;Visit \u0026lt;a href=\\\u0026#34;$$NGINX_PROXY_PATH\\\u0026#34;\u0026gt;$$NGINX_PROXY_PATH\u0026lt;/a\u0026gt; for redisinsight\u0026#34; \u0026gt; /etc/nginx/index.html /docker-entrypoint.sh nginx -g \u0026#34;daemon off;\u0026#34; depends_on: - redisinsight networks: - redis-network networks: redis-network: driver: bridge nginx config server { listen ${NGINX_PORT} default_server; root /etc/nginx; index index.html; location ${NGINX_PROXY_PATH} { # Subpath proxy_pass http://redisinsight:8001/; # Assumes redisinsight runs in this host proxy_read_timeout 900; proxy_set_header Host $host; } } RIPROXYPREFIX dynamic subpath When RIPROXYPREFIX is being set to a value, default being X-Forwarded-Prefix, RedisInsight extracts the path from this field in the HTTP Header. So the subpath is actually set by the proxy server and not RedisInsight and this variable just tells which HTTP header field to check for the proxy subpath. Using this approach, multiple proxies can send requests to same RedisInsight with different proxy subpaths.\nNote: When the dynamic subpath is used, RedisInsight is reachable both from the normal path, i.e., localhost:8001 and the one from dynamic subpath. Example Using the below docker-compose and nginx.conf file, same RedisInsight instance is reachable via:\nhttp://localhost:8001 — Direct path to RedisInsight server http://localhost:9000/tools/redisinsight/ — Proxy subpath 1 from the first proxy server. http://localhost:9000/installed-tools/more/content/here/redisinsight/ — Proxy subpath 2 from the first proxy server. http://localhost:9001/applications/redisinsight/ — Proxy subpath 1 from the second proxy server. http://localhost:9001/helpers/tools/redisinsight/ — Proxy subpath 2 from the second proxy server. docker-compose version: \u0026#34;3.7\u0026#34; services: redisinsight: image: redislabs/redisinsight-dev:master environment: - RIPORT=8001 - RITRUSTEDORIGINS=http://localhost:9000,http://localhost:9001 # Trust multiple proxy origins - RIPROXYENABLE=t # Enable subpath proxy ports: - \u0026#34;8001:8001\u0026#34; networks: - redis-network nginx-subpath-proxy-dynamic-1: image: nginx volumes: # Your first nginx proxy - ./nginx-subpath-proxy-dynamic.conf.template:/etc/nginx/templates/nginx-subpath-proxy-dynamic.conf.template ports: - \u0026#34;9000:9000\u0026#34; environment: - NGINX_PORT=9000 - NGINX_PROXY_PATH=-/tools/redisinsight/ - NGINX_PROXY_PATH_ADDITIONAL=/installed-tools/more/content/here/redisinsight/ command: - bash - -c - | printf \u0026#34;Visit \u0026lt;a href=\\\u0026#34;$$NGINX_PROXY_PATH\\\u0026#34;\u0026gt;$$NGINX_PROXY_PATH\u0026lt;/a\u0026gt; or \u0026lt;a href=\\\u0026#34;$$NGINX_PROXY_PATH_ADDITIONAL\\\u0026#34;\u0026gt;$$NGINX_PROXY_PATH_ADDITIONAL\u0026lt;/a\u0026gt; for redisinsight\u0026#34; \u0026gt; /etc/nginx/index.html /docker-entrypoint.sh nginx -g \u0026#34;daemon off;\u0026#34; depends_on: - redisinsight networks: - redis-network nginx-subpath-proxy-dynamic-2: image: nginx volumes: # Your second nginx proxy - ./nginx-subpath-proxy-dynamic.conf.template:/etc/nginx/templates/nginx-subpath-proxy-dynamic.conf.template ports: - \u0026#34;9001:9001\u0026#34; environment: - NGINX_PORT=9001 - NGINX_PROXY_PATH=/applications/redisinsight/ - NGINX_PROXY_PATH_ADDITIONAL=/helpers/tools/redisinsight/ command: - bash - -c - | printf \u0026#34;Visit \u0026lt;a href=\\\u0026#34;$$NGINX_PROXY_PATH\\\u0026#34;\u0026gt;$$NGINX_PROXY_PATH\u0026lt;/a\u0026gt; or \u0026lt;a href=\\\u0026#34;$$NGINX_PROXY_PATH_ADDITIONAL\\\u0026#34;\u0026gt;$$NGINX_PROXY_PATH_ADDITIONAL\u0026lt;/a\u0026gt; for redisinsight\u0026#34; \u0026gt; /etc/nginx/index.html /docker-entrypoint.sh nginx -g \u0026#34;daemon off;\u0026#34; depends_on: - redisinsight networks: - redis-network networks: redis-network: driver: bridge nginx config server { listen ${NGINX_PORT} default_server; root /etc/nginx; index index.html; location ${NGINX_PROXY_PATH} { # Subpath one proxy_pass http://redisinsight:8001/; # Assumes redisinsight runs in this host proxy_read_timeout 900; proxy_set_header Host $host; proxy_set_header X-Forwarded-Prefix ${NGINX_PROXY_PATH}; # Dynamic subpath 1 } location ${NGINX_PROXY_PATH_ADDITIONAL} { # Subpath two proxy_pass http://redisinsight:8001/; # Assumes redisinsight runs in this host proxy_read_timeout 900; proxy_set_header Host $host; proxy_set_header X-Forwarded-Prefix ${NGINX_PROXY_PATH_ADDITIONAL}; # Dynamic subpath 2 } } ","categories":["RI"]},{"uri":"/ri/using-redisinsight/overview/","uriRel":"/ri/using-redisinsight/overview/","title":"Performance Metrics","tags":[],"keywords":[],"description":"","content":"RedisInsight Overview provides you the quick overview about your Redis instance through graphical representation. It displays the total memory and keys for your instance. Number of connections received, clients connected, Network input and output and various other information.\n","categories":["RI"]},{"uri":"/rs/clusters/add-node/","uriRel":"/rs/clusters/add-node/","title":"Add a cluster node","tags":[],"keywords":[],"description":"Add a node to your existing Redis Enterprise cluster.","content":"When you install Redis Enterprise Software on the first node of a cluster, you create the new cluster. After you install the first node, you can add more nodes to the cluster.\nNote: Before you add a node to the cluster:\nThe clocks on all nodes must always be synchronized.\nIf the clock in the node you are trying to join to the cluster is not synchronized with the nodes already in the cluster, the action fails and an error message is shown indicating that you must synchronize the clocks first.\nYou must update the DNS records each time a node is added or replaced.\nWe recommend that you add nodes one after the other rather than in parallel to avoid errors that occur because the connection to the other nodes in the cluster cannot be verified.\nTo add a node in RS:\nInstall the RS installation package on a clean installation of a supported operating system.\nTo connect to the RS management UI of the new RS installation, go to: \u0026lt;https://URL or IP address:8443\u0026gt;\nFor example, if you installed RS on a machine with IP address 10.0.1.34, go to https://10.0.1.34:8443.\nTip - The RS management UI uses SSL encryption with a default certificate. You can also replace the TLS certificate with a custom certificate. To start configuring RS, click Setup.\nConfigure the RS network and storage settings:\nYou can enter a path for Persistent storage, or leave the default path. You can enter a path for Ephemeral storage, or leave the default path. If you want to enable Redis on Flash, select Enable flash storage support and enter the path to the Flash storage to use as RAM extension. If your machine has multiple IP addresses, in IP Addresses Usage assign a single IPv4 type address for internal traffic and multiple IPv4/IPv6 type addresses for external traffic. Join the new RS node to the cluster:\nIn Cluster configuration, select Join cluster.\nEnter the internal IP address or DNS name of a node that is a cluster member.\nIf the node only has one IP address, enter that IP address.\nEnter the credentials of the cluster administrator.\nThe cluster administrator is the user account that you create when you configure the first node in the cluster.\nClick Next.\nIf the cluster is configured to support rack-zone awareness, you are redirected to a page in which you must set the Rack-zone ID for the new node.\nThe node is added to the cluster. You can see it in the list of nodes in the cluster.\nIf you see an error when you add the node, try adding the node again.\nTip - We recommend that you run the rlcheck utility to verify that the node is functioning properly. ","categories":["RS"]},{"uri":"/rs/security/access-control/manage-users/add-users/","uriRel":"/rs/security/access-control/manage-users/add-users/","title":"Add users","tags":[],"keywords":[],"description":"Add users to the cluster and assign access control roles (ACLs) to them.","content":"To add a user to the cluster:\nFrom the access control tab in the admin console, select .\nEnter the name, email, and password of the new user and select a role to assign to the user.\nSelect internal for Authentication.\nFor Email Alerts, select Edit and then choose the alerts that the user should receive. You can select:\nReceive alerts for databases - The alerts that are enabled for the selected databases will be sent to the user. You can either select All databases, or you can select Customize and select the individual databases to send alerts for.\nReceive cluster alerts - The alerts that are enabled for the cluster in settings \u0026gt; alerts are sent to the user.\nSelect the save icon. ","categories":["RS"]},{"uri":"/rs/security/admin-console-security/","uriRel":"/rs/security/admin-console-security/","title":"Admin console security","tags":[],"keywords":[],"description":"","content":"Redis Enterprise comes with a web-based user interface known as the admin console. The admin console provides the following security features:\nEncryption-in-transit using TLS/SSL User authentication using LDAP Role-based access control We recommend the following practices:\nIntegrate with an external identity provider: Redis Enterprise uses LDAP integration to support external identity providers, such as Active Directory.\nImplement standard authentication practices: If your organization does not support LDAP, you can still use Redis Enterprise\u0026rsquo;s user account security. Features include basic password complexity requirements, password expiration, and user login lockouts.\nLimit session timeouts: Session timeouts, also known as automatic sign out, help prevent unauthorized access. Admin console sessions are allowed to idle for a period of time before users are required to re-authenticate.\nBy default, users are signed out after 15 minutes of inactivity. You can set the timeout period.\nRequire HTTPS for API endpoints - Redis Enterprise comes with a REST API to help automate tasks. This API is available in both an encrypted and unencrypted endpoint for backward compatibility. You can disable the unencrypted endpoint with no loss in functionality.\nConfigure Transport Layer Security (TLS) - A common compliance requirement is to set a minimum version of TLS. This helps to make sure that only secure versions of TLS are allowed when accessing the cluster.\nInstall your own certificates - Redis Enterprise comes with self-signed certificates by default; however, many organizations require that you use specific CA signed certificates.\n","categories":["RS"]},{"uri":"/rc/security/database-security/","uriRel":"/rc/security/database-security/","title":"Cloud database security","tags":[],"keywords":[],"description":"","content":"Redis Cloud provides several features to help you secure your databases. These include password-based authentication and role-based access control, network security, TLS, and encryption-at-rest.\nPasswords, users, and roles All Redis Cloud databases require a password to connect. However, we recommend going further by enabling role-based access control (RBAC). With RBAC, you can define all the roles you need, with the appropriate permissions, and assign those roles to your users.\nNetwork security Redis Cloud supports two types of network security: IP Restrictions and VPCs. We recommend that you employ at least one of these network security options to constrain access to your databases.\nTransport Layer Security (TLS) Redis Cloud supports Transport Layer Security (TLS) for database connections. TLS, often called \u0026ldquo;SSL\u0026rdquo;, ensures the privacy of the TCP connection between your application and database. When client authentication is enabled, TLS also ensures that those clients with an authorized key can connect to your Redis databases.\nWe strongly recommend enabling TLS for any application transmitting sensitive data across the wire.\nDisk encryption Redis Cloud provides encryption for all data stored on disk in Redis databases. See our encrpytion at rest documentation for specific details.\n","categories":["RC"]},{"uri":"/rs/databases/configure/","uriRel":"/rs/databases/configure/","title":"Configure database settings","tags":[],"keywords":[],"description":"Configure settings specific to each database.","content":"You can manage your Redis Enterprise Software databases with several different tools:\nAdmin console (the web-based user interface) Command-line tools REST API Admin console You can change the configuration of a Redis Enterprise Software database at any time.\nTo edit the configuration of a database using the admin console:\nGo to Database and select the database that you want to edit.\nGo to Configuration and click Edit at the bottom of the page. The database settings appear.\nChange any of the configurable database settings.\nNote: For Active-Active database instances, most database settings only apply to the instance that you are editing. Click Update.\nCommand-line tools For details on using command line tools see:\nrladmin for standalone database configuration crdb-cli for Active-Active database configuration redis-cli for open source Redis configuration REST API See the REST API reference for details on using the API to configure your database.\n","categories":["RS"]},{"uri":"/rs/security/access-control/configure-acl/","uriRel":"/rs/security/access-control/configure-acl/","title":"Configure ACLs","tags":[],"keywords":[],"description":"Configure access control lists (ACLs).","content":"Redis ACL command syntax Redis ACLs are defined by a Redis syntax where you specify the commands or command categories that are allowed for specific keys.\nNote: Redis Enterprise modules do not have a command category. Redis Enterprise lets you:\nInclude commands and categories with the \u0026ldquo;+\u0026rdquo; prefix for commands or \u0026ldquo;+@\u0026rdquo; prefix for command categories. Exclude commands and categories with the \u0026ldquo;-\u0026rdquo; prefix for commands or \u0026ldquo;-@\u0026rdquo; prefix for command categories. Include keys or key patterns with the \u0026ldquo;~\u0026rdquo; prefix. Allow access to pub/sub channels with the \u0026ldquo;\u0026amp;\u0026rdquo; prefix (only supported for databases with Redis version 6.2 and later). To define database access control, you can:\nUse the predefined user roles and add Redis ACLs for specific databases. Create new user roles and select the management roles and Redis ACLs that apply to the user roles for specific databases. Assign roles and Redis ACLs to a database in the access control list section of the database configuration. The predefined Redis ACLs are:\nFull Access - All commands are allowed on all keys. Not Dangerous - All commands are allowed except those that are administrative, could affect availability, or could affect performance. Read Only - Only read-only commands are allowed on keys. Configure Redis ACLs To configure a Redis ACL rule that you can assign to a user role:\nFrom access control \u0026gt; redis acls, you can either:\nPoint to a Redis ACL and select to edit an existing Redis ACL.\nSelect to create a new Redis ACL.\nEnter a descriptive name for the Redis ACL. This will be used to reference the ACL rule to the role.\nDefine the ACL rule.\nSelect Save.\nNote: In Redis Enterprise:\nExternal users are not currently supported for database authentication. For multi-key commands on multi-slot keys, the return value is failure but the command runs on the keys that are allowed. Change default pub/sub permissions Pub/sub ACL rules determine which pub/sub channels a user can access.\nAs of Redis Enterprise version 6.4.2, you can configure acl_pubsub_default, which determines the default pub/sub permissions for all databases in the cluster. You can set acl_pubsub_default to the following values:\nresetchannels is restrictive and blocks access to all channels by default.\nallchannels is permissive and allows access to all channels by default.\nRedis Enterprise version 6.4.2 defaults to permissive pub/sub channels for backward compatibility. We recommend you change your cluster\u0026rsquo;s default pub/sub ACLs to be restrictive.\nTo make default pub/sub permissions restrictive:\nUpgrade all databases in the cluster to Redis version 6.2 or later.\nSet the default to resetchannels with rladmin or the REST API.\nMethod 1 - rladmin tune cluster:\nrladmin tune cluster acl_pubsub_default resetchannels Method 2 - Update cluster policy REST API request:\nPUT /v1/cluster/policy { \u0026#34;acl_pubsub_default\u0026#34;: \u0026#34;resetchannels\u0026#34; } Blocked ACL commands The following ACL commands are blocked in Redis Enterprise:\nLOAD SAVE SETUSER DELUSER GENPASS LOG Allowed ACL subcommands The following ACL subcommands are allowed in Redis Enterprise:\nLIST USER GETUSER CAT WHOAMI HELP Note: The MULTI, EXEC, DISCARD commands are always allowed, but ACLs are enforced on MULTI subcommands. ","categories":["RS"]},{"uri":"/rs/databases/connect/","uriRel":"/rs/databases/connect/","title":"Connect to a database","tags":[],"keywords":[],"description":"Learn how to connect your application to a Redis database hosted by Redis Enterprise Software and test your connection.","content":"After you have Set up a cluster and created a Redis database, you can connect to your database.\nTo connect to your database, you need the database endpoint, which includes the cluster name (FQDN) and the database port. Select the Configuration tab in the database screen to find the database endpoint.\nIf you try to connect with the FQDN, and the database does not respond, try connecting with the IP address. If this succeeds, DNS is not properly configured. To set up DNS, see Configure cluster DNS.\nIf you want to secure your connection, set up TLS.\nConnect to a database Use one of the following connection methods to connect to your database:\nredis-cli utility\nRedis client for your preferred programming language\nRedisInsight\nredis-cli The redis-cli utility is installed when you install Redis. It provides a command-line interface that lets you work with your database using core Redis commands.\nredis-cli is located in /opt/redislabs/bin/ on a node with Redis Enterprise installed. To connect to the database from a node in the cluster, run redis-cli with the database port.\n$ sudo /opt/redislabs/bin/redis-cli -p \u0026lt;port\u0026gt; To connect using redis-cli from outside of the cluster, run redis-cli with the database hostname and port.\n$ sudo /opt/redislabs/bin/redis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; Redis client Different programming languages use different connection clients to interact with Redis databases, and each client has its own syntax and installation process. For help with a specific client, see the client\u0026rsquo;s documentation.\nSee the client list to view all Redis clients by language.\nNote: You can\u0026rsquo;t use client libraries to configure Redis Enterprise Software. Instead, use:\nThe Redis Software admin console The REST API Command-line utilities, such as rladmin Code example (Python) A simple python application can also connect to the database.\nNote: The following section assumes you already have Python and redis-py (python library for connecting to Redis) configured. You can find the instructions to configure redis-py on the github page for redis-py. Create a new file called redis_test.py:\nimport redis r = redis.StrictRedis(host=\u0026#39;\u0026lt;host\u0026gt;\u0026#39;, port=\u0026lt;port\u0026gt;) print (\u0026#34;set key1 123\u0026#34;) print (r.set(\u0026#39;key1\u0026#39;, \u0026#39;123\u0026#39;)) print (\u0026#34;get key1\u0026#34;) print(r.get(\u0026#39;key1\u0026#39;)) Replace \u0026lt;host\u0026gt; and \u0026lt;port\u0026gt; with the hostname and port for your database.\nRun the redis_test.py application to store and retrieve a key:\n$ python redis_test.py If the connection is successful, the output of the application looks like this:\nset key1 123 True get key1 123 RedisInsight RedisInsight is a free Redis GUI that is available for MacOS, Windows, and Linux.\nInstall RedisInsight.\nOpen RedisInsight and select Add Redis Database.\nEnter the host and port in the Host and Port fields.\nSelect Use TLS if TLS is set up.\nSelect Add Redis Database to connect to the database.\nSee the RedisInsight documentation for more information.\n","categories":["RS"]},{"uri":"/rs/databases/durability-ha/consistency/","uriRel":"/rs/databases/durability-ha/consistency/","title":"Consistency during replication","tags":[],"keywords":[],"description":"Explains the order write operations are communicated from app to proxy to shards for both the weak consistency model and the strong consistency model.","content":"Redis Enterprise Software comes with the ability to replicate data to another database instance for high availability and persist in-memory data on disk permanently for durability. With the WAIT command, you can control the consistency and durability guarantees for the replicated and persisted database.\nAny updates that are issued to the database are typically performed with the following flow shown below;\nApplication issues a write Proxy communicates with the correct primary (also known as master) \u0026ldquo;shard\u0026rdquo; in the system that contains the given key The shard writes the data and sends an acknowledgment to the proxy The proxy sends the acknowledgment back to the application The write is communicated from master to replica Replica acknowledges the write back to the master The write to a replica is persisted to disk The write is acknowledged within the replica With the WAIT command, applications can ask to wait for acknowledgments only after replication or persistence is confirmed on the replica. The flow of a write operation with the WAIT command is shown below:\nApplication issues a write, Proxy communicates with the correct master \u0026ldquo;shard\u0026rdquo; in the system that contains the given key, Replication communicated the update to the replica shard. Replica persists the update to disk (assuming AOF every write setting is selected). 5-8. The acknowledgment is sent back from the replica all the way to the proxy with steps 5 to 8. With this flow, the application only gets the acknowledgment from the write after durability is achieved with replication to the replica and to the persistent storage.\nWith the WAIT command, applications can have a guarantee that even under a node failure or node restart, an acknowledged write is recorded.\nSee the WAIT command for details on the new durability and consistency options.\n","categories":["RS"]},{"uri":"/rs/clusters/monitoring/console-metrics-definitions/","uriRel":"/rs/clusters/monitoring/console-metrics-definitions/","title":"Metrics in the admin console","tags":[],"keywords":[],"description":"The Redis Enterprise Software (RS) admin console shows metrics with information about the performance of the cluster, node, database, and shard.","content":"The Redis Enterprise Software admin console shows performance metrics for clusters, nodes, databases, and shards.\nStandard metrics Metric Components measured Description More information Connections Cluster, Node, Database Number of connections used to access the database CPU usage Cluster, Node Percent usage of the CPU Evicted objects/sec1 Database, Shard Number of objects evicted per second Objects are evicted if:\nThe database reaches its memory_limitThe eviction policy is not configured to no-evictionThe dataset keys are compliant with the selected eviction policy. For example, with the volatile-lru eviction policy, Redis evicts expired keys. Expired objects/sec1 Database, Shard Number of expired objects per second Fork CPU usage Database, Shard CPU usage of Redis child forks Free disk space Cluster, Node Remaining unused disk space Free ephemeral disk space Cluster, Node Remaining unused disk space on the ephemeral path Free RAM Cluster, Node RAM available for system use Hit ratio Database, Shard Calculated as number of operations on existing keys out of the total number of operations number_of_ops_on_existing_keys / total_ops Incoming traffic1 Cluster, Node, Database Total incoming traffic (in bytes/sec) to the database Incoming traffic compressed1 Active-Active Total incoming compressed traffic (in bytes/sec) to the database Incoming traffic uncompressed1 Active-Active Total incoming uncompressed traffic (in bytes/sec) to the database Latency Database Latency per operation The graph shows average, min, max, and last latency values Main thread CPU usage Database, Shard Percent of the CPU used by the main thread Memory limit Database Memory size limit of the database, enforced on used_memory Used memory does not include:\nFragmentation ratioReplication buffer - By default, set to 10% of used memory and is between 64MB and 2048 MB.Lua memory limit - Does not exceed 1MB. Memory usage1 Database Calculated as used memory out of the memory limit used_memory / memory_limit Ops/sec Cluster, Node, Database, Shard Number of total operations per second Where operations means:\nread operationswrite operationsother commands operations Other cmds/sec Database Number of other commands per second For example: PING, AUTH, INFO Other commands latency Database Latency per other command The graph shows average, min, max, and last latency values Outgoing traffic1 Cluster, Node, Database Total outgoing traffic (in bytes/sec) from the database Pending writes max Active-Active Maximum number of write operations queued Pending writes min Active-Active Minimum number of write operations queued RAM fragmentation Database, Shard Ratio between the used (allocated) memory and the physical RAM that is used. Read misses/sec1 Database, Shard Number of read operations (per sec) on nonexistent keys Reads latency Database Latency per read operation The graph shows average, min, max and last latency values Reads/sec Database Number of total read operations per second For example: GET Total CPU usage Database, Shard Percent usage of the CPU Total keys1 Database, Shard Total number of keys in the dataset (not including replication, even if replication is enabled) Calculated as the sum of all keys of all master shards. Used memory1 Database, Shard Total memory used by the database, including RAM, Flash (if enabled), and replication (if enabled) Does not include:Fragmentation overheadReplica replication buffers at the primary nodesMemory used by Lua scriptsCopy On Write (COW) operation that can be triggered by:\nA full replication processA database snapshot processAOF rewrite process Write misses/sec1 Database, Shard Number of write operations (per sec) on nonexistent keys Writes latency Database Latency per write operation Writes/sec Database Number of total write operations per second For example: SET Redis on Flash (RoF) metrics These metrics are available for Redis on Flash (RoF) databases.\nMeasured metrics Metric Components measured Description % Values in RAM Database, Shard Percent of keys whose values are stored in RAM Flash bytes/sec Cluster, Node, Database, Shard Read+write bytes per second on flash storage device Flash fragmentation Database, Shard Ratio between the (logical) used flash memory and the actual physical flash that is used Flash IOPS Cluster, Node Rate of Input/Output operations per second on flash storage device Flash KV ops Node Rate of operations on flash key values (read + write + del) per second Flash ops/sec Database, Shard Rate of operations on flash key values (read + write + del) per second Free flash Cluster, Node Free space on flash storage RAM dataset overhead Database, Shard Percentage of the RAM limit that is used for anything other than values, such as key names, dictionaries, and other overheads RAM hit ratio Database, Shard Ratio of requests processed directly from RAM to total number of requests processed RAM limit Database RAM limit in bytes RAM usage Database Percentage of the RAM limit usage RAM:Flash access ratio Database, Shard Ratio between logical Redis key value operations and actual flash key value operations Used Flash Database, Shard Total RAM used to store values in flash Used RAM Database, Shard Total size of data stored in RAM, including keys, values, overheads, and replication (if enabled) Values in Flash Database, Shard Number of keys with values stored in flash, not including replication Values in RAM Database, Shard Number of keys with values stored in RAM, not including replication Calculated metrics You can calculate the following RoF metrics with Measured metrics.\nRoF average key size with overhead\navg_key_size = (ram_dataset_overhead * used_ram) / (total_keys * 2) RoF average value size in RAM\navg_ram_value_size = ((1 - ram_dataset_overhead) * used_ram) / (values_in_ram * 2) RoF average value size in flash\navg_flash_value_size = used_flash / values_in_flash Footnotes These metrics are not collected during shard migration. If you view the database or shard metrics while resharding, the graph is blank.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","categories":["RS"]},{"uri":"/rc/api/examples/create-database/","uriRel":"/rc/api/examples/create-database/","title":"Create and manage databases","tags":[],"keywords":[],"description":"This article describes how to create and manage a database using `cURL` commands.","content":"You can use the Redis Enterprise Cloud REST API to create databases.\nThese examples use the cURL utility; you can use any REST client to work with the Redis Cloud REST API.\nCreate a database To create a database, use POST /subscriptions/{subscription-id}/databases\nThe database is created in an existing or a newly created subscription.\nWhen a subscription is created, it is created with at least one database.\nYou can add databases to the subscription; you can also update or delete existing databases.\nCreating a database is an asynchronous operation.\nThe following API call creates a database.\nPOST \u0026#34;https://[host]/v1/subscriptions/$SUBSCRIPTION_ID/databases\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;Database-example-basic\u0026#34;, \u0026#34;memoryLimitInGb\u0026#34;: 10, \u0026#34;password\u0026#34;: \u0026#34;P@ssw0rd\u0026#34; } The JSON body contains only the most basic, required parameters in order to create a database:\nDatabase name - A unique name per subscription that can contain only alphanumeric characters and hyphens Maximum database size in GB Database password Additional database parameters There are many additional parameters and settings that can be defined on database creation:\nReview the database parameters and options in the Swagger OpenAPI documentation. Select POST Create database. Under Database definition, click on the Model options. ","categories":["RC"]},{"uri":"/rs/security/access-control/create-roles/","uriRel":"/rs/security/access-control/create-roles/","title":"Create roles","tags":[],"keywords":[],"description":"Create access control roles.","content":"From access control \u0026gt; roles, you can configure user roles with:\nManagement roles - Management roles define user access to the cluster\u0026rsquo;s admin console and API. Data access controls - Data access controls define the permissions each role has for each database in the cluster. Default management roles Redis Enterprise Software includes five predefined roles that determine a user\u0026rsquo;s level of access to the admin console and REST API.\nNone - Cannot access the admin console or use the REST API DB Viewer - Read database settings DB Member - Administer databases Cluster Viewer - Read cluster settings Cluster Member - Administer the cluster Admin - Full cluster access For more details about the privileges granted by each of these roles, see admin console permissions or REST API permissions.\nAdmin console permissions Here\u0026rsquo;s a summary of the admin console actions permitted by each default management role:\nAction DB Viewer DB Member Cluster Viewer Cluster Member Admin Edit database configuration ❌ No ✅ Yes ❌ No ✅ Yes ✅ Yes Reset slow log ❌ No ✅ Yes ❌ No ✅ Yes ✅ Yes View cluster configuration ❌ No ❌ No ✅ Yes ✅ Yes ✅ Yes View cluster logs ❌ No ✅ Yes ✅ Yes ✅ Yes ✅ Yes\nView cluster metrics ❌ No ❌ No ✅ Yes ✅ Yes ✅ Yes View database configuration ✅ Yes ✅ Yes ✅ Yes ✅ Yes ✅ Yes View database metrics ✅ Yes ✅ Yes ✅ Yes ✅ Yes ✅ Yes View node configuration ❌ No ❌ No ✅ Yes ✅ Yes ✅ Yes View node metrics ❌ No ❌ No ✅ Yes ✅ Yes ✅ Yes View Redis database password ❌ No ✅ Yes ❌ No ✅ Yes ✅ Yes View and edit cluster settings ❌ No ❌ No ❌ No ❌ No ✅ Yes Create roles for database access To create a role that grants database access to users but blocks access to the Redis Enterprise admin console and REST API, set the Cluster management role to None.\nTo define a role for database access:\nFrom access control \u0026gt; roles, you can either:\nPoint to a role and select to edit an existing role.\nSelect to create a new role.\nEnter a descriptive name for the role. This will be used to reference the role when configuring users.\nSelect a Cluster management role. The default is None.\nSelect Add under Redis ACLs .\nSelect the databases the role applies to.\nSelect the Redis ACL to apply to the role.\nSelect the save icon.\nSelect Save.\n","categories":["RS"]},{"uri":"/rc/databases/view-edit-database/","uriRel":"/rc/databases/view-edit-database/","title":"View and edit databases","tags":[],"keywords":[],"description":"","content":"Use the Databases menu of the admin console to manage your subscription databases.\nTo view the details of a database:\nSign in to the Redis Cloud admin console. (Create an account if you don\u0026rsquo;t already have one.)\nLocate the database in the list.\nSelect the database name to open the Database page.\nThe Database screen lets you review:\nConfiguration details of a database Graphs showing performance metrics Recent activity via a \u0026ldquo;slowlog,\u0026rdquo; which lists queries that exceed a certain execution time. For help changing database settings, see Edit database details.\nConfiguration details tab The Configuration details screen is divided into sections, each dedicated to a specific category. Note that not every section or setting is available to every subscription plan.\nGeneral settings The General section defines basic properties about your database.\nThe available settings vary according to your subscription plan, cloud provider, and design choices. For example, if you do not select a module when crating a database, the Module setting is not displayed when you view its configuration details.\nSetting name Description Database Name The name given to your database Public endpoint Public URI used by any application or client to access the database. Private endpoint Private endpoint URI available to approved clients; use CIDR allow list and VPC peering to enabled access (Flexible or Annual subscriptions only) Type Either \u0026lsquo;redis\u0026rsquo; or \u0026lsquo;memcached\u0026rsquo; based on the value selected when the database was created Redis version Redis version used to create the database Redis on Flash Checked when the subscription supports Redis on Flash (Flexible or Annual subscriptions only) Activated on Date and time the database was created Active-Active Redis Checked when the database is part of an Active-Active relationship (coming soon; Flexible or Annual subscriptions only) Last changed Date and time of last update Module This setting appears when when a module is enabled for a database Scalability section The Scalability section describes the memory size, throughput, and hashing policy for a database.\nThe Scalability section is primarily for Flexible and Annual plans. Free and Fixed plans have options for memory limit and memory used.\nSetting name Description Memory limit Maximum size (in GB) for your database Memory used Memory currently used for your database Throughput Defines throughput in terms of maximum operations per second for the database RediSearch databases use the number of shards to determine throughput. To determine how many shards you need for your RediSearch database, use the RediSearch sizing calculator. Hashing policy Defines the hashing policy Cluster OSS Enables the OSS Cluster API for a databaseWhen this option is enabled, you cannot define a custom hashing policy To learn more about these settings and when to use them, see Database clustering.\nDurability section The Durability section helps protect your data when problems occur. These settings define replication, persistence, backup, and eviction policies.\nSetting name Description High availability Replicates your data across multiple nodes, as allowed by your subscription plan Data persistence Defines whether (and how) data is saved to disk; available options depend on your plan type Data eviction policy Defines what happens when your database reaches its memory size limit Remote backup (paid Fixed, Flexible, or Annual subscriptions only) When enabled, identifies a location and interval for data backups. Active-passive Redis (Flexible or Annual subscriptions only) When enabled, identifies a path to the linked database. Security section The Security section helps you control access to your database.\nSetting name Description Default user When enabled, permits access using a simple password Default user password Password assigned to the database when created CIDR allow list (paid Fixed, Flexible, or Annual subscriptions only) Range of IP addresses/security groups allowed to access the database Transport layer security (TLS) (Flexible or Annual subscriptions only) Enables transport security layer(TLS) encryption for database access. Alerts section The Alerts section defines notification emails sent to your account and the conditions that trigger them.\nThe available alerts vary according to the subscription type.\nSetting name Description Dataset size has reached When enabled, sends an an email when the database reaches the defined memory size (Free, Flexible, or Annuals plans only) Total size of datasets under this plan reached When enabled, sends an an email when the database reaches the defined memory size (paid Fixed plans only) Throughput is higher than When enabled, sends an email when the operations per second exceed the defined threshold (paid Fixed, Flexible, or Annuals plans only) Throughput is lower than When enabled, sends an email when the operations per second falls below the defined threshold (paid Fixed, Flexible, or Annuals plans only) Latency is higher than When enabled, sends an an email when the latency exceeds the defined memory size (paid Fixed plans only) Number of connections When enabled, sends an email when the connections exceeds the defined limit. (Free and Fixed plans only) Replica Of - database unable to sync with source When enabled, sends email when the replica database cannot sync with the primary (source) database (Flexible or Annuals plans only) Replica Of - sync lag is higher than When enabled, sends email when the sync lag exceeds the defined threshold (Flexible or Annuals plans only) Danger zone Actions in the Danger Zone are permanent and should not be taken lightly.\nHere, you can:\nDelete the database.\nWhen you choose this action, you\u0026rsquo;re asked to confirm.\nIf you only have one database in your subscription, you can delete both the database and the subscription from the Delete database confirmation dialog:\nDelete both deletes both the database and the subscription.\nDelete database deletes the database but keeps the subscription.\nDatabases must be active and empty before they can be deleted. To learn more, see Delete a database.\nImport data into the database.\nWhen you choose this action, you\u0026rsquo;re asked to specify the source and location of the data to import\nTo learn more, see Import data.\nFor best results, we recommend backing up data before starting any Danger Zone actions.\nManage the database list The Databases list summarizes the status of all databases from the subscriptions associated with your account.\nYou can:\nSort the list in descending or ascending order using the the arrow displayed to right of the field name in the header. Supported fields include Subscription, Name, and Memory.\nSelect the arrow icon to change the sort order. One sort order can be active at any given time.\nUse the Filter icon displayed to the right of the field name in the header to display string matches for that field.\nYou can filter the list on Subscription, Name, Endpoint, and Options. String matches are not case sensitive. You can specify more than one filter expression at a time.\nThe icon is circled when a filter is active.\nUse the controls in the list footer to change the number of items displayed in the list or to navigate.\nSort orders and filter expressions are not saved between console sessions.\nOther actions and info The View Database screen also has tabs that let you view:\nMetrics: a series of graphs showing database performance over time. See Monitor performance\nSlowlog: a log showing recent slow queries run against your database. The log displays when the action started, the duration, the complexity of the operation, and any parameters passed to the operation.\nEdit database details Use the Edit database button to edit database details.\nBecause databases exist within the context of a deployment, certain fields cannot be updated, especially those that might lead to data loss.\nHere\u0026rsquo;s what you can change:\nSection Setting Comments General Database name Scalability Memory limit Flexible and Annual subscriptions only) Throughput Flexible and Annual subscriptions only) Hashing policy Flexible and Annual subscriptions only) Cluster OSS Flexible and Annual subscriptions only) Durability High-availability paid Fixed, Flexible, and Annual subscriptions only) Data persistence paid Fixed, Flexible, and Annual subscriptions only) Data eviction policy Remote backup paid Fixed, Flexible, Annual subscriptions only) Active-passive Redis Flexible and Annual subscriptions only) Security Default user Default user password CIDR allow list paid Fixed, Flexible, and Annual subscriptions only) Transport layer security (TLS) Flexible and Annual subscriptions only) Alerts all available for subscription Choose Save database to save your changes.\nIf you need to change other details, create a new database and then migrate existing data.\n","categories":["RC"]},{"uri":"/rs/databases/import-export/export-data/","uriRel":"/rs/databases/import-export/export-data/","title":"Export data from a database","tags":[],"keywords":[],"description":"You can export data to import it into a new database or to make a backup.  This article shows how to do so.","content":"You can export the data from a specific database at any time. The following destinations are supported:\nFTP server SFTP server Amazon AWS S3 Local mount point Azure Blob Storage Google Cloud Storage If you export a database configured for database clustering, export files are created for each shard.\nStorage space requirements Before exporting data, verify that you have enough space available in the storage destination and on the local storage associated with the node hosting the database.\nExport is a two-step process: a temporary copy of the data is saved to the local storage of the node and then copied to the storage destination. (The temporary file is removed after the copy operation.)\nExport fails when there isn\u0026rsquo;t enough space for either step.\nExport database data To export data from a database:\nSign in to the admin console.\nSelect the Databases command from the console menu.\nThe admin console commands vary according to your level of access. Here, you see commands available to users with full access.\nSelect the database from the database list.\nSelect the Configuration tab.\nSelect the Export button.\nIf the Export button is disabled, you do not have permission to export data.\nEnter the export details.\nThe Choose storage type list defines the destination storage container for the exported data; select the appropriate value and then enter the requested details. Details vary for each storage type. For help, see Supported storage locations.\nSelect the Export button to begin the export process.\nSupported storage locations Data can be exported to a local mount point, transferred to a URI using FTP/SFTP, or stored on cloud provider storage.\nWhen saved to a local mount point or a cloud provider, export locations need to be available to the group and user running Redis Enterprise Software, redislabs:redislabs by default.\nRedis Enterprise Software needs the ability to view permissions and update objects in the storage location. Implementation details vary according to the provider and your configuration. To learn more, consult the provider\u0026rsquo;s documentation.\nThe following sections provide general guidelines. Because provider features change frequently, use your provider\u0026rsquo;s documentation for the latest info.\nFTP server Before exporting data to an FTP server, verify that:\nYour Redis Enterprise cluster can connect and authenticate to the FTP server. The user specified in the FTP server location has read and write privileges. To export data to an FTP server, set Path using the following syntax:\nftp://[username]:[password]@[host]:[port]/[path]/\nWhere:\nprotocol: the server\u0026rsquo;s protocol, can be either ftp or ftps. username: your username, if needed. password: your password, if needed. hostname: the hostname or IP address of the server. port: the port number of the server, if needed. path: the export destination path, if needed. Example: ftp://username:password@10.1.1.1/home/exports/\nThe user account needs permission to write files to the server.\nSFTP server Before exporting data to an SFTP server, make sure that:\nYour Redis Enterprise cluster can connect and authenticate to the SFTP server.\nThe user specified in the SFTP server location has read and write privileges.\nThe SSH private keys are specified correctly. You can use the key generated by the cluster or specify a custom key.\nWhen using the cluster key, copy the Cluster SSH Public Key to the appropriate location on the SFTP server. This is available from the General tab of the Settings menu in the admin console.\nUse the server documentation to determine the appropriate location for the SSH Public Key.\nTo export data to an SFTP server, enter the SFTP server location in the format:\nsftp://user:password@host\u0026lt;:custom_port\u0026gt;/path/ For example: sftp://username:password@10.1.1.1/home/exports/\nLocal mount point Before exporting data to a local mount point, verify that:\nThe node can connect to the destination server, the one hosting the mount point. The redislabs:redislabs user has read and write privileges on the local mount point and on the destination server. The export location has enough disk space for your exported data. To export to a local mount point:\nOn each node in the cluster, create the mount point:\nConnect to a shell running Redis Enterprise Software server hosting the node.\nMount the remote storage to a local mount point.\nFor example:\nsudo mount -t nfs 192.168.10.204:/DataVolume/Public /mnt/Public In the path for the export location, enter the mount point.\nFor example: /mnt/Public\nVerify that the user running Redis Enterprise Software has permissions to access and update files in the mount location.\nAWS Simple Storage Service To export data to an Amazon Web Services (AWS) Simple Storage Service (S3) bucket:\nSign in to the AWS console.\nCreate an S3 bucket if you do not already have one.\nCreate an IAM User with permission to add objects to the bucket.\nCreate an access key for that user if you do not already have one.\nIn the Redis Enterprise Software admin console, when you enter the export location details:\nSelect \u0026ldquo;AWS S3\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Access key ID field, enter the access key ID.\nIn the Secret access key field, enter the secret access key.\nGCP Storage To export to a Google Cloud Platform (GCP) storage bucket:\nSign in to Google Cloud Platform console.\nCreate a JSON service account key if you do not already have one.\nCreate a bucket if you do not already have one.\nAdd a principal to your bucket:\nIn the New principals field, add the client_email from the service account key.\nSelect \u0026ldquo;Storage Legacy Bucket Writer\u0026rdquo; from the Role list.\nIn the Redis Enterprise Software admin console, when you enter the export location details:\nSelect \u0026ldquo;Google Cloud Storage\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Client id field, enter the client_id from the service account key.\nIn the Client email field, enter the client_email from the service account key.\nIn the Private key id field, enter the private_key_id from the service account key.\nIn the Private key field, enter the private_key from the service account key. Replace \\n with new lines, and then select the Save icon.\nAzure Blob Storage To export to Microsoft Azure Blob Storage, sign in to the Azure portal and then:\nCreate an Azure Storage account if you do not already have one.\nCreate a container if you do not already have one.\nManage storage account access keys to find the storage account name and account keys.\nIn the Redis Enterprise Software admin console, when you enter the export location details:\nSelect \u0026ldquo;Azure Blob Storage\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Account name field, enter your storage account name.\nIn the Account key field, enter the storage account key.\nTo learn more, see Authorizing access to data in Azure Storage.\n","categories":["RS"]},{"uri":"/rs/installing-upgrading/file-locations/","uriRel":"/rs/installing-upgrading/file-locations/","title":"File locations","tags":[],"keywords":[],"description":"","content":"To make sure that Redis Enterprise Software functions properly, make sure that you handle the files in the application directories with care.\nApplication directories The directories that Redis Enterprise Software installs into are:\nPath Description /opt/redislabs Main installation directory for all Redis Enterprise Software binaries /opt/redislabs/bin Binaries for all the utilities for command line access and managements such as \u0026ldquo;rladmin\u0026rdquo; or \u0026ldquo;redis-cli\u0026rdquo; /opt/redislabs/config System configuration files /opt/redislabs/lib System library files /opt/redislabs/sbin System binaries for tweaking provisioning Configuration and data directories The default directories that Redis Enterprise Software uses for data and metadata are:\nPath Description /var/opt/redislabs Default storage location for the cluster data, system logs, backups and ephemeral, persisted data /var/opt/redislabs/log System logs for Redis Enterprise Software /var/opt/redislabs/run Socket files for Redis Enterprise Software /etc/opt/redislabs Default location for cluster manager configuration and certificates /tmp Temporary files You can change these file locations for:\nEphemeral and persistence storage during cluster setup Socket files after cluster setup ","categories":["RS"]},{"uri":"/rs/databases/active-active/get-started/","uriRel":"/rs/databases/active-active/get-started/","title":"Get started with Redis Enterprise Active-Active databases","tags":[],"keywords":[],"description":"Quick start guide to create an Active-Active database for test and development.","content":"To get you started, this article will help you set up a Active-Active database, formerly known as CRDB (conflict-free replicated database) spanning across two Redis Enterprise Software clusters for test and development environments. Here are the steps:\nStep 1: Run two Redis Enterprise Software (RS) Docker containers Step 2: Set up each container as a cluster Step 3: Create a new Redis Enterprise Active-Active database Step 4: Test connectivity to the Active-Active database To run an Active-Active database on installations from the RS download package, set up two RS installations and continue from Step 2.\nNote: This getting started guide is for development or demonstration environments. To set up an Active-Active database in a production environment, use the instructions for creating an Active-Active database. Step 1: Run two containers To spin up two RS containers, run these commands:\ndocker run -d --cap-add sys_resource -h rp1_node1 --name rp1_node1 -p 8443:8443 -p 9443:9443 -p 12000:12000 redislabs/redis docker run -d --cap-add sys_resource -h rp2_node1 --name rp2_node1 -p 8445:8443 -p 9445:9443 -p 12002:12000 redislabs/redis The -p options map the admin console port (8443), REST API port (9443), and database access port differently for each container to make sure that all containers can be accessed from the host OS that is running the containers.\nStep 2: Setup two clusters For cluster 1, direct your browser to https://localhost:8443 on the host machine to see the Redis Enterprise Software web console. Simply click the Setup button on the page to get started.\nNote: Depending on your browser, you may see a certificate error. Continue to the website. On the node configuration page, select your default settings and provide a cluster FQDN, for example cluster1.local. Then click Next button.\nIf you don\u0026rsquo;t have a license key, click the Next button to try the trial version of the product.\nOn the next screen, set up a Cluster Administrator account using an email for the login and a password.\nClick OK to confirm that you are aware of the replacement of the HTTPS SSL/TLS certificate on the node, and proceed through the browser warning.\nRepeat the same operations for cluster 2 with these differences:\nIn your web browser, go to https://localhost:8445 to set up the cluster 2. For the Cluster name (FQDN), enter a different name, such as cluster2.local. Now we have two Redis Enterprise Software clusters with FQDNs cluster1.local and cluster2.local.\nNote: Each Active-Active instance must have a unique fully-qualified domain name (FQDN). Step 3: Create a Redis Active-Active database After you login to cluster1.local, select the Redis database and deployment type Geo-Distributed. Then click Next.\nIn create database, click the show advanced option and:\nFor the database name, enter: database1\nFor the endpoint port number, enter: 12000\nIn the participating clusters list, add the address and admin credentials for:\nhttps://cluster1.local:9443 - the cluster you are currently connected to https://cluster2.local:9443 - the other cluster In Database clustering, either:\nMake sure that Database clustering is enabled and select the number of shards that you want to have in the database. When database clustering is enabled, databases are subject to limitations on Multi-key commands. You can increase the number of shards in the database at any time. Clear Database clustering to use only one shard and to avoid Multi-key command limitations. Note: You cannot enable or disable database clustering after the Active-Active database is created. Click Activate to create your Active-Active database.\nNote: \u0026lt;p\u0026gt;If you cannot activate the database because of a memory limitation, make sure that Docker has enough memory allocated in the Docker Settings.\u0026lt;/p\u0026gt; After the Active-Active database is created, access the RS admin console of cluster 1 at https://localhost:8443 and of cluster 2 at https://localhost:8445.\nMake sure that each cluster has an Active-Active database member database with the name database1.\nIn a real-world deployment, cluster 1 and cluster 2 would most likely be in separate data centers in different regions. However, for local testing we created the scale-minimized deployment using two local clusters running on the same host.\nStep 4: Test the connection to your member Redis Active-Active databases With the Redis database created, you are ready to connect to your database. See Connect to Active-Active databases for tutorials and examples of multiple connection methods.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/install/","uriRel":"/modules/redisgears/jvm/install/","title":"Install RedisGears and the JVM plugin","tags":[],"keywords":[],"description":"","content":"Before you can use RedisGears with the JVM, you need to install the RedisGears module and JVM plugin on your Redis Enterprise cluster and enable them for a database.\nPrerequisites Redis Enterprise v6.0.12 or later\nCreated a Redis Enterprise cluster\nAdded nodes to the cluster\nInstalled RedisGears and the JVM plugin\nEnable RedisGears for a database From the Redis Enterprise admin console\u0026rsquo;s databases page, select the Add button to create a new database:\nConfirm that you want to create a new Redis database with the Next button.\nOn the create database page, give your database a name.\nFor Redis Modules, select the Add button and choose RedisGears from the Module dropdown list.\nSelect Add Configuration, enter Plugin gears_jvm in the box, then select the OK button:\nNote: You can configure additional JVM options in this box. For example:\nPlugin gears_jvm JvmOptions '-Dproperty1=value1 -Dproperty2=value2' Select the Activate button.\nVerify the install Run the RG.JSTATS command from a database shard to view statistics and verify that you set up RedisGears and the JVM plugin correctly:\n$ shard-cli 3 172.16.0.1:12345\u0026gt; RG.JSTATS ","categories":["Modules"]},{"uri":"/modules/redisgears/python/install/","uriRel":"/modules/redisgears/python/install/","title":"Install RedisGears and the Python plugin","tags":[],"keywords":[],"description":"","content":"Before you can use RedisGears with Python, you need to install the RedisGears module and Python plugin on your Redis Enterprise cluster and enable them for a database.\nPrerequisites Redis Enterprise v6.0.12 or later\nCreated a Redis Enterprise cluster\nAdded nodes to the cluster\nInstalled RedisGears and the Python plugin\nEnable RedisGears for a database From the Redis Enterprise admin console\u0026rsquo;s databases page, select the Add button to create a new database:\nConfirm that you want to create a new Redis database with the Next button.\nOn the create database page, give your database a name.\nFor Redis Modules, select the Add button and choose RedisGears from the Module dropdown list.\nSelect Add Configuration, enter Plugin gears_python CreateVenv 1 in the box, then select the OK button:\nNote: Only RedisGears v1.2 and later require this configuration. Select the Activate button.\nVerify the install Run the RG.PYSTATS command to view statistics and verify that you set up RedisGears and the Python plugin correctly:\nredis\u0026gt; RG.PYSTATS ","categories":["Modules"]},{"uri":"/kubernetes/re-databases/ingress_routing_with_istio/","uriRel":"/kubernetes/re-databases/ingress_routing_with_istio/","title":"Configure Istio for external routing","tags":[],"keywords":[],"description":"Configure Istio as an ingress controller for access to your Redis Enterprise databases from outside the Kubernetes cluster.","content":"Redis Enterprise for Kubernetes version 6.2.8-11 introduces the ability to use an Istio ingress gateway as an alternative to NGINX or HaProxy ingress controllers.\nIstio can also understand ingress resources, but using that mechanism takes away the advantages and options that the native Istio resources provide. Istio offers its own configuration methods using custom resources.\nTo configure Istio to work with the Redis Kubernetes operator, we will use two custom resources: a Gateway and a VirtualService. Then you\u0026rsquo;ll be able to establish external access to your database.\nInstall and configure Istio for Redis Enterprise Download and install Istio (see instructions from Istio\u0026rsquo;s Getting Started guide).\nOnce the installation is complete, all the deployments, pods, and services will be deployed in a namespace called istio-system. This namespace contains a LoadBalancer type service called service/istio-ingressgateway that exposes the external IP address.\nFind the EXTERNAL-IP for the istio-ingressgateway service.\nkubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.34.67.89 10.145.78.91 15021:12345/TCP,80:67891/TCP,443:23456/TCP,31400:78901/TCP,15443:10112/TCP 3h8m Create a DNS entry that resolves your chosen database hostname (or a wildcard * followed by your domain) to the Istio EXTERNAL-IP. Use this hostname to access your database from outside the cluster.\nIn this example, any hostname that ends with .istio.k8s.my.redisdemo.com will resolve to the Istio LoadBalancer\u0026rsquo;s external IP of 10.145.78.91. Substitute your own values accordingly.\nVerify the record was created successfully.\ndig api.istio.k8s.my.redisdemo.com Look in the ANSWER SECTION for the record you just created.\n;; ANSWER SECTION: api.istio.k8s.my.redisdemo.com. 0 IN A 10.145.78.91 Create custom resources Gateway custom resource On a different namespace from istio-system, create a Gateway custom resource file (redis-gateway.yaml).\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: redis-gateway spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*.istio.k8s.my.redisdemo.com\u0026#39; port: name: https number: 443 protocol: HTTPS tls: mode: PASSTHROUGH Replace .istio.k8s.my.redisdemo.com with the domain that matches your DNS record. TLS passthrough mode is required to allow secure access to the database. Apply gateway.yaml to create the ingress gateway.\nkubectl apply -f gateway.yaml Verify the gateway was created successfully.\nkubectl get gateway NAME AGE redis-gateway 3h33m VirtualService custom resource On a different namespace than istio-system, create the VirtualService custom resource file (redis-vs.yaml).\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: redis-vs spec: gateways: - redis-gateway hosts: - \u0026#34;*.istio.k8s.my.redisdemo.com\u0026#34; tls: - match: - port: 443 sniHosts: - api.istio.k8s.my.redisdemo.com route: - destination: host: rec1 port: number: 9443 - match: - port: 443 sniHosts: - db1.istio.k8s..my.redisdemo.com route: - destination: host: db1 This creates both a route to contact the API server on the REC (rec1) and a route to contact one of the databases (db1).\nReplace .istio.k8s.my.redisdemo.com with the domain that matches your DNS record. The gateway\u0026rsquo;s metadata name must be similar to the gateway\u0026rsquo;s spec name (redis-gateway in this example). When creating an Active-Active database with the crdb-cli command, Apply redis-vs.yaml to create the virtual service.\nkubectl apply -f redis-vs.yaml Verify the virtual service was created successfully.\nkubectl get vs NAME GATEWAYS HOSTS AGE redis-vs [\u0026#34;redis-gateway\u0026#34;] [\u0026#34;*.istio.k8s.my.redisdemo.com\u0026#34;] 3h33m Deploy the operator, Redis Enterprise Cluster (REC), and Redis Enterprise Database (REDB) on the same namespace as the gateway and virtual service.\nTest your external access to the database To test your external access to the database, you need a client that supports TLS and SNI.\nSee Test your access with Openssl or Test your access with Python for more info.\n","categories":["Platforms"]},{"uri":"/rs/clusters/configure/license-keys/","uriRel":"/rs/clusters/configure/license-keys/","title":"Cluster license keys","tags":[],"keywords":[],"description":"The cluster key (or license) enables features and capacity within Redis Enterprise Software","content":"The cluster key (or license) enables features and capacity within Redis Enterprise Software (RS). You can add or update a cluster key at any time in a cluster lifecycle. When the cluster does not have a cluster key, the cluster is in trial mode.\nTrial mode Trial mode is limited to thirty days and a total of four shards, including master and replica shards. Any new installation starts its thirty-day clock from the day the cluster setup was done (with the first cluster node provisioned). This mode allows all features to be enabled, including Redis on Flash, during the trial period.\nViewing the cluster key You can see the cluster key either:\nadmin console - Go to: settings \u0026gt; general\nThe cluster key string is shown.\nREST API - GET https://localhost:9443/v1/license\nThe REST API response includes:\nlicense - The cluster name (FQDN) in the key string expired - If the cluster key is expired (True or False) activation_date - The date of the cluster key activation expiration_date - The date of the cluster key expiration shards_limit - The number of shards allowed by the cluster key Adding or updating a cluster key Note: After you add a cluster key, you cannot remove the key to return the cluster to trial mode. You can add a cluster key to the cluster either:\nDuring cluster setup using the admin console or CLI After cluster setup using the admin console - Go to settings \u0026gt; general, paste your cluster key into the cluster key field, and click the Save button. An existing cluster key can be updated at any time provided the new cluster license key is valid. If saving of a new cluster key fails, the operation returns the error \u0026ldquo;invalid license key\u0026rdquo;. In this case, the existing key stays in effect.\nExpired cluster key When the license is expired:\nYou cannot do these actions:\nChange database settings including security and configuration options Add/remove a database Upgrade a database to a new version Add/remove a node You can do these actions:\nLogin to the admin console and view settings and metrics at all resolutions for the cluster, nodes and databases Change cluster settings including license key, security for administrators, and cluster alerts Failover when a node fails and explicitly migrate shard between nodes Upgrade node to a new RS version ","categories":["RS"]},{"uri":"/rc/api/get-started/manage-api-keys/","uriRel":"/rc/api/get-started/manage-api-keys/","title":"Manage API keys","tags":[],"keywords":[],"description":"How to use the Redis Cloud admin console to create and manage API user keys for your account&#39;s team owners","content":"Every REST API request must include the following API keys:\nThe Account key identifies the account associated with the Redis Enterprise Cloud subscription.\nThe User key identifies the user and (optionally) the context of a request. Generated by account owners.\nUse the API Keys tab of the Access Management screen to manage your keys:\nSign in to your Redis Cloud subscription as an account owner.\nFrom the menu, choose Access Management and then select the API Keys tab.\nIf an Enable API button appears, select it to enable the REST API for your account.\nAPI account key The API account key is used as the value of the x-api-key HTTP header in order to authenticate a REST API request.\nBy default, the API account key is masked; that is, it is obscured for security reasons. You can use the Show button to display the key and the Hide button to mask it.\nThe Copy button copies the account key to the Clipboard.\nAPI user keys API user keys (also known as secret keys) are used as the value of the x-api-secret-key HTTP header used to authenticate a REST API request.\nIn this context, user refers to the account used to sign in to the admin console. Users must be account owners.\nUsers can have more than one user key; however, users should not share user keys.\nCreate a new user key Use the Add button to create a new user key.\nWhen you do this, you\u0026rsquo;re prompted for the Key name and the associated User name.\nThe key name:\nMust be between 10 and 50 characters long Can contain alphanumeric characters, hyphens, and underscores. Spaces are not allowed. The user name must be an account owner.\nSelect Create to create the new key.\nWhen you do this, the API user key dialog appears.\nWarning - This is the only time the value of the user key is available. Save it to a secure location before closing the dialog box. If you lose the user key value, it cannot be retrieved. If this happens, create a new key to replace the lost one. When you\u0026rsquo;ve saved the user key, use the Finish button to close the dialog box.\n(The Finish button is disabled until you copy the key to the Clipboard.)\nDelete a user key To delete a user key:\nUse the API Keys tab of the Access Management screen to locate the target key\nSelect the Delete button displayed to the right.\nThis displays the Delete API secret key dialog box.\nSelect the Delete button to confirm.\nManage CIDR allow list By default, REST API requests are allowed from all IP addresses. To limit access to specific addresses, define a CIDR allow list for the user key.\nTo manage the CIDR allow list:\nUse the API Keys tab of the Access Management screen to locate the target key\nSelect the Manage link in the CIDR allow list column; this displays the Manage CIDR allow list dialog box.\nEnter each allowed IP address in CIDR format (example: 127.1.0.0/24) and then select the Save button.\nUse the Add CIDR rule button to add additional addresses to the list.\nUse the Edit button to change the address for a rule or the Delete button to remove a rule.\n","categories":["RC"]},{"uri":"/rs/security/access-control/manage-users/","uriRel":"/rs/security/access-control/manage-users/","title":"Manage users","tags":[],"keywords":[],"description":"Manage users and user security.","content":"Redis Enterprise supports the following user account security settings:\nPassword complexity Password expiration User lockouts Account inactivity timeout Manage users and user security Add users Add users to the cluster and assign access control roles (ACLs) to them.\nManage passwords Manage user passwords.\nManage user login Manage user login lockout and session timeout.\nDeactivate default user Deactivate a database\u0026#39;s default user.\n","categories":["RS"]},{"uri":"/rs/databases/memory-performance/memory-limit/","uriRel":"/rs/databases/memory-performance/memory-limit/","title":"Database memory limits","tags":[],"keywords":[],"description":"When you set a database&#39;s memory limit, you define the maximum size the database can reach.","content":"When you set a database\u0026rsquo;s memory limit, you define the maximum size the database can reach in the cluster, across all database replicas and shards, including both primary and replica shards.\nIf the total size of the database in the cluster reaches the memory limit, the data eviction policy is applied.\nFactors for sizing Factors to consider when sizing your database:\ndataset size: you want your limit to be above your dataset size to leave room for overhead. database throughput: high throughput needs more shards, leading to a higher memory limit. modules: using modules with your database consumes more memory. database clustering: enables you to spread your data into shards across multiple nodes. database replication: enabling replication doubles memory consumption. Additional factors for Active-Active databases:\nActive-Active replication: enabling Active-Active replication requires double the memory of regular replication, which can be up to two times (2x) the original data size per instance.\ndatabase replication backlog for synchronization between shards. By default, this is set to 1% of the database size.\nActive-Active replication backlog for synchronization between clusters. By default, this is set to 1% of the database size.\nIt\u0026rsquo;s also important to know Active-Active databases have a lower threshold for activating the eviction policy, because it requires propagation to all participating clusters. The eviction policy starts to evict keys when one of the Active-Active instances reaches 80% of its memory limit.\nAdditional factors for Redis on Flash databases:\ndatabase persistence: Redis on Flash uses dual database persistence where both the primary and replica shards persist to disk. This may add some processor and network overhead, especially in cloud configurations with network attached storage. What happens when Redis Enterprise Software is low on RAM? Redis Enterprise Software manages node memory so that data is entirely in RAM (unless using Redis on Flash). If not enough RAM is available, Redis Enterprise prevents adding more data into the databases.\nRedis Enterprise Software protects the existing data and prevents the database from being able to store data into the shards.\nYou can configure the cluster to move the data to another node, or even discard it according to the eviction policy set on each database by the administrator.\nRedis on Flash manages memory so that you can also use flash memory (SSD) to store data.\nOrder of events for low RAM If there are other nodes available, your shards migrate to other nodes. If the eviction policy allows eviction, shards start to release memory, which can result in data loss. If the eviction policy does not allow eviction, you\u0026rsquo;ll receive out of memory (OOM) messages. If shards can\u0026rsquo;t free memory, Redis Enterprise relies on the OS processes to stop replicas, but tries to avoid stopping primary shards. We recommend that you have a monitoring platform that alerts you before a system gets low on RAM. You must maintain sufficient free memory to make sure that you have a healthy Redis Enterprise installation.\nMemory metrics The admin console provides metrics that can help you evaluate your memory use.\nFree RAM RAM fragmentation Used memory Memory usage Memory limit See console metrics for more detailed information.\nRelated info Memory and performance Disk sizing for heavy write scenarios Turn off services to free system memory Eviction policy Shard placement policy Database persistence ","categories":["RS"]},{"uri":"/rs/references/client_references/client_ioredis/","uriRel":"/rs/references/client_references/client_ioredis/","title":"Redis with Node.js (ioredis)","tags":[],"keywords":[],"description":"The ioredis client allows you to use Redis with Node.js.","content":"In order to use Redis with Node.js, you need to install a Node.js Redis client. The following sections demonstrate the use of ioredis, a community-recommended Redis client for Node.js with build-in support for promises.\nAnother community-recommended client for Node.js developers is node_redis. Additional Node.js clients for Redis can be found under the Node.js section of the Redis Clients page.\nInstall ioredis See the ioredis README file for installation instructions.\nTo install ioredis, run:\nnpm install ioredis Connect to Redis This example code creates a connection to Redis:\nconst Redis = require(\u0026#39;ioredis\u0026#39;); const redis = new Redis({ host: \u0026#39;\u0026lt;hostname\u0026gt;\u0026#39;, port: \u0026lt;port\u0026gt;, password: \u0026#39;\u0026lt;password\u0026gt;\u0026#39; }); Replace the values in the example with the values for your Redis instance:\n\u0026lt;hostname\u0026gt; - The name of the host your database runs on \u0026lt;port\u0026gt; - The port that the database is running on (default: 6379) \u0026lt;password\u0026gt; - The default Redis password, if configured Note: Remember to always store passwords outside of your code, for example in environment variables. TLS This example shows how to configure ioredis to make a connection to Redis using TLS:\nconst Redis = require(\u0026#39;ioredis\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const redis = new Redis({ host: \u0026#39;hostname\u0026#39;, port: \u0026lt;port\u0026gt;, tls: { key: fs.readFileSync(\u0026#39;path_to_keyfile\u0026#39;, \u0026#39;ascii\u0026#39;), cert: fs.readFileSync(\u0026#39;path_to_certfile\u0026#39;, \u0026#39;ascii\u0026#39;), ca: [ fs.readFileSync(\u0026#39;path_to_ca_certfile\u0026#39;, \u0026#39;ascii\u0026#39;) ] } }); Where you must provide:\n\u0026lt;hostname\u0026gt; - The name of the host your database runs on \u0026lt;port\u0026gt; - The port that the database is running on (default: 6379) ACL user and password Redis 6 introduced Access Control Lists. ACLs provide the capability to create named user accounts, each having its own password.\nTo connect to Redis as an ACL user, provide the user\u0026rsquo;s username and password when creating the client:\nconst Redis = require(\u0026#39;ioredis\u0026#39;); const redis = new Redis({ host: \u0026#39;hostname\u0026#39;, port: \u0026lt;port\u0026gt;, username: \u0026#39;username\u0026#39;, password: \u0026#39;password\u0026#39; }); Make sure to replace the values in the example with the values for your Redis instance:\n\u0026lt;hostname\u0026gt; - The name of the host your database runs on \u0026lt;port\u0026gt; - The port that the database is running on (default: 6379) \u0026lt;username\u0026gt; - The username of the ACL user \u0026lt;password\u0026gt; - The password of the ACL user Note: ioredis uses the Redis INFO command when it creates a new connection, so your ACL user must have permission to run INFO.\nIf your ACL user does not have the required permissions, connection attempts will fail with a NOPERM error message:\n[ioredis] Unhandled error event: ReplyError: NOPERM this user has no permissions to run the \u0026#39;info\u0026#39; command or its subcommand Example code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet reads the value stored at the key foo and prints it:\n// Open a connection to Redis // \u0026#39;foo\u0026#39; has been set to \u0026#39;bar\u0026#39;. ... client.get(\u0026#39;foo\u0026#39;, (err, reply) =\u0026gt; { if (err) throw err; console.log(reply); }); Example output:\n$ node example_ioredis.js bar The ioredis client exposes a function for each Redis command.\nThe first argument is usually the Redis key to run the command against. You can also add an optional error first callback function after the other arguments.\nIf you do not provide a callback function, the ioredis function returns a promise:\n// Open a connection to Redis // \u0026#39;foo\u0026#39; has been set to \u0026#39;bar\u0026#39;. const redisDemo = async () =\u0026gt; { const reply = await redis.get(\u0026#39;foo\u0026#39;); console.log(reply); }; redisDemo(); Example output:\n$ node example_ioredis.js bar ","categories":["RS"]},{"uri":"/rs/databases/configure/oss-cluster-api/","uriRel":"/rs/databases/configure/oss-cluster-api/","title":"Enable OSS Cluster API","tags":[],"keywords":[],"description":"","content":" Redis OSS Cluster API reduces access times and latency with near-linear scalability. The Redis OSS Cluster API provides a simple mechanism for Redis clients to know the cluster topology.\nClients must first connect to the master node to get the cluster topology, and then they connect directly to the Redis proxy on each node that hosts a master shard.\nNote: You must use a client that supports the OSS cluster API to connect to a database that has the OSS cluster API enabled. Prerequisites The Redis OSS Cluster API is supported only when a database meets specific criteria.\nThe database must:\nUse the standard hashing policy. Have the proxy policy set to either all-master-shards or all-nodes. In addition, the database must not:\nUse node include or exclude in the proxy policy. Use RediSearch, RedisTimeSeries, or RedisGears modules. The OSS Cluster API setting applies to individual databases, as opposed to the overall cluster.\nEnable OSS Cluster API support You can use the admin console or the rladmin utility to enable OSS Cluster API support for a database.\nAdmin console To enable the OSS Cluster API from the admin console for an existing database:\nSign in to the admin console and then select the target database.\nWhen the database details appear, select the Configuration tab.\nThe OSS Cluster API support setting shows the current setting.\nSelect the Edit button.\nPlace a checkmark in the OSS Cluster API support setting and then select Update.\nYou can also use the admin console to enable the setting when creating a new database.\nTo do so, select Advanced options while creating the database in order to display the OSS Cluster API support setting. Place a checkmark in the setting to enable the API when the database is created.\nCommand line (rladmin) You can use the rladmin utility to enable the OSS Cluster API for Redis Enterprise Software databases, including Replica Of (Active-Passive) databases.\nFor Active-Active (CRDB) databases, use the crdb-cli utility.\nTo enable the OSS Cluster API for a Redis database from the command line:\n$ rladmin tune db \u0026lt;database name or ID\u0026gt; oss_cluster enabled To determine the current setting for a database from the command line, use rladmin info db to return the value of the oss_cluster setting.\n$ rladmin info db test | grep oss_cluster: oss_cluster: enabled The Redis OSS Cluster API setting applies to the specified database only; it does not apply to the cluster.\nActive-Active databases Use the crdb-cli utility to enable the OSS Cluster API for Active-Active databases:\n$ crdb-cli crdb update --crdb-guid \u0026lt;GUID\u0026gt; --oss-cluster true For best results, you should do this when you first create the database.\nHere\u0026rsquo;s the basic process:\nCreate the Active-Active database:\n$ crdb-cli crdb create --name \u0026lt;name\u0026gt; \\ --memory-size 10g --port \u0026lt;port\u0026gt; \\ --sharding true --shards-count 2 \\ --replication true --oss-cluster true \\ --instance fqdn=\u0026lt;fqdn\u0026gt;,username=\u0026lt;user\u0026gt;,password=\u0026lt;pass\u0026gt; \\ --instance fqdn=\u0026lt;fqdn\u0026gt;,username=\u0026lt;user\u0026gt;,password=\u0026lt;pass\u0026gt; \\ --instance fqdn=\u0026lt;fqdn\u0026gt;,username=\u0026lt;user\u0026gt;,password=\u0026lt;pass\u0026gt; Obtain the CRDB-GUID ID for the new database:\n$ crdb-cli crdb list CRDB-GUID NAME REPL-ID CLUSTER-FQDN \u0026lt;CRDB-GUID\u0026gt; Test 4 cluster1.local Use the CRDB-GUID ID to enable the OSS Cluster API:\n$ crdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; \\ --oss-cluster true The Redis OSS Cluster API setting applies to all of the instances of the Active-Active database.\nTurn off OSS Cluster API support To deactivate OSS Cluster API support for a database, either:\nUse the admin console to remove the checkmark from the database configuration settings.\nUse the appropriate utility to deactivate the OSS cluster setting.\nFor standard databases, including Replica Of (Active-Passive), use rladmin:\n$ rladmin tune db \u0026lt;Name or ID\u0026gt; oss_cluster disable For Active-Active databases, use crdb-cli:\n$ crdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; \\ --oss-cluster false Multi-key command support When you enable the Redis OSS Cluster API for a database, multi-key commands are only allowed when all keys are mapped to the same slot.\nTo verify that your database meets this requirement, make sure that the CLUSTER KEYSLOT reply is the same for all keys affected by the multi-key command. To learn more, see multi-key operations.\n","categories":["RS"]},{"uri":"/modules/redisai/redisai-quickstart/","uriRel":"/modules/redisai/redisai-quickstart/","title":"RedisAI quick start","tags":[],"keywords":[],"description":"RedisAI quick start","content":"For this quick start tutorial, we recommend that you use:\nEither: A Redis Enterprise Software database with the RedisAI module Contact support if you want to try out RedisAI on Redis Cloud redis-cli with connectivity to a Redis database Getting started You can connect to RedisAI using any Redis client. Better yet, some languages already have client implementations for RedisAI - the list can be found at the Clients page. RedisAI clients wrap the core API and simplify the interaction with the module.\nWe\u0026rsquo;ll begin by using the official redis-cli Redis client. If you have it locally installed feel free to use that, but it is also available from the container:\nredis-cli Using RedisAI tensors A tensor is an n-dimensional array and is the standard representation for data in DL/ML workloads. RedisAI adds to Redis a Tensor data structure that implements the tensor type. Like any datum in Redis, RedisAI\u0026rsquo;s Tensors are identified by key names.\nCreating new RedisAI tensors is done with the AI.TENSORSET command. For example, consider the tensor:\nWe can create the RedisAI Tensor with the key name \u0026rsquo;tA\u0026rsquo; with the following command:\nAI.TENSORSET tA FLOAT 2 VALUES 2 3 Copy the command to your cli and hit the \u0026lt;ENTER\u0026gt; on your keyboard to execute it. It should look as follows:\n$ redis-cli 127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK The reply \u0026lsquo;OK\u0026rsquo; means that the operation was successful. We\u0026rsquo;ve called the AI.TENSORSET command to set the key named \u0026rsquo;tA\u0026rsquo; with the tensor\u0026rsquo;s data, but the name could have been any string value. The FLOAT argument specifies the type of values that the tensor stores, and in this case a single-precision floating-point. After the type argument comes the tensor\u0026rsquo;s shape as a list of its dimensions, or just a single dimension of 2.\nThe VALUES argument tells RedisAI that the tensor\u0026rsquo;s data will be given as a sequence of numeric values and in this case the numbers 2 and 3. This is useful for development purposes and creating small tensors, however for practical purposes the AI.TENSORSET command also supports importing data in binary format.\nThe Redis key \u0026rsquo;tA\u0026rsquo; now stores a RedisAI Tensor. We can verify that using standard Redis commands such as EXISTS and TYPE:\n127.0.0.1:6379\u0026gt; EXISTS tA (integer) 1 127.0.0.1:6379\u0026gt; TYPE tA AI_TENSOR Using AI.TENSORSET with the same key name, as long as it already stores a RedisAI Tensor, will overwrite the existing data with the new. To delete a RedisAI tensor, use the Redis DEL command.\nRedisAI Tensors are used as inputs and outputs in the execution of models and scripts. For reading the data from a RedisAI Tensor value there is the AI.TENSORGET command:\n127.0.0.1:6379\u0026gt; AI.TENSORGET tA VALUES 1) INT8 2) 1) (integer) 2 3) 1) (integer) 2 1) (integer) 3 Loading models A Model is a Deep Learning or Machine Learning frozen graph that was generated by some framework. The RedisAI Model data structure represents a DL/ML model that is stored in the database and can be run.\nModels, like any other Redis and RedisAI data structures, are identified by keys. A Model\u0026rsquo;s key is created using the AI.MODELSET command and requires the graph payload serialized as protobuf for input.\nIn our examples, we\u0026rsquo;ll use one of the graphs that RedisAI uses in its tests, namely \u0026lsquo;graph.pb\u0026rsquo;, which can be downloaded from here. This graph was created using TensorFlow with this script.\nUse a web browser or the command line to download \u0026lsquo;graph.pb\u0026rsquo;:\nwget https://github.com/RedisAI/RedisAI/raw/master/test/test_data/graph.pb You can view the computation graph using Netron, which supports all frameworks supported by RedisAI.\nThis is a great way to inspect a graph and find out node names for inputs and outputs.\nredis-cli doesn\u0026rsquo;t provide a way to read files\u0026rsquo; contents, so to load the model with it we\u0026rsquo;ll use the command line and output pipes:\n$ cat graph.pb | redis-cli -x \\ AI.MODELSET mymodel TF CPU INPUTS a b OUTPUTS c OK Note: For practical purposes, you are encouraged to use a programmatic Redis or RedisAI client in the language of your choice for interacting with RedisAI. Refer to the following pages for further information:\nRedis clients page RedisAI clients page Like most commands, AI.MODELSET\u0026rsquo;s first argument is a key\u0026rsquo;s name, which is \u0026lsquo;mymodel\u0026rsquo; in the example. The next two arguments are the model\u0026rsquo;s DL/ML backend and the device it will be executed on. \u0026lsquo;graph.pb\u0026rsquo; in the example is a TensorFlow graph and is denoted by TF argument. The model will be executed on the CPU as instructed by the CPU argument.\nTensorFlow models also require declaring the names of their inputs and outputs. The inputs for \u0026lsquo;graph.pb\u0026rsquo; are called \u0026lsquo;a\u0026rsquo; and \u0026lsquo;b\u0026rsquo;, whereas its single output is called \u0026lsquo;c\u0026rsquo;. These names are provided as additional arguments after the \u0026lsquo;INPUTS\u0026rsquo; and \u0026lsquo;OUTPUTS\u0026rsquo; arguments, respectively.\nRunning models Once a RedisAI Model key has been set with AI.MODELSET it can be run with any Tensor keys from the database as its input. The model\u0026rsquo;s output, after it was executed, is stored in RedisAI Tensors as well.\nThe model stored at \u0026lsquo;mymodel\u0026rsquo; expects two input tensors so we\u0026rsquo;ll use the previously-created \u0026rsquo;tA\u0026rsquo; and create another input tensor:\nwith the following command:\nAI.TENSORSET tB FLOAT 2 VALUES 3 5 The model can now be run with the AI.MODELRUN command as follows:\nAI.MODELRUN mymodel INPUTS tA tB OUTPUTS tResult For example:\n127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK 127.0.0.1:6379\u0026gt; AI.TENSORSET tB FLOAT 2 VALUES 3 5 OK 127.0.0.1:6379\u0026gt; AI.MODELRUN mymodel INPUTS tA tB OUTPUTS tModel OK The first argument to AI.MODELRUN is the name of the key at which the RedisAI Model is stored. The names of RedisAI Tensor keys that follow the INPUTS argument are used as input for the model. Similarly, following the OUTPUT argument are the key names of RedisAI Tensors that the model outputs.\nThe inputs for the example are the tensors stored under the \u0026rsquo;tA\u0026rsquo; and \u0026rsquo;tB\u0026rsquo; keys. Once the model\u0026rsquo;s run had finished, a new RedisAI Tensor key called \u0026rsquo;tResult\u0026rsquo; is created and stores the model\u0026rsquo;s output.\nFor example:\n127.0.0.1:6379\u0026gt; AI.TENSORGET tModel VALUES 1) FLOAT 2) 1) (integer) 2 3) 1) \u0026#34;6\u0026#34; 2) \u0026#34;15\u0026#34; The model we\u0026rsquo;ve imported from \u0026lsquo;graph.pb\u0026rsquo; takes two input tensors as input and outputs a tensor that is the product of multiplying them. In the case of the example above it looks like this:\nModel management The AI.MODELGET command can be used for retrieving information about a model and its serialized blob. The AI.INFO command shows runtime statistics about the model\u0026rsquo;s runs. Lastly, RedisAI Model keys can be deleted with the AI.MODELDEL command.\nScripting RedisAI makes it possible to run TorchScript with the PyTorch backend. Scripts are useful for performing pre- and post-processing operations on tensors.\nThe RedisAI Script data structure is managed via a set of dedicated commands, similarly to the models. A RedisAI Script key is:\nCreated with the AI.SCRIPTSET command Run with the AI.SCRIPTRUN command Deleted with the AI.SCRIPTSEL command We can create a RedisAI Script that performs the same computation as the \u0026lsquo;graph.pb\u0026rsquo; model. The script can look like this:\ndef multiply(a, b): return a * b Assuming that the script is stored in the \u0026lsquo;myscript.py\u0026rsquo; file it can be uploaded via command line and the AI.SCRIPTSET command as follows:\ncat myscript.py | redis-cli -x AI.SCRIPTSET myscript CPU This will store the PyTorch Script from \u0026lsquo;myscript.py\u0026rsquo; under the \u0026lsquo;myscript\u0026rsquo; key and will associate it with the CPU device for execution. Once loaded, the script can be run with the following:\nAI.SCRIPTRUN myscript multiply INPUTS tA tB OUTPUTS tScript For example:\n127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK 127.0.0.1:6379\u0026gt; AI.TENSORSET tB FLOAT 2 VALUES 3 5 OK 127.0.0.1:6379\u0026gt; AI.SCRIPTRUN myscript multiply INPUTS tA tB OUTPUTS tScript OK 127.0.0.1:6379\u0026gt; AI.TENSORGET tScript VALUES 1) FLOAT 2) 1) (integer) 2 3) 1) \u0026#34;6\u0026#34; 2) \u0026#34;15\u0026#34; ","categories":["Modules"]},{"uri":"/modules/redisbloom/redisbloom-quickstart/","uriRel":"/modules/redisbloom/redisbloom-quickstart/","title":"RedisBloom quick start","tags":[],"keywords":[],"description":"RedisBloom quick start","content":"For this quick start tutorial, you need:\nEither: A Redis Cloud database with the RedisBloom module\nYou can set up a free Redis Cloud database to see the module in action.\nA Redis Enterprise Software database with the RedisBloom module\nredis-cli with connectivity to a Redis database Trying it out You can play with it a bit using redis-cli:\nConnect to redis.\n$ redis-cli -p 12543 127.0.0.1:12543\u0026gt; Run these commands:\n127.0.0.1:12543\u0026gt; BF.ADD bloom kirk 1) (integer) 1 127.0.0.1:12543\u0026gt; BF.ADD bloom redis 1) (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom kirk (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom redis (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom nonexist (integer) 0 127.0.0.1:12543\u0026gt; BF.EXISTS bloom que? (integer) 0 127.0.0.1:12543\u0026gt; 127.0.0.1:12543\u0026gt; BF.MADD bloom elem1 elem2 elem3 1) (integer) 1 2) (integer) 1 3) (integer) 1 127.0.0.1:12543\u0026gt; BF.MEXISTS bloom elem1 elem2 elem3 1) (integer) 1 2) (integer) 1 3) (integer) 1 You can also create a custom Bloom filter. The BF.ADD command creates a new Bloom filter suitable for a small-ish number of items. This consumes less memory but may not be ideal for large filters. In that case:\n127.0.0.1:12543\u0026gt; BF.RESERVE largebloom 0.0001 1000000 OK 127.0.0.1:12543\u0026gt; BF.ADD largebloom kirk 1) (integer) 1 Using Cuckoo filters in Redis Enterprise Software Cuckoo filters can also be used as part of the RedisBloom module. You can play with it using redis-cli:\n127.0.0.1:12543\u0026gt; CF.ADD newcuckoo redis (integer) 1 127.0.0.1:12543\u0026gt; CF.EXISTS newcuckoo redis (integer) 1 127.0.0.1:12543\u0026gt; CF.EXISTS newcuckoo notpresent (integer) 0 127.0.0.1:12543\u0026gt; CF.DEL newcuckoo redis (integer) 1 Debugging Bloom filters Finally, I added a BF.DEBUG command, to see exactly how the filter is being utilized:\n127.0.0.1:6379\u0026gt; BF.DEBUG test 1) \u0026#34;size:987949\u0026#34; 2) \u0026#34;bytes:239627 bits:1917011 hashes:14 capacity:100000 size:100000 ratio:0.0001\u0026#34; 3) \u0026#34;bytes:551388 bits:4411101 hashes:16 capacity:200000 size:200000 ratio:2.5e-05\u0026#34; 4) \u0026#34;bytes:1319180 bits:10553436 hashes:19 capacity:400000 size:400000 ratio:3.125e-06\u0026#34; 5) \u0026#34;bytes:3215438 bits:25723497 hashes:23 capacity:800000 size:287949 ratio:1.95313e-07\u0026#34; This outputs the total number of elements as the first result, and then a list of details for each filter in the chain. As you can see, whenever a new filter is added, its capacity grows exponentially and the strictness for errors increases.\nNote that this filter chain also uses a total of 5MB. This is still much more space efficient than alternative solutions, since we\u0026rsquo;re still at about 5 bytes per element, and the uppermost filter is only at about 12% utilization. Had the initial capacity been greater, more space would have been saved and lookups would have been quicker.\nMore info RedisBloom commands Develop with Bloom filters Original cuckoo filter paper ","categories":["Modules"]},{"uri":"/modules/redisearch/redisearch-quickstart/","uriRel":"/modules/redisearch/redisearch-quickstart/","title":"RediSearch quick start","tags":[],"keywords":[],"description":"RediSearch quick start on how to index hash documents and run search queries.","content":"This quick start shows you how to index documents stored as Redis hashes and run search queries against the index. To learn how to index and search JSON documents, see the Search JSON quick start.\nPrerequisites For this tutorial, you need:\nA Redis database with the RediSearch module enabled. You can use either:\nA Redis Cloud database\nA Redis Enterprise Software database\nredis-cli command-line tool\nredis-py client library v4.0.0 or later\nRediSearch with redis-cli To begin, connect to your database with redis-cli.\nCreate an index The FT.CREATE command lets you create an index. It indexes Redis hashes by default. However, as of RediSearch v2.2, you can also index JSON documents if you have the RedisJSON module enabled.\nTo set the language used for indexing and stemming, include the optional LANGUAGE keyword. If you do not specify a language, it defaults to English. See Supported languages for additional language options.\nWhen you define an index, you also need to define the schema, or structure, of the data you want to add to the index.\nThe schema for this example has four fields:\ntitle (TEXT) body (TEXT) url (TEXT) visits (NUMERIC) To define the schema and create the index, run:\n127.0.0.1:12543\u0026gt; FT.CREATE database_idx PREFIX 1 \u0026#34;doc:\u0026#34; LANGUAGE english SCORE 0.5 SCORE_FIELD \u0026#34;doc_score\u0026#34; SCHEMA title TEXT body TEXT url TEXT visits NUMERIC This command indexes all hashes with the prefix \u0026ldquo;doc:\u0026rdquo;. It will also index any future hashes that have this prefix.\nBy default, all documents have a score of 1.0. However, you can configure an index\u0026rsquo;s default document score with the SCORE \u0026lt;default_score\u0026gt; option.\nIf you want to allow document-specific scores, use the SCORE_FIELD \u0026lt;score_attribute\u0026gt; option.\nAdd documents After you create an index, you can add documents to the index.\nUse the HSET command to add a hash with the key \u0026ldquo;doc:1\u0026rdquo; and the fields:\ntitle: \u0026ldquo;RediSearch\u0026rdquo; body: \u0026ldquo;RediSearch is a powerful indexing, querying, and full-text search engine for Redis\u0026rdquo; url: https://redis.com/modules/redis-search/ visits: 108 127.0.0.1:12543\u0026gt; HSET doc:1 title \u0026#34;RediSearch\u0026#34; body \u0026#34;RediSearch is a powerful indexing, querying, and full-text search engine for Redis\u0026#34; url \u0026#34;\u0026lt;https://redis.com/modules/redis-search/\u0026gt;\u0026#34; visits 108 (integer) 4 If you want to make a document appear higher or lower in the search results, add a document-specific score.\nTo do so, use the score_attribute set by the SCORE_FIELD option during index creation (score_attribute is doc_score in this case):\n127.0.0.1:12543\u0026gt; HSET doc:2 title \u0026#34;Redis\u0026#34; body \u0026#34;Modules\u0026#34; url \u0026#34;\u0026lt;https://redis.com/modules\u0026gt;\u0026#34; visits 102 doc_score 0.8 (integer) 5 Search an index Use FT.SEARCH to search the index for any documents that contain the words \u0026ldquo;search engine\u0026rdquo;:\n127.0.0.1:12543\u0026gt; FT.SEARCH database_idx \u0026#34;search engine\u0026#34; LIMIT 0 10 1) (integer) 1 2) \u0026#34;doc:1\u0026#34; 3) 1) \u0026#34;title\u0026#34; 2) \u0026#34;RediSearch\u0026#34; 3) \u0026#34;body\u0026#34; 4) \u0026#34;RediSearch is a powerful indexing, querying, and full-text search engine for Redis\u0026#34; 5) \u0026#34;url\u0026#34; 6) \u0026#34;\u0026lt;https://redis.com/modules/redis-search/\u0026gt;\u0026#34; 7) \u0026#34;visits\u0026#34; 8) \u0026#34;108\u0026#34; Drop an index To remove the index without deleting any associated documents, run the FT.DROPINDEX command:\n127.0.0.1:12543\u0026gt; FT.DROPINDEX database_idx OK Auto-complete You can use RediSearch suggestion commands to implement auto-complete.\nNote: Active-Active databases do not support RediSearch suggestions. Use FT.SUGADD to add a phrase for the search engine to suggest during auto-completion:\n127.0.0.1:12543\u0026gt; FT.SUGADD autocomplete \u0026#34;primary and caching\u0026#34; 100 \u0026#34;(integer)\u0026#34; 1 Test auto-complete suggestions with FT.SUGGET:\n127.0.0.1:12543\u0026gt; FT.SUGGET autocomplete \u0026#34;pri\u0026#34; 1) \u0026#34;primary and caching\u0026#34; RediSearch with Python If you want to use RediSearch within an application, these client libraries are available.\nThe following example uses the Redis Python client library redis-py, which supports RediSearch commands as of v4.0.0.\nThis Python code creates an index, adds documents to the index, displays search results, and then deletes the index:\nimport redis from redis.commands.search.field import TextField, NumericField from redis.commands.search.indexDefinition import IndexDefinition from redis.commands.search.query import Query # Connect to a database r = redis.Redis(host=\u0026#34;\u0026lt;endpoint\u0026gt;\u0026#34;, port=\u0026#34;\u0026lt;port\u0026gt;\u0026#34;, password=\u0026#34;\u0026lt;password\u0026gt;\u0026#34;) # Options for index creation index_def = IndexDefinition( prefix = [\u0026#34;py_doc:\u0026#34;], score = 0.5, score_field = \u0026#34;doc_score\u0026#34; ) # Schema definition schema = ( TextField(\u0026#34;title\u0026#34;), TextField(\u0026#34;body\u0026#34;), TextField(\u0026#34;url\u0026#34;), NumericField(\u0026#34;visits\u0026#34;) ) # Create an index and pass in the schema r.ft(\u0026#34;py_idx\u0026#34;).create_index(schema, definition = index_def) # A dictionary that represents a document doc1 = { \u0026#34;title\u0026#34;: \u0026#34;RediSearch\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;RediSearch is a powerful indexing, querying, and full-text search engine for Redis\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;\u0026lt;https://redis.com/modules/redis-search/\u0026gt;\u0026#34;, \u0026#34;visits\u0026#34;: 108 } doc2 = { \u0026#34;title\u0026#34;: \u0026#34;Redis\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;Modules\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;\u0026lt;https://redis.com/modules\u0026gt;\u0026#34;, \u0026#34;visits\u0026#34;: 102, \u0026#34;doc_score\u0026#34;: 0.8 } # Add documents to the database and index them r.hset(\u0026#34;py_doc:1\u0026#34;, mapping = doc1) r.hset(\u0026#34;py_doc:2\u0026#34;, mapping = doc2) # Search the index for a string; paging limits the search results to 10 result = r.ft(\u0026#34;py_idx\u0026#34;).search(Query(\u0026#34;search engine\u0026#34;).paging(0, 10)) # The result has the total number of search results and a list of documents print(result.total) print(result.docs) # Delete the index; set delete_documents to True to delete indexed documents as well r.ft(\u0026#34;py_idx\u0026#34;).dropindex(delete_documents=False) Example output:\n$ python3 quick_start.py 1 [Document {\u0026#39;id\u0026#39;: \u0026#39;py_doc:1\u0026#39;, \u0026#39;payload\u0026#39;: None, \u0026#39;visits\u0026#39;: \u0026#39;108\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;RediSearch\u0026#39;, \u0026#39;body\u0026#39;: \u0026#39;RediSearch is a powerful indexing, querying, and full-text search engine for Redis\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;\u0026lt;https://redis.com/modules/redis-search/\u0026gt;\u0026#39;}] More info RediSearch commands RediSearch query syntax Details about RediSearch query features RediSearch client libraries Search JSON quick start ","categories":["Modules"]},{"uri":"/modules/redisgraph/redisgraph-quickstart/","uriRel":"/modules/redisgraph/redisgraph-quickstart/","title":"RedisGraph quick start","tags":[],"keywords":[],"description":"RedisGraph quick start","content":"Prerequisites For this quick start tutorial, you need:\nA Redis database with the RedisGraph module enabled. You can use either:\nA Redis Cloud database\nA Redis Enterprise Software database\nredis-cli command-line tool\nredis-py client library v4.1.0 or later\nRedisGraph with redis-cli To begin, connect to your database with redis-cli.\nCreate a graph When you create a graph, you can define nodes and the relationships between them with this format:\n(:\u0026lt;node 1\u0026gt;)-[:\u0026lt;relationship\u0026gt;]-\u0026gt;(:\u0026lt;node 2\u0026gt;)\nTo define multiple nodes and relationships in a single creation query, separate the entries with commas.\nFor example, use the CREATE query to create a new graph of motorcycle riders and teams participating in the MotoGP league:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;CREATE (:Rider {name:\u0026#39;Valentino Rossi\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Yamaha\u0026#39;}), (:Rider {name:\u0026#39;Dani Pedrosa\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Honda\u0026#39;}), (:Rider {name:\u0026#39;Andrea Dovizioso\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Ducati\u0026#39;})\u0026#34; 1) 1) \u0026#34;Labels added: 2\u0026#34; 2) \u0026#34;Nodes created: 6\u0026#34; 3) \u0026#34;Properties set: 6\u0026#34; 4) \u0026#34;Relationships created: 3\u0026#34; 5) \u0026#34;Cached execution: 0\u0026#34; 6) \u0026#34;Query internal execution time: 0.385472 milliseconds\u0026#34; Add nodes You can add new nodes to a previously created graph:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;CREATE (:Rider {name:\u0026#39;Jorge Lorenzo\u0026#39;})\u0026#34; 1) 1) \u0026#34;Nodes created: 1\u0026#34; 2) \u0026#34;Properties set: 1\u0026#34; 3) \u0026#34;Cached execution: 0\u0026#34; 4) \u0026#34;Query internal execution time: 0.185841 milliseconds\u0026#34; Add relationships To create new relationships between nodes of a graph:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider), (t:Team) WHERE r.name = \u0026#39;Jorge Lorenzo\u0026#39; and t.name = \u0026#39;Honda\u0026#39; CREATE (r)-[:rides]-\u0026gt;(t)\u0026#34; 1) 1) \u0026#34;Relationships created: 1\u0026#34; 2) \u0026#34;Cached execution: 0\u0026#34; 3) \u0026#34;Query internal execution time: 0.356578 milliseconds\u0026#34; Query the graph After you create a graph, you can use the GRAPH.QUERY command to query the graph\u0026rsquo;s data.\nThe following example returns which motorcycle riders compete for team Yamaha:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider)-[:rides]-\u0026gt;(t:Team) WHERE t.name = \u0026#39;Yamaha\u0026#39; RETURN r,t\u0026#34; 1) 1) \u0026#34;r\u0026#34; 2) \u0026#34;t\u0026#34; 2) 1) 1) 1) 1) \u0026#34;id\u0026#34; 2) \u0026#34;0\u0026#34; 2) 1) \u0026#34;labels\u0026#34; 2) 1) \u0026#34;Rider\u0026#34; 3) 1) \u0026#34;properties\u0026#34; 2) 1) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Valentino Rossi\u0026#34; 2) 1) 1) \u0026#34;id\u0026#34; 2) \u0026#34;1\u0026#34; 2) 1) \u0026#34;labels\u0026#34; 2) 1) \u0026#34;Team\u0026#34; 3) 1) \u0026#34;properties\u0026#34; 2) 1) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Yamaha\u0026#34; 3) 1) \u0026#34;Cached execution: 0\u0026#34; 2) \u0026#34;Query internal execution time: 0.500535 milliseconds\u0026#34; You can also use functions to create more complex queries.\nFor example, you can use the count function to check how many riders represent team Honda:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider)-[:rides]-\u0026gt;(t:Team {name:\u0026#39;Honda\u0026#39;}) RETURN count(r)\u0026#34; 1) 1) \u0026#34;count(r)\u0026#34; 2) 1) 1) \u0026#34;2\u0026#34; 3) 1) \u0026#34;Cached execution: 0\u0026#34; 2) \u0026#34;Query internal execution time: 0.445760 milliseconds\u0026#34; Delete nodes You can use the DELETE query to remove a specific node and its relationships from the graph:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider {name: \u0026#39;Dani Pedrosa\u0026#39;}) DELETE r\u0026#34; 1) 1) \u0026#34;Nodes deleted: 1\u0026#34; 2) \u0026#34;Relationships deleted: 1\u0026#34; 3) \u0026#34;Cached execution: 0\u0026#34; 4) \u0026#34;Query internal execution time: 0.276815 milliseconds\u0026#34; Delete relationships You can also use the DELETE query to delete a node\u0026rsquo;s relationships without removing any nodes:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (:Rider {name: \u0026#39;Valentino Rossi\u0026#39;})-[r:rides]-\u0026gt;() DELETE r\u0026#34; 1) 1) \u0026#34;Relationships deleted: 1\u0026#34; 2) \u0026#34;Cached execution: 0\u0026#34; 3) \u0026#34;Query internal execution time: 0.348346 milliseconds\u0026#34; Delete a graph To delete an entire graph, including all nodes and relationships, run the GRAPH.DELETE command:\n127.0.0.1:12543\u0026gt; GRAPH.DELETE MotoGP \u0026#34;Graph removed, internal execution time: 0.013138 milliseconds\u0026#34; RedisGraph with Python If you want to use RedisGraph within an application, you can use one of these client libraries.\nThe following example uses the Redis Python client library redis-py, which supports RedisGraph commands as of v4.1.0.\nThis Python code creates a graph that represents friendships between users on a social media website. It also shows how to run queries and change relationships between users.\nimport redis from redis.commands.graph.edge import Edge from redis.commands.graph.node import Node # Connect to a database r = redis.Redis(host=\u0026#34;\u0026lt;endpoint\u0026gt;\u0026#34;, port=\u0026#34;\u0026lt;port\u0026gt;\u0026#34;, password=\u0026#34;\u0026lt;password\u0026gt;\u0026#34;) # Create nodes that represent users users = { \u0026#34;Alex\u0026#34;: Node(label=\u0026#34;Person\u0026#34;, properties={\u0026#34;name\u0026#34;: \u0026#34;Alex\u0026#34;, \u0026#34;age\u0026#34;: 35}), \u0026#34;Jun\u0026#34;: Node(label=\u0026#34;Person\u0026#34;, properties={\u0026#34;name\u0026#34;: \u0026#34;Jun\u0026#34;, \u0026#34;age\u0026#34;: 33}), \u0026#34;Taylor\u0026#34;: Node(label=\u0026#34;Person\u0026#34;, properties={\u0026#34;name\u0026#34;: \u0026#34;Taylor\u0026#34;, \u0026#34;age\u0026#34;: 28}), \u0026#34;Noor\u0026#34;: Node(label=\u0026#34;Person\u0026#34;, properties={\u0026#34;name\u0026#34;: \u0026#34;Noor\u0026#34;, \u0026#34;age\u0026#34;: 41}) } # Define a graph called SocialMedia social_graph = r.graph(\u0026#34;SocialMedia\u0026#34;) # Add users to the graph as nodes for key in users.keys(): social_graph.add_node(users[key]) # Add relationships between user nodes social_graph.add_edge( Edge(users[\u0026#34;Alex\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Jun\u0026#34;]) ) # Make the relationship bidirectional social_graph.add_edge( Edge(users[\u0026#34;Jun\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Alex\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Jun\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Taylor\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Taylor\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Jun\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Jun\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Noor\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Noor\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Jun\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Alex\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Noor\u0026#34;]) ) social_graph.add_edge( Edge(users[\u0026#34;Noor\u0026#34;], \u0026#34;friends\u0026#34;, users[\u0026#34;Alex\u0026#34;]) ) # Create the graph in the database social_graph.commit() # Query the graph to find out how many friends Alex has result1 = social_graph.query(\u0026#34;MATCH (p1:Person {name: \u0026#39;Alex\u0026#39;})-[:friends]-\u0026gt;(p2:Person) RETURN count(p2)\u0026#34;) print(\u0026#34;Alex\u0026#39;s original friend count:\u0026#34;, result1.result_set) # Delete a relationship without deleting any user nodes social_graph.query(\u0026#34;MATCH (:Person {name: \u0026#39;Alex\u0026#39;})\u0026lt;-[f:friends]-\u0026gt;(:Person {name: \u0026#39;Jun\u0026#39;}) DELETE f\u0026#34;) # Query the graph again to see Alex\u0026#39;s updated friend count result2 = social_graph.query(\u0026#34;MATCH (p1:Person {name: \u0026#39;Alex\u0026#39;})-[:friends]-\u0026gt;(p2:Person) RETURN count(p2)\u0026#34;) print(\u0026#34;Alex\u0026#39;s updated friend count:\u0026#34;, result2.result_set) # Delete the entire graph social_graph.delete() Example output:\n$ ./quick_start.py Alex\u0026#39;s original friend count: [[2]] Alex\u0026#39;s updated friend count: [[1]] Visualize graphs with RedisInsight You can use the RedisInsight workbench to visualize the relationships between the nodes of your graph.\nConnect to your database with RedisInsight. You can connect manually or use the auto-discovery feature.\nSelect the Workbench button:\nEnter a RedisGraph query in the text editor.\nFor example, this query returns all nodes and relationships in the graph:\nGRAPH.QUERY MotoGP \u0026#34;MATCH (n) RETURN n\u0026#34; Select Run:\nAfter you run a query, the output log displays a visual representation of your graph\u0026rsquo;s nodes and relationships:\nMore info RedisGraph commands RedisGraph client libraries ","categories":["Modules"]},{"uri":"/modules/redisjson/redisjson-quickstart/","uriRel":"/modules/redisjson/redisjson-quickstart/","title":"RedisJSON quick start","tags":[],"keywords":[],"description":"RedisJSON quick start","content":"Prerequisites For this quick start tutorial, you need:\nA Redis database with the RedisJSON module enabled. You can use either:\nA Redis Cloud database\nA Redis Enterprise Software database\nredis-cli command-line tool\nredis-py client library v4.0.0 or greater\nRedisJSON with redis-cli The redis-cli command-line tool comes packaged with Redis. You can use it to connect to your Redis database and test RedisJSON commands.\nConnect to a database $ redis-cli --raw -h \u0026lt;endpoint\u0026gt; -p \u0026lt;port\u0026gt; -a \u0026lt;password\u0026gt; 127.0.0.1:12543\u0026gt; The --raw option forces the command to return raw output.\nCreate JSON documents To create a JSON document in a Redis database, you can use the JSON.SET command.\nHere\u0026rsquo;s an example JSON document that represents a shopping list:\n{ \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 } ] } } } To create this JSON document in your database, run JSON.SET:\n127.0.0.1:12543\u0026gt; JSON.SET shopping_list $ \u0026#39;{ \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 } ] } } }\u0026#39; OK JSON.SET accepts a JSON path as a parameter. Paths must start with a $ (JSONPath syntax) or . (legacy path syntax) character, which represents the root of the JSON document. All elements within a JSON document are relative to the root.\nModify JSON documents You can also use JSON.SET to modify existing JSON documents and elements. To modify a specific JSON element, you need to provide the path to the element as a parameter.\nAdd a new JSON object Add a new store named clothing_store to the shopping list and provide an array of items that you need to buy from there:\n127.0.0.1:12543\u0026gt; JSON.SET shopping_list $.stores.clothing_store \u0026#39;{ \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;socks\u0026#34;, \u0026#34;count\u0026#34;: 2 } ] }\u0026#39; OK Append to an array You can use JSON.ARRAPPEND to add a new element to an existing array.\nFor example, add a new item named pears to the grocery_store list:\n127.0.0.1:12543\u0026gt; JSON.ARRAPPEND shopping_list $.stores.grocery_store.items \u0026#39;{ \u0026#34;name\u0026#34; : \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34; : 3 }\u0026#39; 2 The output number indicates how many items are currently in the array.\nIncrease or decrease a number JSON.NUMINCRBY lets you increase (or decrease) a number by a specified value.\nIncrease the count of the first item on the clothing_store list by 2:\n127.0.0.1:12543\u0026gt; JSON.NUMINCRBY shopping_list $.stores.clothing_store.items[0].count 2 [4] Conditional update You can also use filter expressions ?() in the JSONPath to modify JSON elements that match some condition.\nThe following example filters on grocery_store item names to decrease the count of pears by 1:\n127.0.0.1:12543\u0026gt; JSON.NUMINCRBY shopping_list \u0026#39;$.stores.grocery_store.items.*[?(@.name==\u0026#34;pears\u0026#34;)].count\u0026#39; -1 [2] To use double quotes in a filter expression, you must enclose the path within single quotes.\nRead JSON The JSON.GET command lets you retrieve JSON documents stored in the database. If you run redis-cli with the --raw option, you can format the response with the INDENT, NEWLINE, and SPACE options.\nRead entire document To retrieve the entire JSON document, run JSON.GET with root $ as the path:\n127.0.0.1:12543\u0026gt; JSON.GET shopping_list $ INDENT \u0026#34;\\t\u0026#34; NEWLINE \u0026#34;\\n\u0026#34; SPACE \u0026#34; \u0026#34; [ { \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 }, { \u0026#34;name\u0026#34;: \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34;: 2 } ] }, \u0026#34;clothing_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;socks\u0026#34;, \u0026#34;count\u0026#34;: 4 } ] } } } ] Read specific elements You can also use JSON.GET to retrieve specific elements within a JSON document.\nTo return only the items from the grocery_store list, run JSON.GET with the path $.stores.grocery_store:\n127.0.0.1:12543\u0026gt; JSON.GET shopping_list $.stores.grocery_store INDENT \u0026#34;\\t\u0026#34; NEWLINE \u0026#34;\\n\u0026#34; [ { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 }, { \u0026#34;name\u0026#34;: \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34;: 2 } ] } ] To return all items from all stores on the shopping list, use the path $..items[*]:\n127.0.0.1:12543\u0026gt; JSON.GET shopping_list $..items[*] INDENT \u0026#34;\\t\u0026#34; NEWLINE \u0026#34;\\n\u0026#34; [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 }, { \u0026#34;name\u0026#34;: \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34;: 2 }, { \u0026#34;name\u0026#34;: \u0026#34;socks\u0026#34;, \u0026#34;count\u0026#34;: 4 } ] Verify JSON type If you want to verify a JSON element\u0026rsquo;s type, use the JSON.TYPE command:\n127.0.0.1:12543\u0026gt; JSON.TYPE shopping_list $.stores object 127.0.0.1:12543\u0026gt; JSON.TYPE shopping_list $.stores.grocery_store.items array 127.0.0.1:12543\u0026gt; JSON.TYPE shopping_list $.stores.grocery_store.items[0].name string 127.0.0.1:12543\u0026gt; JSON.TYPE shopping_list $.stores.grocery_store.items[0].count integer Delete JSON elements Use JSON.DEL to delete parts of the JSON document:\nRemove the clothing_store JSON object:\n127.0.0.1:12543\u0026gt; JSON.DEL shopping_list $.stores.clothing_store 1 Then remove the second item from the grocery_store list:\n127.0.0.1:12543\u0026gt; JSON.DEL shopping_list $.stores.grocery_store.items[1] 1 If you run JSON.GET, you can verify the removal of the expected JSON elements:\n127.0.0.1:12543\u0026gt; JSON.GET shopping_list $ INDENT \u0026#34;\\t\u0026#34; NEWLINE \u0026#34;\\n\u0026#34; [ { \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 } ] } } } ] Delete JSON document If you run JSON.DEL but don\u0026rsquo;t specify a path, it will delete the entire JSON document:\n127.0.0.1:12543\u0026gt; JSON.DEL shopping_list 1 127.0.0.1:12543\u0026gt; EXISTS shopping_list 0 RedisJSON with Python If you want to use RedisJSON within an application, you can use one of the client libraries.\nThe following example uses the Redis Python client library redis-py, which supports RedisJSON commands as of v4.0.0.\nThis Python code creates a JSON document in a Redis database, modifies the JSON document, and then deletes the document:\nimport redis import json # Connect to a database r = redis.Redis(host=\u0026#34;\u0026lt;endpoint\u0026gt;\u0026#34;, port=\u0026#34;\u0026lt;port\u0026gt;\u0026#34;, password=\u0026#34;\u0026lt;password\u0026gt;\u0026#34;) # Initial JSON document data list_data = { \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34; : { \u0026#34;items\u0026#34; : [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5 } ] } } } # Create the JSON document in the database print(\u0026#34;Creating shopping list...\u0026#34;) r.json().set(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;$\u0026#39;, list_data) # Add a new field to the existing JSON document r.json().set(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;$.stores.grocery_store.items[0].variety\u0026#39;, \u0026#39;Honeycrisp\u0026#39;) # New item data pears_obj = { \u0026#34;name\u0026#34; : \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34; : 3 } # Add a new item to the grocery_store items array r.json().arrappend(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;$.stores.grocery_store.items\u0026#39;, pears_obj) # Get all items on the shopping list reply = r.json().get(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;$..items[*]\u0026#39;) print(\u0026#34;Current list of all items:\u0026#34;) print(json.dumps(reply, indent=4) + \u0026#34;\\n\u0026#34;) # Delete specific parts of the JSON document r.json().delete(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;$.stores.grocery_store.items[1]\u0026#39;) print(\u0026#34;Deleted pears from the grocery list.\u0026#34;) # Get the updated JSON document reply = r.json().get(\u0026#39;shopping_list:py\u0026#39;, \u0026#39;.\u0026#39;) print(\u0026#34;The JSON document now contains:\u0026#34;) print(json.dumps(reply, indent=4) + \u0026#34;\\n\u0026#34;) # Delete the entire JSON document r.json().delete(\u0026#39;shopping_list:py\u0026#39;) print(\u0026#34;Deleted shopping_list:py JSON document.\u0026#34;) Example output:\n$ python3 quick_start.py Creating shopping list... Current list of all items: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5, \u0026#34;variety\u0026#34;: \u0026#34;Honeycrisp\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;pears\u0026#34;, \u0026#34;count\u0026#34;: 3 } ] Deleted pears from the grocery list. The JSON document now contains: { \u0026#34;list_date\u0026#34;: \u0026#34;05/05/2022\u0026#34;, \u0026#34;stores\u0026#34;: { \u0026#34;grocery_store\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;apples\u0026#34;, \u0026#34;count\u0026#34;: 5, \u0026#34;variety\u0026#34;: \u0026#34;Honeycrisp\u0026#34; } ] } } } Deleted shopping_list:py JSON document. More info RedisJSON commands RedisJSON client libraries ","categories":["Modules"]},{"uri":"/modules/redistimeseries/redistimeseries-quickstart/","uriRel":"/modules/redistimeseries/redistimeseries-quickstart/","title":"RedisTimeSeries quick start","tags":[],"keywords":[],"description":"RedisTimeSeries quick start","content":"Prerequisites For this quick start tutorial, you need:\nA Redis database with the RedisTimeSeries module enabled. You can use either:\nA Redis Cloud database\nA Redis Enterprise Software database\nredis-cli command-line tool\nredis-py client library v4.0.0 or later\nRedisTimeSeries with redis-cli The following examples show you how to create a time series that represents weather measurements.\nTo begin, connect to your database with redis-cli.\nCreate a time series Use TS.CREATE to create a new time series.\nCreate a time series with the key name temperature:1:20 to represent the temperatures at sensor 1 and area 20.\n127.0.0.1:12543\u0026gt; TS.CREATE temperature:1:20 LABELS sensor_id 1 area_id 20 data_type \u0026#34;temperature\u0026#34; \u0026#34;OK\u0026#34; The LABELS parameter allows you to label time series keys with metadata that can be filtered.\nCreate a time series with the key name humidity:1:20 to represent the humidity at sensor 1 and area 20.\n127.0.0.1:12543\u0026gt; TS.CREATE humidity:1:20 LABELS sensor_id 1 area_id 20 data_type \u0026#34;humidity\u0026#34; \u0026#34;OK\u0026#34; Create temperature and humidity time series keys for other temperature and humidity sensors in a different area.\n127.0.0.1:12543\u0026gt; TS.CREATE humidity:2:24 LABELS sensor_id 2 area_id 24 data_type \u0026#34;humidity\u0026#34; \u0026#34;OK\u0026#34; 127.0.0.1:12543\u0026gt; TS.CREATE temperature:2:24 LABELS sensor_id 2 area_id 24 data_type \u0026#34;temperature\u0026#34; \u0026#34;OK\u0026#34; Add samples Use TS.ADD to add new data to a time series.\nAdd a few temperature measurements from area 20 to temperature:1:20.\n127.0.0.1:12543\u0026gt; TS.ADD temperature:1:20 1652824475581 23.9 1652824475581 127.0.0.1:12543\u0026gt; TS.ADD temperature:1:20 1652828015127 22.8 1652828015127 127.0.0.1:12543\u0026gt; TS.ADD temperature:1:20 1652831657886 21.7 1652831657886 The second argument in TS.ADD is the UNIX timestamp for the sample in milliseconds. You can also specify * for the current time on the Redis server.\nAdd a few humidity measurements from area 20 to humidity:1:20.\n127.0.0.1:12543\u0026gt; TS.ADD humidity:1:20 1652824475581 29 1652824475581 127.0.0.1:12543\u0026gt; TS.ADD humidity:1:20 1652828015127 27 1652828015127 127.0.0.1:12543\u0026gt; TS.ADD humidity:1:20 1652831657886 28 1652831657886 Update multiple time series You can also update multiple time series at once with TS.MADD.\nUse TS.MADD to add multiple measurements from area 24 to temperature:2:24 and humidity:2:24.\n127.0.0.1:12543\u0026gt; TS.MADD temperature:2:24 1652824475581 18.3 humidity:2:24 1652824475581 51 \\ temperature:2:24 1652828015127 17.2 humidity:2:24 1652828015127 54 \\ temperature:2:24 1652831657886 16.7 humidity:2:24 1652831657886 56 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;1652824475581\u0026#34; 3) \u0026#34;1652828015127\u0026#34; 4) \u0026#34;1652828015127\u0026#34; 5) \u0026#34;1652831657886\u0026#34; 6) \u0026#34;1652831657886\u0026#34; Read time series data Use TS.GET to get the latest entry in the time series.\n127.0.0.1:12543\u0026gt; TS.GET temperature:1:20 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;21.7\u0026#34; Use TS.RANGE to get a range of timestamps and values in the time series, ordered from earliest to latest.\n127.0.0.1:12543\u0026gt; TS.RANGE temperature:2:24 1652824475581 1652828015127 1) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;18.3\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;17.2\u0026#34; TS.REVRANGE orders the values from latest to earliest.\nYou can specify 0 and + for the starting and ending timestamps to get all of the entries in the time series.\n127.0.0.1:12543\u0026gt; TS.REVRANGE temperature:2:24 0 + 1) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;16.7\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;17.2\u0026#34; 3) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;18.3\u0026#34; Read data from multiple time series Use TS.MGET to get the last entry in all time series keys with an area_id of 20.\n127.0.0.1:12543\u0026gt; TS.MGET WITHLABELS FILTER area_id=20 1) 1) \u0026#34;humidity:1:20\u0026#34; 2) 1) 1) \u0026#34;sensor_id\u0026#34; 2) \u0026#34;1\u0026#34; 2) 1) \u0026#34;area_id\u0026#34; 2) \u0026#34;20\u0026#34; 3) 1) \u0026#34;data_type\u0026#34; 2) \u0026#34;humidity\u0026#34; 3) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;28\u0026#34; 2) 1) \u0026#34;temperature:1:20\u0026#34; 2) 1) 1) \u0026#34;sensor_id\u0026#34; 2) \u0026#34;1\u0026#34; 2) 1) \u0026#34;area_id\u0026#34; 2) \u0026#34;20\u0026#34; 3) 1) \u0026#34;data_type\u0026#34; 2) \u0026#34;temperature\u0026#34; 3) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;21.7\u0026#34; Use TS.MRANGE to get a range of entries in all time series keys with a sensor_id of 2 from earliest to latest.\n127.0.0.1:12543\u0026gt; TS.MRANGE 0 + filter sensor_id=2 1) 1) \u0026#34;humidity:2:24\u0026#34; 2) (empty list or set) 3) 1) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;51\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;54\u0026#34; 3) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;56\u0026#34; 2) 1) \u0026#34;temperature:2:24\u0026#34; 2) (empty list or set) 3) 1) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;18.3\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;17.2\u0026#34; 3) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;16.7\u0026#34; Then, use TS.MREVRANGE to get a range of entries in all time series keys that are humidity keys from latest to earliest.\n127.0.0.1:12543\u0026gt; TS.MREVRANGE 0 + FILTER data_type=humidity 1) 1) \u0026#34;humidity:1:20\u0026#34; 2) (empty list or set) 3) 1) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;28\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;27\u0026#34; 3) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;29\u0026#34; 2) 1) \u0026#34;humidity:2:24\u0026#34; 2) (empty list or set) 3) 1) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;56\u0026#34; 2) 1) \u0026#34;1652828015127\u0026#34; 2) \u0026#34;54\u0026#34; 3) 1) \u0026#34;1652824475581\u0026#34; 2) \u0026#34;51\u0026#34; Delete samples Use TS.DEL to delete all samples within a specified range of timestamps:\n127.0.0.1:12543\u0026gt; TS.DEL temperature:1:20 1652824475581 1652828015127 (integer) 2 127.0.0.1:12543\u0026gt; TS.RANGE temperature:1:20 0 + 1) 1) \u0026#34;1652831657886\u0026#34; 2) \u0026#34;21.7\u0026#34; RedisTimeSeries with Python If you want to use RedisTimeSeries within an application, you can use one of the client libraries.\nThe following example uses the Redis Python client library redis-py, which supports RedisTimeSeries commands as of v4.0.0.\nThis Python code adds multiple time series keys to Redis, adds data to the time series, retrieves data from the time series, and then deletes the time series keys.\nimport redis import json # To make the responses easier to read # Data from two sensors sensor1 = { \u0026#34;sensorId\u0026#34;: 1, \u0026#34;areaId\u0026#34;: 20, \u0026#34;data\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1652824475581, \u0026#34;temperature\u0026#34;: 23.9, \u0026#34;humidity\u0026#34;: 29 }, { \u0026#34;timestamp\u0026#34;: 1652828015127, \u0026#34;temperature\u0026#34;: 22.8, \u0026#34;humidity\u0026#34;: 27 }, { \u0026#34;timestamp\u0026#34;: 1652831657886, \u0026#34;temperature\u0026#34;: 21.7, \u0026#34;humidity\u0026#34;: 28 }, { \u0026#34;timestamp\u0026#34;: 1652835287321, \u0026#34;temperature\u0026#34;: 19.4, \u0026#34;humidity\u0026#34;: 35 }, { \u0026#34;timestamp\u0026#34;: 1652838756149, \u0026#34;temperature\u0026#34;: 17.8, \u0026#34;humidity\u0026#34;: 40 } ] } sensor2 = { \u0026#34;sensorId\u0026#34;: 2, \u0026#34;areaId\u0026#34;: 24, \u0026#34;data\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1652824475581, \u0026#34;temperature\u0026#34;: 18.3, \u0026#34;humidity\u0026#34;: 51 }, { \u0026#34;timestamp\u0026#34;: 1652828015127, \u0026#34;temperature\u0026#34;: 17.2, \u0026#34;humidity\u0026#34;: 54 }, { \u0026#34;timestamp\u0026#34;: 1652831657886, \u0026#34;temperature\u0026#34;: 16.7, \u0026#34;humidity\u0026#34;: 56 }, { \u0026#34;timestamp\u0026#34;: 1652835287321, \u0026#34;temperature\u0026#34;: 15.6, \u0026#34;humidity\u0026#34;: 60 }, { \u0026#34;timestamp\u0026#34;: 1652838756149, \u0026#34;temperature\u0026#34;: 14.4, \u0026#34;humidity\u0026#34;: 65 } ] } sensors = [ sensor1, sensor2 ] # Connect to a Redis database r = redis.Redis(host=\u0026#34;\u0026lt;endpoint\u0026gt;\u0026#34;, port=\u0026#34;\u0026lt;port\u0026gt;\u0026#34;, password=\u0026#34;\u0026lt;password\u0026gt;\u0026#34;) # Create a list to store keys key_list = [] # Create a list of keys and values for ts.madd humidity_madd_list = [] # For each sensor: for sensor in sensors: # Create time series keys key_end = \u0026#34;{}:{}\u0026#34;.format(sensor[\u0026#34;sensorId\u0026#34;], sensor[\u0026#34;areaId\u0026#34;]) temperature_key = \u0026#34;temperature-py:\u0026#34; + key_end humidity_key = \u0026#34;humidity-py:\u0026#34; + key_end key_list.extend((temperature_key, humidity_key)) print(\u0026#34;Create {} and {}\u0026#34;.format(temperature_key, humidity_key)) labels = { \u0026#34;sensorId\u0026#34; : sensor[\u0026#34;sensorId\u0026#34;], \u0026#34;areaId\u0026#34; : sensor[\u0026#34;areaId\u0026#34;] } labels[\u0026#34;dataType\u0026#34;] = \u0026#34;temperature\u0026#34; r.ts().create(temperature_key, labels=labels) labels[\u0026#34;dataType\u0026#34;] = \u0026#34;humidity\u0026#34; r.ts().create(humidity_key, labels=labels) # Add temperature data to time series keys with ts.add print(\u0026#34;Add data to {}\u0026#34;.format(temperature_key)) # For each data entry: for entry in sensor[\u0026#34;data\u0026#34;]: r.ts().add(temperature_key, entry[\u0026#34;timestamp\u0026#34;], entry[\u0026#34;temperature\u0026#34;]) #Create a list of humidity data for ts.madd humidity_madd_list.append( (humidity_key, entry[\u0026#34;timestamp\u0026#34;], entry[\u0026#34;humidity\u0026#34;])) #Add humidity data to time series keys with ts.madd print(\u0026#34;Add data with ts.madd:\\n{}\u0026#34;.format(humidity_madd_list)) r.ts().madd(humidity_madd_list) input(\u0026#34;\\nAdded all data. Press enter to continue...\u0026#34;) print(\u0026#34;Get the last value from temperature-py:1:20...\u0026#34;) get_reply = r.ts().get(\u0026#34;temperature-py:1:20\u0026#34;) print(json.dumps(get_reply, indent=4) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Get all of the values from humidity-py:2:24...\u0026#34;) range_reply = r.ts().range(\u0026#34;humidity-py:2:24\u0026#34;, 0, \u0026#34;+\u0026#34;) print(json.dumps(range_reply, indent=4) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Get last values from area 20...\u0026#34;) mget_filters = [\u0026#34;areaId=20\u0026#34;] mget_reply = r.ts().mget(mget_filters, with_labels=True) print(json.dumps(mget_reply, indent=4) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Get all of the values from sensor 2 from earliest to latest...\u0026#34;) mrange_filters = [ \u0026#34;sensorId=2\u0026#34; ] mrange_reply = r.ts().mrange(0, \u0026#34;+\u0026#34;, mrange_filters, with_labels=True) print(json.dumps(mrange_reply, indent=4) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Get all humidity values from latest to earliest...\u0026#34;) mrevrange_filters = [ \u0026#34;dataType=humidity\u0026#34; ] mrevrange_reply = r.ts().mrevrange(0, \u0026#34;+\u0026#34;, mrevrange_filters, with_labels=True) print(json.dumps(mrevrange_reply, indent=4) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Delete range of values from temperature-py:1:20...\u0026#34;) r.ts().delete(\u0026#34;temperature-py:1:20\u0026#34;, 0, 1652831657886) range_reply = r.ts().range(\u0026#34;temperature-py:1:20\u0026#34;, 0, \u0026#34;+\u0026#34;) print(json.dumps(range_reply, indent=4) + \u0026#34;\\n\u0026#34;) input(\u0026#34;Press enter to finish...\u0026#34;) print(\u0026#34;Deleting keys: {}\u0026#34;.format(key_list)) for key in key_list: r.delete(key) print(\u0026#34;Done!\u0026#34;) Example output $ python3 quick_start.py Create temperature-py:1:20 and humidity-py:1:20 Add data to temperature-py:1:20 Create temperature-py:2:24 and humidity-py:2:24 Add data to temperature-py:2:24 Add data with ts.madd: [(\u0026#39;humidity-py:1:20\u0026#39;, 1652824475581, 29), (\u0026#39;humidity-py:1:20\u0026#39;, 1652828015127, 27), (\u0026#39;humidity-py:1:20\u0026#39;, 1652831657886, 28), (\u0026#39;humidity-py:1:20\u0026#39;, 1652835287321, 35), (\u0026#39;humidity-py:1:20\u0026#39;, 1652838756149, 40), (\u0026#39;humidity-py:2:24\u0026#39;, 1652824475581, 51), (\u0026#39;humidity-py:2:24\u0026#39;, 1652828015127, 54), (\u0026#39;humidity-py:2:24\u0026#39;, 1652831657886, 56), (\u0026#39;humidity-py:2:24\u0026#39;, 1652835287321, 60), (\u0026#39;humidity-py:2:24\u0026#39;, 1652838756149, 65)] Added all data. Press enter to continue... Get the last value from temperature-py:1:20... [ 1652838756149, 17.8 ] Get all of the values from humidity-py:2:24... [ [ 1652824475581, 51.0 ], [ 1652828015127, 54.0 ], [ 1652831657886, 56.0 ], [ 1652835287321, 60.0 ], [ 1652838756149, 65.0 ] ] Get last values from area 20... [ { \u0026#34;humidity-py:1:20\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;humidity\u0026#34; }, 1652838756149, 40.0 ] }, { \u0026#34;temperature-py:1:20\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;temperature\u0026#34; }, 1652838756149, 17.8 ] } ] Get all of the values from sensor 2 from earliest to latest... [ { \u0026#34;humidity-py:2:24\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;humidity\u0026#34; }, [ [ 1652824475581, 51.0 ], [ 1652828015127, 54.0 ], [ 1652831657886, 56.0 ], [ 1652835287321, 60.0 ], [ 1652838756149, 65.0 ] ] ] }, { \u0026#34;temperature-py:2:24\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;temperature\u0026#34; }, [ [ 1652824475581, 18.3 ], [ 1652828015127, 17.2 ], [ 1652831657886, 16.7 ], [ 1652835287321, 15.6 ], [ 1652838756149, 14.4 ] ] ] } ] Get all humidity values from latest to earliest... [ { \u0026#34;humidity-py:1:20\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;humidity\u0026#34; }, [ [ 1652838756149, 40.0 ], [ 1652835287321, 35.0 ], [ 1652831657886, 28.0 ], [ 1652828015127, 27.0 ], [ 1652824475581, 29.0 ] ] ] }, { \u0026#34;humidity-py:2:24\u0026#34;: [ { \u0026#34;sensorId\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;areaId\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;dataType\u0026#34;: \u0026#34;humidity\u0026#34; }, [ [ 1652838756149, 65.0 ], [ 1652835287321, 60.0 ], [ 1652831657886, 56.0 ], [ 1652828015127, 54.0 ], [ 1652824475581, 51.0 ] ] ] } ] Delete range of values from temperature-py:1:20... [ [ 1652835287321, 19.4 ], [ 1652838756149, 17.8 ] ] Press enter to finish... Deleting keys: [\u0026#39;temperature-py:1:20\u0026#39;, \u0026#39;humidity-py:1:20\u0026#39;, \u0026#39;temperature-py:2:24\u0026#39;, \u0026#39;humidity-py:2:24\u0026#39;] Done! More info RedisTimeSeries commands RedisTimeSeries client libraries ","categories":["Modules"]},{"uri":"/rs/references/rest-api/quick-start/","uriRel":"/rs/references/rest-api/quick-start/","title":"Redis Enterprise Software REST API quick start","tags":[],"keywords":[],"description":"Redis Enterprise Software REST API quick start","content":"Redis Enterprise Software includes a REST API that allows you to automate certain tasks. This article shows you how to send a request to the Redis Enterprise Software REST API.\nFundamentals No matter which method you use to send API requests, there are a few common concepts to remember.\nType Description Authentication Use Basic Auth with your cluster username (email) and password Ports All calls are made to port 9443 by default Versions Specify the version in the request URI Headers Accept and Content-Type should be application/json Response types and error codes A response of 200 OK means success; otherwise, the request failed due to an error For more information, see Redis Enterprise Software REST API.\ncURL example requests cURL is a command-line tool that allows you to send HTTP requests from a terminal.\nYou can use the following options to build a cURL request:\nOption Description -X Method (GET, PUT, PATCH, POST, or DELETE) -H Request header, can be specified multiple times -u Username and password information -d JSON data for PUT or POST requests -F Form data for PUT or POST requests, such as for the POST /v1/modules or POST /v2/modules endpoint -k Turn off SSL verification -i Show headers and status code as well as the response body See the cURL documentation for more information.\nGET request Use the following cURL command to get a list of databases with the GET /v1/bdbs/ endpoint.\n$ curl -X GET -H \u0026#34;accept: application/json\u0026#34; \\ -u \u0026#34;[username]:[password]\u0026#34; \\ https://[host][:port]/v1/bdbs -k -i HTTP/1.1 200 OK server: envoy date: Tue, 14 Jun 2022 19:24:30 GMT content-type: application/json content-length: 2833 cluster-state-id: 42 x-envoy-upstream-service-time: 25 [ { ... \u0026#34;name\u0026#34;: \u0026#34;tr01\u0026#34;, ... \u0026#34;uid\u0026#34;: 1, \u0026#34;version\u0026#34;: \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34;: true } ] In the response body, the uid is the database ID. You can use the database ID to view or update the database using the API.\nFor more information about the fields returned by GET /v1/bdbs/, see the bdbs object.\nPUT request Once you have the database ID, you can use PUT /v1/bdbs/ to update the configuration of the database.\nFor example, you can pass the database uid 1 as a URL parameter and use the -d option to specify the new name when you send the request. This changes the database\u0026rsquo;s name from tr01 to database1:\n$ curl -X PUT -H \u0026#34;accept: application/json\u0026#34; \\ -H \u0026#34;content-type: application/json\u0026#34; \\ -u \u0026#34;cameron.bates@redis.com:test123\u0026#34; \\ https://[host]:[port]/v1/bdbs/1 \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;database1\u0026#34; }\u0026#39; -k -i HTTP/1.1 200 OK server: envoy date: Tue, 14 Jun 2022 20:00:25 GMT content-type: application/json content-length: 2933 cluster-state-id: 43 x-envoy-upstream-service-time: 159 { ... \u0026#34;name\u0026#34; : \u0026#34;database1\u0026#34;, ... \u0026#34;uid\u0026#34; : 1, \u0026#34;version\u0026#34; : \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34; : true } For more information about the fields you can update with PUT /v1/bdbs/, see the bdbs object.\nClient examples You can also use client libraries to make API requests in your preferred language.\nTo follow these examples, you need:\nA Redis Enterprise Software node Python 3 and the requests Python library node.js and node-fetch Python import json import requests # Required connection information - replace with your host, port, username, and password host = \u0026#34;[host]\u0026#34; port = \u0026#34;[port]\u0026#34; username = \u0026#34;[username]\u0026#34; password = \u0026#34;[password]\u0026#34; # Get the list of databases using GET /v1/bdbs bdbs_uri = \u0026#34;https://{}:{}/v1/bdbs\u0026#34;.format(host, port) print(\u0026#34;GET {}\u0026#34;.format(bdbs_uri)) get_resp = requests.get(bdbs_uri, auth = (username, password), headers = { \u0026#34;accept\u0026#34; : \u0026#34;application/json\u0026#34; }, verify = False) print(\u0026#34;{} {}\u0026#34;.format(get_resp.status_code, get_resp.reason)) for header in get_resp.headers.keys(): print(\u0026#34;{}: {}\u0026#34;.format(header, get_resp.headers[header])) print(\u0026#34;\\n\u0026#34; + json.dumps(get_resp.json(), indent=4)) # Rename all databases using PUT /v1/bdbs for bdb in get_resp.json(): uid = bdb[\u0026#34;uid\u0026#34;] # Get the database ID from the JSON response put_uri = \u0026#34;{}/{}\u0026#34;.format(bdbs_uri, uid) new_name = \u0026#34;database{}\u0026#34;.format(uid) put_data = { \u0026#34;name\u0026#34; : new_name } print(\u0026#34;PUT {} {}\u0026#34;.format(put_uri, json.dumps(put_data))) put_resp = requests.put(put_uri, data = json.dumps(put_data), auth = (username, password), headers = { \u0026#34;content-type\u0026#34; : \u0026#34;application/json\u0026#34; }, verify = False) print(\u0026#34;{} {}\u0026#34;.format(put_resp.status_code, put_resp.reason)) for header in put_resp.headers.keys(): print(\u0026#34;{}: {}\u0026#34;.format(header, put_resp.headers[header])) print(\u0026#34;\\n\u0026#34; + json.dumps(put_resp.json(), indent=4)) See the Python requests library documentation for more information.\nOutput $ python rs_api.py python rs_api.py GET https://[host]:[port]/v1/bdbs InsecureRequestWarning: Unverified HTTPS request is being made to host \u0026#39;[host]\u0026#39;. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings warnings.warn( 200 OK server: envoy date: Wed, 15 Jun 2022 15:49:43 GMT content-type: application/json content-length: 2832 cluster-state-id: 89 x-envoy-upstream-service-time: 27 [ { ... \u0026#34;name\u0026#34;: \u0026#34;tr01\u0026#34;, ... \u0026#34;uid\u0026#34;: 1, \u0026#34;version\u0026#34;: \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34;: true } ] PUT https://[host]:[port]/v1/bdbs/1 {\u0026#34;name\u0026#34;: \u0026#34;database1\u0026#34;} InsecureRequestWarning: Unverified HTTPS request is being made to host \u0026#39;[host]\u0026#39;. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings warnings.warn( 200 OK server: envoy date: Wed, 15 Jun 2022 15:49:43 GMT content-type: application/json content-length: 2933 cluster-state-id: 90 x-envoy-upstream-service-time: 128 { ... \u0026#34;name\u0026#34; : \u0026#34;database1\u0026#34;, ... \u0026#34;uid\u0026#34; : 1, \u0026#34;version\u0026#34; : \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34; : true } node.js import fetch, { Headers } from \u0026#39;node-fetch\u0026#39;; import * as https from \u0026#39;https\u0026#39;; const HOST = \u0026#39;[host]\u0026#39;; const PORT = \u0026#39;[port]\u0026#39;; const USERNAME = \u0026#39;[username]\u0026#39;; const PASSWORD = \u0026#39;[password]\u0026#39;; // Get the list of databases using GET /v1/bdbs const BDBS_URI = `https://${HOST}:${PORT}/v1/bdbs`; const USER_CREDENTIALS = Buffer.from(`${USERNAME}:${PASSWORD}`).toString(\u0026#39;base64\u0026#39;); const AUTH_HEADER = `Basic ${USER_CREDENTIALS}`; console.log(`GET ${BDBS_URI}`); const HTTPS_AGENT = new https.Agent({ rejectUnauthorized: false }); const response = await fetch(BDBS_URI, { method: \u0026#39;GET\u0026#39;, headers: { \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: AUTH_HEADER }, agent: HTTPS_AGENT }); const responseObject = await response.json(); console.log(`${response.status}: ${response.statusText}`); console.log(responseObject); // Rename all databases using PUT /v1/bdbs for (const database of responseObject) { const DATABASE_URI = `${BDBS_URI}/${database.uid}`; const new_name = `database${database.uid}`; console.log(`PUT ${DATABASE_URI}`); const response = await fetch(DATABASE_URI, { method: \u0026#39;PUT\u0026#39;, headers: { \u0026#39;Authorization\u0026#39;: AUTH_HEADER, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ \u0026#39;name\u0026#39;: new_name }), agent: HTTPS_AGENT }); console.log(`${response.status}: ${response.statusText}`); console.log(await(response.json())); } See the node-fetch documentation for more info.\nOutput $ node rs_api.js GET https://[host]:[port]/v1/bdbs 200: OK [ { ... \u0026#34;name\u0026#34;: \u0026#34;tr01\u0026#34;, ... \u0026#34;slave_ha\u0026#34; : false, ... \u0026#34;uid\u0026#34;: 1, \u0026#34;version\u0026#34;: \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34;: true } ] PUT https://[host]:[port]/v1/bdbs/1 200: OK { ... \u0026#34;name\u0026#34; : \u0026#34;tr01\u0026#34;, ... \u0026#34;slave_ha\u0026#34; : true, ... \u0026#34;uid\u0026#34; : 1, \u0026#34;version\u0026#34; : \u0026#34;6.0.16\u0026#34;, \u0026#34;wait_command\u0026#34; : true } More info Redis Enterprise Software REST API Redis Enterprise Software REST API requests ","categories":["RS"]},{"uri":"/kubernetes/re-clusters/cluster-recovery/","uriRel":"/kubernetes/re-clusters/cluster-recovery/","title":"Recover a Redis Enterprise cluster on Kubernetes","tags":[],"keywords":[],"description":"This task describes how to recover a Redis Enterprise cluster on Kubernetes.","content":"When a Redis Enterprise cluster loses contact with more than half of its nodes either because of failed nodes or network split, the cluster stops responding to client connections. When this happens, you must recover the cluster to restore the connections.\nYou can also perform cluster recovery to reset cluster nodes, to troubleshoot issues, or in a case of active/passive failover.\nThe Redis Enterprise for Kubernetes automates these recovery steps:\nRecreates a fresh Redis Enterprise cluster Mounts the persistent storage with the recovery files from the original cluster to the nodes of the new cluster Recovers the cluster configuration on the first node in the new cluster Joins the remaining nodes to the new cluster. Prerequisites For cluster recovery, the cluster must be deployed with persistence. Recover a cluster Edit the REC resource to set the clusterRecovery flag to true.\nkubectl patch rec \u0026lt;cluster-name\u0026gt; --type merge --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;clusterRecovery\u0026#34;:true}}\u0026#39; Note: In some cases, pods do not terminate when the statefulSet is scaled down as part of the cluster recovery. If pods are stuck in terminating or crashLoopBack and do not terminate gracefully, cluster recovery can pause.\nTo work around this, delete the pods manually with:\nkubectl delete pods \u0026lt;pod\u0026gt; --grace-period=0 --force When the last pod is manually deleted, the recovery process resumes.\nWait for the cluster to recover until it is in the \u0026ldquo;Running\u0026rdquo; state.\nTo see the state of the cluster, run:\nwatch \u0026#34;kubectl describe rec | grep State\u0026#34; To recover the database, see Recover a failed database.\n","categories":["Platforms"]},{"uri":"/modules/redisearch/","uriRel":"/modules/redisearch/","title":"RediSearch","tags":[],"keywords":[],"description":"","content":"The RediSearch 2.x module is a source-available project that lets you build powerful search queries for open source Redis databases. When combined with Redis Enterprise Software, you can use the same RediSearch protocols and commands to run geo-replicated queries and full-text searches over efficient in-memory indexes.\nIndex documents The RediSearch engine indexes documents, which are objects that represent data as field-value pairs. You can index more than one field per document, and these fields can represent text, numeric, or geospatial data types.\nAs the documents in your database change, the index automatically processes these changes to keep the search results up to date.\nWith RediSearch indexes, you can do:\nLanguage-aware fuzzy matching Fast auto-complete Exact phrase matching Numeric filtering Geo-radius queries Supported document types You can store documents as Redis hashes or JSON. To use JSON documents with RediSearch, you also need to enable the RedisJSON module in your database.\nHash documents With Redis hashes, each document is assigned to a single key and uses field-value pairs to represent the document\u0026rsquo;s contents.\nYou can run HGETALL to retrieve the entire hash document.\nJSON documents If a database has RedisJSON enabled, you can store documents as JSON and use RediSearch to index and search for them.\nFor more information about how to use RediSearch with JSON documents, see the Search JSON quick start.\nSearch features For full-text searches, you can customize the field queries and ranking of the search results. When querying, you can use multiple predicates that query text, numeric, and geospatial fields in one query. You can also sort by a specific field and limit the results with an offset to produce customized results pages.\nRediSearch supports over 15 natural languages for stemming and includes auto-complete engines with specific commands that can provide real-time interactive search suggestions.\nRediSearch in Active-Active databases As a result of the new RediSearch architecture and methodology, RediSearch 2.x supports Active-Active databases. You can now serve your index information from geo-distributed database instances.\nResharding and RediSearch By moving the index out of the keyspace and structuring the data as hashes, RediSearch 2.x makes it possible to reshard the database. When half of the data moves to the new shard, the index related to that data is created synchronously and RediSearch removes the keys from the index when it detects that the keys were deleted. Because the index on the new shard is created synchronously though, it\u0026rsquo;s expected that the resharding process will take longer than resharding of a database without RediSearch.\nLimitations You cannot use RediSearch with the OSS Cluster API. More info Getting Started with RediSearch 2.0 RediSearch quick start Configure RediSearch RediSearch commands RediSearch references RediSearch source ","categories":["Modules"]},{"uri":"/rc/subscriptions/","uriRel":"/rc/subscriptions/","title":"Manage subscriptions","tags":[],"keywords":[],"description":"","content":"This page helps you manage your Redis Cloud subscriptions; it briefly compares available plans and shows where to find help with common tasks.\nSubscription plans As of February 2021, Redis Enterprise Cloud supports the following subscription plans:\nFree plans are designed for training purposes and prototyping. They can be seamlessly upgraded to Fixed plans with no data loss. (Free plans are a tier of Fixed plans.)\nFixed plans are cost-efficient and designed for low-throughput scenarios. They support a range of availability, persistence, and backup options. Pricing supports low throughput workloads.\nFlexible plans support more databases (and larger sizes), unlimited connections, and greater throughput. Hosted in dedicated VPCs, they feature high-availability in a single or multi-AZ, active-active geo distribution, Redis-on-Flash (RoF), clustering, data persistence, and configurable backups. Pricing is \u0026ldquo;pay as you go\u0026rdquo; to support any dataset size or throughput.\nAnnual plans support the same features as Flexible plans, but at significant savings. Annual plans also provide Premium support. Further, the underlying commitment applies to all workloads across multiple providers and regions.\nHere\u0026rsquo;s a quick comparison:\nFeature Free plan Fixed plan Flexible/Annual plan Number of databases 1 4-64 Unlimited Memory size 30 MB 100 MB-10 GB 50 TB Concurrent connections 30 256-10,000 Unlimited Security role-based authpassword protectionencryption in transit role-based authpassword protectionSSL \u0026amp; SIP authencryption in transit role-based authpassword protectionSSL \u0026amp; SAIP authencryption in transitencryption at rest Admin REST API No No Yes Support Best effort Basic Flexible: StandardAnnual: Premium Selected additional features ReplicationAuto-failover\nDedicated accounts\nRedis on FlashActive/Active To learn more, see Redis Enterprise Cloud Pricing.\nCommon tasks Create a new subscription:\nThe Redis Cloud quick start helps you create a free subscription and your first database. (Start here if you\u0026rsquo;re new.)\nCreate a Fixed subscription\nCreate a Flexible subscription\nTo create an Annual subscription, contact support.\nView subscription details:\nView or update a Fixed subscription\nView Flexible subscription\nDelete a subscription\n","categories":["RC"]},{"uri":"/rs/databases/connect/test-client-connectivity/","uriRel":"/rs/databases/connect/test-client-connectivity/","title":"Test client connection","tags":[],"keywords":[],"description":"","content":"In various scenarios, such as after creating a new cluster or upgrading the cluster, it is highly advisable to verify client connectivity to the database.\nTo test client connectivity:\nCreate a Redis database and get the database endpoint, which contains the cluster name (FQDN). Try to connect to the database endpoint from your client of choice, and execute commands against the database. If the database does not respond, try to connect to the database endpoint by using the IP address rather than the FQDN; if you succeed, it means that the DNS is not properly configured. For additional details, refer to DNS. If any issues are encountered during the connectivity test, contact our support at support@redislabs.com.\nTest connecting to your database With the Redis database created, you are ready to connect to your database to store data. You can use one of the following ways to test connectivity to your database:\nConnecting with redis-cli, the built-in command-line tool Connecting with a Hello World application using Python. Connecting using redis-cli Run redis-cli, located in the /opt/redislabs/bin directory, to connect to port 12000 and store and retrieve a key in database1\n# sudo /opt/redislabs/bin/redis-cli -p 12000 127.0.0.1:16653\u0026gt; set key1 123 OK 127.0.0.1:16653\u0026gt; get key1 \u0026#34;123\u0026#34; Connect with a simple Python app A simple python application running on the host machine can also connect to the database1.\nNote: The following section assumes you already have python and redis-py (python library for connecting to Redis) configured on the host machine running the container. You can find the instructions to configure redis-py on the github page for redis-py. In the command-line Terminal, create a new file called redis_test.py\nvi redis_test.py Paste the following into a file named redis_test.py.\nimport redis r = redis.StrictRedis(host=\u0026#39;localhost\u0026#39;, port=12000, db=0) print (\u0026#34;set key1 123\u0026#34;) print (r.set(\u0026#39;key1\u0026#39;, \u0026#39;123\u0026#39;)) print (\u0026#34;get key1\u0026#34;) print(r.get(\u0026#39;key1\u0026#39;)) Run \u0026ldquo;redis_test.py\u0026rdquo; application to connect to the database and store and retrieve a key using the command-line.\npython redis_test.py The output should look like the following screen if the connection is successful.\nset key1 123 True get key1 123 Test database connectivity with a simple application You can also use a simple application to test connectivity to your database. Here is a simple python app that connects to the database by IP address. The app uses the discovery service that is compliant with Redis Sentinel API.\nIn the IP-based connection method, you only need the database name, not the port number. Here we simply use the discovery service that listens on port 8001 on all nodes of the cluster to discover the endpoint for the database named \u0026ldquo;db1\u0026rdquo;.\nfrom redis.sentinel import Sentinel # with IP based connections, a list of known node IP addresses is constructed # to allow connection even if any one of the nodes in the list is unavailable. sentinel_list = [ (\u0026#39;10.0.0.44\u0026#39;, 8001), (\u0026#39;10.0.0.45\u0026#39;, 8001), (\u0026#39;10.0.0.45\u0026#39;, 8001) ] # change this to the db name you want to connect db_name = \u0026#39;db1\u0026#39; sentinel = Sentinel(sentinel_list, socket_timeout=0.1) r = sentinel.master_for(db_name, socket_timeout=0.1) # set key \u0026#34;foo\u0026#34; to value \u0026#34;bar\u0026#34; print r.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;) # set value for key \u0026#34;foo\u0026#34; print r.get(\u0026#39;foo\u0026#39;) In the URL-based connection method, you need to specify the endpoint and the port number for your database.\nimport redis # the URL provided to redis. Redis method comes from the database configuration # property called \u0026#34;Endpoint\u0026#34;. The endpoint URL generated by the database is a # combination of the cluster name (FQDN) and database port number. r = redis.Redis( host=\u0026#39;redis-19836.c9.us-east-1-2.ec2.cloud.redislabs.com\u0026#39;, port=19836) # set key \u0026#34;foo\u0026#34; to value “bar” print(r.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;)) # set value for key \u0026#34;foo\u0026#34; print(r.get(\u0026#39;foo\u0026#39;)) ","categories":["RS"]},{"uri":"/rc/security/database-security/tls-ssl/","uriRel":"/rc/security/database-security/tls-ssl/","title":"Transport Layer Security (TLS)","tags":[],"keywords":[],"description":"Enable TLS to encrypt data communications between applications and Redis databases.","content":"Transport Layer Security (TLS) uses encryption to secure network communications.\nRedis Cloud Fixed, Flexible, and Annual subscriptions can use TLS to encrypt data communications between applications and Redis databases.\nUse TLS with Redis Cloud TLS is not enabled by default.\nTLS recommendations Because TLS has an impact on performance, you need to determine whether the security benefits of TLS are worth the performance impact. TLS recommendations depend on the subscription plan and whether clients connect to your database using public or private endpoints.\nThis table shows TLS recommendations:\nSubscription Public endpoint Private endpoint Fixed Enable TLS N/A Flexible Enable TLS Enable TLS if security outweighs performance impact Annual Enable TLS Enable TLS if security outweighs performance impact Client authentication When you enable TLS, you can optionally require client authentication (also known as \u0026ldquo;mutual authentication\u0026rdquo;). If enabled, all clients must present a valid client certificate when they connect to the database.\nClient authentication is not required by Redis Cloud; however, it is strongly recommended.\nEnable TLS To enable TLS for a Redis Cloud database:\nSelect Databases from the admin console menu and then select your database from the list.\nFrom the database\u0026rsquo;s Configuration screen, select the Edit database button:\nIn the Security section, use the Transport layer security (TLS) toggle to enable TLS:\nSelect the Download server certificate button to download the Redis Cloud certificate bundle redis_ca.pem:\nDecide whether you want to require client authentication:\nIf you only want clients that present a valid certificate to be able to connect, continue to the next step.\nIf you do not want to require client authentication, skip to the final step to apply your changes.\nTo require client authentication, select the TLS client authentication checkbox.\nEither provide an X.509 certificate that contains a public key for your client or select Generate certificate to create one:\nIf you generate your certificate from the admin console, a ZIP file download will start. The download contains:\nredis_user.crt – the certificate\u0026rsquo;s public key.\nredis_user_private.key – the certificate\u0026rsquo;s private key.\nTo apply your changes and enable TLS, select the Save database button:\nNote: Once you\u0026rsquo;ve enabled TLS, all client connections to your database must use TLS. Unencrypted connections will no longer be permitted. Connect over TLS To connect to a Redis Cloud database over TLS, you need:\nA Redis client that supports TLS Redis Cloud CA certificates Download certificates If you don\u0026rsquo;t have the Redis Cloud CA certificates, you can download them from the admin console:\nEither select Account Settings from the admin console menu or go to the database\u0026rsquo;s Configuration screen.\nGo to the Security section.\nFor Redis Cloud certificate authority, either:\nSelect the Download button to download the certificates from Account Settings:\nSelect the Download server certificate button to download the certificates from the database\u0026rsquo;s Configuration screen:\nThe download contains a file called redis_ca.pem, which includes the following certificates:\nSelf-signed Redis Cloud Fixed plan Root CA (deprecated but still in use)\nSelf-signed Redis Cloud Flexible plan Root CA and intermediate CA (deprecated but still in use)\nPublicly trusted GlobalSign Root CA and intermediate CA\nTo inspect the certificates in redis_ca.pem, run the keytool command:\nkeytool -printcert -file ./redis_ca.pem | grep \u0026#34;Owner:\u0026#34; You can add redis_ca.pem to the trust store or pass it directly to a Redis client.\nIf your database requires client authentication, you also need the public (redis_user.crt) and private (redis_user_private.key) client keys. See Enable TLS for details.\nConnect with the Redis CLI Here\u0026rsquo;s how to use the Redis CLI to connect to a TLS-enabled Redis Cloud database.\nEndpoint and port details are available from the Databases list or the database\u0026rsquo;s Configuration screen.\nWithout client authentication If your database doesn\u0026rsquo;t require client authentication, then provide the Redis Cloud CA certificate bundle (redis_ca.pem) when you connect:\nredis-cli -h \u0026lt;endpoint\u0026gt; -p \u0026lt;port\u0026gt; --tls --cacert redis_ca.pem With client authentication If your database requires client authentication, then you also need to provide your client\u0026rsquo;s private and public keys:\nredis-cli -h \u0026lt;endpoint\u0026gt; -p \u0026lt;port\u0026gt; --tls --cacert redis_ca.pem \\ --cert redis_user.crt --key redis_user_private.key ","categories":["RC"]},{"uri":"/rs/databases/import-export/replica-of/replicaof-repeatedly-fails/","uriRel":"/rs/databases/import-export/replica-of/replicaof-repeatedly-fails/","title":"Replica Of Repeatedly Fails","tags":[],"keywords":[],"description":"Troubleshoot when the Replica Of process repeatedly fails and restarts.","content":"Problem: The Replica Of process repeatedly fails and restarts\nDiagnostic: A log entry in the Redis log of the source database shows repeated failures and restarts.\nCause: The Redis \u0026ldquo;client-output-buffer-limit\u0026rdquo; setting on the source database is configured to a relatively small value, which causes the connection drop.\nResolution: Reconfigure the buffer on the source database to a bigger value:\nIf the source is a Redis database on a Redis Enterprise Software cluster, increase the replica buffer size of the source database with:\nrladmin tune db \u0026lt; db:id | name \u0026gt; slave_buffer \u0026lt; value \u0026gt;\nIf the source is a Redis database not on a Redis Enterprise Software cluster, use the config set command through redis-cli to increase the client output buffer size of the source database with:\nconfig set client-output-buffer-limit \u0026quot;slave \u0026lt;hard_limit\u0026gt; \u0026lt;soft_limit\u0026gt; \u0026lt;soft_seconds\u0026gt;\u0026quot;\nAdditional information: Top Redis Headaches for DevOps - Replication Buffer\n","categories":["RS"]},{"uri":"/rs/security/certificates/updating-certificates/","uriRel":"/rs/security/certificates/updating-certificates/","title":"Update certificates","tags":[],"keywords":[],"description":"Update certificates in a Redis Enterprise cluster.","content":" Warning - When you update the certificates, the new certificate replaces the same certificates on all nodes in the cluster. How to update certificates You can use either the rladmin command-line interface (CLI) or the REST API to update certificates.\nThe new certificates are used the next time the clients connect to the database.\nWhen you upgrade Redis Enterprise Software, the upgrade process copies the certificates that are on the first upgraded node to all of the nodes in the cluster.\nUse the CLI To replace certificates with the rladmin CLI, run the cluster certificate set command:\nrladmin cluster certificate set \u0026lt;cert-name\u0026gt; certificate_file \u0026lt;cert-file-name\u0026gt;.pem key_file \u0026lt;key-file-name\u0026gt;.pem Replace the following variables with your own values:\n\u0026lt;cert-name\u0026gt; - The name of the certificate you want to replace. See the certificates table for the list of valid certificate names. \u0026lt;cert-file-name\u0026gt; - The name of your certificate file \u0026lt;key-file-name\u0026gt; - The name of your key file For example, to replace the admin console (cm) certificate with the private key key.pem and the certificate file cluster.pem:\nrladmin cluster certificate set cm certificate_file cluster.pem key_file key.pem Use the REST API To replace a certificate using the REST API, use PUT /v1/cluster/update_cert:\nPUT https://[host][:port]/v1/cluster/update_cert \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;cert_name\u0026gt;\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;\u0026lt;key\u0026gt;\u0026#34;, \u0026#34;certificate\u0026#34;: \u0026#34;\u0026lt;cert\u0026gt;\u0026#34; }\u0026#39; Replace the following variables with your own values:\n\u0026lt;cert_name\u0026gt; - The name of the certificate to replace. See the certificates table for the list of valid certificate names.\n\u0026lt;key\u0026gt; - The contents of the *_key.pem file\nTip - The key file contains \\n end of line characters (EOL) that you cannot paste into the API call. You can use sed -z 's/\\n/\\\\\\n/g' to escape the EOL characters. \u0026lt;cert\u0026gt; - The contents of the *_cert.pem file\nActive-Passive database certificates Update proxy certificates To update the proxy certificate on clusters running Active-Passive (Replica Of) databases:\nStep 1: Use rladmin or the REST API to update the proxy certificate on the source database cluster. Step 2: From the admin console, update the destination database (replica) configuration with the new certificate. Note: Perform Step 2 as quickly as possible after performing Step 1. Connections using the previous certificate are rejected after applying the new certificate. Until both steps are performed, recovery of the database sync cannot be established. Active-Active database certificates Update proxy certificates To update proxy certificate on clusters running Active-Active databases:\nStep 1: Use rladmin or the REST API to update proxy certificates on a single cluster, multiple clusters, or all participating clusters.\nStep 2: Use the crdb-cli utility to update Active-Active database configuration from the command line. Run the following command once for each Active-Active database residing on the modified clusters:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; --force Note: Perform Step 2 as quickly as possible after performing Step 1. Connections using the previous certificate are rejected after applying the new certificate. Until both steps are performed, recovery of the database sync cannot be established . Do not run any other crdb-cli crdb update operations between the two steps. Update syncer certificates To update your syncer certificate on clusters running Active-Active databases, follow these steps:\nStep 1: Update your syncer certificate on one or more of the participating clusters using the rladmin command, REST API, or admin console. You can update a single cluster, multiple clusters, or all participating clusters.\nStep 2: Update the Active-Active database configuration from the command line with the crdb-cli utility. Run this command once for each Active-Active database that resides on the modified clusters:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; --force Note: Run step 2 as quickly as possible after step 1. Between the two steps, new syncer connections that use the ‘old’ certificate will get rejected by the cluster that has been updated with the new certificate (in step 1). Do not run any other crdb-cli crdb update operations between the two steps. Known limitation: Updating syncer certificate on versions prior to 6.0.20-81 will restart the proxy and syncer connections. We recommend you schedule the certificate replacement carefully. ","categories":["RS"]},{"uri":"/rs/databases/active-active/planning/","uriRel":"/rs/databases/active-active/planning/","title":"Considerations for planning Active-Active databases","tags":[],"keywords":[],"description":"Information about Active-Active database to take into consideration while planning a deployment, such as compatibility, limitations, and special configuration","content":"In Redis Enterprise, Active-Active geo-distribution is based on conflict-free replicated data type (CRDT) technology. Compared to databases without geo-distribution, Active-Active databases have more complex replication and networking, as well as a different data type.\nBecause of the complexities of Active-Active databases, there are special considerations to keep in mind while planning your Active-Active database.\nSee Active-Active Redis for more information about geo-distributed replication. For more info on other high availability features, see Durability and high availability.\nParticipating clusters For Active-Active databases, you need to set up your participating clusters. At least two participating clusters. If your database requires more than ten participating clusters, contact Redis support. You can add or remove participating clusters after database creation.\nChanges made from the admin console to an Active-Active database configuration only apply to the cluster you are editing. For global configuration changes across all clusters, use the crdb-cli command-line utility.\nMemory limits Database memory limits define the maximum size of your database across all database replicas and shards on the cluster. Your memory limit also determines the number of shards.\nBesides your dataset, the memory limit must also account for replication, Active-Active metadata, and module overhead. These features can increase your database size, sometimes increasing it by two times or more.\nFactors to consider when sizing your database:\ndataset size: you want your limit to be above your dataset size to leave room for overhead. database throughput: high throughput needs more shards, leading to a higher memory limit. modules: using modules with your database can consume more memory. database clustering: enables you to spread your data into shards across multiple nodes (scale out). database replication: enabling replication doubles memory consumption Active-Active replication: enabling Active-Active replication requires double the memory of regular replication, which can be up to two times (2x) the original data size per instance. database replication backlog for synchronization between shards. By default, this is set to 1% of the database size. Active-Active replication backlog for synchronization between clusters. By default, this is set to 1% of the database size. It\u0026rsquo;s also important to know Active-Active databases have a lower threshold for activating the eviction policy, because it requires propagation to all participating clusters. The eviction policy starts to evict keys when one of the Active-Active instances reaches 80% of its memory limit.\nFor more information on memory limits, see Memory and performance or Database memory limits.\nNetworking Network requirements for Active-Active databases include:\nA VPN between each network that hosts a cluster with an instance (if your database spans WAN). A network connection to several ports on each cluster from all nodes in all participating clusters. A network time service running on each node in all clusters. Networking between the clusters must be configured before creating an Active-Active database. The setup will fail if there is no connectivity between the clusters.\nNetwork ports Every node must have access to the REST API ports of every other node as well as other ports for proxies, VPNs, and the admin console. See Network port configurations for more details. These ports should be allowed through firewalls that may be positioned between the clusters.\nNetwork Time Service Active-Active databases require a time service like NTP or Chrony to make sure the clocks on all cluster nodes are synchronized. This is critical to avoid problems with internal cluster communications that can impact your data integrity.\nSee Synchronizing cluster node clocks for more information.\nRedis modules Several Redis modules are compatible with Active-Active databases. Find the list of compatible Redis modules. Note: Starting with v6.2.18, you can index, query, and perform full-text searches of nested JSON documents in Active-Active databases by combining RedisJSON and RediSearch. Limitations Active-Active databases have the following limitations:\nAn existing database can\u0026rsquo;t be changed into an Active-Active database. To move data from an existing database to an Active-Active database, you must create a new Active-Active database and migrate the data. Discovery service is not supported with Active-Active databases. Active-Active databases require FQDNs or mDNS. The FLUSH command is not supported from the CLI. To flush your database, use the API or admin console. Cross slot multi commands (such as MSET) are not supported with Active-Active databases. The hashing policy can\u0026rsquo;t be changed after database creation. ","categories":["RS"]},{"uri":"/modules/redisearch/search-json-quickstart/","uriRel":"/modules/redisearch/search-json-quickstart/","title":"Search JSON quick start","tags":[],"keywords":[],"description":"RediSearch quick start on how to index JSON documents and run search queries.","content":"This quick start shows you how to index JSON documents and run search queries against the index.\nPrerequisites For this quick start tutorial, you need:\nEither:\nA Redis Cloud database with Redis Stack\nA Redis Enterprise Software database with RediSearch (v2.2 or later) and RedisJSON (v2.0 or later)\nAnd:\nredis-cli command-line tool\nredis-py client library v4.0.0 or later\nSearch JSON with redis-cli To begin, connect to your database with redis-cli.\nCreate an index To create an index for JSON documents with FT.CREATE, use the ON JSON keyword.\nYou also need to include a schema definition that indicates which JSON elements to index. When you define the schema, use a JSON path expression to map a specific JSON element to a schema field:\nSCHEMA \u0026lt;JSONPath\u0026gt; AS \u0026lt;field_name\u0026gt; \u0026lt;TYPE\u0026gt; The examples in this tutorial use the extended JSONPath syntax.\nThe schema for this example includes four fields:\nname (TEXT) description (TEXT) connectionType (TEXT) price (NUMERIC) This example defines the schema and creates the index:\n127.0.0.1:12543\u0026gt; FT.CREATE itemIdx ON JSON PREFIX 1 item: SCHEMA $.name AS name TEXT $.description as description TEXT $.connection.type AS connectionType TEXT $.price AS price NUMERIC After you create the index, RediSearch automatically adds all existing and future JSON documents prefixed with item: to the index.\nSee Index limitations for more details about schema restrictions for JSON document indexes.\nAdd JSON documents You can create and store JSON documents in the database with the JSON.SET command.\nThe following examples use these JSON documents to represent individual inventory items.\nItem 1 JSON document:\n{ \u0026#34;name\u0026#34;: \u0026#34;Noise-cancelling Bluetooth headphones\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Wireless Bluetooth headphones with noise-cancelling technology\u0026#34;, \u0026#34;connection\u0026#34;: { \u0026#34;wireless\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;Bluetooth\u0026#34; }, \u0026#34;price\u0026#34;: 99.98, \u0026#34;stock\u0026#34;: 25, \u0026#34;colors\u0026#34;: [ \u0026#34;black\u0026#34;, \u0026#34;silver\u0026#34; ] } Item 2 JSON document:\n{ \u0026#34;name\u0026#34;: \u0026#34;Wireless earbuds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Wireless Bluetooth in-ear headphones\u0026#34;, \u0026#34;connection\u0026#34;: { \u0026#34;wireless\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;Bluetooth\u0026#34; }, \u0026#34;price\u0026#34;: 64.99, \u0026#34;stock\u0026#34;: 17, \u0026#34;colors\u0026#34;: [ \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34; ] } To store these JSON documents in the database, run the following commands:\n127.0.0.1:12543\u0026gt; JSON.SET item:1 $ \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Noise-cancelling Bluetooth headphones\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Wireless Bluetooth headphones with noise-cancelling technology\u0026#34;,\u0026#34;connection\u0026#34;:{\u0026#34;wireless\u0026#34;:true,\u0026#34;type\u0026#34;:\u0026#34;Bluetooth\u0026#34;},\u0026#34;price\u0026#34;:99.98,\u0026#34;stock\u0026#34;:25,\u0026#34;colors\u0026#34;:[\u0026#34;black\u0026#34;,\u0026#34;silver\u0026#34;]}\u0026#39; \u0026#34;OK\u0026#34; 127.0.0.1:12543\u0026gt; JSON.SET item:2 $ \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Wireless earbuds\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Wireless Bluetooth in-ear headphones\u0026#34;,\u0026#34;connection\u0026#34;:{\u0026#34;wireless\u0026#34;:true,\u0026#34;type\u0026#34;:\u0026#34;Bluetooth\u0026#34;},\u0026#34;price\u0026#34;:64.99,\u0026#34;stock\u0026#34;:17,\u0026#34;colors\u0026#34;:[\u0026#34;black\u0026#34;,\u0026#34;white\u0026#34;]}\u0026#39; \u0026#34;OK\u0026#34; Search the index To search the index for JSON documents that match a specified condition, use the FT.SEARCH command. You can search any field defined in the index schema.\nFor more information about search queries, see Search query syntax.\nReturn the entire document Search query results include entire JSON documents by default.\nFor example, search for Bluetooth headphones with a price less than 70:\n127.0.0.1:6379\u0026gt; FT.SEARCH itemIdx \u0026#39;@description:(headphones) @connectionType:(bluetooth) @price:[0 70]\u0026#39; 1) \u0026#34;1\u0026#34; 2) \u0026#34;item:2\u0026#34; 3) 1) \u0026#34;$\u0026#34; 2) \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Wireless earbuds\\\u0026#34;,\\\u0026#34;description\\\u0026#34;:\\\u0026#34;Wireless Bluetooth in-ear headphones\\\u0026#34;,\\\u0026#34;connection\\\u0026#34;:{\\\u0026#34;wireless\\\u0026#34;:true,\\\u0026#34;connection\\\u0026#34;:\\\u0026#34;Bluetooth\\\u0026#34;},\\\u0026#34;price\\\u0026#34;:64.99,\\\u0026#34;stock\\\u0026#34;:17,\\\u0026#34;colors\\\u0026#34;:[\\\u0026#34;black\\\u0026#34;,\\\u0026#34;white\\\u0026#34;]}\u0026#34; Return specific fields If you want to limit the search results to include only specific parts of a JSON document, use field projection. The RETURN option lets you specify which fields to include.\nThe following query uses the JSONPath expression $.stock to return each item\u0026rsquo;s stock in addition to the name and price:\n127.0.0.1:6379\u0026gt; FT.SEARCH itemIdx \u0026#39;@description:(headphones)\u0026#39; RETURN 5 name price $.stock AS stock 1) \u0026#34;2\u0026#34; 2) \u0026#34;item:1\u0026#34; 3) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Noise-cancelling Bluetooth headphones\u0026#34; 3) \u0026#34;price\u0026#34; 4) \u0026#34;99.98\u0026#34; 5) \u0026#34;stock\u0026#34; 6) \u0026#34;25\u0026#34; 4) \u0026#34;item:2\u0026#34; 5) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Wireless earbuds\u0026#34; 3) \u0026#34;price\u0026#34; 4) \u0026#34;64.99\u0026#34; 5) \u0026#34;stock\u0026#34; 6) \u0026#34;17\u0026#34; Aggregate search results The FT.AGGREGATE command lets you run a search query and modify the results with operations such as SORTBY, REDUCE, LIMIT, FILTER, and more. For a detailed list of available operations and examples, see Aggregations.\nTo run aggregations on JSON documents, pass JSON path expressions to the LOAD option. You can use any JSON element, even elements that are not included in the index schema.\nThis example uses aggregation operations to calculate a 10% price discount for each item and then sorts the items from least expensive to most expensive:\n127.0.0.1:6379\u0026gt; FT.AGGREGATE itemIdx \u0026#39;*\u0026#39; LOAD 4 name $.price AS originalPrice APPLY \u0026#39;@originalPrice - (@originalPrice * 0.10)\u0026#39; AS salePrice SORTBY 2 @salePrice ASC 1) \u0026#34;2\u0026#34; 2) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Wireless earbuds\u0026#34; 3) \u0026#34;originalPrice\u0026#34; 4) \u0026#34;64.99\u0026#34; 5) \u0026#34;salePrice\u0026#34; 6) \u0026#34;58.491\u0026#34; 3) 1) \u0026#34;name\u0026#34; 2) \u0026#34;Noise-cancelling Bluetooth headphones\u0026#34; 3) \u0026#34;originalPrice\u0026#34; 4) \u0026#34;99.98\u0026#34; 5) \u0026#34;salePrice\u0026#34; 6) \u0026#34;89.982\u0026#34; Drop the index To remove the index without deleting any associated documents, run the FT.DROPINDEX command:\n127.0.0.1:12543\u0026gt; FT.DROPINDEX itemIdx OK Search JSON with Python If you want to use RediSearch within an application, these client libraries are available.\nThe following example uses the Redis Python client library redis-py, which supports RediSearch commands as of v4.0.0.\nThis Python code indexes JSON documents, runs search and aggregation queries, and then deletes the index:\nimport redis from redis.commands.search.field import TextField, NumericField from redis.commands.search.indexDefinition import IndexDefinition, IndexType from redis.commands.search.query import Query from redis.commands.search.aggregation import AggregateRequest, Asc # Connect to a database r = redis.Redis(host=\u0026#34;\u0026lt;endpoint\u0026gt;\u0026#34;, port=\u0026#34;\u0026lt;port\u0026gt;\u0026#34;, password=\u0026#34;\u0026lt;password\u0026gt;\u0026#34;) # Options for index creation index_def = IndexDefinition( index_type=IndexType.JSON, prefix = [\u0026#39;item:\u0026#39;], score = 0.5, score_field = \u0026#39;doc_score\u0026#39; ) # Schema definition schema = ( TextField(\u0026#39;$.name\u0026#39;, as_name=\u0026#39;name\u0026#39;), TextField(\u0026#39;$.description\u0026#39;, as_name=\u0026#39;description\u0026#39;), TextField(\u0026#39;$.connection.type\u0026#39;, as_name=\u0026#39;connectionType\u0026#39;), NumericField(\u0026#39;$.price\u0026#39;, as_name=\u0026#39;price\u0026#39;) ) # Create an index and pass in the schema r.ft(\u0026#39;py_item_idx\u0026#39;).create_index(schema, definition = index_def) # Dictionaries that represent JSON documents doc1 = { \u0026#34;name\u0026#34;: \u0026#34;Noise-cancelling Bluetooth headphones\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Wireless Bluetooth headphones with noise-cancelling technology\u0026#34;, \u0026#34;connection\u0026#34;: { \u0026#34;wireless\u0026#34;: True, \u0026#34;type\u0026#34;: \u0026#34;Bluetooth\u0026#34; }, \u0026#34;price\u0026#34;: 99.98, \u0026#34;stock\u0026#34;: 25, \u0026#34;colors\u0026#34;: [ \u0026#34;black\u0026#34;, \u0026#34;silver\u0026#34; ] } doc2 = { \u0026#34;name\u0026#34;: \u0026#34;Wireless earbuds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Wireless Bluetooth in-ear headphones\u0026#34;, \u0026#34;connection\u0026#34;: { \u0026#34;wireless\u0026#34;: True, \u0026#34;type\u0026#34;: \u0026#34;Bluetooth\u0026#34; }, \u0026#34;price\u0026#34;: 64.99, \u0026#34;stock\u0026#34;: 17, \u0026#34;colors\u0026#34;: [ \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34; ] } # Add documents to the database and index them r.json().set(\u0026#39;item:1\u0026#39;, \u0026#39;$\u0026#39;, doc1) r.json().set(\u0026#39;item:2\u0026#39;, \u0026#39;$\u0026#39;, doc2) # Search the index for a string search_result = r.ft(\u0026#39;py_item_idx\u0026#39;).search(Query(\u0026#39;@name:(earbuds)\u0026#39;) .return_field(\u0026#39;name\u0026#39;) .return_field(\u0026#39;price\u0026#39;) .return_field(\u0026#39;$.stock\u0026#39;, as_field=\u0026#39;stock\u0026#39;)) # The result has the total number of search results and a list of documents print(\u0026#39;Results for \u0026#34;earbuds\u0026#34;:\u0026#39;) print(search_result.total) for doc in search_result.docs: print(doc) print() # Use aggregation to calculate a 10% price discount for each item and sort them from least expensive to most expensive aggregate_query = AggregateRequest(\u0026#39;*\u0026#39;).load(\u0026#39;name\u0026#39;, \u0026#39;price\u0026#39;).apply(salePrice=\u0026#39;@price - (@price * 0.10)\u0026#39;).sort_by(Asc(\u0026#39;@salePrice\u0026#39;)) aggregate_result = r.ft(\u0026#39;py_item_idx\u0026#39;).aggregate(aggregate_query).rows # Display the aggregation result print(\u0026#39;Aggregation result:\u0026#39;) for result in aggregate_result: print(result) # Delete the index; set delete_documents to True to delete indexed documents as well r.ft(\u0026#39;py_item_idx\u0026#39;).dropindex(delete_documents=False) Example output:\n$ python3 quick_start.py Results for \u0026#34;earbuds\u0026#34;: 1 Document {\u0026#39;id\u0026#39;: \u0026#39;item:2\u0026#39;, \u0026#39;payload\u0026#39;: None, \u0026#39;name\u0026#39;: \u0026#39;Wireless earbuds\u0026#39;, \u0026#39;price\u0026#39;: \u0026#39;64.99\u0026#39;, \u0026#39;stock\u0026#39;: \u0026#39;17\u0026#39;} Aggregation result: [b\u0026#39;name\u0026#39;, b\u0026#39;Wireless earbuds\u0026#39;, b\u0026#39;price\u0026#39;, b\u0026#39;64.99\u0026#39;, b\u0026#39;salePrice\u0026#39;, b\u0026#39;58.491\u0026#39;] [b\u0026#39;name\u0026#39;, b\u0026#39;Noise-cancelling Bluetooth headphones\u0026#39;, b\u0026#39;price\u0026#39;, b\u0026#39;99.98\u0026#39;, b\u0026#39;salePrice\u0026#39;, b\u0026#39;89.982\u0026#39;] More info RediSearch commands RedisJSON commands RediSearch query syntax Details about RediSearch query features RediSearch client libraries ","categories":["Modules"]},{"uri":"/modules/redisbloom/commands/","uriRel":"/modules/redisbloom/commands/","title":"RedisBloom commands","tags":[],"keywords":[],"description":"Lists RedisBloom commands and provides links to the command reference pages.","content":"The following tables list RedisBloom commands. See the command links for more information about each command\u0026rsquo;s syntax, arguments, and examples.\nBloom filter commands Command Redis\nEnterprise Redis\nCloud Description BF.ADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an item to the filter. BF.EXISTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Checks if an item exists in the filter. BF.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns information about a Bloom filter. BF.INSERT ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds multiple items to a filter. If the key does not exist, it creates a new filter. BF.LOADCHUNK ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Restores a Bloom filter previously saved with BF.SCANDUMP. BF.MADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds multiple items to the filter. BF.MEXISTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed For multiple items, checks if each item exists in the filter. BF.RESERVE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates a Bloom filter. Sets the false positive rate and capacity. BF.SCANDUMP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Starts an incremental save of a Bloom filter. Cuckoo filter commands Command Redis\nEnterprise Redis\nCloud Description CF.ADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an item to a filter. CF.ADDNX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an item to a filter only if the item does not already exist. CF.COUNT ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the probable number of times an item occurs in the filter. CF.DEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes one instance of an item from the filter. CF.EXISTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Checks if an item exists in the filter. CF.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns information about a cuckoo filter. CF.INSERT ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds multiple items to a filter. Optionally sets the capacity if the filter does not already exist. CF.INSERTNX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds multiple items to a filter if they do not already exist. Optionally sets the capacity if the filter does not already exist. CF.LOADCHUNK ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Restores a cuckoo filter previously saved with CF.SCANDUMP. CF.MEXISTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed For multiple items, checks if each item exists in the filter. CF.RESERVE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates a cuckoo filter and sets its capacity. CF.SCANDUMP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Starts an incremental save of a cuckoo filter. Count-min sketch commands Command Redis\nEnterprise Redis\nCloud Description CMS.INCRBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Increases item counts. CMS.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns width, depth, and total count of the sketch. CMS.INITBYDIM ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Initializes a count-min sketch to the specified dimensions (width and depth). CMS.INITBYPROB ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Initializes a count-min sketch to allow the specified overestimation percent for the item count and the probability of overestimation. CMS.MERGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Merges several sketches into one sketch. CMS.QUERY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the count for one or more items in a sketch. Top-k commands Command Redis\nEnterprise Redis\nCloud Description TOPK.ADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an item to the data structure. TOPK.COUNT ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns probable item counts. TOPK.INCRBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Increases the score of an item by the specified number. TOPK.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the number of required items (k), width, depth, and decay values. TOPK.LIST ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the keys of items in the top-k list. Optionally returns their item counts. TOPK.QUERY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Checks whether an item is one of top-k items. TOPK.RESERVE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Initializes a top-k with the specified number of top occurring items to keep, width, depth, and decay. T-digest sketch commands Command Redis\nEnterprise Redis\nCloud Description TDIGEST.ADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds one or more samples to a t-digest sketch. TDIGEST.CDF ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Estimates the fraction of all observations which are less than or equal to the specified value. TDIGEST.CREATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Allocates memory and initializes a t-digest sketch. TDIGEST.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns information about the t-digest sketch. TDIGEST.MAX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the maximum value from the sketch. TDIGEST.MERGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Copies values from one sketch to another. TDIGEST.MERGESTORE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Merges multiple sketches and stores the combined sketch in a new key. TDIGEST.MIN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the minimum value from the sketch. TDIGEST.QUANTILE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Estimates one or more cutoffs. TDIGEST.RESET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Resets the sketch and reinitializes it. TDIGEST.TRIMMED_MEAN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Estimates the mean value from the sketch, excluding values outside the specified range. ","categories":["Modules"]},{"uri":"/modules/redisearch/commands/","uriRel":"/modules/redisearch/commands/","title":"RediSearch commands","tags":[],"keywords":[],"description":"Lists RediSearch commands and provides links to the command reference pages.","content":"The following table lists RediSearch commands. See the command links for more information about each command\u0026rsquo;s syntax, arguments, and examples.\nCommand Redis\nEnterprise Redis\nCloud Description FT.AGGREGATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Runs a search query on an index and groups, sorts, transforms, limits, and/or filters the results. FT.ALIASADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an alias to an index. FT.ALIASDEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes an alias from an index. FT.ALIASUPDATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds an alias to an index. If the alias already exists for a different index, it updates the alias to point to the specified index instead. FT.ALTER ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds a new field to an index. FT.CONFIG GET ❌ Not supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Displays configuration options. FT.CONFIG HELP ❌ Not supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Describes configuration options. FT.CONFIG SET ❌ Not supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Sets configuration options. FT.CREATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates an index. FT.CURSOR DEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Deletes a cursor. FT.CURSOR READ ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Reads the next results from an existing cursor. FT.DICTADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Adds terms to a dictionary. FT.DICTDEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Deletes terms from a dictionary. FT.DICTDUMP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns all terms in the specified dictionary. FT.DROPINDEX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Deletes an index. FT.EXPLAIN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the execution plan for a complex query as a string. FT.EXPLAINCLI ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the execution plan for a complex query as an array. FT.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns index information and statistics. FT._LIST ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Lists all indexes. FT.PROFILE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Runs FT.SEARCH or FT.AGGREGATE and reports performance information. FT.SEARCH ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Searches an index for a text query and returns matching documents or document IDs. FT.SPELLCHECK ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Suggests spelling corrections for misspelled terms in a query. FT.SYNDUMP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns a list of synonym terms and their synonym group IDs. FT.SYNUPDATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates or updates a synonym group with additional terms. FT.TAGVALS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns all distinct values indexed in a tag field. ","categories":["Modules"]},{"uri":"/modules/redisgraph/commands/","uriRel":"/modules/redisgraph/commands/","title":"RedisGraph commands","tags":[],"keywords":[],"description":"Lists RedisGraph commands and provides links to the command reference pages.","content":"The following table lists RedisGraph commands. See the command links for more information about each command\u0026rsquo;s syntax, arguments, and examples.\nCommand Redis\nEnterprise Redis\nCloud Description GRAPH.CONFIG GET ✅ Supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Returns the current value of a RedisGraph configuration parameter. GRAPH.CONFIG SET ✅ Supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Changes the value of a RedisGraph configuration parameter. GRAPH.DELETE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes the graph and its entities. GRAPH.EXPLAIN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Displays the query execution plan but does not run the query. GRAPH.LIST ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Lists all graph keys. GRAPH.PROFILE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Runs a query and displays the execution plan with metrics for each operation. GRAPH.QUERY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Runs a query against a graph. Supports a variety of clauses and functions. GRAPH.RO_QUERY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Runs a read-only query against a graph. GRAPH.SLOWLOG ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the slowest 10 queries run against a specific graph. ","categories":["Modules"]},{"uri":"/modules/redisjson/commands/","uriRel":"/modules/redisjson/commands/","title":"RedisJSON commands","tags":[],"keywords":[],"description":"Lists RedisJSON commands and provides links to the command reference pages.","content":"The following table lists RedisJSON commands. See the command links for more information about each command\u0026rsquo;s syntax, arguments, and examples.\nCommand Redis\nEnterprise Redis\nCloud Description JSON.ARRAPPEND ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Appends an element to a JSON array. JSON.ARRINDEX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the index of a value\u0026rsquo;s first occurrence in a JSON array. JSON.ARRINSERT ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Inserts JSON values into a JSON array before the given index. JSON.ARRLEN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the length of a JSON array. JSON.ARRPOP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes and returns an element located at the index in the JSON array. JSON.ARRTRIM ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Trims a JSON array so that it contains only the specified inclusive range of elements. JSON.CLEAR ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Clears container values (arrays/objects) and sets numeric values to 0. JSON.DEBUG ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Debugging container command. JSON.DEBUG HELP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns helpful information about the JSON.DEBUG command. JSON.DEBUG MEMORY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Reports a JSON element\u0026rsquo;s memory usage in bytes. JSON.DEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes a JSON element. JSON.FORGET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes a JSON element, the same as JSON.DEL. JSON.GET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the value of an element in JSON-serialized form. JSON.MGET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the values of multiple elements. JSON.NUMINCRBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Increments the number stored at path by the specified number. JSON.NUMMULTBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Multiplies the number stored at path by the specified number. (deprecated as of RedisJSON v2.0) JSON.OBJKEYS ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the keys contained in the specified JSON object. JSON.OBJLEN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the number of keys in the specified JSON object. JSON.RESP ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns a JSON element in Redis Serialization Protocol (RESP) format. JSON.SET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Sets the value of a JSON element. JSON.STRAPPEND ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Appends the given string to the specified key\u0026rsquo;s existing strings. JSON.STRLEN ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the length of a string. JSON.TOGGLE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed If the boolean is true, changes it to false. If the boolean is false, changes it to true. JSON.TYPE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns a JSON element\u0026rsquo;s type. ","categories":["Modules"]},{"uri":"/modules/redistimeseries/commands/","uriRel":"/modules/redistimeseries/commands/","title":"RedisTimeSeries commands","tags":[],"keywords":[],"description":"Lists RedisTimeSeries commands and provides links to the command reference pages.","content":"The following table lists RedisTimeSeries commands. See the command links for more information about each command\u0026rsquo;s syntax, arguments, and examples.\nCommand Redis\nEnterprise Redis\nCloud Description TS.ADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Appends a sample to a time series. TS.ALTER ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Updates the retention, chunk size, duplicate policy, or labels for an existing time series. TS.CREATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates a new time series. TS.CREATERULE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Creates a compaction rule for downsampling. TS.DECRBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Decreases the value of the latest sample in a time series by the specified number. Either modifies the existing sample or adds the decreased value as a new sample, depending on the timestamp option. TS.DEL ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes all samples between two timestamps for a given time series. TS.DELETERULE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Removes a compaction rule. TS.GET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the last sample in a time series. TS.INCRBY ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Increases the value of the latest sample in a time series by the specified number. Either modifies the existing sample or adds the increased value as a new sample, depending on the timestamp option. TS.INFO ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns time series information and statistics. TS.MADD ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Appends multiple samples to one or more time series. TS.MGET ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns multiple samples with labels that match the filter. TS.MRANGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed For multiple time series, runs a query against samples within a range of timestamps, from earliest to latest. Supports filtering and aggregation. TS.MREVRANGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed For multiple time series, runs a query against samples within a range of timestamps in reverse order, from latest to earliest. Supports filtering and aggregation. TS.QUERYINDEX ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed Returns the keys of all time series with labels that match the given filters. TS.RANGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed From the start of a single time series, runs a query against samples within a range of timestamps. Supports filtering and aggregation. TS.REVRANGE ✅ Supported\n✅ Flexible \u0026amp; Annual\n✅ Free \u0026amp; Fixed From the end of a single time series, runs a query against samples within a range of timestamps in reverse order. Supports filtering and aggregation. ","categories":["Modules"]},{"uri":"/rs/databases/active-active/create/","uriRel":"/rs/databases/active-active/create/","title":"Create an Active-Active geo-replicated database","tags":[],"keywords":[],"description":"How to create an Active-Active database and things to consider when setting it up.","content":"Active-Active geo-replicated databases (formerly known as CRDBs) give applications write access to replicas of the dataset in different geographical locations.\nThe participating Redis Enterprise Software clusters that host the instances can be in distributed geographic locations. Every instance of an Active-Active database can receive write operations, and all operations are synchronized to all of the instances without conflict.\nSteps to create an Active-Active database Create a service account - On each participating cluster, create a dedicated user account with the Admin role. Confirm connectivity - Confirm network connectivity between the participating clusters. Create Active-Active database - Connect to one of your clusters and create a new Active-Active database. Add participating clusters - Add the participating clusters to the Active-Active database with the user credentials for the service account. Verify creation - Log in to each of the participating clusters and verify your Active-Active database was created on them. Confirm Active-Active database synchronization - Test writing to one cluster and reading from a different cluster. Prerequisites Two or more machines with the same version of RS installed Network connectivity and cluster FQDN name resolution between all participating clusters Network time service listener (ntpd) configured and running on each node in all clusters Create an Active-Active database To create service accounts, on each participating cluster:\nIn your web browser, open the admin console of the cluster that you want to connect to in order to create the Active-Active database. By default, the address is: https://\u0026lt;RS_address\u0026gt;:8443 Go to access control \u0026gt; users and select . Enter the name, email, and password for the user, select the Admin role, and select . To make sure that there is network connectivity between the participating clusters, telnet on port 9443 from each participating cluster to each of the other participating clusters.\ntelnet \u0026lt;target FQDN\u0026gt; 9443 In your web browser, open the admin console of the cluster that you want to connect to in order to create the Active-Active database. By default, the address is: https://\u0026lt;RS_address\u0026gt;:8443\nIn databases, click .\nIf you do not have any databases on the node, you are prompted to create a database.\nIn the Deployment box, select Geo-Distributed and click Next to create an Active-Active database on RAM.\nIf your cluster supports Redis on Flash, in Runs on you can select Flash so that your database uses Flash memory. We recommend that you use AOF every 1 sec for the best performance during the initial Active-Active database sync of a new replica.\nEnter the name of the new Active-Active database and select from the options:\nNote: You cannot enable or disable database clustering after the Active-Active database is created. Replication - We recommend that all Active-Active database use replication for best intercluster synchronization performance. When replication is enabled, every Active-Active database master shard is replicated to a corresponding replica shard. The replica shards are then used to synchronize data between the instances, and the master shards are dedicated to handling client requests. We also recommend that you enable replica HA to ensure that the replica shards are highly-available for this synchronization.\nData persistence - To protect against loss of data stored in RAM, you can enable data persistence and select to store a copy of the data on disk with snapshots or Append Only File (AOF). AOF provides the fastest and most reliable method for instance failure recovery.\nDefault database access - When you configure a password for your database, all connections to the database must authenticate with the AUTH command. If you also configure an access control list, connections can specify other users for authentication, and requests are allowed according to the Redis ACLs specified for that user.\nConfigure the advanced options that you want for the database:\nAccess Control List - You can specify the user roles that have access to the database and the Redis ACLs that apply to those connections. You can only configure access control after the Active-Active database is created.\nTo define an access control list:\nIn the Access control list section of the database configuration, click . Select the role that you want to have access to the database. Select the ACL that you want the role to have in the database. Click Save to save the ACL. Click Update to save the changes to the database. Endpoint port number (Required) - The port in the range 10000-19999 that clients must use to connect to the Active-Active database.\nIn the Database clustering option, you can either:\nMake sure the Database clustering is enabled and select the number of shards that you want to have in the database. When database clustering is enabled, databases are subject to limitations on Multi-key commands. You can increase the number of shards in the database at any time. Clear the Database clustering option to use only one shard so that you can use Multi-key commands without the limitations. OSS Cluster API - Redis OSS Cluster API reduces access times and latency with near-linear scalability. The Redis OSS Cluster API provides a simple mechanism for Redis clients to know the cluster topology.\nClients must first connect to the master node to get the cluster topology, and then they connect directly to the Redis proxy on each node that hosts a master shard.\nNote: You must use a client that supports the OSS cluster API to connect to a database that has the OSS cluster API enabled. Eviction policy - The default eviction policy for Active-Active databases is noeviction. Redis Enterprise version 6.0.20 and later support all eviction policies for Active-Active databases, unless Redis on Flash is enabled.\nParticipating Clusters - You must specify the URL of the clusters that you want to host instances of an Active-Active database and the admin user account to connect to each cluster.\nIn the Participating Clusters list, click to add clusters. For each cluster, enter the URL for the cluster (https://\u0026lt;cluster_fqdn\u0026gt;:9443), enter the credentials (email address and password) for the service account that you created, and click . Causal Consistency - Causal Consistency in an Active-Active database guarantees that the order of operations on a specific key is maintained across all instances of an Active-Active database. To enable Causal Consistency for an existing Active-Active database, use the REST API.\nTLS - If you enable TLS when you create the Active-Active database, the nodes use the TLS mode Require TLS for CRDB communication only to require TLS authentication and encryption for communications between participating clusters. After you create the Active-Active database, you can set the TLS mode to Require TLS for all communications so that client communication from applications are also authenticated and encryption.\nTest the connection to your member Redis Active-Active databases With the Redis database created, you are ready to connect to your database. See Connect to Active-Active databases for tutorials and examples of multiple connection methods.\n","categories":["RS"]},{"uri":"/rs/security/access-control/ldap/enable-role-based-ldap/","uriRel":"/rs/security/access-control/ldap/enable-role-based-ldap/","title":"Enable role-based LDAP","tags":[],"keywords":[],"description":"Describes how to enable role-based LDAP authentication and authorization using the Redis Enterprise admin console.","content":"Redis Enterprise Software uses a role-based mechanism to enable LDAP authentication and authorization.\nWhen a user attempts to access Redis Enterprise resources using LDAP credentials, the credentials are passed to the LDAP server in a bind request. If the request succeeds, the user’s groups are searched for a group that authorizes access to the original resource.\nRole-based LDAP lets you authorize admin console admins (previously known as external users) as well as database users. As with any access control role, you can define the level of access authorized by the role.\nSet up LDAP connection To enable and configure LDAP, sign into the Redis Enterprise admin console and then select Settings \u0026gt; LDAP.\nWarning - If LDAP is already enabled, you may already be using the cluster-based LDAP integration. If so, follow the migration process to enable role-based LDAP. You have to delete external users from the admin console before you enable role-based LDAP. When LDAP is enabled, use the info you gathered to populate the following settings.\nLDAP server settings The LDAP Server settings define the communication settings used for LDAP authentication and authorization. These include:\nSetting Description Protocol Underlying communication protocol; must be LDAP, LDAPS, or STARTTLS Host URL of the LDAP server Port LDAP server port number Trusted CA certificate (LDAPS or STARTTLS protocols only) Certificate for the trusted certificate authority (CA) When defining multiple LDAP hosts, the organization tree structure must be identical for all hosts.\nBind credentials These settings define the credentials for the bind query:\nSetting Description Distinguished Name Example: cd=admin,dc=example,dc=org Password Example: admin1 Client certificate authentication (LDAPS or STARTTLS protocols only) Place checkmark to enable Client public key (LDAPS or STARTTLS protocols only) The client public key for authentication Client private key (LDAPS or STARTTLS protocols only) The client private key for authentication Authentication query These settings define the authentication query:\nSetting Description Search user by Either Template or Query Template (template search) Example: cn=%u,ou=dev,dc=example,dc=com Base (query search) Example: ou=dev,dc=example,dc=com Filter (query search) Example: (cn=%u) Scope (query search) Must be baseObject, singleLevel, or wholeSubtree In this example, %u is replaced by the username attempting to access the Redis Enterprise resource.\nAuthorization query These settings define the group authorization query:\nSetting Description Search groups by Either Attribute or Query Attribute (attribute search) Example: memberOf (case-sensitive) Base (query search) Example: ou=groups,dc=example,dc=com Filter (query search) Example: (members=%D) Scope (query search) Must be baseObject, singleLevel, or wholeSubtree In this example, %D is replaced by the Distinguished Name of the user attempting to access the Redis Enterprise resource.\nSave settings When finished, select the Save button to save your changes.\nMore info Map LDAP groups to access control roles Update database ACLs to authorize LDAP access Learn more about Redis Software security and practices ","categories":["RS"]},{"uri":"/rs/security/access-control/ldap/","uriRel":"/rs/security/access-control/ldap/","title":"LDAP authentication","tags":[],"keywords":[],"description":"Describes how Redis Enterprise Software integrates LDAP authentication and authorization. Also describes how to enable LDAP for your deployment of Redis Enterprise Software.","content":"Redis Enterprise Software supports Lightweight Directory Access Protocol (LDAP) authentication and authorization through its role-based access controls (RBAC). You can use LDAP to authorize access to the admin console and to control database access.\nYou can configure LDAP roles using the Redis Enterprise admin console or REST API.\nPreviously, Redis Enterprise Software supported a cluster-based LDAP integration; however, support for this approach was removed in v6.2.12.\nIf you are using the earlier cluster-based LDAP mechanism, you need to migrate to role-based LDAP before upgrading to v6.2.12.\nHow it works Here\u0026rsquo;s how role-based LDAP integration works:\nA user signs in with their LDAP credentials.\nBased on the LDAP configuration details, the username is mapped to an LDAP Distinguished Name.\nA simple LDAP bind request is attempted using the Distinguished Name and the password. The sign-in fails if the bind fails.\nObtain the user’s LDAP group memberships.\nUsing configured LDAP details, obtain a list of the user’s group memberships.\nCompare the user’s LDAP group memberships to those mapped to local roles.\nDetermine if one of the user\u0026rsquo;s groups is authorized to access the target resource. If so, the user is granted the level of access authorized to the role.\nTo access the admin console, the user needs to belong to an LDAP group mapped to an administrative role.\nFor database access, the user needs to belong to an LDAP group mapped to a role listed in the database’s access control list (ACL). The rights granted to the group determine the user\u0026rsquo;s level of access.\nPrerequisites Before you enable LDAP in Redis Enterprise, you need:\nThe LDAP groups that correspond to the levels of access you wish to authorize. Each LDAP group will be mapped to a Redis Enterprise access control role.\nA Redis Enterprise access control role for each LDAP group. Before you enable LDAP, you need to set up role-based access controls (RBAC).\nThe following LDAP details:\nServer URI, including host, port, and protocol details. Certificate details for secure protocols. Bind credentials, including Distinguished Name, password, and (optionally) client public and private keys for certificate authentication. Authentication query details, whether template or query. Authorization query details, whether attribute or query. The Distinguished Names of LDAP groups you’ll use to authorize access to Redis Enterprise resources. Enable LDAP To enable LDAP:\nFrom Settings \u0026gt; LDAP in the admin console, enable LDAP access.\nMap LDAP groups to access control roles.\nUpdate database access control lists (ACLs) to authorize role access.\nIf you already have appropriate roles, you can update them to include LDAP groups.\nMore info Enable and configure role-based LDAP Map LDAP groups to access control roles Update database ACLs to authorize LDAP access Learn more about Redis Enterprise Software security and practices ","categories":["RS"]},{"uri":"/rs/databases/active-active/connect/","uriRel":"/rs/databases/active-active/connect/","title":"Connect to your Active-Active databases","tags":[],"keywords":[],"description":"How to connect to an Active-Active database using redis-cli or a sample Python application.","content":"With the Redis database created, you are ready to connect to your database to store data. You can use one of the following ways to test connectivity to your database:\nConnect with redis-cli, the built-in command-line tool Connect with a Hello World application written in Python Remember we have two member Active-Active databases that are available for connections and concurrent reads and writes. The member Active-Active databases are using bi-directional replication to for the global Active-Active database.\nConnecting using redis-cli redis-cli is a simple command-line tool to interact with redis database.\nTo use redis-cli on port 12000 from the node 1 terminal, run:\nredis-cli -p 12000 Store and retrieve a key in the database to test the connection with these commands:\nset key1 123 get key1 The output of the command looks like this:\n127.0.0.1:12000\u0026gt; set key1 123 OK 127.0.0.1:12000\u0026gt; get key1 \u0026#34;123\u0026#34; Enter the terminal of node 1 in cluster 2, run the redis-cli, and retrieve key1.\nThe output of the commands looks like this:\n$ redis-cli -p 12000 127.0.0.1:12000\u0026gt; get key1 \u0026#34;123\u0026#34; Connecting using Hello World application in Python A simple python application running on the host machine can also connect to the database.\nNote: Before you continue, you must have python and redis-py (python library for connecting to Redis) configured on the host machine running the container. In the command-line terminal, create a new file called \u0026ldquo;redis_test.py\u0026rdquo;\nvi redis_test.py Paste this code into the \u0026ldquo;redis_test.py\u0026rdquo; file.\nThis application stores a value in key1 in cluster 1, gets that value from key1 in cluster 1, and gets the value from key1 in cluster 2.\nimport redis rp1 = redis.StrictRedis(host=\u0026#39;localhost\u0026#39;, port=12000, db=0) rp2 = redis.StrictRedis(host=\u0026#39;localhost\u0026#39;, port=12002, db=0) print (\u0026#34;set key1 123 in cluster 1\u0026#34;) print (rp1.set(\u0026#39;key1\u0026#39;, \u0026#39;123\u0026#39;)) print (\u0026#34;get key1 cluster 1\u0026#34;) print (rp1.get(\u0026#39;key1\u0026#39;)) print (\u0026#34;get key1 from cluster 2\u0026#34;) print (rp2.get(\u0026#39;key1\u0026#39;)) To run the \u0026ldquo;redis_test.py\u0026rdquo; application, run:\npython redis_test.py If the connection is successful, the output of the application looks like:\nset key1 123 in cluster 1 True get key1 cluster 1 \u0026#34;123\u0026#34; get key1 from cluster 2 \u0026#34;123\u0026#34; ","categories":["RS"]},{"uri":"/ri/using-redisinsight/cluster-management/","uriRel":"/ri/using-redisinsight/cluster-management/","title":"Cluster Management","tags":[],"keywords":[],"description":"","content":"RedisInsight Cluster Management provides a graphical user interface (GUI) to manage your Redis Cluster. Cluster Management comes with three different views to analyze your cluster architecture.\nMaster Layout - This view only contains information about the masters present in the Redis Cluster. The information present is - slot ranges, host, port and few metrics gathered from redis INFO Command. Master-Replica Layout - This view contains masters along with their replicas. This view contains information about slots ranges, host, port, etc for both master and replica. Physical Layout - This view gives you a representation of your server i.e. it groups all nodes according to the physical server they reside in. Cluster Management actions Cluster Management not only gives you a representation of your cluster but also helps you administer your cluster by using the following actions -\nCluster Rebalance - Cluster Rebalance helps you migrate all slots according to slot percentages amongst cluster nodes. Manual Resharding - Manual Resharding asks for source and destination nodes with slot ranges. Using this you can migrate the specified slot range from source to destination node in just a few clicks. Add Node to Cluster- Cluster Management allows you to add a new node to your cluster as a master. The node should be empty and should have cluster enabled. Manual Failover - Using this you can manually failover a replica node to become the master. Delete Node in Cluster - Use this to delete a replica. (Master nodes cannot be deleted.) Make a node Replica of Master - Cluster management includes a feature to make the selected node replica of a master. If the node is already a replica, no work is required. If the node is a master, then that should be empty i.e. - no hash slots and keys. Cluster Health Monitoring \u0026amp; Alerts - Cluster Management constantly runs health checks for your cluster and returns alerts if there is any issue with the cluster. It also provides alerts if the master and replica are on the same server. The alerts for the cluster can be fixed by using the Fix Cluster feature. Compatibility RedisInsight Cluster Management capabilities are available for OSS Redis Clusters Currently we do not support Redis Enterprise Software clusters that have the OSS Cluster API enabled.\nNote: RedisInsight Cluster Management tool for TLS databases is not supported on Microsoft Windows because Windows does not include the OpenSSL library. You can run RedisInsight using Docker on Windows to manage TLS databases. ","categories":["RI"]},{"uri":"/rs/references/client_references/client_csharp/","uriRel":"/rs/references/client_references/client_csharp/","title":"Redis with .NET","tags":[],"keywords":[],"description":"The StackExchange.Redis client allows you to use Redis with .NET.","content":"In order to use Redis with .NET, you need a .NET Redis client. This article shows how to use StackExchange.Redis, a general purpose Redis client. More .NET Redis clients can be found in the C# section of the Redis Clients page.\nInstall StackExchange.Redis There are several ways to install this package including:\nWith the .NET CLI: dotnet add package StackExchange.Redis With the package manager console: PM\u0026gt; Install-Package StackExchange.Redis With the NuGet GUI in Visual Studio Connect to Redis The following code creates a connection to Redis using StackExchange.Redis in the context of a console application:\nusing StackExchange.Redis; using System; using System.Threading.Tasks; namespace ReferenceConsoleRedisApp { class Program { static readonly ConnectionMultiplexer redis = ConnectionMultiplexer.Connect( new ConfigurationOptions{ EndPoints = { \u0026#34;redis-12000.cluster.redis.com:12000\u0026#34; }, }); static async Task Main(string[] args) { var db = redis.GetDatabase(); var pong = await db.PingAsync(); Console.WriteLine(pong); } } } The above example assumes that you have a Redis Server running locally.\nTo configure the connection to your environment, adjust the parameters in the ConfigurationOptions object appropriately. For the remainder of the examples, the configuration uses localhost.\nConnection pooling StackExchange.Redis does not support conventional connection pooling. As an alternative solution, you can share and reuse the ConnectionMultiplexer object.\nDo not create a separate ConnectionMultiplexer for each operation. Instead, create an instance at the beginning and then reuse the object throughout your process.\nConnectionMultiplexer is thread-safe, so it can be safely shared between threads. For more information, see the Basic Usage.\nDependency injection of the ConnectionMultiplexer As the ConnectionMultiplexer must be shared and reused within a runtime, it\u0026rsquo;s recommended that you use dependency injection to pass it where it\u0026rsquo;s needed. There\u0026rsquo;s a few flavors of dependency injection depending on what you\u0026rsquo;re using.\nASP.NET Core A single ConnectionMultiplexer instance should be shared throughout the runtime.\nUse the AddSingleton method of IServiceCollection to inject your instance as a dependency in ASP.NET Core when configuring your app\u0026rsquo;s services in Startup.cs:\npublic void ConfigureServices(IServiceCollection services) { //Configure other services up here var multiplexer = ConnectionMultiplexer.Connect(\u0026#34;localhost\u0026#34;); services.AddSingleton\u0026lt;IConnectionMultiplexer\u0026gt;(multiplexer); } Once the service is registered, you can inject it into anything that allows dependency injection, such as MVC Controllers, API Controllers, Blazor Server Components, and more.\nThe following code shows how to pass the service to a RedisController instance:\n[Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] public class RedisController : ControllerBase { private readonly IConnectionMultiplexer _redis; public RedisController(IConnectionMultiplexer redis) { _redis = redis; } [HttpGet(\u0026#34;foo\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; Foo() { var db = _redis.GetDatabase(); var foo = await db.StringGetAsync(\u0026#34;foo\u0026#34;); return Ok(foo.ToString()); } } Azure Functions There are two types of Azure Functions to consider: in-process and out-of-process. Both handle dependency injection differently.\nIn-process Azure Functions In-process Azure Functions handle dependency injection similarly to ASP.NET Core.\nTo use dependency injection, follow these steps:\nCreate a Startup.cs file Extend FunctionsStartup Override the Configure method Add the multiplexer as a singleton service for the function. using Microsoft.Azure.Functions.Extensions.DependencyInjection; using Microsoft.Extensions.DependencyInjection; using StackExchange.Redis; [assembly: FunctionsStartup(typeof(MyNamespace.Startup))] namespace MyNamespace { public class Startup : FunctionsStartup { public override void Configure(IFunctionsHostBuilder builder) { var muxer = ConnectionMultiplexer.Connect(\u0026#34;localhost\u0026#34;); builder.Services.AddSingleton\u0026lt;IConnectionMultiplexer\u0026gt;(muxer); } } } Out-of-process Azure Functions Unlike in-process functions, out-of-process functions handle dependency injection in the Main method of the Program class.\nModify the HostBuilder build pipeline to call ConfigureServices and configure the ConnectionMultiplexer, as shown in the following example:\npublic static void Main() { var host = new HostBuilder() .ConfigureFunctionsWorkerDefaults() .ConfigureServices(s=\u0026gt;s.AddSingleton\u0026lt;IConnectionMultiplexer\u0026gt;(ConnectionMultiplexer.Connect(\u0026#34;localhost\u0026#34;))) // add this line .Build(); host.Run(); } For both in-process and out-of-process functions, you can pass the Multiplexer by injecting it into the constructor of the class housing your functions:\npublic class RedisTrigger { private readonly IConnectionMultiplexer _redis; public RedisTrigger(IConnectionMultiplexer redis){ _redis = redis; } [FunctionName(\u0026#34;RedisTrigger\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; Foo( [HttpTrigger(AuthorizationLevel.Function, \u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;, Route = null)] HttpRequest req, ILogger log) { var db = _redis.GetDatabase(); var bar = await db.StringGetAsync(\u0026#34;foo\u0026#34;); return new OkObjectResult(bar.ToString()); } } TLS StackExchange.Redis natively support TLS, as shown here:\nusing StackExchange.Redis; using System; using System.Security.Cryptography.X509Certificates; using System.Threading.Tasks; namespace ConnectToRedisWithTls { class Program { const string PATH_TO_CERT_FILE = \u0026#34;c:\\\\PATH\\\\TO\\\\CERT.pfx\u0026#34;; static async Task Main(string[] args) { var configurationOptions = new ConfigurationOptions { EndPoints = { \u0026#34;redis-12000.cluster.redis.com:12000\u0026#34; }, Ssl = true }; configurationOptions.CertificateSelection += delegate { var cert = new X509Certificate2(PATH_TO_CERT_FILE, \u0026#34;\u0026#34;); return cert; }; var redis = await ConnectionMultiplexer.ConnectAsync(configurationOptions); Console.WriteLine(redis.GetDatabase().Ping()); } } } Modify the PATH_TO_CERT_FILE to match the path to your certificate Modify the EndPoints setting to point to your endpoint(s) If necessary, add a Password to the ConfigurationOptions To learn how to run Redis Enterprise with TLS enabled, see TLS Support.\nConvert certificate format To easily convert a .key certificate to .pfx format, use OpenSSL:\nopenssl pkcs12 -export -in user.crt -inkey user_private.key -certfile garantia_ca.pem -out certificate.pfx Note: If you\u0026rsquo;re using a self-signed certificate, remember to install it on your server with the Certificate Manager tool. Use SSL and a StackExchange.Redis-based provider Sometimes you need to use a 3rd-party library, such as when running a session on a cache provider that connects to Redis with the StackExchange.Redis client. When you need to provide an SSL certificate for the connection and the 3rd-party library does not expose a public interface for it, you can \u0026ldquo;sideload\u0026rdquo; the certificate to StackExchange.Redis by setting the following environment variables:\nSet SERedis_ClientCertPfxPath to the path of your .pfx file Set SERedis_ClientCertPassword to the password of your .pfx file ","categories":["RS"]},{"uri":"/rs/installing-upgrading/configuring/configuring-aws-instances/","uriRel":"/rs/installing-upgrading/configuring/configuring-aws-instances/","title":"Configure AWS EC2 instances for Redis Enterprise Software","tags":[],"keywords":[],"description":"","content":"There are some special considerations for installing and running Redis Enterprise Software on Amazon Elastic Cloud Compute (EC2) instances.\nThese include:\nStorage considerations Instance types Security group configuration Storage considerations AWS EC2 instances are ephemeral, but your persistent database storage should not be. If you require a persistent storage location for your database, the storage must be located outside of the instance. Therefore, when you set up an instance make sure that it has a properly sized EBS backed volume connected. Later, when setting up RS on the instance, make sure that the persistence storage is configured to use this volume.\nNote: After installing the RS package on the instance and before running through the setup process, you must give the group \u0026lsquo;redislabs\u0026rsquo; permissions to the EBS volume by running the following command from the OS command-line interface (CLI): chown redislabs:redislabs /\u0026lt; ebs folder name\u0026gt; Another feature that may be of importance to you is the use of Provisioned IOPS for EBS backed volumes. Provisioned IOPS guarantee a certain level of disk performance. There are two features in RS where this feature could be critical to use:\nWhen using Redis on Flash When using AOF on every write and there is a high write load. In this case, the provisioned IOPS should be on the nodes used as replicas in the cluster. Instance types Choose an instance type that has (at minimum) enough free memory and disk space to meet RS\u0026rsquo;s hardware requirements.\nIn addition, some instance types are optimized for EBS backed volumes and some are not. If you are using persistent storage, you should use an instance type that is, especially if disk drain rate matters to your database implementation.\nSecurity group configuration When configuring the Security Group:\nDefine a custom TCP rule for port 8443 to allow web browser access to the RS management UI from the IP address/ range you use to access the UI. If you are using the DNS resolving option with RS, define a DNS UDP rule for port 53 to allow access to the databases\u0026rsquo; endpoints by using the DNS resolving mechanism. To create a cluster that has multiple nodes all running as instances on AWS, you need to define a security group that has an All TCP rule for all ports, 0 - 65535, and add it to all instances that are part of the cluster. This makes sure that all nodes are able to communicate with each other. To limit the number of open ports, you can open just the ports used by RS. After successfully launching the instances:\nInstall Redis Enterprise Software from the Linux package or AWS AMI. Set up the cluster. ","categories":["RS"]},{"uri":"/rc/security/cidr-whitelist/","uriRel":"/rc/security/cidr-whitelist/","title":"Configure CIDR allow list","tags":[],"keywords":[],"description":"The CIDR allow list permits traffic between a range of IP addresses and the Redis Cloud VPC.","content":"The CIDR allow list lets you restrict traffic to your Redis Cloud database. When you configure an allow list, only the IP addresses defined in the list can connect to the database. Traffic from all other IP addresses is blocked.\nDatabase allow list You can configure your database\u0026rsquo;s CIDR allow list to restrict client connections to a specific range of IP addresses.\nDefine CIDR allow list To define the CIDR allow list for a database:\nSelect Databases from the admin console menu and then select your database from the list.\nFrom the database\u0026rsquo;s Configuration screen, select the Edit database button:\nIn the Security section, turn on the CIDR allow list toggle:\nEnter the first IP address (in CIDR format) you want to allow in the text box and then select the check mark to add it to the allow list:\nTo allow additional IP addresses:\nSelect the Add CIDR button:\nEnter the new IP address in the text box and then select the check mark to add it to the allow list:\nSelect the Save database button to apply your changes:\nSubscription allow list If you use a self-managed, external cloud account to host your Redis Cloud deployment, you can configure a subscription-wide allow list to restrict traffic to all databases associated with the subscription.\nThe subscription CIDR allow list defines a range of IP addresses and AWS security groups that control inbound and outbound traffic to the Redis Cloud VPC. When you add security groups to the allow list, you can also use the same security groups to manage access to your application.\nAllow IP address or security group To add IP addresses or AWS security groups to a subscription\u0026rsquo;s allow list:\nFrom the admin console menu, select Subscriptions and then select your subscription from the list.\nSelect the Connectivity tab and then select Allow List.\nIf the allow list is empty, select the Add allow list button:\nSelect an entry Type from the list:\nIn the Value box, enter either:\nAn IP address in CIDR format\nThe AWS security group ID\nSelect the check mark to add the entry to the allow list:\nTo allow additional IP addresses or security groups:\nSelect the Add entry button:\nThen select the new entry\u0026rsquo;s Type, enter the Value, and select the check mark to add it to the allow list:\nSelect the Apply all changes button to apply the allow list updates:\n","categories":["RC"]},{"uri":"/modules/redisbloom/config/","uriRel":"/modules/redisbloom/config/","title":"RedisBloom configuration compatibility with Redis Enterprise","tags":[],"keywords":[],"description":"RedisBloom configuration settings supported by Redis Enterprise.","content":"Redis Enterprise Software lets you manually change any RedisBloom configuration setting.\nRedis Enterprise Cloud does not let you configure RedisBloom manually. However, if you have a Flexible or Annual subscription, you can contact support to request a configuration change. You cannot change RedisBloom configuration for Free or Fixed subscriptions.\nSetting Redis\nEnterprise Redis\nCloud Notes CF_MAX_EXPANSIONS ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 32 ERROR_RATE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 0.01 INITIAL_SIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100 ","categories":["Modules"]},{"uri":"/modules/redisearch/config/","uriRel":"/modules/redisearch/config/","title":"RediSearch configuration compatibility with Redis Enterprise","tags":[],"keywords":[],"description":"RediSearch configuration settings supported by Redis Enterprise.","content":"You cannot use FT.CONFIG SET to configure RediSearch in Redis Enterprise Software or Redis Enterprise Cloud. Instead, use one of the following methods.\nFor Redis Cloud:\nFlexible or Annual subscriptions: contact support to request a configuration change.\nFree or Fixed subscriptions: you cannot change RediSearch configuration.\nFor Redis Enterprise Software, use either:\nrladmin tune db:\n$ rladmin tune db db:\u0026lt;ID|name\u0026gt; module_name search \\ module_config_params \u0026#34;setting-name setting-value\u0026#34; Configure module REST API request:\nPOST /v1/modules/config/bdb/\u0026lt;ID\u0026gt; { \u0026#34;modules\u0026#34;: [ { \u0026#34;module_name\u0026#34;: \u0026#34;search\u0026#34;, \u0026#34;module_args\u0026#34;: \u0026#34;setting-name setting-value\u0026#34; } ] } Setting Redis\nEnterprise Redis\nCloud Notes CONCURRENT_WRITE_MODE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: Not enabled CURSOR_MAX_IDLE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 300000 CURSOR_READ_SIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 1000 DEFAULT_DIALECT ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 1 EXTLOAD ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: None FORK_GC_CLEAN_THRESHOLD ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100 FORK_GC_RETRY_INTERVAL ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 5 FORK_GC_RUN_INTERVAL ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 30 FRISOINI ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: Not set GC_POLICY ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: FORK GC_SCANSIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100 MAXAGGREGATERESULTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Redis Enterprise default: Unlimited\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: Unlimited\n• Free \u0026amp; Fixed: 10000\nMAXDOCTABLESIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 1000000 MAXPREFIXEXPANSIONS ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 200 MAXSEARCHRESULTS ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Redis Enterprise default: 1000000\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: 1000000\n• Free \u0026amp; Fixed: 10000\nMINPREFIX ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 2 NOGC ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: Not set ON_TIMEOUT ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: RETURN OSS_GLOBAL_PASSWORD ✅ Supported\n❌ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed PARTIAL_INDEXED_DOCS ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 0 TIMEOUT ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Redis Enterprise default: 500\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: 500\n• Free \u0026amp; Fixed: 100\nUNION_ITERATOR_HEAP ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed UPGRADE_INDEX ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: No default index name ","categories":["Modules"]},{"uri":"/modules/redisgraph/config/","uriRel":"/modules/redisgraph/config/","title":"RedisGraph configuration compatibility with Redis Enterprise","tags":[],"keywords":[],"description":"RedisGraph configuration settings supported by Redis Enterprise.","content":"Redis Enterprise Software lets you manually change any RedisGraph configuration setting with the GRAPH.CONFIG SET command.\nRedis Enterprise Cloud does not let you use GRAPH.CONFIG SET to configure RedisGraph. However, if you have a Flexible or Annual subscription, you can contact support to request a configuration change. You cannot change RedisGraph configuration for Free or Fixed subscriptions.\nSetting Redis\nEnterprise Redis\nCloud Notes CACHE_SIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 25 MAX_QUEUED_QUERIES ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 25 NODE_CREATION_BUFFER ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 16384 OMP_THREAD_COUNT ✅ Supported\n✅ Flexible \u0026amp; Annual*\n❌ Free \u0026amp; Fixed * Updates automatically when you change your plan.\nRedis Enterprise default: Set by plan\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: Set by plan\n• Free \u0026amp; Fixed: 1\nQUERY_MEM_CAPACITY ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100000000 RESULTSET_SIZE ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 10000 THREAD_COUNT ✅ Supported\n✅ Flexible \u0026amp; Annual*\n❌ Free \u0026amp; Fixed * Updates automatically when you change your plan.\nRedis Enterprise default: Set by plan\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: Set by plan\n• Free \u0026amp; Fixed: 1\nTIMEOUT ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100 VKEY_MAX_ENTITY_COUNT ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 100000 ","categories":["Modules"]},{"uri":"/modules/redistimeseries/config/","uriRel":"/modules/redistimeseries/config/","title":"RedisTimeSeries configuration compatibility with Redis Enterprise","tags":[],"keywords":[],"description":"RedisTimeSeries configuration settings supported by Redis Enterprise.","content":"Redis Enterprise Software lets you manually change any RedisTimeSeries configuration setting.\nRedis Enterprise Cloud does not let you configure RedisTimeSeries manually. However, if you have a Flexible or Annual subscription, you can contact support to request a configuration change. You cannot change RedisTimeSeries configuration for Free or Fixed subscriptions.\nSetting Redis\nEnterprise Redis\nCloud Notes CHUNK_SIZE_BYTES ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 4096 COMPACTION_POLICY ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: No default compaction rules DUPLICATE_POLICY ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: BLOCK ENCODING ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: COMPRESSED NUM_THREADS ✅ Supported\n✅ Flexible \u0026amp; Annual*\n❌ Free \u0026amp; Fixed * Updates automatically when you change your plan.\nRedis Enterprise default: Set by plan\nRedis Cloud defaults:\n• Flexible \u0026amp; Annual: Set by plan\n• Free \u0026amp; Fixed: 1\nRETENTION_POLICY ✅ Supported\n✅ Flexible \u0026amp; Annual\n❌ Free \u0026amp; Fixed Default: 0 ","categories":["Modules"]},{"uri":"/rc/subscriptions/create-fixed-subscription/","uriRel":"/rc/subscriptions/create-fixed-subscription/","title":"Create a Fixed subscription","tags":[],"keywords":[],"description":"","content":"Fixed Size subscription plans support low throughput workflows. Several tiers are available, each designed for different memory sizes and integration requirements.\nWhen creating your subscription, you\u0026rsquo;ll need to know which tier to choose.\n(If you\u0026rsquo;re new to Redis Enterprise Cloud, the quick start helps you create an account with a free subscription and an initial database. You also learn how to connect to your database.)\nFixed plan subscription tiers Fixed plan pricing scales according to the memory size of all databases defined in the subscription. Additional limits also apply, as shown here (updated February 2021):\nMax DB Size Number ofDatabases ConcurrentConnections Security Groups Max IP Auth rules (Free) 30 MB 1 30 N/A N/A 100 MB 4 256 1 4 250 MB 8 256 1 4 500 MB 12 512 1 4 1 GB 16 1024 2 8 2½ GB 24 2500 2 8 5 GB 32 5000 4 16 10 GB 64 10000 4 32 The 30 MB Fixed plan is free; it\u0026rsquo;s designed for training and prototype purposes.\nAll paid (100 MB and above) fixed plans support replication and backups (daily and instant).\nIf you need additional resources, you can update your subscription at any time.\nCreate a Fixed subscription To create a Fixed subscription:\nFrom the admin console menu, select the New Subscription button.\nWhen the New subscription page appears, select Fixed plans and then scroll to the cloud vendor options.\nChoose a Cloud Provider and a Region.\nIn the High-availability panel, select your replication settings.\nNo-replication means that you will have a single copy of your database. Single-zone replication means that your database will have a primary and a replica located in the same cloud region. If anything happens to the primary, the replica takes over and becomes the new primary. Multi-zone replication means that the primary and the replicas are stored in different region zones, which provides additional protection by distributing the replicas. Select the desired plan size.\nTo create a Free subscription, select the 30 MB plan size.\nEnter a subscription name and payment details.\nLocate and then select the Create Subscription button, which is located below the Credit card details.\nHere are few details to keep in mind:\nYou can create a Free subscription without being prompted for payment details.\nYou can only have one free subscription at a time.\nIf you\u0026rsquo;re creating a paid subscription and haven\u0026rsquo;t previously entered a payment method, use the Add Credit Card button to add one.\nWhen you create your subscription, there\u0026rsquo;s a brief pause while your request is processed and then you\u0026rsquo;re taken to the Subscription details page.\nTo create your first database, select the New Database button and then fill in the appropriate details.\nTo learn more, see Create a database.\n","categories":["RC"]},{"uri":"/rc/subscriptions/create-flexible-subscription/","uriRel":"/rc/subscriptions/create-flexible-subscription/","title":"Create a Flexible subscription","tags":[],"keywords":[],"description":"","content":"Flexible subscriptions support any dataset size or throughput. Pricing is based on your workload requirements (database size and throughput.)\nWhen you create a Flexible subscription, a cost estimate is provided to help you understand the impact of your requirements.\nCreate a Flexible subscription To create a Flexible subscription:\nFrom the admin console menu, select the New Subscription button.\nWhen the New subscription page appears, select Flexible plans and then scroll to the cloud vendor options.\nFrom here, you need to:\nSet up the deployment options for your subscription, include cloud vendor details, high availability settings, and advanced options.\nDefine the database size requirements for your subscription.\nReview your choices, provide payment details, and then create the subscription.\nThe following sections provide more information.\nSet up deployment details The Setup tab specifies general settings for the deployment of your subscription.\nThe two sections to this tab:\nGeneral settings include the cloud provider details, the subscription name, and specific configuration options. Advanced options define settings for high availability and security. Configurable settings vary according to cloud provider. To continue to the Sizing tag, locate and select the Continue button, which appears below the Advanced options section\nGeneral settings The following settings are defined in the General settings of the Setup tab:\nGeneral setting Description Cloud vendor The public cloud vendor to deploy your subscription. (required) Region The vendor region where you wish to deploy your subscription. (required) Subscription Name A custom name for your subscription (required) Active-Active Redis Hosts your datasets in multiple read-write locations to support distributed applications and disaster recovery. See Active-Active geo-distributed Redis Redis on Flash Determines if your databases are stored only in memory (RAM) or are split between memory and Flash storage (RAM+Flash). See Redis on Flash Advanced options The following settings are defined in the Advanced options of the Setup tab:\nAdvanced option Description Multi-AZ Determines if replication spans multiple Availability Zones, which provides automatic failover when problems occur. Cloud account To deploy this subscription to an existing cloud account, select it here. Use the Add button to add a new cloud account.(Available only if self-managed cloud vendor accounts are enabled) VPC configuration Select In a new VPC to deploy to a new virtual private cloud (VPC).To deploy this subscription to an existing virtual private cloud, select In existing VPC and then set VPC ID to the appropriate ID value.(Available only if self-managed cloud vendor accounts are enabled) Deployment CIDR The CIDR range of IP addresses for your deployment. Because Redis creates a new subnet for the Deployment CIDR in your virtual private cloud (VPC), it cannot overlap with the CIDR ranges of other subnets used by your subscription.For deployments in an existing VPC, the Deployment CIDR must be within your VPC\u0026rsquo;s primary CIDR range (secondary CIDRs are not supported). Allowed Availability Zones The availability zones for your selected region.If you choose Manual selection, you must select at least one zone ID from the Zone IDs list. For more information, see Availability zones. When finished, choose Continue to determine your subscription size requirements.\nAvailability zones You can reduce network transfer costs and network latency by ensuring your Redis Cloud cluster and your application are located in the same availability zone.\nTo specify the availability zone for your cluster, select Manual Selection under Allowed Availability Zones.\nFor GCP clusters and self-managed AWS cloud accounts, select an availability zone from the Zone name list.\nFor all other AWS clusters, select an availability zone ID from the Zone IDs list. For more information on how to find an availability zone ID, see the AWS docs.\nIf Multi-AZ is enabled, you must select three availability zones from the list.\nFor more information on availability zones, see the GCP docs or the AWS docs.\nSizing tab The Sizing tab helps you specify the database, memory, and throughput requirements for your subscription.\nWhen you first visit the Sizing tab, there are no databases defined. Select the Add button to create one.\nThis opens the New Database dialog, which lets you define the requirements for your new database.\nBy default, you\u0026rsquo;re shown basic settings, which include:\nDatabase setting Description Name A custom name for your database (required) Throughput/Shards Identifies maximum throughput for the database, which can be specified in terms of operations per second (Ops/sec) or number of shards dedicated to the database (Shards).throughput is measured for the database, either operations per second (Ops/sec) or Number of shards. Memory Limit (GB) The size limit for the database. Specify small sizes as decimals of 1.0 GB; example: 0.1 GB (minimum). High Availability Indicates whether a replica copy of the database is maintained in case the primary database becomes unavailable. (Warning: Doubles memory consumption). Quantity Identifies the number of databases to create with the selected settings. Advanced options are also available.\nSelect Advanced options to specify values for the following settings:\nAdvanced option Description OSS Cluster API Enable to use the open-source Redis Cluster API. Type Set to Memcached database to support the legacy database; otherwise leave as Redis Data Persistence Defines the data persistence policy, if any. See Database persistence Modules Identifies a module used by the database. Choose from RedisSearch 2, RedisJSON, RedisGraph, RedisBloom, or RedisTimeSeries. When finished, select Save Database to create your database.\nUse the Add database button to define additional databases or select the Continue button to display the Review and create tab.\nUse the Edit icon to change a database or the Delete icon to remove a database from the list.\nReview and Create tab The Review \u0026amp; Create tab provides a cost estimate for your Flexible plan:\nSelect Back to Sizing to make changes or Create subscription to create your new Flexible subscription.\nNote that subscriptions are created in the background. While they are provisioning, you aren\u0026rsquo;t allowed to make changes. (The process generally takes 10-15 minutes.)\nUse the Subscriptions list to check the status of your subscription. You will also receive an email when your subscription is ready to use.\nShard types The shard types associated with your subscription depend on your database memory size and throughput requirements.\nShard type Capacity (Memory/Throughput) Micro 1GB / 1K ops/sec High-throughput 2.5GB / 25K ops/sec Small 12.5GB / 12.5K ops/sec Large 25GB / 25K ops/sec Very large 50GB / 5.0K ops/sec Prices vary according to the cloud provider and region. Minimum prices apply. To learn more, see Cloud pricing.\n","categories":["RC"]},{"uri":"/rs/installing-upgrading/customize-install-directories/","uriRel":"/rs/installing-upgrading/customize-install-directories/","title":"Customize installation directories","tags":[],"keywords":[],"description":"","content":"When installing Redis Enterprise Software, you can customize the installation directories.\nThe files are installed in the redislabs directory located in the path that you specify.\nNote: Custom installation directories are supported on RedHat Enterprise Linux version 7 and 8. When you install with custom directories, the installation does not run as an RPM file. If a redislabs directory already exists in the path that you specify, the installation fails. All nodes in a cluster must be installed with the same file locations. Custom installation directories are not supported for databases using Redis on Flash. You can specify any or all of these file locations:\nFiles Installer flag Example parameter Example file location Binaries files \u0026ndash;install-dir /opt /opt/redislabs Configuration files \u0026ndash;config-dir /etc/opt /etc/opt/redislabs Data and log files \u0026ndash;var-dir /var/opt /var/opt/redislabs These files are not in the custom directories:\nOS files\n/etc/cron.d/redislabs /etc/firewalld/services /etc/firewalld/services/redislabs-clients.xml /etc/firewalld/services/redislabs.xml /etc/ld.so.conf.d/redislabs_ldconfig.conf.tmpl /etc/logrotate.d/redislabs /etc/profile.d/redislabs_env.sh /usr/lib/systemd/system/rlec_supervisor.service.tmpl /usr/share/selinux/mls/redislabs.pp /usr/share/selinux/targeted/redislabs.pp Installation reference files\n/etc/opt/redislabs/redislabs_custom_install_version /etc/opt/redislabs/redislabs_env_config.sh To install to specific directories, run:\nsudo ./install.sh --install-dir \u0026lt;path\u0026gt; --config-dir \u0026lt;path\u0026gt; --var-dir \u0026lt;path\u0026gt; ","categories":["RS"]},{"uri":"/rs/installing-upgrading/customize-user-and-group/","uriRel":"/rs/installing-upgrading/customize-user-and-group/","title":"Customize system user and group","tags":[],"keywords":[],"description":"","content":"By default, Redis Enterprise Software is installed with the user:group redislabs:redislabs.\nDuring the installation, you can specify the user and group that own all Redis Enterprise Software processes.\nIf you specify the user only, then installation is run with the primary group that the user belongs to.\nNote: Custom installation user is supported on RedHat Enterprise Linux. When you install with custom directories, the installation does not run as an RPM file. You must create the user and group before attempting to install Redis Software. You can specify an LDAP user as the installation user. Use command-line options for the install script to customize the user or group:\nsudo ./install.sh --os-user \u0026lt;user\u0026gt; --os-group \u0026lt;group\u0026gt; ","categories":["RS"]},{"uri":"/rc/databases/delete-database/","uriRel":"/rc/databases/delete-database/","title":"Delete database","tags":[],"keywords":[],"description":"","content":"To delete a database, use the Delete button. It\u0026rsquo;s located in the Danger zone section of the database\u0026rsquo;s Configuration tab.\nDatabases must be empty before they can be deleted. Deleted databases cannot be recovered. (We recommend making a backup, just in case.)\nThis command requires the account owner role.\nStep-by-step Sign in to the Redis Cloud admin portal.\nIf you have more than one subscription, select the target subscription from the list. This displays the Databases tab for the selected subscription.\nSelect the database from the list. The Configuration tab is selected by default.\nScroll to the Danger zone.\nSelect the Delete button.\nWhen the Delete database dialog appears, use the Delete database button to confirm your choice.\nIf you only have one database in your subscription, you can delete both the database and the subscription from the Delete database confirmation dialog:\nDelete both deletes both the database and the subscription.\nDelete database deletes the database but keeps the subscription.\nWhen the operation completes, the database and its data are deleted.\n","categories":["RC"]},{"uri":"/rs/databases/durability-ha/discovery-service/","uriRel":"/rs/databases/durability-ha/discovery-service/","title":"Discovery service","tags":[],"keywords":[],"description":"","content":"The Discovery Service provides an IP-based connection management service used when connecting to Redis Enterprise Software databases. When used in conjunction with Redis Enterprise Software\u0026rsquo;s other high availability features, the Discovery Service assists an application scope with topology changes such as adding, removing of nodes, node failovers and so on. It does this by providing your application with the ability to easily discover which node hosts the database endpoint. The API used for discovery service is compliant with the Redis Sentinel API.\nDiscovery Service is an alternative for applications that do not want to depend on DNS name resolution for their connectivity. Discovery Service and DNS based connectivity are not mutually exclusive. They can be used side by side in a given cluster where some clients can use Discovery Service based connection while others can use DNS name resolution when connecting to databases.\nHow discovery service works The Discovery Service is available for querying on each node of the cluster, listening on port 8001. To employ it, your application utilizes a Redis Sentinel enabled client library to connect to the Discovery Service and request the endpoint for the given database. The Discovery Service replies with the database\u0026rsquo;s endpoint for that database. In case of a node failure, the Discovery Service is updated by the cluster manager with the new endpoint and clients unable to connect to the database endpoint due to the failover, can re-query the discovery service for the new endpoint for the database.\nThe Discovery Service can return either the internal or external endpoint for a database. If you query the discovery service for the endpoint of a database named \u0026ldquo;db1\u0026rdquo;, the Discovery Service returns the external endpoint information by default. If only an internal endpoint exists with no external endpoint the default behavior is to return the internal endpoint. The \u0026ldquo;@internal\u0026rdquo; is added to the end of the database name to explicitly ask for the internal endpoint. to query the internal endpoint explicitly with database name \u0026ldquo;db1\u0026rdquo;, you can pass in the database name as \u0026ldquo;db1@internal\u0026rdquo;.\nIf you\u0026rsquo;d like to examine the metadata returned from Redis Enterprise Software Discovery Service you can connect to port 8001 with redis-cli utility and execute \u0026ldquo;SENTINEL masters\u0026rdquo;. Following is a sample output from one of the nodes of a Redis Enterprise Software cluster:\n$ ./redis-cli -p 8001 127.0.0.1:8001\u0026gt; SENTINEL masters 1) 1) \u0026#34;name\u0026#34; 2) \u0026#34;db1@internal\u0026#34; 3) \u0026#34;ip\u0026#34; 4) \u0026#34;10.0.0.45\u0026#34; 5) \u0026#34;port\u0026#34; 6) \u0026#34;12000\u0026#34; 7) \u0026#34;flags\u0026#34; 8) \u0026#34;master,disconnected\u0026#34; 9) \u0026#34;num-other-sentinels\u0026#34; 10) \u0026#34;0\u0026#34; 2) 1) \u0026#34;name\u0026#34; 2) \u0026#34;db1\u0026#34; 3) \u0026#34;ip\u0026#34; 4) \u0026#34;10.0.0.45\u0026#34; 5) \u0026#34;port\u0026#34; 6) \u0026#34;12000\u0026#34; 7) \u0026#34;flags\u0026#34; 8) \u0026#34;master,disconnected\u0026#34; 9) \u0026#34;num-other-sentinels\u0026#34; 10) \u0026#34;0\u0026#34; It is important to note that, the Discovery Service is not a full implementation of the Redis Sentinel protocol. There are aspects of the protocol that are not applicable or would be duplication with existing technology in Redis Enterprise Software. The Discovery Service implements only the parts required to provide applications with easy High Availability, be compatible with the protocol, and not rely on DNS to derive which node in the cluster to communicate with.\nNote: To use Redis Sentinel, every database name must be unique across the cluster. Redis client support We recommend these clients that are tested for use with the Discovery Service that uses the Redis Sentinel API:\nRedis-py (Python redis client) HiRedis (C redis client) Jedis (Java redis client) Ioredis (NodeJS redis client) If you need to use another client, consider using Sentinel Tunnel to discover the current Redis master with Sentinel and create a TCP tunnel between a local port on the client and the master.\nNote: Redis Sentinel API can return endpoints for both master and replica endpoints. Discovery Service only supports master endpoints and does not support returning replica endpoints for a database. ","categories":["RS"]},{"uri":"/modules/install/add-module-to-database/","uriRel":"/modules/install/add-module-to-database/","title":"Enable a module for a database","tags":[],"keywords":[],"description":"","content":"Modules add additional functionality to Redis databases for specific use cases. You can enable modules when you create a database.\nPrerequisites Installed the module on the cluster Upgraded the module to the latest version Create a database with a module Note: You can only add modules to a database when you first create it. You cannot add modules to an existing database. In the Redis Enterprise admin console, follow these steps to add modules to a database:\nFrom the databases page, select the Add button to create a new database:\nConfirm that you want to create a new Redis database with the Next button.\nConfigure the database settings.\nFor Redis Modules:\nSelect the Add button:\nSelect the module from the Module dropdown list.\nTo use custom configuration with a module, select Add configuration and enter the configuration options.\nSelect the OK button to confirm that you want to enable the module:\nRepeat these steps to add additional modules to the database.\nNote: You cannot use RediSearch 1.x and RediSearch 2.x in the same database. Select Show advanced options and enter a port number for the database, such as 12543.\nNote: Depending on the features supported by an enabled module, certain database configuration fields may not be available. Select the Activate button.\nModule configuration options RediSearch configuration options\nRedisGraph configuration options\nRedisTimeSeries configuration options\n","categories":["Modules"]},{"uri":"/rs/security/admin-console-security/encryption/","uriRel":"/rs/security/admin-console-security/encryption/","title":"Encrypt REST API requests","tags":[],"keywords":[],"description":"","content":"This section details how you can configure encryption for Redis Enterprise Software.\nRequire HTTPS for API endpoints By default, the Redis Enterprise Software API supports communication over HTTP and HTTPS. However, you can turn off support for HTTP to ensure that API requests are encrypted.\nBefore you turn off HTTP support, be sure to migrate any scripts or proxy configurations that use HTTP to the encrypted API endpoint to prevent broken connections.\nTo turn off HTTP support for API endpoints, run:\nrladmin cluster config http_support disabled ","categories":["RS"]},{"uri":"/rc/api/examples/","uriRel":"/rc/api/examples/","title":"REST API Examples","tags":[],"keywords":[],"description":"These examples show how to use the API and the results to expect.","content":" Create and manage subscriptions This article describes how to create and manage a subscription using `cURL` commands.\nCreate and manage databases This article describes how to create and manage a database using `cURL` commands.\nUpdate databases How to construct requests that update an existing database.\nDatabase backup and import When you create or update a database, you can specify the backup path. The import API operation lets you import data from various source types and specified locations.\nEstimate cost How to evaluate the cost of a specific subscription or database without changing existing resources.\nAudit using Service Log Use the service log to track and audit actions performed in the account\nView account information Get initial information on account parameters\nCreate and manage cloud accounts Cloud accounts specify which account to use when creating and modifying infrastructure resources.\n","categories":["RC"]},{"uri":"/rs/databases/import-export/","uriRel":"/rs/databases/import-export/","title":"Import and export data","tags":[],"keywords":[],"description":"How to import, export, flush, and migrate your data.","content":"You can import, export, or back up a Redis Enterprise database.\nImport data Import data from a backup or another Redis database. You can import from a single file or multiple files, such as when you want to import a backup of a clustered database.\nExport data Export data from a Redis Enterprise database to a local mount point, an FTP or SFTP server, or cloud provider storage.\nSchedule automatic backups Schedule backups of your databases to make sure you always have valid backups.\nMigrate to Active-Active Migrate a database to an Active-Active database using Replica Of.\n","categories":["RS"]},{"uri":"/ri/installing/","uriRel":"/ri/installing/","title":"Installation","tags":[],"keywords":[],"description":"","content":"We offer various ways to install RedisInsight:\nDesktop Docker Kubernetes Amazon AWS EC2 ","categories":["RI"]},{"uri":"/rs/databases/active-active/manage/","uriRel":"/rs/databases/active-active/manage/","title":"Manage Active-Active databases","tags":[],"keywords":[],"description":"Manage your Active-Active database settings.","content":"You can configure and manage your Active-Active database from either the admin console or the command line.\nTo change the global configuration of the Active-Active database, use the crdb-cli.\nIf you need to apply changes locally to one database instance, you use the admin console or the rladmin CLI.\nDatabase settings The following table shows a list of database settings, tools you can use to change those settings, and links to more information.\nMuch of the Active-Active database settings can be changed after the database has been created. One notable exception is database clustering. Database clustering can\u0026rsquo;t be turned on or off after the database has been created and will remain the same through the lifetime of the database.\nParticipating clusters You can add and remove participating clusters of an Active-Active database to change the topology. To manage the changes to Active-Active topology, use the crdb-cli tool or the participating clusters list in the admin console. You can make multiple changes at once and changes will be committed when the database configuration is saved.\nAdd participating clusters All of the existing participating clusters must be online and in a syncing state when you add new participating clusters.\nNew participating clusters create the Active-Active database instance based on the global Active-Active database configuration. After you add new participating clusters to an existing Active-Active database, the new database instance can accept connections and read operations. The new instance does not accept write operations until it is in the syncing state.\nRemove participating clusters All of the existing participating clusters must be online and in a syncing state when you remove an online participating clusters. If you must remove offline participating clusters, you can do this with forced removal. If a participating cluster that was removed forcefully returns attempts to re-join the cluster, it will have an out of date on Active-Active database membership. The joined participating clusters reject updates sent from the removed participating cluster. To avoid re-join attempts, purge the forcefully removed instance from the participating cluster.\nReplication backlog Redis databases that use replication for high availability maintain a replication backlog (per shard) to synchronize the primary and replica shards of a database. In addition to the database replication backlog, Active-Active databases maintain a backlog (per shard) to synchronize the database instances between clusters.\nBy default, both the database and Active-Active replication backlogs are set to one percent (1%) of the database size divided by the number of shards. This can range between 1MB to 250MB per shard for each backlog.\nChange the replication backlog size Use the crdb-cli utility to control the size of the replication backlogs. You can set it to auto or set a specific size.\nUpdate the database replication backlog configuration with the crdb-cli command shown below.\ncrdb-cli crdb update --crdb-guid \u0026lt;crdb_guid\u0026gt; --default-db-config \u0026#34;{\\\u0026#34;repl_backlog_size\\\u0026#34;: \u0026lt;size in MB | \u0026#39;auto\u0026#39;\u0026gt;}\u0026#34; Update the Active-Active (CRDT) replication backlog with the command shown below:\ncrdb-cli crdb update --crdb-guid \u0026lt;crdb_guid\u0026gt; --default-db-config \u0026#34;{\\\u0026#34;crdt_repl_backlog_size\\\u0026#34;: \u0026lt;size in MB | \u0026#39;auto\u0026#39;\u0026gt;}\u0026#34; Data persistence Active-Active supports AOF (Append-Only File) data persistence only. Snapshot persistence is not supported and should not be used. The snapshot option for data persistence on Active-Active databases is not supported and should not be used (Althought possible to be configured). If an Active-Active database is currently using snapshot data persistence, use crdb-cli to switch to AOF persistence:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB_GUID\u0026gt; --default-db-config \u0026#39;{\u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;aof_policy\u0026#34;:\u0026#34;appendfsync-every-sec\u0026#34;}\u0026#39; ","categories":["RS"]},{"uri":"/rs/installing-upgrading/manage-installation-questions/","uriRel":"/rs/installing-upgrading/manage-installation-questions/","title":"Manage installation questions","tags":[],"keywords":[],"description":"","content":"Several questions are displayed during the Redis Software installation process.\nHere, you\u0026rsquo;ll find a list of these questions and learn how to automatically answer these questions to perform a silent install.\nInstallation questions Several questions appear during installation:\nLinux swap file - Swap is enabled. Do you want to proceed? [Y/N]?\nWe recommend that you disable Linux swap in the operating system configuration to give Redis Software control of the memory allocation.\nAutomatic OS tuning - Do you want to automatically tune the system for best performance [Y/N]?\nTo allow the installation process to optimize the OS for Redis Software, answer Y. The installation process prompts you for additional information.\nThe /opt/redislabs/sbin/systune.sh file contains details about the tuning process.\nNetwork time - Do you want to set up NTP time synchronization now [Y/N]?\nRedis Software requires that all cluster nodes have synchronized time. You can either let the installation process configure NTP or you can configure NTP manually.\nFirewall ports - Would you like to open RedisLabs cluster ports on the default firewall zone [Y/N]?\nRedis Enterprise Software requires that all nodes have specific network ports open. You can either:\nAnswer Y to let the installation process open these ports. Answer N and configure the firewall manually for RHEL/CentOS firewall. Answer N and configure the firewall on the node manually for your OS. Installation verification (rlcheck) - Would you like to run rlcheck to verify proper configuration? [Y/N]?\nWe recommend running the rlcheck installation verification to make sure that the installation completed successfully. If you want to run this verification at a later time, you can run: /opt/redislabs/bin/rlcheck\nAnswer install questions automatically To perform a silent (or automated) install, answer the questions when you start the install.\nThere are two ways to do so:\nTo automatically answer yes to all questions (which accepts the default values), start the install script with the -y parameter:\n`% ./install.sh -y` Use an answer file to manage your response:\nCreate a text file to serve as an answer file. The answer file can contain any of the parameters for the installation questions and indicate the answer for the question with yes or no.\nFor example:\nignore_swap=no systune=yes ntp=no firewall=no rlcheck=yes If you use systune=yes, the installation answers yes to all of the system tuning questions.\nStart the install script with the -c command-line option and add the path to the answer file. For example:\n./install.sh -c /home/user/answers ","categories":["RS"]},{"uri":"/rs/security/access-control/manage-users/manage-passwords/","uriRel":"/rs/security/access-control/manage-users/manage-passwords/","title":"Manage passwords","tags":[],"keywords":[],"description":"Manage user passwords.","content":"Redis Enterprise Software provides several ways to manage the passwords of local accounts, including:\nPassword complexity rules\nPassword expiration\nPassword rotation policies\nYou can also manage a user\u0026rsquo;s ability to sign in and control session timeout.\nTo enforce more advanced password policies, we recommend using LDAP integration with an external identity provider, such as Active Directory.\nPassword complexity rules Redis Enterprise Software provides optional password complexity rules that meet common requirements. When enabled, these rules require the password to have:\nAt least 8 characters At least one uppercase character At least one lowercase character At least one number At least one special character These requirements reflect v6.2.12 and later. Earlier versions did not support numbers or special characters as the first or the last character of a password. This restriction was removed in v6.2.12.\nIn addition, the password:\nCannot contain the user\u0026rsquo;s email address or the reverse of the email address. Cannot have more than three repeating characters. Password complexity rules apply complexity rules are applied when a new user account is created and when the password is changed. Password complexity rules are not applied to accounts authenticated by an external identity provider.\nYou can use the admin console or the REST API to enable password complexity rules.\nEnable using the admin console To enable password complexity rules using the admin console:\nSign in to the Redis Enterprise Software admin console using an administrator account\nFrom the main menu, select Settings | Preferences\nPlace a checkmark next to Enable password complexity rules\nSave your changes\nEnable using the REST API To use the REST API to enable password complexity rules:\nPUT https://[host][:port]/v1/cluster {\u0026#34;password_complexity\u0026#34;:true} Deactivate password complexity rules To deactivate password complexity rules:\nRemove the checkmark from the Enable password complexity rules setting in the admin console\nUse the cluster REST API endpoint to set password_complexity to false\nPassword expiration To enforce an expiration of a user\u0026rsquo;s password after a specified number of days:\nUse the admin console to place a checkmark next to the Enable password expiration preference setting\nUse the cluster endpoint of the REST API\nPUT https://[host][:port]/v1/cluster {\u0026#34;password_expiration_duration\u0026#34;:\u0026lt;number_of_days\u0026gt;} To deactivate password expiration:\nRemove the checkmark next to the to the Enable password expiration preference setting.\nFor help locating the setting, see Password complexity rules.\nUse the cluster REST API endpoint to set password_expiration_duration to 0 (zero).\nPassword rotation policies Redis Enterprise Software lets you implement password rotation policies using its REST API.\nYou can add a new password for a database user without immediately invalidating the old one (which might cause authentication errors in production).\nNote: Password rotation does not work for the default user. Add additional users to enable password rotation. For user access to the Redis Enterprise Software admin console, you can set a password expiration policy to prompt the user to change their password.\nHowever, for database connections that rely on password authentication, you need to allow for authentication with the existing password while you roll out the new password to your systems.\nWith the Redis Enterprise Software REST API, you can add additional passwords to a user account for authentication to the database or the admin console and API.\nOnce the old password is replaced in the database connections, you can delete the old password to finish the password rotation process.\nWarning - Multiple passwords are only supported using the REST API. If you reset the password for a user in the admin console, the new password replaces all other passwords for that user. The new password cannot already exist as a password for the user and must meet the password complexity requirements, if enabled.\nRotate password To rotate the password of a user account:\nAdd an additional password to a user account with POST /v1/users/password:\nPOST https://[host][:port]/v1/users/password \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;old_password\u0026#34;:\u0026#34;\u0026lt;an_existing_password\u0026gt;\u0026#34;, \u0026#34;new_password\u0026#34;:\u0026#34;\u0026lt;a_new_password\u0026gt;\u0026#34;}\u0026#39; After you send this request, you can authenticate with both the old and the new password.\nUpdate the password in all database connections that connect with the user account.\nDelete the original password with DELETE /v1/users/password:\nDELETE https://[host][:port]/v1/users/password \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;old_password\u0026#34;:\u0026#34;\u0026lt;an_existing_password\u0026gt;\u0026#34;}\u0026#39; If there is only one valid password for a user account, you cannot delete that password.\nReplace all passwords You can also replace all existing passwords for a user account with a single password that does not match any existing passwords. This can be helpful if you suspect that your passwords are compromised and you want to quickly resecure the account.\nTo replace all existing passwords for a user account with a single new password, use PUT /v1/users/password:\nPUT https://[host][:port]/v1/users/password \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;old_password\u0026#34;:\u0026#34;\u0026lt;an_existing_password\u0026gt;\u0026#34;, \u0026#34;new_password\u0026#34;:\u0026#34;\u0026lt;a_new_password\u0026gt;\u0026#34;}\u0026#39; All of the existing passwords are deleted and only the new password is valid.\nNote: If you send the above request without specifying it is a PUT request, the new password is added to the list of existing passwords. ","categories":["RS"]},{"uri":"/rc/security/database-security/network-security/","uriRel":"/rc/security/database-security/network-security/","title":"Network security","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Cloud supports two types of network security: database-level CIDR allow lists and VPC-wide CIDR allow lists.\nThese features are available in most Redis Cloud configurations, as shown here:\nCloud provider VPC peering IP restrictions AWS Flexible and Annual Fixed (paid), Flexible, and Annual GCP Flexible and Annual Fixed (paid), Flexible, and Annual Azure Annual Annual IP and subnet restrictions You can restrict database access to a configurable set of source IP addresses and subnets. This is roughly equivalent to using iptables to limit access to a host.\nTo restrict a database to a specific set of source IP addresses or subnets, see Configure CIDR allow list.\nVirtual private clouds A Virtual Private Cloud (VPC) is an isolated set of resources within a public cloud, usually having its own subnets and VLAN.\nDatabases in Flexible and Annual subscriptions are almost always deployed in a Redis VPC. In most cases, you\u0026rsquo;ll need to create a VPC peering connection to access these databases. A VPC peering connection allows unrestricted network access between two VPCs.\nHow you create these connections and the features supported vary somewhat by public cloud provider. You can read about VPC usage for AWS, GCP, and Azure below.\nVPCs with AWS Subscriptions that run on AWS support two VPC options. To ensure that that you can securely connect to your database, you need to create a VPC peering connection.\nIf you create a VPC peering connection, you can also configure a CIDR allow list to allow connections only from specific IP address blocks or security groups.\nVPCs with GCP Subscriptions that run on GCP require a VPC peering connection. See GCP VPC peering to learn how to set up VPC peering for GCP.\nVPCs with Azure When you request a Redis Cloud Annual subscription, all databases will be deployed in your own Azure VPC.\n","categories":["RC"]},{"uri":"/rs/databases/configure/database-persistence/","uriRel":"/rs/databases/configure/database-persistence/","title":"Configure database persistence","tags":[],"keywords":[],"description":"How to configure database persistence with either an append-only file (AOF) or snapshots.","content":"All data is stored and managed exclusively in either RAM or RAM + flash Memory (Redis on Flash) and therefore, is at risk of being lost upon a process or server failure. As Redis Enterprise Software is not just a caching solution, but also a full-fledged database, persistence to disk is critical. Therefore, Redis Enterprise Software supports persisting data to disk on a per-database basis and in multiple ways.\nPersistence can be configured either during database creation or by editing an existing database\u0026rsquo;s configuration. While the persistence model can be changed dynamically, just know that it can take time for your database to switch from one persistence model to the other. It depends on what you are switching from and to, but also on the size of your database.\nConfigure persistence for your database In databases, either: Click Add (+) to create a new database. Click on the database that you want to configure and at the bottom of the page click edit. Navigate to Persistence Select your database persistence option Select save or update Data persistence options There are six options for persistence in Redis Enterprise Software:\nOptions Description None Data is not persisted to disk at all. Append Only File (AoF) on every write Data is fsynced to disk with every write. Append Only File (AoF) one second Data is fsynced to disk every second. Snapshot every 1 hour A snapshot of the database is created every hour. Snapshot every 6 hours A snapshot of the database is created every 6 hours. Snapshot every 12 hours A snapshot of the database is created every 12 hours. Selecting a persistence strategy When selecting your persistence strategy, you should take into account your tolerance for data loss and performance needs. There will always be tradeoffs between the two. The fsync() system call syncs data from file buffers to disk. You can configure how often Redis performs an fsync() to most effectively make tradeoffs between performance and durability for your use case. Redis supports three fsync policies: every write, every second, and disabled.\nRedis also allows snapshots through RDB files for persistence. Within Redis Enterprise, you can configure both snapshots and fsync policies.\nFor any high availability needs, replication may also be used to further reduce any risk of data loss and is highly recommended.\nFor use cases where data loss has a high cost:\nAppend-only file (AOF) - Fsync every everywrite - Redis Enterprise sets the open-source Redis directive appendfsyncalways. With this policy, Redis will wait for the write and the fsync to complete prior to sending an acknowledgement to the client that the data has written. This introduces the performance overhead of the fsync in addition to the execution of the command. The fsync policy always favors durability over performance and should be used when there is a high cost for data loss. For use cases where data loss is tolerable only limitedly:\nAppend-only file (AOF) - Fsync every 1 sec - Redis will fsync any newly written data every second. This policy balances performance and durability and should be used when minimal data loss is acceptable in the event of a failure. This is the default Redis policy. This policy could result in between 1 and 2 seconds worth of data loss but on average this will be closer to one second. Note: For performance reasons, if you are going to be using AOF, it is highly recommended to make sure replication is enabled for that database as well. When these two features are enabled, persistence is performed on the database replica and does not impact performance on the master. For use cases where data loss is tolerable or recoverable for extended periods of time:\nSnapshot, every 1 hour - Sets a full backup every 1 hour. Snapshot, every 6 hour - Sets a full backup every 6 hours. Snapshot, every 12 hour - Sets a full backup every 12 hours. None - Does not backup or persist data at all. Append-only file (AOF) vs snapshot (RDB) Now that you know the available options, to assist in making a decision on which option is right for your use case, here is a table about the two:\nAppend-only File (AOF) Snapshot (RDB) More resource intensive Less resource intensive Provides better durability (recover the latest point in time) Less durable Slower time to recover (Larger files) Faster recovery time More disk space required (files tend to grow large and require compaction) Requires less resource (I/O once every several hours and no compaction required) Active-Active data persistence Active-Active databases support AOF persistence only. Snapshot persistence is not supported for Active-Active databases.\nIf an Active-Active database is using snapshot persistence, use crdb-cli to switch to AOF persistence:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB_GUID\u0026gt; --default-db-config \\ \u0026#39;{\u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;aof_policy\u0026#34;:\u0026#34;appendfsync-every-sec\u0026#34;}\u0026#39; Redis on Flash data persistence If you are enabling data persistence for databases running on Redis Enterprise Flash, by default both master and replica shards are configured to write to disk. This is unlike a standard Redis Enterprise Software database where only the replica shards persist to disk. This master and replica dual data persistence with replication is done to better protect the database against node failures. Flash-based databases are expected to hold larger datasets and repair times for shards can be longer under node failures. Having dual-persistence provides better protection against failures under these longer repair times.\nHowever, the dual data persistence with replication adds some processor and network overhead, especially in the case of cloud configurations with persistent storage that is network attached (e.g. EBS-backed volumes in AWS).\nThere may be times where performance is critical for your use case and you don\u0026rsquo;t want to risk data persistence adding latency. If that is the case, you can disable data-persistence on the master shards using the following rladmin command:\nrladmin tune db \u0026lt;database_ID_or_name\u0026gt; master_persistence disabled ","categories":["RS"]},{"uri":"/rs/clusters/monitoring/prometheus-integration/","uriRel":"/rs/clusters/monitoring/prometheus-integration/","title":"Prometheus integration with Redis Enterprise Software","tags":[],"keywords":[],"description":"To collect and display metrics data from your databases and other cluster components, you can connect your Prometheus or Grafana server to your Redis Enterprise cluster.","content":"To collect and display metrics data from your databases and other cluster components, you can connect your Prometheus or Grafana server to your Redis Enterprise Software cluster. Metrics are exposed at the node, database, shard and proxy levels.\nPrometheus is an open-source systems monitoring and alerting toolkit that can scrape metrics from different sources. Grafana is an open-source, feature-rich metrics dashboard and graph editor that can process Prometheus data. You can use Prometheus and Grafana to:\nCollect and display data metrics not available in the admin console\nSet up automatic alerts for node or cluster events\nDisplay Redis Enterprise Software metric data alongside data from other applications\nIn each cluster, the metrics_exporter component listens on port 8070 and serves as a Prometheus scraping endpoint for obtaining metrics.\nQuick start To get started with custom monitoring:\nCreate a directory called \u0026lsquo;prometheus\u0026rsquo; on your local machine.\nWithin that directory, create a file called prometheus.yml.\nAdd the following contents to the yml file and replace \u0026lt;cluster_name\u0026gt; with your Redis cluster\u0026rsquo;s FQDN:\nNote: This is for development testing only (running in Docker). global: scrape_interval: 15s evaluation_interval: 15s # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: \u0026#34;prometheus-stack-monitor\u0026#34; # Load and evaluate rules in this file every \u0026#39;evaluation_interval\u0026#39; seconds. #rule_files: # - \u0026#34;first.rules\u0026#34; # - \u0026#34;second.rules\u0026#34; scrape_configs: # scrape Prometheus itself - job_name: prometheus scrape_interval: 10s scrape_timeout: 5s static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] # scrape Redis Enterprise - job_name: redis-enterprise scrape_interval: 30s scrape_timeout: 30s metrics_path: / scheme: https tls_config: insecure_skip_verify: true static_configs: - targets: [\u0026#34;\u0026lt;cluster_name\u0026gt;:8070\u0026#34;] Set up your Prometheus and Grafana servers. To set up Prometheus and Grafana on Docker containers:\nCreate a docker-compose.yml file:\nversion: \u0026#39;3\u0026#39; services: prometheus-server: image: prom/prometheus ports: - 9090:9090 volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml grafana-ui: image: grafana/grafana ports: - 3000:3000 environment: - GF_SECURITY_ADMIN_PASSWORD=secret links: - prometheus-server:prometheus To start the containers, run:\n$ docker compose up -d To check that all the containers are up, run: docker ps\nIn your browser, sign in to Prometheus at http://localhost:9090 to make sure the server is running.\nSelect Status and then Targets to check that Prometheus is collecting data from the Redis Enterprise cluster.\nIf Prometheus is connected to the cluster, you can type node_up in the Expression field on the Prometheus home page to see the cluster metrics.\nConfigure the Grafana datasource:\nSign in to Grafana. If you installed Grafana locally, go to http://localhost:3000 and sign in with:\nUsername: admin Password: secret In the Grafana configuration menu, select Data Sources.\nSelect Add data source.\nSelect Prometheus from the list of data source types.\nEnter the Prometheus information:\nName: redis-enterprise URL: http://\u0026lt;your prometheus address\u0026gt;:9090 Access: Server Note: If the network port is not accessible to the Grafana server, select the Browser option from the Access menu. In a testing environment, you can select Skip TLS verification. Add dashboards for cluster, node, and database metrics. To add preconfigured dashboards:\nIn the Grafana dashboards menu, select Manage. Click Import. Copy one of the configurations into the Paste JSON field. database.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;refresh\u0026#34;: false, \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 121, \u0026#34;panels\u0026#34;: [ { \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$status\u0026lt;b\u0026gt;\\n\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$newStatus\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;id\u0026#34;: 9, \u0026#34;links\u0026#34;: [], \u0026#34;mode\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;span\u0026#34;: 2, \u0026#34;style\u0026#34;: { \u0026#34;font-size\u0026#34;: \u0026#34;72pt\u0026#34; }, \u0026#34;title\u0026#34;: \u0026#34;Status for BDB:$bdb\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 4, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb used memory\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Keys\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 6, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Connections\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 7, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(listener_total_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Listeners\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 12, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_no_of_keys{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}) or count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 258, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_write_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_write_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_read_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_read_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_other_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_other_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;max_over_time(bdb_avg_latency_max{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_latency (max over $aggregation)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg latency for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 55, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;listener_acc_latency{bdb=~\\\u0026#34;$bdb.*\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / listener_total_started_res{bdb=~\\\u0026#34;$bdb.*\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg listener latency for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 44, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*limit.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;used_memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_memory_limit{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026lt; bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} * 1.5\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;limit (when low, near used)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Memory for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 306, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 10, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;listener_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;endpoint {{proxy}} node: {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb Connections\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 14, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_other_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;other requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;read requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;write requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_total_req_max{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests (max)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;max_over_time(bdb_total_req_max{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[$aggregation])\\n\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests (max over $aggregation)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Requests[total,read,write,other] for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 16, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_egress_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ingress_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb Ingress/Egress\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 43, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_main_thread_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_main_thread_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;main threads (masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_shard_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_shard_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all threads (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_fork_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_fork_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all forks (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_main_thread_cpu_system_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_main_thread_cpu_user_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;main threads max (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_shard_cpu_system_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_shard_cpu_user_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all threads max (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum((delta(redis_process_main_thread_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_main_thread_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Slave main threads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum((delta(redis_process_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Slave all threads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB CPU utilization for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 13, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(delta(redis_process_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60*100 or redis_cpu_percent{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard: {{redis}} role: {{role}} node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU usage by shards for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 53, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*maxmemory.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, role:{{role}}, node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_maxmemory{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;maxmemory: redis:{{redis}}, bdb:{{bdb}}, role:{{role}}, node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Sizes of shards for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 196, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 17, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} or redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, role: {{role}}, node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;# keys/shard\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 11, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;num of keys\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_db0_expires{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;num of volatile keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;ram keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;disk keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;(sum(max(redis_bigdb0_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) + sum(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))) - sum(redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;overlapping keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_dirty{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_dirty{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dirty keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_clean{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_clean{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;clean keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;#Keys for BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 20, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_evicted_objects{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB:$bdb\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Evicted objects for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 21, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_expired_objects{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB:$bdb\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Expired objects for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 202, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 48, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_rewrites{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}[$aggregation]) !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard AOF Rewrites / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 57, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_bgsave_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BGSAVE {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_rewrite_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;AOFRW {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;forks\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 49, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_misses{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Read Misses\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_misses{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Write Misses\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_hits{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Read Hits\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_hits{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Write Hits\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Read/Write Misses\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 23, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_fragmentation{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} or redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} ({{role}})\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard RSS fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 26, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(bdb_read_hits{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_write_hits{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) / (bdb_read_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_write_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;DB hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DB Hit Ratio (hits / requests)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 47, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_replicaof_syncer_local_ingress_lag_time{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB $bdb src_id {{src_id}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_crdt_syncer_local_ingress_lag_time{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB $bdb crdt src_id {{src_id}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Syncer lag\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 34, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / count(redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis_mem_fragmentation_ratio\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Shard allocator fragmentation\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_resident{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard allocator rss\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb total fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 50, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_allocator_active\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard allocator fragmentation % (defraggable)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 30, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_active_defrag_running{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis active defrag running\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 22, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_delayed_fsync{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} ({{role}})\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_aof_delayed_fsync\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Delayed fsync - (events / $aggregation)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 58, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_allocator_active\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard allocator fragmentation bytes (defraggable)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: true, \u0026#34;height\u0026#34;: 285, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 51, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*limit.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_used_ram\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_limit{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;limit (when low, near used)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB (ROF) used RAM for $bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 25, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;read hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;write hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;del hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) / (bdb_big_fetch_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_fetch_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;read hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_write_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;write hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_del_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;del hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Positive is RAM, Negative is Flash\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 52, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/User*/\u0026#34;, \u0026#34;dashes\u0026#34;: true } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]))/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;RAM Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]) * -1)/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Flash Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_user_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]))/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;User RAM Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_user_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]) * -1)/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;User Flash Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB RAM:Flash access ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Positive is RAM, Negative is Flash\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 54, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_big_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[1m])/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;RAM Ops for Redis {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_big_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[1m]) / -60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Flash Ops for Redis {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;RAM:Flash access ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_read_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Reads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_write_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Writes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Flash IO Banwidth\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 18, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_reads{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Reads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_writes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Writes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_dels{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Deletes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Flash IOPS\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 33, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_wait_busy_key{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis wait busy key / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 31, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_blocking_reads{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis blocking reads / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 37, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;frag ratio\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_bigstore{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;used\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_bigstore{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} * bdb_disk_frag_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;actual\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_disk_frag_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;frag ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB used flash\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 29, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_postponed_clients{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB postponed clients\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 27, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_blocked_clients{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Number of blocked clients\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_big_blocked_clients\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB blocked clients\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;ratiojj\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;actual (bytes)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 120 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} / bdb_ram_limit{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} / redis_max_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis: {{redis}} role:{{role}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB ram overhead\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 32, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(redis_blocking_writes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis blocking writes\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 36, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_big_inst_avg_read_io_queue{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis big inst avg read io queue\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 35, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_big_inst_avg_write_io_queue{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis big inst avg write io queue\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 40, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;objects in flash (sum of all master shards)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;objects in ram (sum of all master shards)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB objects(values) in FLASH\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 46, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/BDB.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/average.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/lowest.*/\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB % values in RAM (masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average master shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average slave shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis) * -1) *-1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest master shard % values in RAM - redsi:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis) * -1) *-1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest slave shard % values in RAM - redsi:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} / redis_bigdb0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest master shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} / redis_bigdb0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest slave shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 10 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB % values in RAM\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 39, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*average.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*lowest.*/\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}} (sum of masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average number of values in ram per slave shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average number of values in ram per master shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per slave shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per master shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster) \u0026lt; on(cluster,bdb,redis) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} ) by (bdb, redis) * -1) * -1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per slave shard - redis:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster) \u0026lt; on(cluster,bdb,redis) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (bdb, redis) * -1) * -1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per master shard - redis:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard objects(values) in RAM\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 42, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_ram_dirty_evictions{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB dirty RAM evictions\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 41, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_ram_clean_evictions{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB clean RAM evictions\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 56, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/stalls/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;/slowdowns/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_comp_started{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;compactions\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_started{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;flushes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_writes_slowdown{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;slowdowns / $aggregation\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_writes_stop{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;stalls / $aggregation\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;RocksDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;events\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;ROF Metrics\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: \u0026#34;BDB Id\u0026#34;, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;bdb\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;}, bdb)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_status{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}, status)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;newStatus\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_up{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}, status)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;auto\u0026#34;: true, \u0026#34;auto_count\u0026#34;: 100, \u0026#34;auto_min\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;current\u0026#34;: { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, \u0026#34;hide\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;aggregation interval\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;aggregation\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;6h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;6h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;12h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;12h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;7d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;14d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;14d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30d\u0026#34; } ], \u0026#34;query\u0026#34;: \u0026#34;1m,10m,30m,1h,6h,12h,1d,7d,14d,30d\u0026#34;, \u0026#34;refresh\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;interval\u0026#34; } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 176 } node.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 174, \u0026#34;panels\u0026#34;: [ { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}) by (bdb))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# BDBs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: false }, \u0026#34;id\u0026#34;: 26, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;, role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Master Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: false }, \u0026#34;id\u0026#34;: 7, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;, role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Slave Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: 2, \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: true, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 9, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_system{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + node_cpu_user{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 7200 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;70,85\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CPU Utilization\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;70%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 10, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Free RAM\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$version\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;id\u0026#34;: 14, \u0026#34;links\u0026#34;: [], \u0026#34;mode\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;span\u0026#34;: 2, \u0026#34;title\u0026#34;: \u0026#34;Node version\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 312, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: 2, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 3, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*dmcproxy.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;stack\u0026#34;: false }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*shards.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;stack\u0026#34;: false } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34; node_cpu_user{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} + node_cpu_system{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;user+system\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;1- node_cpu_idle{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} - node_cpu_system{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} - node_cpu_user{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} unless node_cpu_nice{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;other\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 300 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_steal{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;steal\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_nice{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;nice\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_irqs{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;irqs\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;dmcproxy_process_cpu_usage_percent{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}/100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dmcproxy (100% = 1 core)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34; node_cpu_iowait{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;iowait\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_cpu_usage_percent{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}/100)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shards (100% = 1 core)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;H\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU Usage on node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;percents\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;1.5\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 297, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;display the egress and ingress traffic on the node\u0026#39;s NIC\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 4, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: null, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_ingress_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_egress_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;egress for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node $node In/Out Traffic\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;max\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;display the egress and ingress traffic on the node\u0026#39;s NIC\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 32, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total requests for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node $node req/sec\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;max\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 12, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_egress_bytes{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Egress listener {{proxy}} for {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_ingress_bytes{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress listener {{proxy}} for {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 listeners Egress/Ingrees\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 27, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(5, listener_acc_latency{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / listener_total_started_res{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg listener latency for node:$node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 325, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, redis_used_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} or redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, bdb:{{bdb}}, role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;dmcproxy_process_resident_memory_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dmcproxy\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 shards by used ram\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 13, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, bdb:{{bdb}}, role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Size of shards on node (Ability for sorting like by size)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;BIGSTORE IOPS\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 322, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 11, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 active listeners by connections\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 17, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 active listeners by requests\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 243, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 6, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}) by (bdb)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;#Shards per BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 8, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;available memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Available Memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 29, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total shards memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total shards ram\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;total shards memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 30, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;active / allocated (defraggable)\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;resident / active (purgable)\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;active defrag running\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;process rss / allocator rss\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_allocated{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - 1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active / allocated (defraggable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - 1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;resident / active (purgable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_allocated{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active - allocated (defraggable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;resident - active (purgable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_active_defrag_running{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})/100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active defrag running\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_resident_memory_bytes{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;process rss - allocator rss\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_process_resident_memory_bytes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_resident_memory_bytes{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;process rss / allocator rss\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_process_resident_memory_bytes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 16, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(delta(redis_process_cpu_system_seconds_total{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60*100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard: {{redis}} role: {{role}} bdb: {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard\u0026#39;s CPU on Node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 2, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(dmcproxy_process_cpu_user_seconds_total{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;CPU User\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;rate(dmcproxy_process_cpu_system_seconds_total{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;CPU System\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;dmcproxy_process_cpu_system_seconds_total\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DMC Proxy CPU Usage\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 22, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*cow.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0 }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*mem.*/\u0026#34;, \u0026#34;fill\u0026#34;: 8 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_last_cow_size{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard last RDB cow - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_last_cow_size{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard last AOF cow - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_mem_aof_buffer{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard aofrw buffers - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_mem_clients_slaves{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard salve buffers - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;fork memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 25, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_bgsave_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BGSAVE {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_rewrite_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}!=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;AOFRW {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;forks\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 23, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_rewrites{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation]) !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard AOF Rewrites / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 24, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_aof_delayed_fsync{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;delayed fsync\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;delayed fsync - (events / $aggregation)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: true, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 20, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_throughput{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_throughput on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_read_throughput{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_throughput on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE throughput\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 18, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_read_iops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_iops on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;node_flash_read_iops\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_iops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_iops on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE IOPS\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 21, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_time{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} / 1000\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_time on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34; node_flash_read_time {cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} / 1000\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_time on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE wait time\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_bigstore_kv_ops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_bigstore_kv_ops on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE KV\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 31, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_disk{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total used flash\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_disk_allocation{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total flash allocation\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_bigstore_free{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;free flash\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node used flash\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;ROF metrics\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;}, node)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 3, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;version\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}, cnm_version)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;auto\u0026#34;: true, \u0026#34;auto_count\u0026#34;: 100, \u0026#34;auto_min\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;current\u0026#34;: { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, \u0026#34;hide\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;interval aggregation\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;aggregation\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;6h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;6h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;12h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;12h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;7d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;14d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;14d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30d\u0026#34; } ], \u0026#34;query\u0026#34;: \u0026#34;1m,10m,30m,1h,6h,12h,1d,7d,14d,30d\u0026#34;, \u0026#34;refresh\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;interval\u0026#34; } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Node Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 81 } cluster.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Table\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;refresh\u0026#34;: false, \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 137, \u0026#34;panels\u0026#34;: [ { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 1, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count (bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 7200 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;# BDBs\u0026#34;, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 3, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_connected_clients{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Nodes\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: true, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(247, 43, 43, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 14, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;dashUri\u0026#34;: \u0026#34;db/alerts\u0026#34;, \u0026#34;dashboard\u0026#34;: \u0026#34;Alerts\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;targetBlank\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;Alerts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dashboard\u0026#34; } ], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(ALERTS{cluster=\\\u0026#34;$cluster\\\u0026#34;, alertstate=\\\u0026#34;firing\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;0:1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Number of active Alerts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 277, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 3, \u0026#34;id\u0026#34;: 2, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;Total Available\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Provisional (No Overbooking)\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Provisional\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Used\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Available\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_provisional_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Provisional\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_provisional_memory_no_overbooking{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Provisional (No Overbooking)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 10, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;hideEmpty\u0026#34;: true, \u0026#34;hideZero\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: false, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m])) by (bdb) \u0026gt; 100 * 1024*1024\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB #{{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Grow over 1min (Over 100MiB)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 1, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 280, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 4, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: true, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ {} ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, sum(bdb_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (bdb) \u0026gt; 0)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;#{{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Requests by BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 6, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ {} ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;100 - node_cpu_idle{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}*100\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU Usage for node $node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;total\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 15, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Requests Per Listener\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 2, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB: {{bdb}} Redis: {{redis}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Mem By redis for Node $node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 246, \u0026#34;panels\u0026#34;: [ { \u0026#34;columns\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;Avg\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;avg\u0026#34; }, { \u0026#34;text\u0026#34;: \u0026#34;Current\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;filterNull\u0026#34;: false, \u0026#34;fontSize\u0026#34;: \u0026#34;100%\u0026#34;, \u0026#34;id\u0026#34;: 23, \u0026#34;links\u0026#34;: [], \u0026#34;pageSize\u0026#34;: null, \u0026#34;scroll\u0026#34;: true, \u0026#34;showHeader\u0026#34;: true, \u0026#34;sort\u0026#34;: { \u0026#34;col\u0026#34;: 2, \u0026#34;desc\u0026#34;: true }, \u0026#34;span\u0026#34;: 4, \u0026#34;styles\u0026#34;: [ { \u0026#34;dateFormat\u0026#34;: \u0026#34;YYYY-MM-DD HH:mm:ss\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;Time\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;colorMode\u0026#34;: null, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;decimals\u0026#34;: 2, \u0026#34;pattern\u0026#34;: \u0026#34;/.*/\u0026#34;, \u0026#34;thresholds\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;decbytes\u0026#34; } ], \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;title\u0026#34;: \u0026#34;List of BDBs\u0026#34;, \u0026#34;transform\u0026#34;: \u0026#34;timeseries_aggregations\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;table\u0026#34; }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 8, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026gt; 30 * 1024 * 1024\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Mem By BDB over 30MB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 35, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Conns for BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_conns\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener #{{listener}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Used conns By BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;id\u0026#34;: 47, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10,bdb_avg_latency{cluster=\\\u0026#34;$cluster\\\u0026#34;} )\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Latency for BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_conns\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top bdb by latency in cluster\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: true, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;}, node)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 3, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Cluster Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 38 } In the Import options, select the redis-enterprise datasource and click Import. The dashboards that you create from the configurations are sample dashboards. For more information about configuring dashboards, see the Grafana documentation.\n","categories":["RS"]},{"uri":"/kubernetes/re-clusters/","uriRel":"/kubernetes/re-clusters/","title":"Redis Enterprise clusters (REC)","tags":[],"keywords":[],"description":"Articles to help you manage your Redis Enterprise clusters (REC).","content":"This section contains articles to help you manage your Redis Enterprise clusters (REC).\nConnect to the admin console Connect to the Redis Enterprise admin console to manage your Redis Enterprise cluster.\nCreate Active-Active databases on Kubernetes This section how to set up an Active-Active Redis Enterprise database on Kubernetes using the Redis Enterprise Software operator.\nUse Redis on Flash on Kubernetes Deploy a cluster with Redis on Flash on Kubernetes.\nManage databases in multiple namespaces Redis Enterprise for Kubernetes allows you to deploy to multiple namespaces within your Kubernetes cluster. This article shows you how to configure your Redis Enterprise cluster to connect to databases in multiple namespaces\nUpgrade a Redis Enterprise cluster (REC) on Kubernetes This task describes how to upgrade a Redis Enterprise cluster via the operator.\nRecover a Redis Enterprise cluster on Kubernetes This task describes how to recover a Redis Enterprise cluster on Kubernetes.\nDelete custom resources This article explains how to delete Redis Enterprise clusters and Redis Enterprise databases from your Kubernetes environment.\nConnect the Prometheus operator to Redis Enterprise for Kubernetes This article describes how to configure a Prometheus operator custom resource to allow it to export metrics from Redis Enterprise for Kubernetes.\n","categories":["Platforms"]},{"uri":"/kubernetes/re-databases/","uriRel":"/kubernetes/re-databases/","title":"Redis Enterprise databases (REDB)","tags":[],"keywords":[],"description":"Articles to help you manage your Redis Enterprise databases (REDBs).","content":"This section contains articles to help you manage your Redis Enterprise databases (REDBs).\nManage Redis Enterprise databases for Kubernetes This section describes how the database controller provides the ability to create, manage, and use databases via a database custom resource.\nEstablish external routing with an ingress controller Configure an ingress controller to access your Redis Enterprise databases from outside the Kubernetes cluster.\nUse OpenShift routes for external database access Every time the Redis Enterprise operator creates a Redis Enterprise database (REDB), it creates a service that allows requests to be routed to that database. Redis Enterprise supports three types of services for accessing databases: ClusterIP, headless, or LoadBalancer. By default, REDB creates a ClusterIP type service, which exposes a cluster-internal IP and can only be accessed from within the cluster. OpenShift routes allow requests to be routed to the REDB from outside the cluster.\nConfigure Istio for external routing Configure Istio as an ingress controller for access to your Redis Enterprise databases from outside the Kubernetes cluster.\nCreate replica databases on Kubernetes How to create and automate database replicas using the database controller\n","categories":["Platforms"]},{"uri":"/modules/redisjson/","uriRel":"/modules/redisjson/","title":"RedisJSON","tags":[],"keywords":[],"description":"The RedisJSON module adds support for JSON to Redis databases.","content":"The RedisJSON module adds support for the JSON data structure to Redis databases.\nApplications developed with the open source version of RedisJSON are 100% compatible with RedisJSON in Redis Enterprise Software.\nRedisJSON paths Paths let you traverse the structure of a JSON document, starting from the root, and interact only with the data you want. You can also use paths to perform operations on specific JSON elements.\nSince there is no standard for JSON path syntax, RedisJSON implements its own.\nJSONPath syntax RedisJSON v2.0 and later support the JSONPath syntax, which resembles Goessner\u0026rsquo;s design:\nPaths start with a dollar sign ($), which represents the root of the JSON document.\nSee the JSONPath syntax table to learn how to access various elements within a JSON document.\nThe following path refers to headphones, which is a child of inventory under the root:\n$.inventory.headphones\nSee JSONPath examples for examples with more complex syntax.\nLegacy path syntax The legacy path syntax refers to the path implementation in RedisJSON v1. RedisJSON v2 still supports this legacy path syntax in addition to JSONPath syntax.\nThe legacy path syntax works as follows:\nA period character represents the root.\nFor paths to the root\u0026rsquo;s children, it is optional to prefix the path with a period.\nSupports both dot notation and bracket notation for JSON object key access.\nThe following paths refer to headphones, which is a child of inventory under the root:\n.inventory.headphones\ninventory[\u0026quot;headphones\u0026quot;]\n['inventory'][\u0026quot;headphones\u0026quot;]\nKey name rules You can only use the legacy path syntax to access JSON keys if they follow these name syntax rules:\nKey names must begin with a letter, a dollar sign ($), or an underscore (_). Key names can contain letters, digits, dollar signs, and underscores. Key names are case-sensitive. Index and search JSON documents If a Redis database has both RedisJSON and RediSearch modules enabled, you can index and search stored JSON documents.\nFor more information about how to use RediSearch with JSON documents, see the Search JSON quick start.\nRedisJSON in Active-Active databases RedisJSON v2.2 and later support the JSON data structure as a conflict-free replicated data type (CRDT) in Active-Active Redis Enterprise databases.\nFor details about how RedisJSON resolves operation conflicts that can arise when replicas attempt to sync, see the RedisJSON conflict resolution rules.\nMore info RedisJSON quick start RedisJSON commands RedisJSON source ","categories":["Modules"]},{"uri":"/rs/references/rest-api/requests/","uriRel":"/rs/references/rest-api/requests/","title":"Redis Enterprise REST API requests","tags":[],"keywords":[],"description":"Documents the requests supported by the Redis Enterprise Software REST API calls.","content":"A REST API request requires the following components:\nHTTP method (GET, PUT, PATCH, POST, DELETE) Base URL Endpoint Some requests may also require:\nURL parameters Query parameters JSON request body Permissions Request Description actions Actions requests bdbs Database requests bootstrap Bootstrap requests cluster Cluster settings requests crdb_tasks Active-Active database task status requests crdbs Active-Active database requests endpoints/stats Endpoint statistics requests jsonschema API object JSON schema requests ldap_mappings LDAP mappings requests license License requests logs Cluster event logs requests modules Redis modules requests nodes Node requests ocsp OCSP requests redis_acls Redis access control list (ACL) requests roles Roles requests shards/stats Shard statistics requests suffix DNS suffix requests suffixes DNS suffixes requests users User requests ","categories":["RS"]},{"uri":"/modules/redisearch/redisearch-active-active/","uriRel":"/modules/redisearch/redisearch-active-active/","title":"RediSearch in Active-Active databases","tags":[],"keywords":[],"description":"Search Active-Active databases.","content":"Starting with RediSearch 2.x, supported in Redis Enterprise Software (RS) 6.0 and later, you can add the RediSearch module to an Active-Active database at the time of creation.\nYou can run search operations on any instance of an Active-Active database.\nHow it works Create an Active-Active database with RediSearch 2.x enabled. Create the index on each instance of the database. If you are using synonyms, you need to add them to each replica. The index is maintained by each instance outside of the database keyspace, so only updates to the hashes in the databases are synchronized. Command compatibility Active-Active databases do not support the following RediSearch commands:\nFT.DROP FT.SUGADD FT.SUGGET FT.SUGDEL FT.SUGLEN Example Here\u0026rsquo;s an example to help visualize Active-Active RediSearch:\nTime Description CRDB Instance1 RediSearch Instance 1 CRDB Instance 2 RediSearch Instance 2 t0 Create the index on each instance FT.CREATE idx \u0026hellip;. FT.CREATE idx \u0026hellip;. t1 Add doc1 as a hash on instance 1; RediSearch indexes doc1 on instance 1 HSET doc1 field1 \u0026ldquo;a\u0026rdquo; (Index doc1 field1 \u0026ldquo;a\u0026rdquo;) t2 Add doc2 as a hash on instance 2; RediSearch indexes doc2 on instance 2 HSET doc1 field2 \u0026ldquo;b\u0026rdquo; (Index doc1 field2 \u0026ldquo;b\u0026rdquo;) t3 Searching for \u0026ldquo;a\u0026rdquo; in each instance only finds the result in instance 1 FT.Search idx \u0026ldquo;a\u0026rdquo;1) 12) doc1 FT.Search idx \u0026ldquo;a\u0026rdquo;1) 0 t4 Active-Active synchronization - Sync - - Sync - t5 Both hashes are found in each instance HGETALL doc11) \u0026ldquo;field2\u0026rdquo;2) \u0026ldquo;b\u0026rdquo;3) \u0026ldquo;field1\u0026rdquo;4) \u0026ldquo;a\u0026rdquo; HGETALL doc11) \u0026ldquo;field2\u0026rdquo;2) \u0026ldquo;b\u0026rdquo;3) \u0026ldquo;field1\u0026rdquo;4) \u0026ldquo;a\u0026rdquo; t6 Searching for \u0026ldquo;a\u0026rdquo; in each instance finds both documents FT.Search idx \u0026ldquo;a\u0026rdquo;1) 12) doc1 FT.Search idx \u0026ldquo;a\u0026rdquo;1) 12) doc1 The practical result is that you have a geo-distributed database with a high level of consistency that can also run search operations on any instance.\n","categories":["Modules"]},{"uri":"/rs/databases/memory-performance/shard-placement-policy/","uriRel":"/rs/databases/memory-performance/shard-placement-policy/","title":"Shard placement policy","tags":[],"keywords":[],"description":"Detailed info about the shard placement policy.","content":"In Redis Enterprise Software, the location of master and replica shards on the cluster nodes can impact the database and node performance. Master shards and their corresponding replica shards are always placed on separate nodes for data resiliency. The shard placement policy helps to maintain optimal performance and resiliency.\nIn addition to the shard placement policy, considerations that determine shard placement are:\nSeparation of master and replica shards Available persistence and Redis on Flash (RoF) storage Rack awareness Memory available to host the database when fully populated The shard placement policies are:\ndense - Place as many shards as possible on the smallest number of nodes to reduce the latency between the proxy and the database shards; Recommended for Redis on RAM databases to optimize memory resources sparse - Spread the shards across as many nodes in the cluster as possible to spread the traffic across cluster nodes; Recommended for Redis on Flash databases to optimize disk resources When you create a Redis Enterprise Software cluster, the default shard placement policy (dense) is assigned to all databases that you create on the cluster.\nYou can:\nChange the default shard placement policy for the cluster to sparse so that the cluster applies that policy to all databases that you create Change the shard placement policy for each database after the database is created Shard placement policies Dense shard placement policy In the dense policy, the cluster places the database shards on as few nodes as possible. When the node is not able to host all of the shards, some shards are moved to another node to maintain optimal node health.\nFor example, for a database with two master and two replica shards on a cluster with three nodes and a dense shard placement policy, the two master shards are hosted on one node and the two replica shards are hosted on another node.\nFor Redis on RAM databases without the OSS cluster API enabled, use the dense policy to optimize performance.\nFigure: Three nodes with two master shards (red) and two replica shards (grey) with a dense placement policy\nSparse shard placement policy In the sparse policy, the cluster places shards on as many nodes as possible to distribute the shards of a database across all available nodes. When all nodes have database shards, the shards are distributed evenly across the nodes to maintain optimal node health.\nFor example, for a database with two master and two replica shards on a cluster with three nodes and a sparse shard placement policy:\nNode 1 hosts one of the master shards Node 2 hosts the replica for the first master shard Node 3 hosts the second master shard Node 1 hosts for the replica shard for master shard 2 For Redis on RAM databases with OSS cluster API enabled and for Redis on Flash databases, use the sparse policy to optimize performance.\nFigure: Three nodes with two master shards (red) and two replica shards (grey) with a sparse placement policy\nRelated articles You can configure the shard placement policy for each database.\n","categories":["RS"]},{"uri":"/kubernetes/reference/supported_k8s_distributions/","uriRel":"/kubernetes/reference/supported_k8s_distributions/","title":"Supported distributions","tags":[],"keywords":[],"description":"Support matrix for the current Redis Enterprise K8s operator","content":"Each release of Redis Enterprise for Kubernetes is thoroughly tested against a set of Kubernetes distributions. The table below lists the current release\u0026rsquo;s support status for each distribution.\n\u0026ldquo;supported\u0026rdquo; indicates this distribution is supported for this release. \u0026ldquo;deprecated\u0026rdquo; indicates this distribution is supported for this release, but will be dropped in a future release. Any distribution not listed below is not supported for production workloads. Kubernetes version 1.21 1.22 1.23 1.24 1.25 Community Kubernetes supported supported supported supported Amazon EKS deprecated supported supported Azure AKS supported supported supported Google GKE deprecated supported supported supported supported* Rancher 2.6 supported supported supported supported* VMware TKG 1.6 supported* supported* OpenShift version 4.8 4.9 4.10 4.11 deprecated supported supported* VMware TKGI version 1.12 1.13 1.14 1.15 supported supported supported* * Support added in this release\n","categories":["Platforms"]},{"uri":"/rs/clusters/configure/sync-clocks/","uriRel":"/rs/clusters/configure/sync-clocks/","title":"Synchronize cluster node clocks","tags":[],"keywords":[],"description":"Sync node clocks to avoid problems with internal custer communication.","content":"To avoid problems with internal cluster communications that can impact your data integrity, make sure that the clocks on all of the cluster nodes are synchronized using Chrony and/or NTP.\nWhen you install Redis Enterprise Software, the install script checks if Chrony or NTP is running. If they are not, the installation process asks for permission to configure a scheduled Cron job. This should make sure that the node\u0026rsquo;s clock is always synchronized. If you did not confirm configuring this job during the installation process, you must use the Network Time Protocol (NTP) regularly to make sure that all server clocks are synchronized.\nTo synchronize the server clock, run the command that is appropriate for your operating system. For example, in Ubuntu, the following command can be used to synchronize a server\u0026rsquo;s clock to an NTP server:\nsudo /etc/network/if-up.d/ntpdate If you are using Active-Active databases, you must use Network Time Service (ntpd) to synchronize OS clocks consistent across clusters to handle conflict resolution according to the OS time.\n","categories":["RS"]},{"uri":"/rc/api/examples/update-database/","uriRel":"/rc/api/examples/update-database/","title":"Update databases","tags":[],"keywords":[],"description":"How to construct requests that update an existing database.","content":"The API operation that updates an existing database is: PUT /subscriptions/{subscription-id}/databases/{database-id}\nThis API operation uses the same provisioning lifecycle as the create database operation.\nDatabase update request JSON body The primary component of a database update request is the JSON request body that contains the details of the requested database changes.\nYou can see the complete set of JSON elements accepted by the database update API operation in the Swagger UI. To see the JSON elements, expand the specific API operation and, in the request section, click Model.\nHere are several examples of JSON requests to update a database:\nAdd or remove Replica Of Setting one or more source Redis databases configures the updated database as a Replica Of destination database for the specified Redis databases.\nAdd a source database The following JSON request specifies two source databases for the updated database:\n{ \u0026#34;replicaOf\u0026#34;: [ \u0026#34;redis://redis-12345.c9876.us-east-1-mz.ec2.cloud.rlrcp.com:12345\u0026#34; , \u0026#34;redis://redis-54321.internal.c9876.us-east-1-mz.ec2.cloud.rlrcp.com:54321\u0026#34; ] } The replicaOf array contains one or more URIs with the format: redis://user:password@host:port If the URI provided belongs to the same account, you can provide just the host and port (example: [\u0026quot;redis://endpoint1:6379', \u0026quot;redis://endpoint2:6380\u0026quot;]) Warning - If a source database is already defined for a specific database, and the goal is to add an additional source database, the source databases URI for the existing source must be included in the database updates JSON request. Remove a source database To remove a source database from the replicaOf list, submit a JSON request that does not include the specific source database URI.\nExample: Given a database that has two defined source databases:\n{ \u0026#34;replicaOf\u0026#34;: [ \u0026#34;redis://redis-12345.c9876.us-east-1-mz.ec2.cloud.rlrcp.com:12345\u0026#34; , \u0026#34;redis://redis-54321.internal.c9876.us-east-1-mz.ec2.cloud.rlrcp.com:54321\u0026#34; ] } You can use this JSON request to remove one of the two source databases: { \u0026#34;replicaOf\u0026#34;: [ \u0026#34;redis://redis-12345.c9876.us-east-1-mz.ec2.cloud.rlrcp.com:12345\u0026#34; ] } You can use this JSON request to remove all source databases: { \u0026#34;replicaOf\u0026#34;: [] } Viewing database Replica Of information The API operation GET /subscriptions/{subscription-id}/databases/{database-id} returns information on a specific database, including the Replica Of endpoints defined for the specific database.\n","categories":["RC"]},{"uri":"/modules/redisearch/redisearch-2-upgrade/","uriRel":"/modules/redisearch/redisearch-2-upgrade/","title":"Upgrade a database to RediSearch 2.x","tags":[],"keywords":[],"description":"Upgrade a database from RediSearch 1.x to RediSearch 2.x.","content":"RediSearch 2.x includes some significant architectural changes that improve functionality. The main change is that RediSearch 2.x stores indexes outside of the Redis database that contains the data. This makes commands more efficient and improves replication between clusters because the index changes are managed by the participating clusters rather than being synchronized with the data.\nThis change allows databases with RediSearch to support:\nActive-Active databases Database cluster re-sharding Replica Of to a sharded destination database EXPIRE of documents reflected in the index In addition, RediSearch 2.x indexes data that already existed in the database at the time that the index was created.\nTo upgrade a Redis Enterprise Software database with RediSearch 1.x to RediSearch 2.x, you have to set up a new database with RediSearch 2.x and use the RediSearch_Syncer.py script to replicate the data from the old database into the new database.\nNote: After you create the database or after you replicate the data, create an index with a prefix or filter that defines the keys that you want to index. Prerequisites Install Python 3 on the host where you plan to run the synchronization script:\nsudo apt install python3 Limitations Suggestions (FT.SUG APIs) and spell check dictionaries are not replicated from the source database. You need to add them manually. If there are multiple indexes on the source and the documents do not have prefixes that identify them with an index, RediSearch 2.x can\u0026rsquo;t index the documents in their respective indexes. The NOSAVE option is no longer supported. Indexes created with the NOSAVE option can\u0026rsquo;t be upgraded. Databases that contain temporary indexes can\u0026rsquo;t be upgraded. Any attempt to add, delete, or modify an index during the upgrade will cause the replication process to fail. During the upgrade, the source database can only receive FT.ADD and FT.DEL commands. Replicate data from RediSearch 1.x to RediSearch 2.x To replicate a RediSearch 1.x database to a RediSearch 2.x database:\nSign in to the admin console of the Redis Enterprise cluster where you want to host the new RediSearch 2.x database.\nAdd the RediSearch 2.x module to the cluster.\nCreate a new database with RediSearch 2.x.\nMigrate data to the RediSearch 2.x database.\nAdd RediSearch 2.x to the cluster Download the RediSearch 2.x module package from the Redis Download Center.\nIn the Redis Enterprise admin console, select settings.\nIn redis modules, select the Add module button.\nUse the file browser to select the module package and upload it to the cluster.\nCreate a RediSearch 2.x database Create a database and configure its settings.\nIn the Redis Modules section:\nSelect the Add button:\nSelect RediSearch 2 from the dropdown list.\nSelect the OK button to confirm:\nSelect the Activate button to create the database.\nMigrate data Download the RediSearch_Syncer package for your operating system:\nRedHat Enterprise Linux 7 Ubuntu 18.04 Ubuntu 16.04 Ubuntu 14.04 Extract the package:\nunzip \u0026lt;package_name\u0026gt; In the extracted directory, run the RediSearch_Syncer.py script:\npython3 RediSearch_Syncer.py -d \u0026lt;destination_url\u0026gt; -s \u0026lt;source_url\u0026gt; [--add-prefix \u0026lt;prefix\u0026gt;] Replace the following variables with your own values:\ndestination url - The replication URL of the RediSearch 2.x database. To find this value, go to the database configuration screen in the admin console and select Get Replica of source URL.\nsource url - The replication URL of the RediSearch 1.x database. To find this value, go to the database configuration screen in the admin console and select Get Replica of source URL.\n--add-prefix \u0026lt;prefix\u0026gt; (optional) - Adds a prefix to all of the hashes that are replicated to the new database.\nNote: Only use the add-prefix option if you want to index all of the hashes in the same index in the source database. The script shows a table with the progress of the replication process. Press F5 to see the updated status.\nFor example:\npython RediSearch_Syncer.py -d redis://admin:IBrS0xaL6rShfB1wKTtUkcQG1g3UWAlTd53AotPdTcQvGIVP@redis-19472.cluster1.local:19472 -s redis://admin:1GjFuUbBqTSPDbRfaxEPLSoXpFmRdFxmBKMD0BuxwMJ2DEaO@redis-19636.cluster1.local:19636 Stop the processes that are sending requests to the source database so all of the data gets synchronized to the destination database.\nRun FT.INFO on both source and destination databases and compare the number of indexed documents. The replication process is complete when the number of indexed documents is the same in both databases.\nWhen the status field is st_in_sync, you can press Ctrl-C to cancel the synchronization process.\nPress Q to quit the RediSearch_Syncer.py.\nYou can now redirect your database connections to the RediSearch 2.x database.\n","categories":["Modules"]},{"uri":"/kubernetes/architecture/operator/","uriRel":"/kubernetes/architecture/operator/","title":"Redis Enterprise for Kubernetes operator-based architecture","tags":[],"keywords":[],"description":"This section provides a description of the design of Redis Enterprise for Kubernetes.","content":"Redis Enterprise is the fastest, most efficient way to deploy and maintain a Redis Enterprise cluster in Kubernetes.\nWhat is an operator? An operator is a Kubernetes custom controller which extends the native K8s API.\nOperators were developed to handle sophisticated, stateful applications that the default K8s controllers aren’t able to handle. While stock Kubernetes controllers—for example, StatefulSets—are ideal for deploying, maintaining and scaling simple stateless applications, they are not equipped to handle access to stateful resources, upgrade, resize and backup of more elaborate, clustered applications such as databases.\nWhat does an operator do? In abstract terms, Operators encode human operational knowledge into software that can reliably manage an application in an extensible, modular way and do not hinder the basic primitives that comprise the K8s architecture.\nRedis created an operator that deploys and manages the lifecycle of a Redis Enterprise Cluster.\nThe Redis Enterprise operator acts as a custom controller for the custom resource RedisEnterpriseCluster, or ‘rec’, which is defined through K8s CRD (customer resource definition) and deployed with a yaml file.\nThe operator functions include:\nValidating the deployed Cluster spec (for example, requiring the deployment of an odd number of nodes) Implementing a reconciliation loop to monitor all the applicable resources Logging events Enabling a simple mechanism for editing the Cluster spec The Redis Enterprise operator functions as the logic “glue” between the K8s infrastructure and the Redis Enterprise Cluster.\nThe operator creates the following resources:\nService account Service account role Service account role binding Secret – holds the cluster username, password, and license Statefulset – holds Redis Enterprise nodes The Services Manager deployment – exposes databases and tags nodes The Redis UI service The service that runs the REST API + Sentinel Pod Disruption Budget Optionally: a deployment for the Service Broker, including services and a PVC The following diagram shows the high-level architecture of the Redis Enterprise operator:\n","categories":["Platforms"]},{"uri":"/rc/subscriptions/create-active-active-subscription/","uriRel":"/rc/subscriptions/create-active-active-subscription/","title":"Create an Active-Active subscription","tags":[],"keywords":[],"description":"Shows what changes when you create an Active-Active subscription (Flexible or Annual)","content":"To deploy Active-Active databases in Redis Enterprise Cloud, you need to create a Flexible or Annual subscription with Active-Active enabled.\nOverall, the process is similar to creating a traditional subscription. However, there are some differences; you need to:\nDefine the regions for each database instance Define unique CIDR addresses for each instance Define throughput (read and write operations) for each region Active-Active databases consist of multiple copies (also called instances) deployed to different regions throughout the world.\nThis reduces latency for local users and improves availability should a region fail.\nConsistency between each instance is maintained in the background; that is, each copy eventually includes updates from every region. As a result, memory size and throughput increase.\nDefine regions When you create a new Flexible subscription, the Active-Active Redis option appears to the right of the cloud providers.\nWhen you enable Active-Active Redis, the region control changes to a default showing two regions. Select the drop-down arrow to display a list of provider regions that support Active-Active databases.\nUse the checkboxes in the list to select or remove regions. The Search box lets you locate specific regions.\nYou can use a region\u0026rsquo;s Remove button to remove it from the list.\nDefine CIDR addresses To properly route network traffic between each Active-Active database instance and your consumer VPCs, use care to specify unique CIDR address blocks when using VPC peering. The block regions should not overlap between the Redis server and your app consumer VPCs.\nIn addition, CIDR blocks should not overlap between cluster instances. Every CIDR block should be unique.\nUse the VPC configuration section of the Advanced options to define unique address blocks for each region.\nWhen all Deployment CIDR regions display a green checkmark, you\u0026rsquo;re ready to continue.\nRed exclamation marks indicate error conditions; the tooltip provides additional details.\nDefine throughput Each Active-Active instance coordinates changes with every other instance, which increases memory use and throughput.\nWhen you create an Active-Active database, you define the throughput for each instance.\nRead and write operations are factored into the total throughput. Because each instance needs the ability to write to every other instance, write operations significantly affect the total, as shown in the following table:\nNumber of regions Read operations Write operations Total operations Two 1,000 each 1,000 each 6,000(2,000 reads; 4,000 writes) Two 1,500 each 1,000 each 7,000(3,000 reads; 4,000 writes) Two 1,000 each 1,500 each 8,000(2,000 reads; 6,000 writes) Three 1,000 each 1,000 each 12,000(3,000 reads; 9,000 writes) The total operations per second:\nCombines the total read ops/sec for each region Applies the write ops/sec for each region across every region. Throughput requirements grow dramatically as regions increase. As a result, consider your requirements carefully.\nMore info Create a Flexible subscription Database memory size Redis Enterprise Cloud subscription plans Redis Enterprise Cloud pricing ","categories":["RC"]},{"uri":"/kubernetes/reference/cluster-options/","uriRel":"/kubernetes/reference/cluster-options/","title":"REC custom resource options","tags":[],"keywords":[],"description":"A primer for the configuration options for Redis Enterprise cluster Custom Resource Definitions.","content":"A Redis Enterprise cluster (REC) is defined in a custom resource definition (CRD). The REC options are specified in the spec section of the custom resource. Any field not specified in the custom resource will be set to default values by the operator. Changes made to the custom resource will override changes made to the cluster via the admin console or rladmin commands.\nThe most common options you might specify are listed below. For a full list of options, see the Redis Enterprise Cluster API.\nname: rec This is the cluster name that the operator uses to name various resources in the Kubernetes cluster and also name the CRD.\nNote: There is a binding between the SCC and the service account. You can create this binding manually, but it is not recommended. nodes: nnn This must be an odd number that is 3 or higher.\nuiServiceType: service_type This controls how the Redis Enterprise UI is exposed on the cluster. The service_type must be either ClusterIP or LoadBalancer (default: ClusterIP). It is an optional configuration based on k8s service types.\npersistentSpec You can add a storageClassName that specifies the Storage Class used for your nodes’ persistent disks. For example, AWS uses “gp2” as a default, GKE uses “standard” and Azure uses \u0026ldquo;default\u0026rdquo;.\nAlso, adding a volumeSize lets you control the size of the persistent volume attached to the Redis Enterprise pods.\npersistentSpec: enabled: true volumeSize: \u0026#34;10Gi\u0026#34; storageClassName: \u0026#34;gp2\u0026#34; redisEnterpriseNodeResources The compute resources required for each node (see limits and requests). Kubernetes accepts only integers as sizing numbers for requests and limits.\nResource limits are recommended to equal requests, see guaranteed quality of service for more info.\nlimits The max resources (in integers) for a Redis node (similar to pod limits).\nFor example:\nlimits: cpu: \u0026#34;4000m\u0026#34; memory: 4Gi The default is 4 cores (4000m) and 4GB (4Gi).\nrequests The minimum resources (in integers) for a Redis node (similar to pod requests).\nFor example:\nrequests: cpu: \u0026#34;4000m\u0026#34; memory: 4Gi The default is 4 cores (4000m) and 4GB (4Gi).\nredisEnterpriseImageSpec This configuration controls the Redis Enterprise version used, and where it is fetched from. This is an optional field. The operator automatically uses the matching RHEL image version for the release.\nThe value is structured as follows with the policy values from OpenShift:\nimagePullPolicy: IfNotPresent Repository: redislabs/redis versionTag: 6.0.6-39 The version tag is as it appears on your repository, such as in DockerHub.\nredisUpgradePolicy Redis upgrade policy that controls the default Redis database version when creating or updating databases.\nThe supported values are:\nmajor: limits Redis database version to the most recent major release latest: sets default database version to the latest minor release More info:\nRedis upgrade policy Upgrade policy values Sample REC custom resource apiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec namespace: my-project spec: nodes: 3 persistentSpec: enabled: true uiServiceType: ClusterIP username: demo@redislabs.com redisEnterpriseNodeResources: limits: cpu: 4000m memory: 4Gi requests: cpu: 4000m memory: 4Gi redisEnterpriseImageSpec: imagePullPolicy: IfNotPresent repository: redislabs/redis versionTag: 6.0.6-39.rhel7-openshift This example may be useful to get started with a test or development deployment. You can modify it for your required deployment use case. This file and other references are available in the RedisLabs/redis-enterprise-k8s-docs GitHub repository.\n","categories":["Platforms"]},{"uri":"/rc/databases/back-up-data/","uriRel":"/rc/databases/back-up-data/","title":"Back up a database","tags":[],"keywords":[],"description":"","content":"The backup options for Redis Enterprise Cloud databases depend on your plan:\nFlexible and Annual subscriptions can perform backups on-demand and schedule daily backups that occur during a set hour.\nPaid Fixed plans can perform backups on-demand and schedule backups that occur every 24 hours.\nFree plans cannot perform backups through the Redis Cloud console.\nBackups are saved to predefined storage locations available to your subscription.\nBackup locations need to be available before you turn on database backups. To learn more, see Set up backup storage locations\nHere, you\u0026rsquo;ll learn how to store backups using different cloud providers.\nTurn on backups To turn on database backups:\nSign in to the Redis Cloud admin portal. (Create an account if you don\u0026rsquo;t already have one.)\nIf you have more than one subscription, select the target subscription from the list. This displays the Databases tab for the selected subscription.\nSelect the database to open the Database page and then select Edit database.\nIn the Durability section of the Configuration tab, locate the Remote backup setting:\nWhen you enable Remote backup, additional options appear. The options vary according to your subscription.\nSetting name Description Interval Defines the frequency of automatic backups. Paid fixed accounts are backed up every 24 hours. Flexible and Annual subscriptions can be set to 24, 12, 6, 4, 2, or 1 hour backup intervals. Set backup time When checked, this lets you set the hour of the Backup time. (Flexible and Annual subscriptions only) Backup time Defines the hour automatic backups are made. Note that actual backup times will vary up in order to minimize customer access disruptions. (Flexible and Annual subscriptions only) Times are expressed in Coordinated Universal Time (UTC). Storage type Defines the provider of the storage location, which can be: AWS S3, Google Cloud Storage, Azure Blob Storage, or FTP (FTPS). Backup destination Defines a URI representing the backup storage location. Back up data on demand After backups are turned on, you can back up your data at any time. Use the Backup now button in the Durability section.\nYou can only use the Backup now button after you turn on backups.\nSet up backup storage locations Database backups can be stored to a cloud provider service or saved to a URI using FTP/FTPS.\nWhen stored to a cloud provider, backup locations need to be available on the same provider in the same region as your subscription.\nYour subscription needs the ability to view permissions and update objects in the storage location. Specific details vary according to the provider. To learn more, consult the provider\u0026rsquo;s documentation.\nThe following sections help set things up; however, provider features change frequently. For best results, use your provider\u0026rsquo;s documentation for the latest info.\nAWS S3 To store backups in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket:\nIn the AWS Management Console, use the Services menu to locate and select Storage \u0026gt; S3. This takes you to the Amazon S3 admin panel.\nUse the Buckets list to locate and select your bucket. When the settings appear, select the Permissions tab, locate the Access control list (ACL) section, and then select the Edit button.\nWhen the Edit access control list (ACL) screen appears, configure the bucket\u0026rsquo;s Access Control List to give access to Redis Enterprise Cloud.\nSelect Add grantee. In the Grantee field, enter: fd1b05415aa5ea3a310265ddb13b156c7c76260dbc87e037a8fc290c3c86b614 In the Objects list, select Write. In the Bucket ACL list, select Read and Write. Select Save. Once the bucket is available and the permissions are set, use the name of your bucket as the Backup destination for your database\u0026rsquo;s Remote backup settings. For example, suppose your bucket is named backups-bucket. In that case, set Backup destination to s3://backups-bucket.\nTo learn more, see Configuring ACLs for buckets on the AWS docs.\nGCP Storage For Google Cloud Platform (GCP) console subscriptions, store your backups in a Google Cloud Storage bucket:\nSign in to Google Cloud Platform console.\nIn the admin console menu, locate the Storage section than select Cloud Storage \u0026gt; Browser.\nCreate or select a bucket.\nSelect the overflow menu (three dots, stacked) and then select the Edit Bucket Permissions command.\nSelect the Add members button and then add:\nservice@redislabs-prod-clusters.iam.gserviceaccount.com\nSet Role to Storage Legacy | Storage Legacy Bucket Writer.\nSave your changes.\nVerify that your bucket does not have a set retention policy.\nTo do so:\nView the details of your bucket.\nSelect the Retention tab.\nVerify that there is no retention policy.\nIf a policy is defined and you cannot delete it, you need to use a different bucket.\nUse the bucket details Configuration tab to locate the gsutil URI. This is the value you\u0026rsquo;ll assign to your resource\u0026rsquo;s backup path.\nTo learn more, see Use IAM permissions.\nAzure Blob Storage To store your backup in Microsoft Azure Blob Storage, sign in to the Azure portal and then:\nCreate an Azure Storage account if you do not already have one.\nCreate a container if you do not already have one.\nManage storage account access keys\nSet your resource\u0026rsquo;s Backup Path to the path of your storage account.\nThe syntax for creating the backup varies according to your authorization mechanism. For example:\nabs://:storage_account_access_key@storage_account_name/container_name/[path/]\nWhere:\nstorage_account_access_key: the primary access key to the storage account storage_account_name: the storage account name container_name: the name of the container, if needed. path: the backups path, if needed. To learn more, see Authorizing access to data in Azure Storage.\nFTP Server To store your backups on an FTP server, set its Backup Path using the following syntax:\n\u0026lt;protocol\u0026gt;://[username]:[password]@[hostname]:[port]/[path]/\nWhere:\nprotocol: the server\u0026rsquo;s protocol, can be either ftp or ftps. username: your username, if needed. password: your password, if needed. hostname: the hostname or IP address of the server. port: the port number of the server, if needed. path: the backup path, if needed. The user account needs permission to write files to the server.\n","categories":["RC"]},{"uri":"/rc/databases/configuration/","uriRel":"/rc/databases/configuration/","title":"Configure databases","tags":[],"keywords":[],"description":"Provides background information to help you configure Redis Enterprise Cloud databases to best fit your needs.","content":"Here, you\u0026rsquo;ll find background info and conceptual details to help you customize your Redis Enterprise Cloud databases to meet your needs.\nClustering Redis Databases Redis Enterprise Cloud uses clustering to manage very large databases (25 GB and larger). Here, you\u0026#39;ll learn how to manage clustering and how to use hashing policies to control how data is managed.\nData eviction policies Data eviction policies control what happens when new data exceeds the memory limits of a database. Here, you\u0026#39;ll learn the available policies and how to change which one is used for a database.\nData persistence Data persistence enables recovery in the event of memory loss or other catastrophic failure. Here, you learn data persistence options, when they\u0026#39;re available, and how to apply specific settings to individual databases.\nHigh availability and replication Describes database replication and high availability as it affects Redis Enterprise Cloud.\n","categories":["RC"]},{"uri":"/rc/databases/","uriRel":"/rc/databases/","title":"Manage databases","tags":[],"keywords":[],"description":"","content":"Databases are the heart of any Redis Enterprise Cloud subscription.\nHere\u0026rsquo;s how to perform a variety of tasks:\nCommon database tasks Create a database\nView or edit a database\nDelete a database\nIf you\u0026rsquo;re new to Redis Enterprise Cloud, consider the Quick Start\nAdditional tasks Monitor database performance\nImport data into databases\nBack up databases\nSecure database access\nFlush database data\nConfiguration details These topics provide background details that can help you tailor your databases to better fit your needs.\nClustering Redis databases: Redis Enterprise Cloud uses clustering to manage very large databases (25 GB and larger). Learn how to manage clustering and how to use hashing policies to manage the process.\nData eviction policies: Data eviction policies control what happens when new data exceeds the memory limits of a database. Learn the available policies and how to control them.\nData persistence: Data persistence enables recovery in the event of memory loss or other catastrophic failure. Learn which options are available and how to apply specific settings to individual databases.\nHigh availability and replication: Replication allows for automatic failover and greater fault tolerance. It can prevent data loss in the event of a hardware or zone failure. Learn which options are available for Redis Enterprise Cloud subscriptions.\nCompatibility Although Redis Cloud follows open source Redis specifications, it does not support certain commands. Instead of using these commands, Redis Cloud automatically handles features like replication and lets you manage your database from the admin console or REST API.\nFor more details, see:\nRedis Enterprise compatibility with open source Redis\nCommand compatibility\nConfiguration compatibility\n","categories":["RC"]},{"uri":"/rs/databases/active-active/delete/","uriRel":"/rs/databases/active-active/delete/","title":"Delete Active-Active databases","tags":[],"keywords":[],"description":"Considerations while deleting Active-Active databases.","content":"When you delete an Active-Active database (formerly known as CRDB), all instances of the Active-Active database are deleted from all participating clusters.\nWarning - This action is immediate, non-reversible, and has no rollback. Because Active-Active databases are made up of instances on multiple participating clusters, to restore a deleted Active-Active database you must create the database again with all of its instances and then restore the data to the database from backup.\nWe recommended that you:\nBack up your data and test the restore on another database before you delete an Active-Active database. Consider flushing the data from the database so that you can keep the Active-Active database configuration and restore the data to it if necessary. ","categories":["RS"]},{"uri":"/rc/databases/flush-data/","uriRel":"/rc/databases/flush-data/","title":"Flush data","tags":[],"keywords":[],"description":"","content":"The FLUSHALL command provides a fast way to remove all data from a database.\nTo use it, connect your database and then issue the command.\nThere are several ways to do this, depending on your circumstances and environment.\nNote: When you flush a database, you remove all data. This is a prerequisite to deleting a database.\nThis permanently removes all data from the database. The data cannot be recovered, except by restoring from earlier backups.\nWe strongly recommend backing up databases before flushing them.\nThe Redis-CLI utility To use the redis-cli utility:\nredis-cli -h \u0026lt;hostname\u0026gt; -p \u0026lt;portnumber\u0026gt; -a \u0026lt;password\u0026gt; flushall Example:\nredis-cli -h redis-12345.server.cloud.redislabs.example.com -p 12345 -a xyz flushall The netcat utility If you have shell access to the server hosting your database, you can use the netcat (nc) to send the flush_all command to your database:\necho \u0026#34;flush_all\u0026#34; | nc redis-12345.server.cloud.redislabs.example.com 12345 RedisInsight CLI If you\u0026rsquo;ve installed RedisInsight and added your database, you can use the RedisInsight command-line interface (CLI) to issue commands:\nStart RedisInsight and connect to your database.\nFrom the RedisInsight menu, select CLI and wait for the client to connect to your database.\nIn the command area, enter flushall and then press Enter.\nThe \u0026lsquo;OK\u0026rsquo; response indicates that the command executed properly.\nSASL connection If you do not have permission to access the command shell of the server hosting your database or are unable to use RedisInsight, but you have connection credentials and your database supports Simple Authentication and Security Layer connections, you can use an SASL-enabled command-line client.\nFor example, suppose you\u0026rsquo;re using Memcached Enterprise Cloud and that your database has SASL enabled. In this case, you can can use the bmemcached-CLI client to connect and issue commands to your database.\nSetup instructions vary according to the environment. Many Linux systems, such as Ubuntu, follow this process:\n$ wget https://github.com/RedisLabs/bmemcached-cli/archive/master.zip $ sudo apt-get install unzip python-pip $ unzip master.zip -d bmemcached-cli $ cd bmemcached-cli/bmemcached-cli-master/ $ sudo pip install --upgrade pip $ sudo pip install . -r requirements.pip Adjust as needed for your operating system and configuration.\nWhen the client is properly installed, you can use it to run the flush_all command:\nbmemcached-cli [user]:[password]@[host]:[port] Here\u0026rsquo;s an example:\n$ bmemcached-cli username:password@redis-12345.server.cloud.redislabs.example.com:12345 ([B]memcached) flush_all True exit ","categories":["RC"]},{"uri":"/rc/databases/import-data/","uriRel":"/rc/databases/import-data/","title":"Import data into a database","tags":[],"keywords":[],"description":"","content":"You can import an existing dataset into your Redis Cloud instance from an existing Redis server or an RDB file.\nNote: Expired keys are not imported. As a result, the number of keys in the source and destination databases can be different after the import is complete. Import from a Redis server To import a dataset from any publicly available Redis server:\nSelect Databases from the admin console menu and then select the target database from the database list. In the Danger Zone, select Import. Enter the source database details: Source type - Select Redis. Redis Hostname/IP Address - Enter the hostname or the public IP address of the source Redis server. Redis port - Enter the port of the source Redis server if it is not the default value of 6379. Password - Enter the password, if required by the Redis database. Select Import. Import from an RDB file If you have an RDB or a compressed RDB file from a Redis database, you can import data from that file into your Redis Enterprise Cloud database.\nNote: In order to import a sharded database that has multiple RDB files, you must first merge the files into a single RDB. For assistance, contact Support. Via FTP or HTTP To import an RDB file stored on an FTP or HTTP server:\nSelect Databases from the admin console menu and then select your database from the list.\nIn the Danger Zone, select Import.\nEnter the details for the RDB file:\nSource type - Select FTP or HTTP. Source path - Enter the URL for the RDB file: \u0026lt;protocol\u0026gt;://[username][:password]@[:port]/[path/]filename.rdb[.gz] Where:\nprotocol - Server protocol: ftp, ftps, http, https username - Your username, if necessary password - Your password, if necessary hostname - Hostname or IP address of the server port - Port number of the server, if not 6379 path - Path to the file, if necessary filename - Filename of the RDB file, including the .gz suffix if the file is compressed Select Import.\nVia AWS S3 To use the Redis Cloud admin console to import your data, you must first share the file from the Amazon Web Services (AWS) management console.\nTo share and import an RDB file that is stored in an AWS Simple Storage Service (S3) bucket:\nIn the AWS management console, configure the file’s Access Control List to give read-only access to Redis Enterprise Cloud:\nGo to the bucket in the AWS S3 console. In the location where the file is stored, select the RDB file. Select Permissions. Select Edit. Select Add grantee. In the Grantee field, enter: fd1b05415aa5ea3a310265ddb13b156c7c76260dbc87e037a8fc290c3c86b614 In the Read column, select Yes. Select Save. For more info, see Configuring ACLs for objects.\nIn the Redis Cloud admin console, select the target database from the database list.\nIn the Danger Zone, select Import.\nEnter the details for the RDB file:\nSource type - Select AWS S3.\nSource path - Enter the URL for the RDB file: s3://bucketname/[path/]filename.rdb[.gz]\nWhere:\nbucketname - Name of the S3 bucket path - Path to the file, if necessary filename - Filename of the RDB file, including the .gz suffix if the file is compressed Select Import.\nVia GCP Storage To use the Redis Cloud admin console to import your data, you must first share the file from the Google Cloud Platform (GCP) console.\nTo share and import an RDB file that is stored in a GCP Storage bucket:\nIn the GCP Storage bucket, edit the file\u0026rsquo;s Access Control List to give read access to Redis Enterprise Cloud:\nSelect Edit access in the RDB file menu. Select Add item. Enter the user details and access: In the Entity field of the new item, select User. In the Name field of the new item, enter: service@redislabs-prod-clusters.iam.gserviceaccount.com In the Access field of the new item, select Reader. Select Save. For more info, see Set ACLs.\nIn the Redis Cloud admin console, select the target database from the database list.\nIn the Danger Zone, select Import.\nEnter the details for the RDB file:\nSource type - Select Google Cloud Storage.\nSource path - Enter the URL for the RDB file: gs://bucketname/[path/]filename.rdb[.gz]\nWhere:\nbucketname - Name of the GCS bucket path - Path to the file filename - Filename of the RDB file, including the .gz suffix if the file is compressed Select Import.\nVia Azure Blob Storage container To import an RDB file stored in a Microsoft Azure Blog storage container:\nIn the Redis Cloud admin console, select the target database from the database list.\nIn the Danger Zone, select Import.\nEnter the details for the RDB file:\nSource type - Select Azure Blob Storage.\nSource path - Enter the URL for the RDB file: abs://:storage_account_access_key@storage_account_name/[container/]filename.rdb[.gz]\nWhere:\nstorage_account_access_key - Primary access key to the storage account storage_account_name - Name of the storage account url - URL of the storage account container - Name of the container, if necessary filename - Filename of the RDB file, including the .gz suffix if the file is compressed Select Import.\n","categories":["RC"]},{"uri":"/rs/installing-upgrading/","uriRel":"/rs/installing-upgrading/","title":"Install and setup","tags":[],"keywords":[],"description":"","content":"This guide shows how to install Redis Enterprise Software, which includes several steps:\nPlan your deployment Download the installation package Prepare to install Perform the install Depending on your needs, you may also want to customize the installation.\nHere, you\u0026rsquo;ll learn how to perform each step.\nPlan your deployment Before installing Redis Enterprise Software, you need to:\nSet up your hardware.\nChoose your deployment platform.\nRedis Enterprise Software supports a variety of platforms, including:\nMultiple Linux distributions (Ubuntu, RedHat Enterprise Linux (RHEL)/IBM CentOS, Oracle Linux) Amazon AWS AMI Docker container (for development and testing only) Kubernetes For complete details, see Supported platforms\nOpen appropriate network ports in the firewall to allow connections to the nodes.\nConfigure cluster DNS so that cluster nodes can reach each other by DNS names.\nBy default, the install process requires an Internet connection to install dependencies and to synchronize the operating system clock. To learn more, see Offline installation.\nDownload the installation package To download the installation package for any of the supported platforms:\nGo to the Redis download page. Sign in with your Redis credentials or create a new account. In the Downloads section for Redis Enterprise Software, select the installation package for your platform then select Go. Note: Before you install the Linux package or AWS AMI on an AWS EC2 instance, review the configuration requirements for AWS EC2 instances. Prepare to install on Linux Before installing, review these notes:\nReview the security considerations for your deployment.\nIf you want to use Redis on Flash (RoF) for your databases, review the prerequisites, storage requirements, and other considerations for RoF databases and prepare and format the flash memory.\nUse the prepare_flash script to prepare and format flash memory:\nsudo /opt/redislabs/sbin/prepare_flash.sh This script finds unformatted disks and mounts them as RAID partitions in /var/opt/redislabs/flash.\nTo verify the disk configuration, run:\nsudo lsblk Disable Linux swap on all cluster nodes.\nMake sure that you have root-level access to each node, either directly or with sudo.\nIf you require the redislabs UID (user ID) and GID (group ID) numbers to be the same on all the nodes, create the redislabs user and group with the required numbers on each node.\nWhen port 53 is in use, the installation fails. This is known to happen in default Ubuntu 18.04 installations in which systemd-resolved (DNS server) is running.\nTo work around this issue, change the system configuration to make this port available before installation. Here\u0026rsquo;s one way to do so:\nRun: sudo vi /etc/systemd/resolved.conf Add DNSStubListener=no as the last line in the file and save the file. Run: sudo mv /etc/resolv.conf /etc/resolv.conf.orig Run: sudo ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf Run: sudo service systemd-resolved restart Make sure that the ports Redis assigns to databases are available; that is, they\u0026rsquo;re not being used by the operating system or other processes.\nTo avoid port collision, we recommend updating /etc/sysctl.conf to include:\nnet.ipv4.ip_local_port_range = 30000 65535 Install on Linux After you download the .tar file installation package, install the package on one of the nodes in the cluster.\nTo install from the CLI:\nCopy the installation package to the node.\n(Optional) Use the GPG key file to confirm authenticity of Ubuntu/Debian or RHEL RPM packages:\nFor Ubuntu:\nImport the key with: gpg --import \u0026lt;path to GPG key\u0026gt; Verify the package signature with: dpkg-sig --verify \u0026lt;path to installation package\u0026gt; For RHEL:\nImport the key with: rpm --import \u0026lt;path to GPG key\u0026gt; Verify the package signature with: rpm --checksig \u0026lt;path to installation package\u0026gt; On the node, change to the directory where the installation package is and extract the installation files:\ntar vxf \u0026lt;tarfile name\u0026gt; To install, run:\nsudo ./install.sh Note: The Redis Enterprise Software files are installed in the default file locations. By default, Redis Enterprise Software runs on the OS as the redislabs user and redislabs group. If needed, you can specify a different user and group during the installation. You must either be logged in as the root user or use sudo to run the install process. Answer the installation questions when shown to complete the installation process, including the rlcheck installation verification.\nNote: To install without answering the installation questions, either:\nRun ./install.sh -y to answer yes to all of the questions. Use an answer file to answer installation questions automatically. When the installation completes successfully, the IP address of the admin console is displayed:\nSummary: ------- ALL TESTS PASSED. 2017-04-24 10:54:15 [!] Please logout and login again to make sure all environment changes are applied. 2017-04-24 10:54:15 [!] Point your browser at the following URL to continue: 2017-04-24 10:54:15 [!] https://\u0026lt;your_ip_here\u0026gt;:8443 Redis Enterprise Software is now installed on the node.\nRepeat this process for each node in the cluster.\nCreate or join an existing Redis Enterprise Software cluster.\nCreate a database.\nFor geo-distributed Active-Active replication, create an Active-Active database.\nPermissions and access Redis Enterprise Software installation creates the redislabs:redislabs user and group.\nAssigning other users to the redislabs group is optional. Users belonging to the redislabs group have permission to read and execute (e.g. use the rladmin status command) but are not allowed to write (or delete) files or directories.\nRedis Enterprise Software is certified to run with permissions set to 750, an industry standard.\nWe recommend against reducing permissions to 700; this configuration has not been tested and is not supported.\nInstall command line-options Run ./install.sh --help to view command-line options supported by the install script.\nAt this time, the following options are supported:\nOption Description -y Automatically answers yes to all install prompts, accepting all default valuesSee Manage install questions -c \u0026lt;answer file\u0026gt; Specify answer file used to respond to install promptsSee Manage install questions -s \u0026lt;socket dir\u0026gt; Specify directory for redislabs unix sockets (new installs only) --install-dir \u0026lt;dir\u0026gt; Specifies installation directory (new installs only) See Customize install locations --config-dir \u0026lt;dir\u0026gt; Configuration file directory (new installs only) See Customize install locations --var-dir \u0026lt;dir\u0026gt; Var dir used for installation (new installs only) See Customize install locations --os-user \u0026lt;user\u0026gt; Operating system user account associated with install; default: redislabsSee Customize user and group (new installs only) --os-group \u0026lt;group\u0026gt; Operating system group associated with install; default: redislabsSee Customize user and group (new installs only) The next section provides additional configuration details.\nMore info and options If you\u0026rsquo;ve already installed Redis Enterprise Software, you can also:\nUpgrade an existing deployment\nUninstall an existing deployment\nMore info is available to help with customization and related questions:\nAWS EC2 configuration CentOS/RHEL Firewall configuration Change socket file location Cluster DNS configuration Cluster load balancer setup File locations Supported platforms Linux swap space configuration Manage installation questions mDNS client prerequisites Offline installation User and group ownership Next steps Now that your cluster is set up with nodes, you can:\nAdd users to the cluster with specific permissions. To begin, start with Access control. Create databases to use with your applications. ","categories":["RS"]},{"uri":"/rs/security/access-control/ldap/map-ldap-groups-to-roles/","uriRel":"/rs/security/access-control/ldap/map-ldap-groups-to-roles/","title":"Map LDAP groups to roles","tags":[],"keywords":[],"description":"Describes how to map LDAP authorization groups to Redis Enterprise roles using the admin console.","content":"Redis Enterprise Software uses a role-based mechanism to enable LDAP authentication and authorization.\nOnce LDAP is enabled, you need to map LDAP groups to Redis Enterprise access control roles.\nMap LDAP groups to roles To map LDAP groups to access control roles:\nFrom the admin console menu, select Access control \u0026gt; LDAP mappings.\nIf you see an \u0026ldquo;LDAP configuration is disabled\u0026rdquo; message, go to Settings \u0026gt; LDAP to enable role-based LDAP.\nYou can map LDAP roles when LDAP configuration is not enabled, but they won\u0026rsquo;t have any effect until you configure and enable LDAP.\nSelect the Add button to create a new mapping and then enter the following details:\nSetting Description Name A descriptive, unique name for the mapping Distinguished Name The distinguished name of the LDAP group to be mapped. Example: cn=admins,ou=groups,dc=example,dc=com Role The Redis Software access control role defined for this group Notified email (Optional) An address to receive alerts Email alerts Selections identifying the desired alerts. Select Edit to change. When finished, select the Save button.\nCreate a mapping for each LDAP group used to authenticate and/or authorize access to Redis Enterprise Software resources.\nThe scope of the authorization depends on the access control role:\nIf the role authorizes admin management, LDAP users are authorized as admin console administrators.\nIf the role authorizes database access, LDAP users are authorized to use the database to the limits specified in the role.\nTo authorize LDAP users to specific databases, update the database access control lists (ACLs) to include the mapped LDAP role.\nMore info Enable and configure role-based LDAP Update database ACLs to authorize LDAP access Learn more about Redis Enterprise Software security and practices ","categories":["RS"]},{"uri":"/rc/databases/migrate-databases/","uriRel":"/rc/databases/migrate-databases/","title":"Migrate data to new subscription","tags":[],"keywords":[],"description":"Shows two ways to migrate data to a database in a new subscription.","content":"There are times when you need to migrate data from one database to another.\nHere are two common ways to do this.\nEach approach is suitable for different situations and the steps can vary according to your needs.\nTransfer via import The most common way to transfer data to a new database is to import a copy of the data into it.\nHere\u0026rsquo;s how it works:\nSelect an export storage destination and verify that it\u0026rsquo;s ready for use and has sufficient space.\nExport the data from the original database to the storage location.\nImport the exported data into the target database, the one hosted by the new subscription.\nThe migrated data reflects the state of the data at the time it was originally exported.\nIf you have apps or other connections actively using the source database, consider scheduling downtime for the migration to avoid loss.\nThis approach also lets you transfer data between databases hosted by different services.\nSync using Active-Passive If your target database is hosted on a Flexible (or Annual) subscription, you can use Active-Passive to sync (synchronize) the source database to the target database. (The source database can be hosted on a Fixed, Flexible, or Annual subscription.)\nThe source database remains active while the data migrates.\nTo do this, specify the target database as an Active-Passive replica of the the source database. The general process is:\nGet the public endpoint of the source database Enable the target database as an Active-Passive replica for the source Wait for the data to sync Switch apps and other connections to the target database Disable Active-Passive for the target database You need the public endpoint details for the source database. These are available from the database list and the General section of the Configuration tab for the source database.\nHere\u0026rsquo;s how this works for databases hosted on the same account:\nSelect Databases from the admin menu and locate the source database in the list.\nSelect the Copy button in the Endpoint column for the source database, which copies the endpoint details to the Clipboard.\n(You can also use the Copy button next to the Public endpoint details in the General section of the Configuration tab for the source database.)\nUse the database list drop-down to select the target database.\nFrom the Configuration tab of the target database, select the Edit database button.\nIn the Durability section, enable Active-Passive Redis and then select the Add URI button.\nIn the text box, type redis:// and then paste in the public endpoint details.\nUse the Save button to make sure you\u0026rsquo;ve specified the source URI correctly.\nIf the endpoint cannot be verified, make sure that you\u0026rsquo;ve copied the details directly from the source database and that the value you entered starts with redis://.\nSelect the Save Database button to begin updating the database.\nInitially, the database status is Pending, which means the update task is still running.\nThe sync process doesn\u0026rsquo;t begin until the database becomes Active.\nWhen data has fully migrated to the target database, database status reports Synced.\nActive-Passive sync lets you migrate data while apps and other connections are using the source database. Once the data is migrated, you should migrate active connections to the target database.\nActive-Passive memory requirements Active-Passive sync requires more memory than data import. On average, you need an extra 25% memory on top of other requirements, though specific requirements depend on the data types and other factors.\nTo illustrate, suppose you want to migrate a 1 GB source database without replication to a target database with replication enabled. Here, the target database memory limit should be at least 2½ GB to avoid data loss.\nOnce the databases are synced, you can disable Active-Passive for the target database. Before doing so, however, verify that apps and other connections have switched to the target database; otherwise, you may lose data.\n","categories":["RC"]},{"uri":"/rc/databases/monitor-performance/","uriRel":"/rc/databases/monitor-performance/","title":"Monitor database performance","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Cloud provides a variety of metrics to help you monitor database performance. You can view graphs of performance data at any time and receive emails when performance crosses defined threshholds.\nView database metrics The Metrics tab of the View Database screen provides a series of graphs showing performance data for your database.\nPerformance data provides insight into how your database is being used and how well it is performing.\nThe interval scrollbar controls the time period displayed in the graphs.\nPromote metric graphs The Metrics screen shows two primary graphs and a collection of smaller ones. You can promote any smaller graph to a primary position.\nWhen you use the mouse to point to a smaller graph, three things appear:\nA promotion icon pointing left A promotion icon pointing right A summary panel showing the minimum, average, maximum, and most recent values displayed in the graph. Use the promotion icons to promote the smaller graph to one of the primary positions. The left icon promotes the smaller graph to the left position and the right icon promotes the smaller graph to the right position.\nMetric definitions Several metric graphs are available:\nMetric Description Ops/sec The number of overall operations per sec for all Redis commands Reads/sec The number of read operations per second Writes/sec The number of write operations per second Other cmds/sec The number of other Redis commands per second Latency (in milliseconds) Latency per write operation Reads Latency (in milliseconds) The average, min, max, and last values are also shown Writes Latency (in milliseconds) Latency per write operation Other Latency (in milliseconds) Latency per other commands Used Memory The amount of memory used by the database Total Keys The total number of keys in the database Connections The total number of connections to the endpoint Evicted Objects/sec Number of objects evicted from the database per second Expired Objects/sec Number of expired objects per sec. An expired object is an object with expired TTL that was deleted from the database. Hit Ratio (percentage) The number of operations on existing keys divided by total database operations. For more detailed analysis, consider tools similar to RedisInsight.\nConfigure metric alerts Depending on your subscription plan, you can enable alerts for several metrics for a given database.\nTo do so, go to the Configuration tab of the database and then locate the Alerts section.\nFor details, including a breakdown of alerts available for each subscription type, see Alerts section.\nAlert settings are specific to a given database. To receive alerts for multiple databases, make sure each is configured accordingly.\nChange alert recipients Any member of the account team can receive alert emails.\nTo update alert settings for one or more team members, select Access Management from the admin console menu and then select the Team tab.\nFor details, see Access management.\nIf you subscribe to Redis Enterprise Cloud through a Platform-as-a-Service (PaaS) provider (such as Heroku), you will need to review your provider\u0026rsquo;s documentation for help managing your team.\n","categories":["RC"]},{"uri":"/rs/databases/recover/","uriRel":"/rs/databases/recover/","title":"Recover a failed database","tags":[],"keywords":[],"description":"Recover a database after the cluster fails or the database is corrupted.","content":"When a cluster fails or a database is corrupted, you must:\nRestore the cluster configuration from the CCS files Recover the databases with their previous configuration and data To restore the data that was in the databases to databases in the new cluster you must restore the database persistence files (backup, AOF, or snapshot files) to the databases. These files are stored in the persistence storage location.\nThe database recovery process includes:\nIf the cluster failed, recover the cluster. Identify recoverable databases. Restore the database data. Verify that the databases are active. Prerequisites Before you start database recovery, make sure that the cluster that hosts the database is healthy. In the case of a cluster failure, you must recover the cluster before you recover the databases.\nWe recommend that you allocate new persistent storage drives for the new cluster nodes. If you use the original storage drives, make sure that you backup all files on the old persistent storage drives to another location.\nRecovering the databases After you prepare the cluster that hosts the database, you can run the recovery process from the rladmin command-line interface (CLI).\nTo recover the database:\nMount the persistent storage drives with the recovery files to the new nodes. These drives must contain the cluster configuration backup files and database persistence files.\nNote: Make sure that the user redislabs has permissions to access the storage location of the configuration and persistence files on each of the nodes. If you use local persistent storage, place all of the recovery files on each of the cluster nodes.\nTo see which databases are recoverable, run:\nrladmin recover list The status for each database can be either ready for recovery or missing files. An indication of missing files in any of the databases can result from:\nThe storage location is not found - Make sure that on all of the nodes in the cluster the recovery path is set correctly. Files are not found in the storage location - Move the files to the storage location. No permission to read the files - Change the file permissions so that redislabs:redislabs has 640 permissions. Files are corrupted - Locate copies of the files that are not corrupted. If you cannot resolve the issues, contact Redis support.\nRecover the database, either:\nRecover the databases all at once from the persistence files located in the persistent storage drives: rladmin recover all Recover a single database from the persistence files located in the persistent storage drives: rladmin recover db \u0026lt;database_id|name\u0026gt; Recover only the database configuration for a single database (without the data): recover db only_configuration \u0026lt;db_name\u0026gt; Note: If persistence was not configure for the database, the database is restored empty. For Active-Active databases that still have live instances, we recommend that you recover the configuration for the failed instances and let the data update from the other instances. For Active-Active databases that all instances need to be recovered, we recommend that you recover one instance with the data and only recover the configuration for the other instances. The empty instances then update from the recovered data. If the persistence files of the databases from the old cluster are not stored in the persistent storage location of the new node, you must first map the recovery path of each node to the location of the old persistence files. To do this, run the node \u0026lt;id\u0026gt; recovery_path set command in rladmin. The persistence files for each database are located in the persistent storage path of the nodes from the old cluster, usually under /var/opt/redislabs/persist/redis. To verify that the recovered databases are now active, run: rladmin status\nAfter the databases are recovered, make sure that your redis clients can successfully connect to the databases.\n","categories":["RS"]},{"uri":"/kubernetes/reference/db-options/","uriRel":"/kubernetes/reference/db-options/","title":"REDB custom resource options","tags":[],"keywords":[],"description":"A primer for the configuration options for Redis Enterprise database custom resource definitions.","content":"The database options are specified in the spec section of the database custom resource. These options include options that you can change and options that are created by the controller for applications or developers. Changes made to the REDB custom resource will override changes made to the database via the admin console or rladmin commands.\nThe most common options are listed below. For a complete list of options, see Redis Enterprise Database API.\ndatabasePort Manually sets the TCP port on which the database is available. If the port number is not specified, it will be automatically generated.\nWarning - databasePort cannot be added, removed, or changed after database creation. If the admission controller is installed and configured, it will run validity checks before the REDB creation. If the port is not available, you\u0026rsquo;ll get an error for \u0026ldquo;port is unavailable.\u0026rdquo; If the port is not valid, you\u0026rsquo;ll get an error for \u0026ldquo;change databasePort is not allowed.\u0026rdquo;\ndatabaseSecretName A string containing the name of a secret that contains the desired database password.\nIf you specify a secret name, you must create an opaque secret before you create the database resource. The operator takes the password from the password key in the secret.\nTo disable authentication for the database, use an empty string as a value for the password key.\nIf you do not specify a secret name, the operator will create a secret for you with the name constructed from the database name (metadata.name) with \u0026ldquo;redb-\u0026rdquo; as a prefix.\nWhen the database is created, the secret is updated to include the port and service name for the database, but the password does not change. If you did not create the secret, it is also updated with the generated database password.\nevictionPolicy An eviction policy (default: volatile-lru)\nmemorySize The amount of memory to allocate that is at least 100MB. Values are an integer suffixed with a unit. For example, values like 1GB, 250MB, etc.\npersistence The value for the database persistence setting.\nThe value is a keyword with the values:\nValue Description disabled Data is not persisted (default) aofEverySecond Data is synced to disk every second aofAlways Data is synced to disk with every write. snapshotEvery1Hour A snapshot of the database is created every hour snapshotEvery6Hour A snapshot of the database is created every 6 hours. snapshotEvery12Hour A snapshot of the database is created every 12 hours. rackAware A boolean that indicates whether the database is rack-zone aware (default: the cluster setting)\nredisEnterpriseCluster The name of the cluster to create the database on.\nThe value has a single name property. For example, to refer to the rec cluster:\nredisEnterpriseCluster: name: rec redisVersion The upgrade policy specific to an REDB: major or latest.\nSpecifying this field during REDB creation will determine if the database version is the latest available, or the latest major version available.\nIf it is not specified, the database will follow the upgrade policy set on the cluster. If the cluster version is set to major, only major will be accepted on REDBs. If the cluster is set to latest, both latest and major are valid for the REDB.\nreplication A boolean that indicates whether in-memory database replication is enabled (default: false).\nWhen enabled, the database has a replica shard for every master.\nshardCount The number of database shards (default: 1).\ntlsMode Controls SSL authentication and encryption for connections to the database.\nValue Description disabled no incoming connection to the Database uses SSL (default) enabled all incoming connections to the Database must use SSL. replica_ssl databases that replicate from this database must use SSL. ","categories":["Platforms"]},{"uri":"/rc/databases/system-logs/","uriRel":"/rc/databases/system-logs/","title":"System logs","tags":[],"keywords":[],"description":"","content":"The Logs page contains events, alerts, and logs from the activities, databases, and subscriptions associated with your account.\nYou can:\nSort the list by a specific field in descending or ascending order. Supported fields include Time, Originator, Database name, and Activity.\nSelect the arrow icon to change the sort order. You can only sort by one field at a time.\nUse the Export all button to export all logs as a comma-separated values (CSV) file for use in other systems and programs.\nUse the refresh button to refresh the system logs.\nUse the search bar to search for specific entries. Supported fields include Originator, Database name, Activity, and Description.\n","categories":["RC"]},{"uri":"/rs/databases/delete/","uriRel":"/rs/databases/delete/","title":"Delete databases","tags":[],"keywords":[],"description":"Delete a database from the admin console.","content":"When you delete a database, the database configuration and data are deleted.\nUse the admin console to delete a database:\nClick the relevant database row in the Databases page. The selected database page appears. Select the Configuration tab. Click Delete at the bottom of the page. Confirm the deletion. ","categories":["RS"]},{"uri":"/rs/databases/","uriRel":"/rs/databases/","title":"Manage databases","tags":[],"keywords":[],"description":"This page will help you find database management information in the Databases section.","content":"You can manage your Redis Enterprise Software databases with several different tools:\nAdmin console (the web-based user interface) Command-line tools (rladmin, redis-cli, crdb-cli) REST API Create a Redis Enterprise Software database Create a database with Redis Enterprise Software.\nConfigure database settings Configure settings specific to each database.\nConnect to a database Learn how to connect your application to a Redis database hosted by Redis Enterprise Software and test your connection.\nImport and export data How to import, export, flush, and migrate your data.\nRecover a failed database Recover a database after the cluster fails or the database is corrupted.\nDelete databases Delete a database from the admin console.\nActive-Active geo-distributed Redis Overview of the Active-Active database in Redis Enterprise Software\nRedis on Flash Redis on Flash enables your data to span both RAM and dedicated flash memory.\nDurability and high availability Overview of Redis Enterprise durability features such as replication, clustering, and rack-zone awareness.\nMemory and performance Learn more about managing your memory and optimizing performance for your database.\n","categories":["RS"]},{"uri":"/rs/clusters/","uriRel":"/rs/clusters/","title":"Manage clusters","tags":[],"keywords":[],"description":"Administrative tasks and information related to the Redis Enterprise cluster.","content":"You can manage your Redis Enterprise Software clusters with several different tools:\nAdmin console (the web-based user interface) Command-line tools (rladmin, redis-cli, crdb-cli) REST API Manage your cluster Set up cluster Set up a new cluster using the admin console.\nAdd a node Add a node to an existing Redis Enterprise cluster.\nConfigure clusters Change cluster settings.\nOptimize clusters Find information and configuration settings to improve the performance of Redis Enterprise Software.\nMonitor cluster Monitor the cluster and database activity with cluster logs and metrics.\n","categories":["RS"]},{"uri":"/rc/accounts/","uriRel":"/rc/accounts/","title":"Manage accounts and settings","tags":[],"keywords":[],"description":"Describes how to work with Redis Cloud accounts and manage their settings.","content":"Here, you learn how to manage Redis Cloud accounts and their settings:\nManage account settings\nUpdate user profile\nSwitch between Redis Cloud accounts\n","categories":["RC"]},{"uri":"/rc/billing-and-payments/","uriRel":"/rc/billing-and-payments/","title":"Billing &amp; payments","tags":[],"keywords":[],"description":"Describes how to view billing transactions and manage payment methods for Redis Enterprise Cloud subscriptions.","content":"The Billing \u0026amp; Payments screen:\nShows recent transactions for your account Helps you manage your payment methods Applies coupon credits to your account The following tabs are available:\nThe Billing History tab displays recent charges and payments. Each transaction includes the following details:\nDetail Description Date Date the transaction was recorded Description Description of the transaction Reference Reference number Amount Transaction amount Billing details may vary between regions.\nWhen you select a billing transaction, a Download icon appears to the right of the amount.\nSelect this to download a PDF invoice for the associated charge.\nA Pay Now button appears in selected regions.\nSelect this button to pay your invoice.\nThe Payment Methods tab lists your current payment methods. You can add a new payment method, associate different payment methods with specific subscriptions, and remove payment methods.\nSelect Add Credit Card to enter new credit card details.\nThe Credits tab shows coupon credits that have been applied to your account, if any.\nDetail Description Code Coupon code Coupon Amount Amount credited to your account Current Balance Amount left Date added Date applied to your account Expiration Date Date the amount expires To apply a coupon, enter the code and then select the Apply button.\nDownload invoice To download an invoice:\nSign in to the Redis Cloud admin portal. (Create an account if you don\u0026rsquo;t already have one.)\nUse the admin console menu to select Billing \u0026amp; Payments and then make sure the Billing History tab is selected.\nLocate and select the invoice.\nSelect the Download invoice icon displayed to the right of the invoice amount.\nThe invoice is downloaded as an Acrobat PDF file. Use your browser\u0026rsquo;s download features to manage the file.\nAdd payment method To add a new payment method:\nSign in to the Redis Cloud admin portal. (Create an account if you don\u0026rsquo;t already have one.)\nUse the admin console menu to select Billing \u0026amp; Payments and then select the Payment Methods tab.\nSelect Add credit card and then provide the details.\nIf your billing address is different from your account address, locate the Billing address section, deactivate Use account address, and then provide the appropriate details.\nUse the Save Card button to save your changes.\nApply coupon Coupons apply credits to your Redis Enterprise Cloud account. To redeem a coupon:\nSign in to the Redis Cloud admin portal. (Create an account if you don\u0026rsquo;t already have one.)\nUse the admin console menu to select Billing \u0026amp; Payments and then select the Credits tab.\nEnter the coupon code and then select the Apply button.\nThe value of the coupon is applied to your account when accepted.\nFor help, contact Support.\n","categories":["RC"]},{"uri":"/rs/networking/","uriRel":"/rs/networking/","title":"Manage networks","tags":[],"keywords":[],"description":"Networking features and considerations designing your Redis Enteprise Software deployment.","content":"When designing a Redis Enterprise Software solution, there are some specific networking features that are worth your time to understand and implement.\nConfigure cluster DNS Configure DNS to communicate between cluster nodes.\nAWS Route53 DNS management Client prerequisites for mDNS for development and test environments Cluster load balancer setup Set up a load balancer to direct traffic to cluster nodes when DNS is not available.\nMulti-IP and IPv6 Requirements for using multiple IP addresses or IPv6 addresses with Redis Enterprise Software.\nNetwork port configurations Describes the port ranges that Redis Enterprise Software uses.\nPublic and private endpoints Enable public and private endpoints for your databases.\n","categories":["RS"]},{"uri":"/ri/using-redisinsight/browser/","uriRel":"/ri/using-redisinsight/browser/","title":"Browser","tags":[],"keywords":[],"description":"","content":"RedisInsight Browser lets you explore keys in your redis server. You can add, edit and delete a key. You can even update the key expiry and copy the key name to be used in different places of the application.\nNote: RedisInsight restricts visualization of keys that are greater than 2GB in size.\n","categories":["RI"]},{"uri":"/ri/using-redisinsight/cli/","uriRel":"/ri/using-redisinsight/cli/","title":"CLI","tags":[],"keywords":[],"description":"","content":"RedisInsight CLI lets you run commands against a redis server. You don\u0026rsquo;t need to remember the syntax - the integrated help shows you all the arguments and validates your command as you type.\nKey Bindings Emacs and Vim keybindings are supported.\nAction Emacs Vim Previous history item C-p k Next history item C-n j Move cursor to the begining C-a 0 Move cursor to the end C-e $ Move cursor to the right by one character C-f h Move cursor to the left by one character C-b l Enter insert mode - i Enter normal mode - ESC ","categories":["RI"]},{"uri":"/ri/installing/install-docker/","uriRel":"/ri/installing/install-docker/","title":"Install RedisInsight on Docker","tags":[],"keywords":[],"description":"","content":"This tutorial shows how to install RedisInsight on Docker so you can use RedisInsight in development. See a separate guide for installing RedisInsight on AWS.\nInstall Docker The first step is to install Docker for your operating system. On Windows and Mac, install Docker version 18.03 or higher. To check which Docker version you have, run docker version.\nRun RedisInsight Docker image Next, run the RedisInsight container.\ndocker run -v redisinsight:/db -p 8001:8001 redislabs/redisinsight:latest Then, point your browser to http://localhost:8001.\nRedisInsight also provides a health check endpoint at http://localhost:8001/healthcheck/ to monitor the health of the running container.\nIf everything worked, you should see the following output in the terminal:\nStarting webserver... Visit http://0.0.0.0:8001 in your web browser. Press CTRL-C to exit. Resolving permission errors If the previous command returns a permissions error, ensure the directory you pass as a volume to the container has necessary permissions for the container to access it. Run the following command:\nchown -R 1001 redisinsight Adding flags to the run command You can use additional flags with the docker run command:\nYou can add the -it flag to see the logs and view the progress.\nOn Linux, you can add --network host. This makes it easy to work with redis running on your local machine.\nTo analyze RDB files stored in S3, you can add the access key and secret access key as environment variables using the -e flag.\nFor example: -e AWS_ACCESS_KEY=\u0026lt;aws access key\u0026gt; -e AWS_SECRET_KEY=\u0026lt;aws secret access key\u0026gt;\n","categories":["RI"]},{"uri":"/ri/installing/install-k8s/","uriRel":"/ri/installing/install-k8s/","title":"Install RedisInsight on Kubernetes","tags":[],"keywords":[],"description":"","content":"In this walkthrough, we will install RedisInsight on Kubernetes. This is an easy way to use RedisInsight with a Redis Enterprise K8s deployment.\nCreate the RedisInsight deployment and service Below is an annotated YAML file that will create a RedisInsight deployment and a service in a k8s cluster.\nCreate a new file redisinsight.yaml with the content below # RedisInsight service with name \u0026#39;redisinsight-service\u0026#39; apiVersion: v1 kind: Service metadata: name: redisinsight-service # name should not be \u0026#39;redisinsight\u0026#39; # since the service creates # environment variables that # conflicts with redisinsight # application\u0026#39;s environment # variables `REDISINSIGHT_HOST` and # `REDISINSIGHT_PORT` spec: type: LoadBalancer ports: - port: 80 targetPort: 8001 selector: app: redisinsight --- # RedisInsight deployment with name \u0026#39;redisinsight\u0026#39; apiVersion: apps/v1 kind: Deployment metadata: name: redisinsight #deployment name labels: app: redisinsight #deployment label spec: replicas: 1 #a single replica pod selector: matchLabels: app: redisinsight #which pods is the deployment managing, as defined by the pod template template: #pod template metadata: labels: app: redisinsight #label for pod/s spec: containers: - name: redisinsight #Container name (DNS_LABEL, unique) image: redislabs/redisinsight:latest #repo/image imagePullPolicy: IfNotPresent #Always pull image volumeMounts: - name: db #Pod volumes to mount into the container\u0026#39;s filesystem. Cannot be updated. mountPath: /db ports: - containerPort: 8001 #exposed container port and protocol protocol: TCP volumes: - name: db emptyDir: {} # node-ephemeral volume https://kubernetes.io/docs/concepts/storage/volumes/#emptydir Create the RedisInsight deployment and service kubectl apply -f redisinsight.yaml Once the deployment and service are successfully applied and complete, access RedisInsight. This can be accomplished by listing the using the \u0026lt;external-ip\u0026gt; of the service we created to reach redisinsight. $ kubectl get svc redisinsight-service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE redisinsight-service \u0026lt;cluster-ip\u0026gt; \u0026lt;external-ip\u0026gt; 80:32143/TCP 1m If you are using minikube, run minikube list to list the service and access RedisInsight at http://\u0026lt;minikube-ip\u0026gt;:\u0026lt;minikube-service-port\u0026gt;. $ minikube list |-------------|----------------------|--------------|---------------------------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|----------------------|--------------|---------------------------------------------| | default | kubernetes | No node port | | | default | redisinsight-service | 80 | http://\u0026lt;minikube-ip\u0026gt;:\u0026lt;minikubeservice-port\u0026gt; | | kube-system | kube-dns | No node port | | |-------------|----------------------|--------------|---------------------------------------------| Create the RedisInsight deployment with persistant storage Below is an annotated YAML file that will create a RedisInsight deployment in a K8s cluster. It will assign a peristent volume created from a volume claim template. Write access to the container is configured in an init container. When using deployments with persistent writeable volumes, it\u0026rsquo;s best to set the strategy to Recreate. Otherwise you may find yourself with two pods trying to use the same volume.\nCreate a new file redisinsight.yaml with the content below. # RedisInsight service with name \u0026#39;redisinsight-service\u0026#39; apiVersion: v1 kind: Service metadata: name: redisinsight-service # name should not be \u0026#39;redisinsight\u0026#39; # since the service creates # environment variables that # conflicts with redisinsight # application\u0026#39;s environment # variables `REDISINSIGHT_HOST` and # `REDISINSIGHT_PORT` spec: type: LoadBalancer ports: - port: 80 targetPort: 8001 selector: app: redisinsight --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: redisinsight-pv-claim labels: app: redisinsight spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: default --- # RedisInsight deployment with name \u0026#39;redisinsight\u0026#39; apiVersion: apps/v1 kind: Deployment metadata: name: redisinsight #deployment name labels: app: redisinsight #deployment label spec: replicas: 1 #a single replica pod strategy: type: Recreate selector: matchLabels: app: redisinsight #which pods is the deployment managing, as defined by the pod template template: #pod template metadata: labels: app: redisinsight #label for pod/s spec: volumes: - name: db persistentVolumeClaim: claimName: redisinsight-pv-claim initContainers: - name: init image: busybox command: - /bin/sh - \u0026#39;-c\u0026#39; - | chown -R 1001 /db resources: {} volumeMounts: - name: db mountPath: /db terminationMessagePath: /dev/termination-log terminationMessagePolicy: File containers: - name: redisinsight #Container name (DNS_LABEL, unique) image: redislabs/redisinsight:latest #repo/image imagePullPolicy: IfNotPresent #Always pull image volumeMounts: - name: db #Pod volumes to mount into the container\u0026#39;s filesystem. Cannot be updated. mountPath: /db ports: - containerPort: 8001 #exposed container port and protocol protocol: TCP Create the RedisInsight deployment and service. kubectl apply -f redisinsight.yaml Create the RedisInsight deployment without a service. Below is an annotated YAML file that will create a RedisInsight deployment in a K8s cluster.\nCreate a new file redisinsight.yaml with the content below apiVersion: apps/v1 kind: Deployment metadata: name: redisinsight #deployment name labels: app: redisinsight #deployment label spec: replicas: 1 #a single replica pod selector: matchLabels: app: redisinsight #which pods is the deployment managing, as defined by the pod template template: #pod template metadata: labels: app: redisinsight #label for pod/s spec: containers: - name: redisinsight #Container name (DNS_LABEL, unique) image: redislabs/redisinsight:latest #repo/image imagePullPolicy: IfNotPresent #Always pull image env: # If there\u0026#39;s a service named \u0026#39;redisinsight\u0026#39; that exposes the # deployment, we manually set `REDISINSIGHT_HOST` and # `REDISINSIGHT_PORT` to override the service environment # variables. - name: REDISINSIGHT_HOST value: \u0026#34;0.0.0.0\u0026#34; - name: REDISINSIGHT_PORT value: \u0026#34;8001\u0026#34; volumeMounts: - name: db #Pod volumes to mount into the container\u0026#39;s filesystem. Cannot be updated. mountPath: /db ports: - containerPort: 8001 #exposed conainer port and protocol protocol: TCP livenessProbe: httpGet: path : /healthcheck/ # exposed RI endpoint for healthcheck port: 8001 # exposed container port initialDelaySeconds: 5 # number of seconds to wait after the container starts to perform liveness probe periodSeconds: 5 # period in seconds after which liveness probe is performed failureThreshold: 1 # number of liveness probe failures after which container restarts volumes: - name: db emptyDir: {} # node-ephemeral volume https://kubernetes.io/docs/concepts/storage/volumes/#emptydir Create the RedisInsight deployment kubectl apply -f redisinsight.yaml Note: If the deployment will be exposed by a service whose name is \u0026lsquo;redisinsight\u0026rsquo;, set REDISINSIGHT_HOST and REDISINSIGHT_PORT environment variables to override the environment variables created by the service. Once the deployment has been successfully applied and the deployment complete, access RedisInsight. This can be accomplished by exposing the deployment as a K8s Service or by using port forwarding, as in the example below: kubectl port-forward deployment/redisinsight 8001 Open your browser and point to http://localhost:8001\nHelm Chart (Experimental) You can download the RedisInsight helm chart from here.\nAfter downloading, install the helm chart using the following command: helm install redisinsight redisinsight-chart-0.1.0.tgz --set service.type=NodePort Note: The service type is NodePort which allows us to access redisinsight from outside k8s cluster. You get the following output:\nNAME: redisinsight LAST DEPLOYED: Wed Dec 16 10:46:08 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services redisinsight-redisinsight-chart) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT Run the commands mentioned in the output to get the end point: export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services redisinsight-redisinsight-chart) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT #Example: http://172.17.0.2:32388 Open your browser and point to http://\u0026lt;endpoint\u0026gt;:\u0026lt;port\u0026gt; from the previous command.\nTo uninstall the helm chart:\nhelm uninstall redisinsight ","categories":["RI"]},{"uri":"/rs/databases/active-active/","uriRel":"/rs/databases/active-active/","title":"Active-Active geo-distributed Redis","tags":[],"keywords":[],"description":"Overview of the Active-Active database in Redis Enterprise Software","content":"In Redis Enterprise, Active-Active geo-distribution is based on CRDT technology. The Redis Enterprise implementation of CRDT is called an Active-Active database (formerly known as CRDB). With Active-Active databases, applications can read and write to the same data set from different geographical locations seamlessly and with latency less than one millisecond (ms), without changing the way the application connects to the database.\nActive-Active databases also provide disaster recovery and accelerated data read-access for geographically distributed users.\nHigh availability The high availability that Active-Active replication provides is built upon a number of Redis Enterprise Software features (such as clustering, replication, and replica HA) as well as some features unique to Active-Active (multi-primary replication, automatic conflict resolution, and strong eventual consistency).\nClustering and replication are used together in Active-Active databases to distribute multiple copies of the dataset across multiple nodes and multiple clusters. As a result, a node or cluster is less likely to become a single point of failure. If a primary node or primary shard fails, a replica is automatically promoted to primary. To avoid having one node hold all copies of certain data, the replica HA feature (enabled by default) automatically migrates replica shards to available nodes.\nMulti-primary replication In Redis Enterprise Software, replication copies data from primary shards to replica shards. Active-Active geo-distributed replication also copies both primary and replica shards to other clusters. Each Active-Active database needs to span at least two clusters; these are called participating clusters.\nEach participating cluster hosts an instance of your database, and each instance has its own primary node. Having multiple primary nodes means you can connect to the proxy in any of your participating clusters. Connecting to the closest cluster geographically enables near-local latency. Multi-primary replication (previously referred to as multi-master replication) also means that your users still have access to the database if one of the participating clusters fails.\nNote: Active-Active databases do not replicate the entire database, only the data. Database configurations, LUA scripts, and other support info are not replicated. Syncer Keeping multiple copies of the dataset consistent across multiple clusters is no small task. To achieve consistency between participating clusters, Redis Active-Active replication uses a process called the syncer.\nThe syncer keeps a replication backlog, which stores changes to the dataset that the syncer sends to other participating clusters. The syncer uses partial syncs to keep replicas up to date with changes, or a full sync in the event a replica or primary is lost.\nConflict resolution Because you can connect to any participating cluster to perform a write operation, concurrent and conflicting writes are always possible. Conflict resolution is an important part of the Active-Active technology. Active-Active databases only use conflict-free replicated data types (CRDTs). These data types provide a predictable conflict resolution and don\u0026rsquo;t require any additional work from the application or client side.\nWhen developing with CRDTs for Active-Active databases, you need to consider some important differences. See Develop applications with Active-Active databases for related information.\nStrong eventual consistency Maintaining strong consistency for replicated databases comes with tradeoffs in scalability and availability. Redis Active-Active databases use a strong eventual consistency model, which means that local values may differ across replicas for short periods of time, but they all eventually converge to one consistent state. Redis uses vector clocks and the CRDT conflict resolution to strengthen consistency between replicas. You can also enable the causal consistency feature to preserve the order of operations as they are synchronized among replicas.\nOther Redis Enterprise Software features can also be used to enhance the performance, scalability, or durability of your Active-Active database. These include data persistence, multiple active proxies, distributed synchronization, the OSS Cluster API, and rack-zone awareness.\nNext steps Plan your Active-Active deployment Get started with Active-Active Create an Active-Active database ","categories":["RS"]},{"uri":"/rc/cloud-integrations/aws-cloud-accounts/","uriRel":"/rc/cloud-integrations/aws-cloud-accounts/","title":"Manage AWS cloud accounts","tags":[],"keywords":[],"description":"Describes how to provision your Redis Enterprise Cloud subscription to use existing cloud provider accounts.","content":"Many customers use cloud provider accounts provisioned and maintained by Redis.\nCustomers with existing Amazon Web Services (AWS) accounts can provision their Flexible or Annual subscriptions to use their existing AWS accounts.\nTo do so, you associate your existing AWS account as a cloud account for your subscription. This requires setting up and entering credentials that enable monitoring, maintenance, and technical support of your subscription.\nTo do this, you need:\nA programmatic user with an access key and a secret access key for that user. A console role that allows administrative access to the cloud account. These resources need to exist before adding the cloud account to your subscription. To learn more, see Create IAM resources.\nNote: Once an AWS account has been configured as a cloud account, you must not:\nManually change the configuration of required resources, such as security groups Manually suspend or stop (terminate) provisioned resources View cloud account settings To create or edit a cloud account in Redis Enterprise Cloud:\nSign in to the admin console and then select the target subscription.\nFrom the console menu, select Account Settings and then select the Cloud Account tab.\nThis displays a list of cloud accounts associated with your Redis Cloud subscription.\nThe Cloud account tab lets you manage cloud accounts associated with your Redis Cloud subscription.\nThe Cloud Account tab is not available (or supported) for Free or Fixed Redis Cloud subscriptions. If you do not see a Cloud Account tab on the Account Settings page, verify that you have selected a Flexible or Annual subscription.\nAdd a new cloud account To add a new cloud account to your Redis Cloud subscription, select the Add button from the Cloud Account tab of the Account Settings screen.\nThis displays the Add cloud account dialog\nEach of the following fields are required.\nSetting Description Account name A descriptive name for your cloud account settings AWS access key The AWS access key for the programmatic user created to support your cloud account settings AWS secret key The AWS secret key for the programmatic user created to support your cloud account settings IAM role name The name of the AWS console role with access to the AWS console Use the Add account button to save your cloud account details.\nBe sure to create the resources before adding the cloud account to your subscription, as they\u0026rsquo;re used to verify access to the cloud account. The details can be saved only after access is verified.\nWhen problems occur, an information icon appears and the field is highlighted in red. When this happens, the icon includes a tooltip that explains the issue.\nIf the Add account button is inactive, verify that:\nYou\u0026rsquo;ve specified all field values correctly The resources exist in your AWS account Each resource provides the required level of access For help, see Create IAM resources.\nEdit cloud account details To update the details of a cloud account associated with your Redis Cloud subscription, select the cloud account from the Cloud account tab and then select the Edit button.\nThis displays the Edit cloud account dialog:\nSetting Description Account name A descriptive name for your cloud account settings AWS access key The AWS access key for the programmatic user created to support your cloud account settings AWS secret key The AWS secret key for the programmatic user created to support your cloud account settings AWS console username The username for the AWS console AWS console password The password for AWS console access Use the Update account button to save your changes.\nDelete cloud account details To remove a cloud account from your Redis cloud subscription, select the cloud account from the Cloud account tab and then select the Delete button.\nDedicated IAM resources We strongly recommend using dedicated identity and access management (IAM) resources to manage your AWS cloud accounts. These resources should not be shared with any other task, account, or process.\nTo learn more, see Create IAM resources for AWS cloud accounts.\n","categories":["RC"]},{"uri":"/rc/cloud-integrations/","uriRel":"/rc/cloud-integrations/","title":"Manage cloud integrations","tags":[],"keywords":[],"description":"Describes how to integrate Redis Enterprise Cloud subscriptions into existing cloud provider services, whether existing subscriptions or through vendor marketplaces.","content":"By default, Redis Enterprise Cloud subscriptions are hosted in cloud vendor accounts owned and managed by Redis, Inc.\nTo integrate Redis Enterprise Cloud into an existing cloud vendor account, you can:\nDesignate an existing AWS subscription as an AWS cloud account for your Redis Cloud subscription.\nSubscribe to Redis Enterprise Cloud through AWS Marketplace.\nSubscribe to Redis Enterprise Cloud through GCP Marketplace.\nWhen you subscribe to Redis Enterprise Cloud through a cloud vendor marketplace, billing is handled through the marketplace.\nMarketplace billing considerations Cloud vendor marketplaces provide a convenient way to handle multiple subscription fees. However, this also means that billing issues impact multiple subscriptions, including Redis Enterprise Cloud.\nWhen billing details change, you should verify that each service is operating normally and reflects the updated billing details. Otherwise, you might experience unexpected consequences, such as data loss or subscription removal.\nFor best results, we recommend:\nBacking up all data before updating billing details.\nVerifying that all accounts operate normally after updating billing details, especially after updating payment methods.\nMaking sure that billing alerts are sent to actively monitored accounts.\nUpdate marketplace billing details To change billing details for an AWS marketplace subscription, we recommend creating a second subscription using the updated billing details and then migrating your existing data to the new subscription.\nIf you\u0026rsquo;re using GCP, you can migrate a GCP project to a new billing account without creating a new subscription. To do so:\nCreate a second project and associate with it your new billing account. With your second project, purchase Redis Enterprise via the GCP Marketplace. Activate the service by signing in to Redis Enterprise console using your original SSO credentials. Change the billing account for your original project to the new billing account. (Optional) Remove your second project. ","categories":["RC"]},{"uri":"/modules/redisgears/jvm/commands/","uriRel":"/modules/redisgears/jvm/commands/","title":"RedisGears JVM commands","tags":[],"keywords":[],"description":"","content":"Use a Redis client like redis-cli to send commands to the RedisGears JVM plugin.\nJVM plugin commands Command Description RG.JDUMPSESSIONS Outputs information about existing Java sessions. RG.JEXECUTE Executes a Java function. See the general RedisGears commands page for more commands.\nNote: Ignore any commands that start with RG.PY while using the JVM plugin. The RG.PY commands are for the Python plugin. ","categories":["Modules"]},{"uri":"/rc/api/examples/back-up-and-import-data/","uriRel":"/rc/api/examples/back-up-and-import-data/","title":"Database backup and import","tags":[],"keywords":[],"description":"When you create or update a database, you can specify the backup path. The import API operation lets you import data from various source types and specified locations.","content":"Back up a database When you create or update a database in a Flexible or Annual account, you can specify the (optional) periodicBackupPath parameter with a backup path. This parameter enables periodic and on-demand backup operations for the specified database.\nThe API operation for on-demand backups is POST /subscriptions/{subscriptionId}/databases/{databaseId}/backup. On-demand database backup is an asynchronous operation.\nPOST \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscriptionId\u0026gt;/databases/\u0026lt;databaseId\u0026gt;/backup\u0026#34; The backup database API does not require a body. Instead, the periodicBackupPath must be set to a valid path with available storage capacity to store the backup files for the specific database.\nImport a database You can import data into an existing database from multiple storage sources, including AWS S3, Redis, and FTP. Database import is an asynchronous operation.\nThe API operation for performing on-demand backup is POST /v1/subscriptions/{subscriptionId}/databases/{databaseId}/import.\nThe requirements for data import are:\nThe URI of the data The source URI must be accessible to the importing database The data format must be a Redis backup file or a Redis database The subscription ID and database ID of the destination database The duration of the import process depends on the amount of data imported and the network bandwidth between the data source and the importing database.\nWarning - Data imported into an existing database overwrites any existing data. To import the data, run:\nPOST \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscriptionId\u0026gt;/databases/{databaseId}/import\u0026#34; \\ { \u0026#34;sourceType\u0026#34;: \u0026#34;aws-s3\u0026#34;, \u0026#34;importFromUri\u0026#34;: [ \u0026#34;s3://bucketname/filename-dbForAWSBackup-1_of_3.rdb.gz\u0026#34;, \u0026#34;s3://bucketname/filename-dbForAWSBackup-2_of_3.rdb.gz\u0026#34;, \u0026#34;s3://bucketname/filename-dbForAWSBackup-3_of_3.rdb.gz\u0026#34; ] } You can specify the backup location with the sourceType and importFromUri values for these sources:\nData location sourceType importFromUri Amazon AWS S3 aws-s3 s3://bucketname/[path/]filename.rdb[.gz] FTP ftp ftp://[username][:password]@[:port]/[path/]filename.rdb[.gz] Google Blob Storage google-blob-storage gs://bucketname/[path/]filename.rdb[.gz] Microsoft Azure Blob Storage azure-blob-storage abs://:storage_account_access_key@storage_account_name/[container/]filename.rdb[.gz] Redis server redis redis://[db_password]@[host]:[port] Web server HTTP HTTP://[username][:password]@[:port]/[path/]filename.rdb[.gz] ","categories":["RC"]},{"uri":"/rs/security/tls/enable-tls/","uriRel":"/rs/security/tls/enable-tls/","title":"Enable TLS","tags":[],"keywords":[],"description":"Shows how to enable TLS.","content":"You can use TLS authentication for one or more of the following types of communication:\nCommunication from clients (applications) to your database Communication from your database to other clusters for replication using Replica Of Communication to and from your database to other clusters for synchronization using Active-Active Enable TLS for client connections You can enable TLS by editing the configuration of an existing database (as shown below) or by selecting Advanced Options when you are creating a new database.\nSelect your database from your database list and navigate to the configuration tab.\nSelect Edit at the bottom of your screen.\nEnable TLS.\nEnforce client authentication is selected by default. If you clear this option, you will still enforce encryption, but TLS client authentication will be deactivated. Select Advanced Options and Require TLS for All Communications from the dropdown menu. Select Add Paste your certificate or certificate authority (CA) into the text box. Save the certificate. Repeat for each client certificate you need to add.\nIf your database uses Replica Of or Active-Active replication, you will need to add the syncer certificates for the participating clusters. The steps for each are below. You can configure Additional certificate validations to further limit connections to clients with valid certificates.\nAdditional certificate validations occur only when loading a certificate chain that includes the root certificate and intermediate CA certificate but does not include a leaf (end-entity) certificate. If you include a leaf certificate, mutual client authentication skips any additional certificate validations.\nSelect a certificate validation option.\nValidation option Description No validation Authenticates clients with valid certificates. No additional validations are enforced. By Subject Alternative Name (SAN) / Common Name (CN) only A client certificate is valid only if its Common Name (CN) matches an entry in the list of valid subjects. Ignores other Subject attributes. By full subject A client certificate is valid only if its Subject attributes match an entry in the list of valid subjects. If you selected No validation, you can skip this step. Otherwise, select Add to create a new entry and then enter valid Subject attributes for your client certificates. All Subject attributes are case-sensitive.\nSubject attribute\n(case-sensitive) Description Common Name (CN) Name of the client authenticated by the certificate (required) Organization (O) The client\u0026rsquo;s organization or company name Organizational Unit (OU) Name of the unit or department within the organization Locality (L) The organization\u0026rsquo;s city State / Province (ST) The organization\u0026rsquo;s state or province Country (C) 2-letter code that represents the organization\u0026rsquo;s country You can only enter a single value for each field. If your client certificate has a Subject attribute with multiple values (such as OU=unit1; OU=unit2), add a separate Subject entry for each.\nBreaking change: If you use the REST API instead of the admin console to configure additional certificate validations, note that authorized_names is deprecated as of Redis Enterprise v6.4.2. Use authorized_subjects instead. See the BDB object reference for more details.\nSelect Update at the bottom of the screen to save your configuration.\nOptional: By default, Redis Enterprise Software validates client certificate expiration dates. You can use rladmin to turn off this behavior.\nrladmin tune db \u0026lt; db:id | name \u0026gt; mtls_allow_outdated_certs enabled Enable TLS for Active-Active cluster connections Note: You cannot enable or turn off TLS after the Active-Active database is created, but you can change the TLS configuration. Retrieve syncer certificates For each participating cluster, copy the syncer certificate from the general settings tab. Configure TLS certificates for Active-Active During database creation (see Create an Active-Active Geo-Replicated Database, select Edit from the configuration tab. Enable TLS. Enforce client authentication is selected by default. If you clear this option, you will still enforce encryption, but TLS client authentication will be deactivated. Select Require TLS for CRDB communication only from the dropdown menu. Select Add Paste a syncer certificate into the text box. Save the syncer certificate. Repeat this process, adding the syncer certificate for each participating cluster. Optional: If also you want to require TLS for client connections, select Require TLS for All Communications from the dropdown and add client certificates as well. Select Update at the bottom of the screen to save your configuration. Configure TLS on all participating clusters Repeat this process on all participating clusters.\nTo enforce TLS authentication, Active-Active databases require syncer certificates for each cluster connection. If every participating cluster doesn\u0026rsquo;t have a syncer certificate for every other participating cluster, synchronization will fail.\nEnable TLS for Replica Of cluster connections You can enable TLS by editing the configuration of an existing database (as shown below) or by selecting Advanced Options when you are creating a new database.\nFor each cluster hosting a replica, copy the syncer certificate from the general settings tab. Select your database from your database list and navigate to the configuration tab. Select Edit at the bottom of your screen. Enable TLS. Enforce client authentication is selected by default. If you clear this option, you will still enforce encryption, but TLS client authentication will be deactivated. Under Advanced Options, Select Require TLS for Replica Of Only from the dropdown menu. Select Add Paste a syncer certificate into the text box. Save the syncer certificate. Repeat this process, adding the syncer certificate for each cluster hosting a replica of this database. Optional: If you also want to require TLS for client connections, select Require TLS for All Communications from the dropdown and add client certificates as well. Select Update at the bottom of the screen to save your configuration. ","categories":["RS"]},{"uri":"/rc/security/encryption-at-rest/","uriRel":"/rc/security/encryption-at-rest/","title":"Encryption at rest","tags":[],"keywords":[],"description":"Describes when data is encrypted at rest.","content":"Redis Cloud databases write their data to disk whenever persistence is enabled.\nRedis Cloud deployments are always encrypted at rest.\nEncryption at rest on AWS Persistent data is written to encrypted EBS volumes.\nWhen Redis on Flash is enabled, the flash memory data is written to encrypted NVMe SSD volumes.\nDisk encryption on GCP All data written to disk on GCP-based Redis Cloud deployments is encrypted by default. When deploying a Redis Cloud database on GCP, you don\u0026rsquo;t need to take any actions to enable this encryption.\nTo learn more, see the GCP encryption at rest documentation.\nDisk encryption on Azure All data written to disk on Azure-based Redis Cloud deployments is encrypted by default. When deploying a Redis Cloud database on Azure, you don\u0026rsquo;t need to take any actions to enable this encryption.\nTo learn more, see the Azure encryption at rest documentation.\n","categories":["RC"]},{"uri":"/rs/databases/import-export/flush/","uriRel":"/rs/databases/import-export/flush/","title":"Flush database data","tags":[],"keywords":[],"description":"To delete the data in a database without deleting the database, you can use Redis CLI to flush it from the database.  You can also use Redis CLI, the admin console, and the Redis Software REST API to flush data from Active-Active databases.","content":"To delete the data in a database without deleting the database configuration, you can flush the data from the database.\nYou can use the admin console to flush data from Active-Active databases.\nWarning - The flush command deletes ALL in-memory and persistence data in the database. We recommend that you back up your database before you flush the data. Flush data from a database From the command line, you can flush a database with the redis-cli command or with your favorite Redis client.\nTo flush data from a database with the redis-cli, run:\nredis-cli -h \u0026lt;hostname\u0026gt; -p \u0026lt;portnumber\u0026gt; -a \u0026lt;password\u0026gt; flushall Example:\nredis-cli -h redis-12345.cluster.local -p 12345 -a xyz flushall Flush data from an Active-Active database When you flush an Active-Active database (formerly known as CRDB), all of the replicas flush their data at the same time.\nTo flush data from an Active-Active database:\nadmin console\nGo to database and select the Active-Active database that you want to flush. Go to configuration and click Flush at the bottom of the page. Enter the name of the Active-Active database to confirm that you want to flush the data. Command line\nTo find the ID of the Active-Active database, run:\ncrdb-cli crdb list For example:\n$ crdb-cli crdb list CRDB-GUID NAME REPL-ID CLUSTER-FQDN a16fe643-4a7b-4380-a5b2-96109d2e8bca crdb1 1 cluster1.local a16fe643-4a7b-4380-a5b2-96109d2e8bca crdb1 2 cluster2.local a16fe643-4a7b-4380-a5b2-96109d2e8bca crdb1 3 cluster3.local To flush the Active-Active database, run:\ncrdb-cli crdb flush --crdb-guid \u0026lt;CRDB-GUID\u0026gt; The command output contains the task ID of the flush task, for example:\n$ crdb-cli crdb flush --crdb-guid a16fe643-4a7b-4380-a5b2-96109d2e8bca Task 63239280-d060-4639-9bba-fc6a242c19fc created ---\u0026gt; Status changed: queued -\u0026gt; started To check the status of the flush task, run:\ncrdb-cli task status --task-id \u0026lt;Task-ID\u0026gt; For example:\n$ crdb-cli task status --task-id 63239280-d060-4639-9bba-fc6a242c19fc Task-ID: 63239280-d060-4639-9bba-fc6a242c19fc CRDB-GUID: - Status: finished REST API\nTo find the ID of the Active-Active database, use GET /v1/crdbs:\nGET https://[host][:port]/v1/crdbs To flush the Active-Active database, use PUT /v1/crdbs/{guid}/flush:\nPUT https://[host][:port]/v1/crdbs/\u0026lt;guid\u0026gt;/flush The command output contains the task ID of the flush task.\nTo check the status of the flush task, use GET /v1/crdb_tasks:\nGET https://[host][:port]/v1/crdb_tasks/\u0026lt;task-id\u0026gt; ","categories":["RS"]},{"uri":"/rs/security/access-control/manage-users/login-lockout/","uriRel":"/rs/security/access-control/manage-users/login-lockout/","title":"Manage user login","tags":[],"keywords":[],"description":"Manage user login lockout and session timeout.","content":"Redis Enterprise Software secures user access in a few different ways, including automatically:\nLocking user accounts after a series of authentication failures (invalid passwords)\nSigning sessions out after a period of inactivity\nHere, you learn how to configure the relevant settings.\nUser login lockout The parameters for the user login lockout are:\nLogin Lockout Threshold - The number of failed login attempts allowed before the user account is locked. (Default: 5 minutes) Login Lockout Counter Reset - The amount of time during which failed login attempts are counted. (Default: 15 minutes) Login Lockout Duration - The amount of time that the user account is locked after excessive failed login attempts. (Default: 30 minutes) By default, after 5 failed login attempts within 15 minutes, the user account is locked for 30 minutes.\nYou can view the user login restrictions for your cluster with:\nrladmin info cluster | grep login_lockout Change the login lockout threshold You can set the login lockout threshold with the command:\nrladmin tune cluster login_lockout_threshold \u0026lt;login_lockout_threshold\u0026gt; For example, to set the lockout threshold to 10 failed login attempts, run:\nrladmin tune cluster login_lockout_threshold 10 If you set the lockout threshold to 0, it turns off account lockout. In this case, the cluster settings show login_lockout_threshold: disabled.\nChange the login lockout counter You can set the login lockout reset counter in seconds with the command:\nrladmin tune cluster login_lockout_counter_reset_after \u0026lt;login_lockout_counter_reset_after\u0026gt; To set the lockout reset to 1 hour, run:\nrladmin tune cluster login_lockout_counter_reset_after 3600 Change the login lockout duration You can set the login lockout duration in seconds with the command:\nrladmin tune cluster login_lockout_duration \u0026lt;login_lockout_duration\u0026gt; For example, to set the lockout duration to 1 hour, run:\nrladmin tune cluster login_lockout_duration 3600 If you set the lockout duration to 0, then the account can be unlocked only when an administrator changes the account\u0026rsquo;s password. In this case, the cluster settings show login_lockout_duration: admin-release.\nUnlock locked user accounts To unlock a user account or reset a user password with rladmin, run:\nrladmin cluster reset_password \u0026lt;user email\u0026gt; To unlock a user account or reset a user password with the REST API, use PUT /v1/users:\nPUT https://[host][:port]/v1/users \u0026#39;{\u0026#34;password\u0026#34;: \u0026#34;\u0026lt;new_password\u0026gt;\u0026#34;}\u0026#39; Session timeout The Redis Enterprise admin console supports session timeouts. By default, users are automatically logged out after 15 minutes of inactivity.\nTo customize the session timeout, run:\nrladmin cluster config cm_session_timeout_minutes \u0026lt;number_of_min\u0026gt; The number_of_min is the number of minutes after which sessions will time out.\n","categories":["RS"]},{"uri":"/rc/security/multi-factor-authentication/","uriRel":"/rc/security/multi-factor-authentication/","title":"Multi-factor authentication","tags":[],"keywords":[],"description":"","content":"To reduce the chances of unauthorized access, Redis Enterprise Cloud allows users to enable multi-factor authentication (MFA).\nWhen MFA is enabled, users must enter their username, password, and an authentication code when signing in. MFA requires a mobile device that can receive these authentication codes over text messaging. In addition, you may use an authenticator app such as Google Authenticator as one of your factors.\nTo further increase the security of the account, the account owner can require MFA enforcement for all users.\nNote: Once you enable MFA, MFA will be required to access every account that belongs to you. Deactivating MFA enforcement on an account will not deactivate MFA enforcement for other users of that account that have defined a phone number. In order to deactivate MFA enforcement for other users, those users will each need to deactivate MFA enforcement for their own user accounts. Enable MFA Each user can enable and configure MFA for their account. The default MFA configuration sends an authentication code by text message that users must enter when they sign in.\nTo configure MFA for your user account:\nSign in to your account.\nFrom the Redis Enterprise Cloud menu, select your name to enter the User Profile view.\nUnder your user profile, locate Multi-factor authentication.\nEnter your mobile phone number in the Text message box and then select Send code.\nYou will receive a confirmation code sent by text message. Enter the code when prompted by the admin console and select Verify.\nYour account is now configured for MFA.\nNote: We recommend that you also configure MFA for an authenticator app as an additional factor. If you cannot sign in to your account because of MFA, please contact support.\nIf your mobile phone is lost or stolen, make sure that you update the MFA configuration to prevent unauthorized sign-ins.\nChange your MFA phone number To change the mobile phone number used for MFA:\nNavigate to the Multi-Factor Authentication view. Select Change number. Enter the new mobile phone number, and complete the verification process as described above. Configure MFA for an authenticator app After you configure MFA for text messages, you can also configure MFA to work with a Time-based One-Time Password (TOTP) app such as Google Authenticator.\nWhen you sign in to the Redis Cloud Admin Console, you can select either an authentication code sent by text message or an authentication code shown in your authenticator app.\nTo configure MFA for an authenticator app:\nInstall an authenticator app on your mobile phone. Add Redis Cloud to the app: From the User Profile view in your Redis Cloud account, locate Multi-Factor Authentication. Select Connect for the authenticator app. A QR code will appear on screen requesting verification. Scan the QR code using your phone\u0026rsquo;s authenticator app. Enter the code generated by your authenticator app to verify the setup. You can now use either a text message code or an authenticator app code as your second factor when signing in.\nDeactivate MFA You can deactivate MFA for your user account. To deactivate MFA, go to your profile, locate Multi-Factor Authentication, and select Deactivate.\nEnforce MFA for all user accounts Account owner users can enable MFA enforcement for all users in their account. After MFA is enforced for the account, all users who do not have MFA enabled will be required to configure MFA the next time they sign in to the Redis Cloud Admin Console.\nTo enable MFA enforcement for all user accounts:\nSign in as an account owner. Go to Settings \u0026gt; Account. Under MFA enforcement, select the toggle. When you enable MFA enforcement, users cannot disable MFA for their account. When you disable MFA enforcement, users can disable MFA for their account. Tip - We recommend that you notify all of your Redis Cloud Admin Console users before enabling MFA enforcement. ","categories":["RC"]},{"uri":"/rs/references/rest-api/objects/","uriRel":"/rs/references/rest-api/objects/","title":"Redis Enterprise REST API objects","tags":[],"keywords":[],"description":"Documents the objects used with Redis Enterprise Software REST API calls.","content":"Certain REST API requests require you to include specific objects in the request body. Many requests also return objects in the response body.\nBoth REST API requests and responses represent these objects as JSON.\nObject Description action An object that represents cluster actions alert An object that contains alert info bdb An object that represents a database bdb_group An object that represents a group of databases with a shared memory pool bootstrap An object for bootstrap configuration check_result An object that contains the results of a cluster check cluster An object that represents a cluster cluster_settings An object for cluster resource management settings crdb An object that represents an Active-Active database crdb_task An object that represents a CRDB task db_alerts_settings An object for database alerts configuration db_conns_auditing_config An object for database connection auditing settings job_scheduler An object for job scheduler settings jwt_authorize An object for user authentication or a JW token refresh request ldap An object that contains the cluster\u0026#39;s LDAP configuration ldap_mapping An object that represents a mapping between an LDAP group and roles module An object that represents a Redis module node An object that represents a node in the cluster ocsp An object that represents the cluster\u0026#39;s OCSP configuration ocsp_status An object that represents the cluster\u0026#39;s OCSP status proxy An object that represents a proxy in the cluster redis_acl An object that represents a Redis access control list (ACL) role An object that represents a role services_configuration An object for optional cluster services settings shard An object that represents a database shard state-machine An object that represents a state machine. statistics An object that contains metrics for clusters, databases, nodes, or shards suffix An object that represents a DNS suffix user An object that represents a Redis Enterprise user ","categories":["RS"]},{"uri":"/rs/installing-upgrading/offline-installation/","uriRel":"/rs/installing-upgrading/offline-installation/","title":"Offline Installation","tags":[],"keywords":[],"description":"","content":"By default, the installation process requires an Internet connection to enable installing dependency packages and for synchronizing the operating system clock against an NTP server.\nIf you install Redis Enterprise Software (RS) on a machine with no Internet connection, you need to perform these two tasks manually, as follows:\nFirst, you need to install the required dependency packages. When RS is installed on a machine that is not connected to the Internet, the installation process fails and displays an error message informing you it failed to automatically install dependencies. Review the installation steps in the console to see which missing dependencies the process attempted to install. Install all these dependency packages and then run the installation again.\nAt the end of the installation, the process asks you whether you would like to set up NTP time synchronization. If you choose \u0026ldquo;Yes\u0026rdquo; while you are not connected to the Internet, the action fails and displays the appropriate error message, but the installation completes successfully. Despite the successful completion of the installation, you still have to configure all nodes for NTP time synchronization.\n","categories":["RS"]},{"uri":"/kubernetes/memory/persistent-volumes/","uriRel":"/kubernetes/memory/persistent-volumes/","title":"Use persistent volumes in Redis Enterprise clusters","tags":[],"keywords":[],"description":"This section covers details about how persistent volumes are sized and specified for Redis Enterprise cluster deployments.","content":"To deploy a Redis Enterprise cluster with Redis Enterprise operator the spec should include a persistentSpec section, in the redis-enterprise-cluster.yaml file:\nspec: nodes: 3 persistentSpec: enabled: true storageClassName: \u0026quot;standard\u0026quot; volumeSize: \u0026quot;23Gi” #optional Persistence storage is a requirement for this deployment type.\nNote: For production deployments of Redis Enterprise Cluster on Kubenetes, the Redis Enterprise Cluster (REC) must be deployed with persistence enabled. The REC deployment files in the Kubernetes documentation contain this declaration by default. Volume size volumeSize is an optional definition. By default, if the definition is omitted, operator allocates five times (5x) the amount of memory (RAM) defined for nodes (see example below), which is the recommended persistent storage size as described in the Hardware requirements article.\nTo explicitly specify the persistent storage size, use the volumeSize property as described in the example above.\nWarning - Be aware the persistent volume size cannot be changed after deployment. Trying to change this value after deployment could result in unexpected and potentially damaging behavior. Please be sure your specified volumeSize is correct at the time of creation. Note: We recommend that you omit the volumeSize definition from the REC declaration so that the Redis Enterprise Cluster deployment on Kubenetes use the default volume size. Storage class name storageClassName determines the Storage Class resource, which is defined by the Kubernetes cluster administrator, to be used for persistent storage.\nDifferent Kubernetes distributions and different deployments use different Storage Class resources.\nIn order to determine the Storage Class resources available for your K8s deployment, use the following command:\nkubectl get StorageClass Typically, AWS provides “gp2” as the Storage Class name while GKE uses “standard.” Azure provides two Storage Classes: \u0026ldquo;default\u0026rdquo; using HDDs, and \u0026ldquo;managed-premium\u0026rdquo; using SSDs.\nBelow is an example of a response to the command.\nName: gp2 IsDefaultClass: Yes Annotations: storageclass.beta.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: encrypted=false,kmsKeyId=,type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; Note: storageClassName must be specified for this deployment type. Warning - The storage class cannot be changed after deployment. Trying to change this value after deployment could result in unexpected and potentially damaging behavior. Example of the redisEnterpriseNodeResources definition:\nredisEnterpriseNodeResources: limits: cpu: “4000m” memory: 4Gi requests: cpu: “4000m” memory: 4Gi ","categories":["Platforms"]},{"uri":"/rs/databases/configure/proxy-policy/","uriRel":"/rs/databases/configure/proxy-policy/","title":"Configure proxy policy","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) provides high-performance data access through a proxy process that manages and optimizes access to shards within the RS cluster. Each node contains a single proxy process. Each proxy can be active and take incoming traffic or it can be passive and wait for failovers.\nProxy policies A database can have one of these proxy policies:\nProxy Policy Description Single There is only a single proxy that is bound to the database. This is the default database configuration and preferable in most use cases. All Master Shards There are multiple proxies that are bound to the database, one on each node that hosts a database master shard. This mode fits most use cases that require multiple proxies. All Nodes There are multiple proxies that are bound to the database, one on each node in the cluster, regardless of whether or not there is a shard from this database on the node. This mode should be used only in special cases. Note: Manual intervention is also available via the rladmin bind add and remove commands. Database configuration A database can be configured with a proxy policy using rladmin bind.\nWarning: Any configuration update which causes existing proxies to be unbounded can cause existing client connections to get disconnected.\nYou can run rladmin to control and view the existing settings for proxy configuration.\nThe info command on cluster returns the existing proxy policy for sharded and non-sharded (single shard) databases.\n$ rladmin info cluster cluster configuration: repl_diskless: enabled default_non_sharded_proxy_policy: single default_sharded_proxy_policy: single default_shards_placement: dense default_shards_overbooking: disabled default_fork_evict_ram: enabled default_redis_version: 3.2 redis_migrate_node_threshold: 0KB (0 bytes) redis_migrate_node_threshold_percent: 8 (%) redis_provision_node_threshold: 0KB (0 bytes) redis_provision_node_threshold_percent: 12 (%) max_simultaneous_backups: 4 watchdog profile: local-network You can configure the proxy policy using the bind command in rladmin. The following command is an example that changes the bind policy for a database called \u0026ldquo;db1\u0026rdquo; with an endpoint id \u0026ldquo;1:1\u0026rdquo; to \u0026ldquo;All Master Shards\u0026rdquo; proxy policy.\nrladmin bind db db1 endpoint 1:1 policy all-master-shards Note: You can find the endpoint id for the endpoint argument by running status command for rladmin. Look for the endpoint id information under the ENDPOINT section of the output. Reapply policies after topology changes If you want to reapply the policy after topology changes, such as node restarts, failovers and migrations, run this command to reset the policy:\nrladmin bind db \u0026lt;db_name\u0026gt; endpoint \u0026lt;endpoint id\u0026gt; policy \u0026lt;all-master-shards|all-nodes\u0026gt; This is not required with single policies.\nOther implications During the regular operation of the cluster different actions might take place, such as automatic migration or automatic failover, which change what proxy needs to be bound to what database. When such actions take place the cluster attempts, as much as possible, to automatically change proxy bindings to adhere to the defined policies. That said, the cluster attempts to prevent any existing client connections from being disconnected, and hence might not entirely enforce the policies. In such cases, you can enforce the policy using the appropriate rladmin commands.\nAbout multiple active proxy support RS allows multiple databases to be created. Each database gets an endpoint (a unique URL and port on the FQDN). This endpoint receives all the traffic for all operations for that database. By default, RS binds this database endpoint to one of the proxies on a single node in the cluster. This proxy becomes an active proxy and receives all the operations for the given database. (note that if the node with the active proxy fails, a new proxy on another node takes over as part of the failover process automatically).\nIn most cases, a single proxy can handle a large number of operations without consuming additional resources. However, under high load, network bandwidth or a high rate of packets per second (PPS) on the single active proxy can become a bottleneck to how fast database operation can be performed. In such cases, having multiple active proxies, across multiple nodes, mapped to the same external database endpoint, can significantly improve throughput.\nWith the multiple active proxies capability, RS enables you to configure a database to have multiple internal proxies in order to improve performance, in some cases. It is important to note that, even though multiple active proxies can help improve the throughput of database operations, configuring multiple active proxies may cause additional latency in operations as the shards and proxies are spread across multiple nodes in the cluster.\nNote: When the network on a single active proxy becomes the bottleneck, you might also look into enabling the multiple NIC support in RS. With nodes that have multiple physical NICs (Network Interface Cards), you can configure RS to separate internal and external traffic onto independent physical NICs. For more details, refer to Multi-IP \u0026amp; IPv6. Having multiple proxies for a database can improve RS\u0026rsquo;s ability for fast failover in case of proxy and/or node failure. With multiple proxies for a database, there is no need for a client to wait for the cluster to spin up another proxy and a DNS change in most cases, the client just uses the next IP in the list to connect to another proxy.\n","categories":["RS"]},{"uri":"/rs/references/client_references/client_python/","uriRel":"/rs/references/client_references/client_python/","title":"Redis with Python","tags":[],"keywords":[],"description":"The redis-py client allows you to use Redis with Python.","content":"To use Redis with Python, you need a Python Redis client. The following sections demonstrate the use of redis-py, a Redis Python Client. Additional Python clients for Redis can be found under the Python section of the Redis Clients page.\nInstall redis-py See redis-py\u0026rsquo;s README file for installation instructions.\nUse pip to install redis-py:\npip install redis You can also download the latest redis-py release from the GitHub repository. To install it, extract the source and run the following commands:\n$ cd redis-py $ python setup.py install Connect to Redis The following code creates a connection to Redis using redis-py:\nimport redis r = redis.Redis( host=\u0026#39;hostname\u0026#39;, port=port, password=\u0026#39;password\u0026#39;) To adapt this example to your code, replace the following values with your database\u0026rsquo;s values:\nIn line 4, set host to your database\u0026rsquo;s hostname or IP address In line 5, set port to your database\u0026rsquo;s port In line 6, set password to your database\u0026rsquo;s password Example code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n# open a connection to Redis ... r.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;) value = r.get(\u0026#39;foo\u0026#39;) print(value) Example output:\n$ python example_redis-py.py bar Connection pooling The redis-py client pools connections by default. No special actions are required to use it.\nSSL The redis-py client natively supports SSL.\nUse the SSLConnection class or instantiate your connection pool using a rediss:// URL and the from_url method, like so:\nr = redis.Redis.from_url( url=\u0026#39;rediss://:password@hostname:port/0\u0026#39;, password=\u0026#39;password\u0026#39;, ssl_keyfile=\u0026#39;path_to_keyfile\u0026#39;, ssl_certfile=\u0026#39;path_to_certfile\u0026#39;, ssl_cert_reqs=\u0026#39;required\u0026#39;, ssl_ca_certs=\u0026#39;path_to_ca_cert\u0026#39;) ","categories":["RS"]},{"uri":"/modules/redisgraph/","uriRel":"/modules/redisgraph/","title":"RedisGraph","tags":[],"keywords":[],"description":"","content":"RedisGraph is the first queryable Property Graph database to use sparse matrices to represent the adjacency matrix in graphs and linear algebra to query the graph.\nPrimary features:\nBased on the Property Graph Model Nodes (vertices) and Relationships (edges) that may have attributes Nodes that can be labeled Relationships have a relationship type Graphs represented as sparse adjacency matrices Cypher as query language Cypher queries translated into linear algebra expressions More info RedisGraph commands RedisGraph configuration RedisGraph source ","categories":["Modules"]},{"uri":"/rs/databases/durability-ha/replication/","uriRel":"/rs/databases/durability-ha/replication/","title":"Database replication","tags":[],"keywords":[],"description":"","content":"Database replication helps ensure high availability. When replication is enabled, your dataset is replicated to a replica shard, which is constantly synchronized with the primary shard. If the primary shard fails, an automatic failover happens and the replica shard is promoted. That is, it becomes the new primary shard.\nWhen the old primary shard recovers, it becomes the replica shard of the new primary shard. This auto-failover mechanism guarantees that data is served with minimal interruption.\nYou can tune your high availability configuration with:\nRack/Zone Awareness - When rack-zone awareness is used additional logic ensures that master and replica shards never share the same rack, thus ensuring availability even under loss of an entire rack. High Availability for Replica Shards - When high availability for replica shards is used, the replica shard is automatically migrated on node failover to maintain high availability. Warning - Enabling replication has implications for the total database size, as explained in Database memory limits. Redis on Flash replication considerations We recommend that you set the sequential replication feature using rladmin. This is due to the potential for relatively slow replication times that can occur with Redis on Flash enabled databases. In some cases, if sequential replication is not set up, you may run out of memory.\nWhile it does not cause data loss on the primary shards, the replication to replica shards may not succeed as long as there is high write-rate traffic on the primary and multiple replications at the same time.\nThe following rladmin command sets the number of primary shards eligible to be replicated from the same cluster node, as well as the number of replica shards on the same cluster node that can run the replication process at any given time.\nThe recommended sequential replication configuration is two, i.e.:\nrladmin tune cluster max_redis_forks 1 max_slave_full_syncs 1 Note: This means that at any given time, only one primary and one replica can be part of a full sync replication process. Database replication backlog Redis databases that use replication for high availability maintain a replication backlog (per shard) to synchronize the primary and replica shards of a database. By default, the replication backlog is set to one percent (1%) of the database size divided by the database number of shards and ranges between 1MB to 250MB per shard. Use the rladmin and the crdb-cli utilities to control the size of the replication backlog. You can set it to auto or set a specific size.\nThe syntax varies between regular and Active-Active databases.\nFor a regular Redis database:\nrladmin tune db \u0026lt;db:id | name\u0026gt; repl_backlog \u0026lt;Backlog size in MB | \u0026#39;auto\u0026#39;\u0026gt; For an Active-Active database:\ncrdb-cli crdb update --crdb-guid \u0026lt;crdb_guid\u0026gt; --default-db-config \u0026#34;{\\\u0026#34;repl_backlog_size\\\u0026#34;: \u0026lt;size in MB | \u0026#39;auto\u0026#39;\u0026gt;}\u0026#34; Active-Active replication backlog In addition to the database replication backlog, Active-Active databases maintain a backlog (per shard) to synchronize the database instances between clusters. By default, the Active-Active replication backlog is set to one percent (1%) of the database size divided by the database number of shards, and ranges between 1MB to 250MB per shard. Use the crdb-cli utility to control the size of the CRDT replication backlog. You can set it to auto or set a specific size:\ncrdb-cli crdb update --crdb-guid \u0026lt;crdb_guid\u0026gt; --default-db-config \u0026#34;{\\\u0026#34;crdt_repl_backlog_size\\\u0026#34;: \u0026lt;size in MB | \u0026#39;auto\u0026#39;\u0026gt;}\u0026#34; For Redis Software versions earlier than 6.0.20: The replication backlog and the CRDT replication backlog defaults are set to 1MB and cannot be set dynamically with \u0026lsquo;auto\u0026rsquo; mode. To control the size of the replication log, use rladmin to tune the local database instance in each cluster.\nrladmin tune db \u0026lt;db:id | name\u0026gt; repl_backlog \u0026lt;Backlog size in MB (or if ending with bytes, KB or GB, in the respective unit)\u0026gt; ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/commands/rg-jdumpsessions/","uriRel":"/modules/redisgears/jvm/commands/rg-jdumpsessions/","title":"RG.JDUMPSESSIONS","tags":[],"keywords":[],"description":"Outputs information about existing Java sessions.","content":"RG.JDUMPSESSIONS [VERBOSE] [SESSIONS s1 s2 ...] Outputs information about existing Java sessions.\nNote: When you run the RG.JEXECUTE command, it creates a Java session. Arguments Name Description VERBOSE Output more details about registrations. SESSIONS Only output information about sessions that appears in the given list. Can only be the last argument. Returns Returns information about existing Java sessions.\nExamples Get information for all sessions:\n\u0026gt; redis-cli RG.JDUMPSESSIONS 1) 1) \u0026#34;mainClass\u0026#34; 2) \u0026#34;gears_experiments.test\u0026#34; 3) \u0026#34;version\u0026#34; 4) (integer) 1 5) \u0026#34;description\u0026#34; 6) \u0026#34;foo\u0026#34; 7) \u0026#34;upgradeData\u0026#34; 8) (nil) 9) \u0026#34;jar\u0026#34; 10) \u0026#34;/home/user/work/RedisGears/plugins/jvmplugin/-jars/6876b8b78ccfc2ad764edc7ede590f573bd7260b.jar\u0026#34; 11) \u0026#34;refCount\u0026#34; 12) (integer) 2 13) \u0026#34;linked\u0026#34; 14) \u0026#34;true\u0026#34; 15) \u0026#34;ts\u0026#34; 16) \u0026#34;false\u0026#34; 17) \u0026#34;registrations\u0026#34; 18) 1) \u0026#34;0000000000000000000000000000000000000000-1\u0026#34; Get more detailed information about a specific session:\n\u0026gt; redis-cli RG.JDUMPSESSIONS VERBOSE SESSIONS gears_experiments.test 1) 1) \u0026#34;mainClass\u0026#34; 2) \u0026#34;gears_experiments.test\u0026#34; 3) \u0026#34;version\u0026#34; 4) (integer) 1 5) \u0026#34;description\u0026#34; 6) \u0026#34;foo\u0026#34; 7) \u0026#34;upgradeData\u0026#34; 8) (nil) 9) \u0026#34;jar\u0026#34; 10) \u0026#34;/home/user/work/RedisGears/plugins/jvmplugin/-jars/6876b8b78ccfc2ad764edc7ede590f573bd7260b.jar\u0026#34; 11) \u0026#34;refCount\u0026#34; 12) (integer) 2 13) \u0026#34;linked\u0026#34; 14) \u0026#34;true\u0026#34; 15) \u0026#34;ts\u0026#34; 16) \u0026#34;false\u0026#34; 17) \u0026#34;registrations\u0026#34; 18) 1) 1) \u0026#34;id\u0026#34; 2) \u0026#34;0000000000000000000000000000000000000000-1\u0026#34; 3) \u0026#34;reader\u0026#34; 4) \u0026#34;CommandReader\u0026#34; 5) \u0026#34;desc\u0026#34; 6) (nil) 7) \u0026#34;RegistrationData\u0026#34; 8) 1) \u0026#34;mode\u0026#34; 2) \u0026#34;async\u0026#34; 3) \u0026#34;numTriggered\u0026#34; 4) (integer) 0 5) \u0026#34;numSuccess\u0026#34; 6) (integer) 0 7) \u0026#34;numFailures\u0026#34; 8) (integer) 0 9) \u0026#34;numAborted\u0026#34; 10) (integer) 0 11) \u0026#34;lastRunDurationMS\u0026#34; 12) (integer) 0 13) \u0026#34;totalRunDurationMS\u0026#34; 14) (integer) 0 15) \u0026#34;avgRunDurationMS\u0026#34; 16) \u0026#34;-nan\u0026#34; 17) \u0026#34;lastError\u0026#34; 18) (nil) 19) \u0026#34;args\u0026#34; 20) 1) \u0026#34;trigger\u0026#34; 2) \u0026#34;test\u0026#34; 3) \u0026#34;inorder\u0026#34; 4) (integer) 0 9) \u0026#34;ExecutionThreadPool\u0026#34; 10) \u0026#34;JVMPool\u0026#34; ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/commands/rg-jexecute/","uriRel":"/modules/redisgears/jvm/commands/rg-jexecute/","title":"RG.JEXECUTE","tags":[],"keywords":[],"description":"Executes a Java function.","content":"RG.JEXECUTE \u0026lt;path.to.main.class\u0026gt; [UPGRADE] \u0026lt;JAR file\u0026gt; Executes a Java function.\nThe code runs immediately if it uses GearsBuilder.run(). Code that uses GearsBuilder.register() will run later, every time certain events occur in the database.\nArguments Name Description path.to.main.class The path to the main class in the JAR JAR file A JAR file that contains the RedisGears code to run or register UPGRADE Upgrades registered code to a new version Returns If the executed code calls GearsBuilder.run(), it returns the output of the executed code.\nFor registered code, it returns the string \u0026ldquo;OK\u0026rdquo; instead.\nExamples The executed code in this example runs immediately:\n$ redis-cli -x RG.JEXECUTE com.domain.packagename.Reviews \u0026lt; /tmp/rgjvmtest-0.0.1-SNAPSHOT.jar 1) 1) \u0026#34;3.6666666666666665\u0026#34; 2) (empty array) This example registers the RedisGears code to run every time certain database events occur:\n$ redis-cli -x RG.JEXECUTE com.domain.packagename.App \u0026lt; /tmp/rgjvmtest-0.0.1-SNAPSHOT.jar OK Here\u0026rsquo;s an example of how to upgrade registered code to a new version:\n$ redis-cli -x RG.JEXECUTE com.domain.packagename.App UPGRADE \u0026lt; /tmp/rgjvmtest-0.0.2-SNAPSHOT.jar OK ","categories":["Modules"]},{"uri":"/rs/databases/import-export/schedule-backups/","uriRel":"/rs/databases/import-export/schedule-backups/","title":"Schedule periodic backups","tags":[],"keywords":[],"description":"You can manually export your data from a specific Redis Enterprise Software database at any time. You can also schedule backups of your databases to make sure you always have valid backups.","content":"Periodic backups provide a way to restore data with minimal data loss. With Redis Enterprise Software, you can schedule periodic backups to occur once a day (every 24 hours), twice a day (every twelve hours), every four hours, or every hour.\nAs of v6.2.8, you can specify the start time for twenty-four or twelve hour backups.\nTo make an on-demand backup, export your data.\nYou can schedule backups to a variety of locations, including:\nFTP server SFTP server Local mount point Amazon Simple Storage Service (S3) Azure Blob Storage Google Cloud Storage The backup process creates compressed (.gz) RDB files that you can import into a database.\nWhen you back up a database configured for database clustering, Redis Enterprise Software creates a backup file for each shard in the configuration. All backup files are copied to the storage location.\nNote: Make sure that you have enough space available in your storage location. If there is not enough space in the backup location, the backup fails. The backup configuration only applies to the database it is configured on. Schedule periodic backups Before scheduling periodic backups, verify that your storage location exists and is available to the user running Redis Enterprise Software (redislabs by default). You should verify that:\nPermissions are set correctly The user running Redis Enterprise Software is authorized to access the storage location The authorization credentials work Storage location access is verified before periodic backups are scheduled.\nTo schedule periodic backups for a database:\nSign in to the Redis Enterprise Software admin console using admin credentials.\nFrom the admin console, choose Databases and then select your database.\nSelect the Edit button.\nLocate and enable the Periodic backup checkbox.\nUse the following table to help specify the details:\nSetting Description Interval Specifies the frequency of the backup; that is, the time between each backup snapshot.Supported values include Every 24 hours, Every 12 hours, Every 4 hours, and Every hour. Set starting time v6.2.8 or later: Specifies the start time for the backup; available when Interval is set to Every 24 hours or Every 12 hours.If not specified, defaults to a time selected by Redis Enterprise Software. Choose storage type Specifies the storage type for the backup. Supported options vary and might require additional details. To learn more, see Supported storage locations. Select Update to apply your changes.\nAccess to the storage location is verified when you apply your updates. This means the location, credentials, and other details must exist and function before you can enable periodic backups.\nDefault backup start time If you do not specify a start time for twenty-four or twelve hour backups, Redis Enterprise Software chooses a random starting time for you.\nThis choice assumes that your database is deployed to a multi-tenant cluster containing multiple databases. This means that default start times are staggered (offset) to ensure availability. This is done by calculating a random offset which specifies a number of seconds added to the start time.\nHere\u0026rsquo;s how it works:\nAssume you\u0026rsquo;re enabling the backup at 4:00 pm (1600 hours). You choose to back up your database every 12 hours. Because you didn\u0026rsquo;t set a start time, the cluster randomly chooses an offset of 4,320 seconds (or 72 minutes). This means your first periodic backup occurs 72 minutes after the time you enabled periodic backups (4:00 pm + 72 minutes). Backups repeat every twelve hours at roughly same time.\nThe backup time is imprecise because they\u0026rsquo;re started by a trigger process that runs every five minutes. When the process wakes, it compares the current time to the scheduled backup time. If that time has passed, it triggers a backup.\nIf the previous backup fails, the trigger process retries the backup until it succeeds.\nIn addition, throttling and resource limits also affect backup times.\nFor help with specific backup issues, contact support.\nSupported storage locations Database backups can be saved to a local mount point, transferred to a URI using FTP/SFTP, or stored on cloud provider storage.\nWhen saved to a local mount point or a cloud provider, backup locations need to be available to the group and user running Redis Enterprise Software, redislabs:redislabs by default.\nRedis Enterprise Software needs the ability to view permissions and update objects in the storage location. Implementation details vary according to the provider and your configuration. To learn more, consult the provider\u0026rsquo;s documentation.\nThe following sections provide general guidelines. Because provider features change frequently, use your provider\u0026rsquo;s documentation for the latest info.\nFTP server Before enabling backups to an FTP server, verify that:\nYour Redis Enterprise cluster can connect and authenticate to the FTP server. The user specified in the FTP server location has read and write privileges. To store your backups on an FTP server, set its Backup Path using the following syntax:\nftp://[username]:[password]@[host]:[port]/[path]/\nWhere:\nprotocol: the server\u0026rsquo;s protocol, can be either ftp or ftps. username: your username, if needed. password: your password, if needed. hostname: the hostname or IP address of the server. port: the port number of the server, if needed. path: the backup path, if needed. Example: ftp://username:password@10.1.1.1/home/backups/\nThe user account needs permission to write files to the server.\nSFTP server Before enabling backups to an SFTP server, make sure that:\nYour Redis Enterprise cluster can connect and authenticate to the SFTP server.\nThe user specified in the SFTP server location has read and write privileges.\nThe SSH private keys are specified correctly. You can use the key generated by the cluster or specify a custom key.\nWhen using the cluster auto generated key, copy the Cluster SSH Public Key to the appropriate location on the SFTP server. This is available from the General tab of the Settings menu in the admin console.\nUse the server documentation to determine the appropriate location for the SSH Public Key.\nTo backup to an SFTP server, enter the SFTP server location in the format:\nsftp://user:password@host\u0026lt;:custom_port\u0026gt;/path/ For example: sftp://username:password@10.1.1.1/home/backups/\nLocal mount point Before enabling periodic backups to a local mount point, verify that:\nThe node can connect to the destination server, the one hosting the mount point. The redislabs:redislabs user has read and write privileges on the local mount point and on the destination server. The backup location has enough disk space for your backup files. Backup files are saved with filenames that include the timestamp, which means that earlier backups are not overwritten. To back up to a local mount point:\nOn each node in the cluster, create the mount point:\nConnect to a shell running on Redis Enterprise Software server hosting the node.\nMount the remote storage to a local mount point.\nFor example:\nsudo mount -t nfs 192.168.10.204:/DataVolume/Public /mnt/Public In the path for the backup location, enter the mount point.\nFor example: /mnt/Public\nVerify that the user running Redis Enterprise Software has permissions to access and update files in the mount location.\nAWS Simple Storage Service To store backups in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket:\nSign in to the AWS Management Console.\nCreate an S3 bucket if you do not already have one.\nCreate an IAM User with permission to add objects to the bucket.\nCreate an access key for that user if you do not already have one.\nIn the Redis Enterprise Software admin console, when you enter the backup location details:\nSelect \u0026ldquo;AWS S3\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Access key ID field, enter the access key ID.\nIn the Secret access key field, enter the secret access key.\nGCP Storage For Google Cloud Platform (GCP) console subscriptions, store your backups in a Google Cloud Storage bucket:\nSign in to Google Cloud Platform console.\nCreate a JSON service account key if you do not already have one.\nCreate a bucket if you do not already have one.\nAdd a principal to your bucket:\nIn the New principals field, add the client_email from the service account key.\nSelect \u0026ldquo;Storage Legacy Bucket Writer\u0026rdquo; from the Role list.\nIn the Redis Enterprise Software admin console, when you enter the backup location details:\nSelect \u0026ldquo;Google Cloud Storage\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Client id field, enter the client_id from the service account key.\nIn the Client email field, enter the client_email from the service account key.\nIn the Private key id field, enter the private_key_id from the service account key.\nIn the Private key field, enter the private_key from the service account key. Replace \\n with new lines, and then select the Save icon.\nAzure Blob Storage To store your backup in Microsoft Azure Blob Storage, sign in to the Azure portal and then:\nTo export to Microsoft Azure Blob Storage, sign in to the Azure portal and then:\nCreate an Azure Storage account if you do not already have one.\nCreate a container if you do not already have one.\nManage storage account access keys to find the storage account name and account keys.\nIn the Redis Enterprise Software admin console, when you enter the backup location details:\nSelect \u0026ldquo;Azure Blob Storage\u0026rdquo; from the Choose storage type drop-down.\nIn the Path field, enter the path of your bucket.\nIn the Account name field, enter your storage account name.\nIn the Account key field, enter the storage account key.\nTo learn more, see Authorizing access to data in Azure Storage.\n","categories":["RS"]},{"uri":"/rc/api/get-started/use-rest-api/","uriRel":"/rc/api/get-started/use-rest-api/","title":"Use the REST API","tags":[],"keywords":[],"description":"How to use the API with various tools (especially `cURL`)","content":"You can access and use the API endpoint URI (https://api.redislabs.com/v1) with any of the following tools:\nThe Swagger user interface The cURL HTTP client An HTTP client in any programming language Swagger user interface The Swagger UI is useful for initial introduction and for learning about API operations, models, and simulated usage.\nAuthenticate to Swagger To authenticate to the Swagger UI:\nOpen the Swagger UI page in a browser.\nSelect Authorize.\nThe Available Authorizations box is shown with the headers and values that are used for authentication in all API interactions with Swagger.\nInsert the API Key values:\nEnter the Account Key as the x-api-key value and then choose Authorize. Enter the Secret Key as the x-api-secret-key value and then choose Authorize. Select Close. Note: The key values are not saved when you refresh the page. When authorization is successful, the lock icon displays a closed lock.\nMake API requests After you complete the authorization in the Swagger UI, you can make an API request:\nOpen an action category and select an API operation.\nFor example, in the Account category select the GET /payment-methods operation.\nSelect Try it out and then select Execute.\nThe API response is shown in the Responses section of the API operation. The results include an example of how to execute the same operation in a standard command-line utility using cURL.\nInputs for operations in Swagger Some API operations require input, such as:\nParameters - When an API operation requires URI parameters, such as \u0026ldquo;get subscription by subscription id\u0026rdquo;, you can enter the values for the parameters.\nJSON Request Body - For API operations that require a JSON request body, you can either:\nUse the model display to write the request based on the expected JSON structure and parameters.\nUse the Try it now sample JSON created by Swagger as a base template that you can edit and execute.\nWarning - The Swagger UI generates default JSON examples for POST and PUT operations. You should modify these examples to suit your specific needs and account settings. The examples will fail if used as-is. For more examples showing how to use specific endpoints, see REST API examples. Use the cURL HTTP client cURL is a popular command line tool used to perform HTTP requests, either as individual commands or within shell scripts (such as bash and zsh). For an introduction, see How to start using cURL and why: a hands-on introduction.\nFYI - Our examples use cURL and Linux shell scripts to demonstrate the API; you can use any standard REST client or library. Our examples also use jq, a JSON parser. Use your package manager to install it (Example: sudo apt install jq) For example, a standard API call to get System Log information looks like this in cURL:\ncurl -s -X GET \u0026#34;https://$HOST/logs\u0026#34; \\ -H \u0026#34;accept: application/json\u0026#34; \\ -H \u0026#34;x-api-key: $ACCOUNT_KEY\u0026#34; \\ -H \u0026#34;x-api-secret-key: $SECRET_KEY\u0026#34; \\ | jq -r . The example expects several variables to be set in the Linux shell:\n$HOST - The URI of the REST API host (api.redislabs.com/v1) $ACCOUNT_KEY - The account key value $SECRET_KEY - The personal secret key value The line \u0026ldquo;| jq -r .\u0026rdquo; means that the HTTP response will be piped (forwarded) to the jq JSON parser, and it will display only the raw output (\u0026quot;-r\u0026quot;) of the root element (\u0026quot;.\u0026quot;)\nYou can set the variables using shell commands like the following:\nexport HOST=api.redislabs.com/v1 export ACCOUNT_KEY={replace-with-your-account-key} export SECRET_KEY={replace-with-your-secret-key} ","categories":["RC"]},{"uri":"/rc/subscriptions/view-fixed-subscription/","uriRel":"/rc/subscriptions/view-fixed-subscription/","title":"View or change a Fixed subscription","tags":[],"keywords":[],"description":"","content":"To view the details of a Fixed subscription:\nSign in to the admin console.\nIf you have more than one subscription, select the target subscription from the subscription list.\nYour subscription details appear, along with a summary of your database details.\nFrom here, you can:\nSelect the Change Plan button to update the subscription plan tier, high availability settings, or payment method.\nSelect the New Database button to create a new database for your subscription.\nSelect the Overview tab to view and edit subscription details.\nThe following sections provide more details.\nChange subscription plan Use the Change plan button to update your Fixed subscription tier, your high availability settings, or your payment method.\nChange subscription tier To change your subscription tier, select the desired tier from the list:\nEach tier in a Fixed plan provides a variety of benefits, including increased memory, number of databases, connections, and so on.\nFor a comparison of available tiers, see Fixed size subscription tiers.\nWhen you change your plan tier, your data and endpoints are not disrupted.\nIf you upgrade a free plan to a paid tier, you need to add a payment method.\nIf you change your subscription to a lower tier, make sure your data (and databases) fit within the limits of the new tier; otherwise, the change attempt will fail.\nChange high availability To change your plan\u0026rsquo;s high availability settings, select the desired setting in the High availability panel.\nFixed plans support either no replication or single-zone replication.\nFree tiers do not support replication.\nChange payment method To change your subscription payment method, update the Credit card settings. You can select a known payment method from the drop-down list or use the Add button to add a new one.\nPayment method changes require the Owner role. If your sign-on is not a subscription owner, you cannot change the payment method.\nTo verify your role, select Access Management from the admin menu and then locate your credentials in the Team tab.\nSave changes Use the Change plan button to save changes.\nSubscription overview The Overview tab summarizes your Fixed subscription details using a series of panels:\nThe following details are displayed:\nDetail Description Cloud vendor Your subscription cloud vendor Plan description Brief summary of subscription, including the plan type, cloud provider, region, and data size limit Availability Describes high availability settings Region The region your subscription is deployed to Plan The tier of your Fixed plan, expressed in terms of maximum database size. Also displays the cost for paid plans. Databases Maximum number of databases for your plan Connections Maximum number of concurrent connections CIDR allow rules Maximum number of authorization rules Data persistence Indicates whether persistence is supported for your subscription Daily \u0026amp; instant backups Indicates whether backups are supported for your subscription Replication Indicates whether replication is supported for your subscription Clustering Indicates whether clustering is supported for your subscription Select the Edit button to change the subscription name.\nThe Cancel subscription button appears below the Overview details; it lets you delete your subscription\n","categories":["RC"]},{"uri":"/rc/subscriptions/view-flexible-subscription/","uriRel":"/rc/subscriptions/view-flexible-subscription/","title":"View Flexible subscription details","tags":[],"keywords":[],"description":"","content":"To view the details of a Flexible subscription:\nSign in to the admin console.\nIf you have more than one subscription, select the target subscription from the subscription list.\nYour subscription details appear, along with a summary of your database details.\nFrom here, you can:\nSelect the New database button to add a database to your subscription.\nView the Status icon to learn the status of your subscription. Active subscriptions display a green circle with a check mark. Pending subscriptions display an animated, grey circle.\nBecause subscriptions represent active deployments, there aren\u0026rsquo;t many details you can change. If your needs change, create a new subscription and then migrate the existing data to the new databases.\nIn addition, three tabs are available:\nThe Databases tab lists the databases in your subscription and summarizes their settings.\nThe Overview tab displays subscription settings for your Flexible subscription.\nThe Connectivity tab lets you limit access to the subscription by defining a VPC peering relationship or by setting up an allow list.\nThe following sections provide more info.\nDatabases tab The Databases tab summarizes the databases in your subscription.\nThe following details are provided:\nDetail Description Status An icon indicating whether the database is active (a green circle) or pending (yellow circle) Name The database name Endpoint Use the Copy button to copy the endpoint URI to the Clipboard Memory Memory size of the database, showing the current size and the maximum size Throughput Maximum operations per second supported for the database Modules Identifies modules attached to the database Options Icons showing options associated with the database To view full details of a database, click its name in the list.\nOverview tab The Overview summarizes the options use to created the subscription.\nThe general settings panel describes the cloud vendor, region, and high-availability settings for your subscription.\nSelect the Edit button to change the name of the subscription.\nSetting Description Cloud vendor Your subscription cloud vendor Plan description Brief summary of subscription, including the plan type, cloud provider, and region Redis on Flash Checked when Redis on Flash is enabled Multi-AZ Checked when multiple availability zones are enabled Active-Active Redis Checked when Active-Active Redis is enabled for your subscription Region Describes the region your subscription is deployed to The Price panel shows the monthly cost of your Flexible subscription\nThe Payment info panel shows your payment details.\nThe Cost Estimate section describes the shards required to deploy the subscription based on the choices made when the subscription was created.\nPayment Method shows the current payment details.\nSelect the button to change the credit card associated with this subscription.\nThe Required cloud resources panel shows the storage resources used by your subscription.\nIf your subscription is attached to a cloud account, the details appear in the panel header.\nThe Redis price panel breaks down your subscription price.\nConnectivity tab The Connectivity tabs helps secure your subscription.\nHere, you can:\nSet up a VPC peering relationship between the virtual PC (VPC) hosting your subscription and another virtual PC.\nSet up an access allow list containing IP addresses or security groups (AWS only) permitted to access your subscription.\nSee the individual links to learn more.\n","categories":["RC"]},{"uri":"/kubernetes/re-databases/replica-redb/","uriRel":"/kubernetes/re-databases/replica-redb/","title":"Create replica databases on Kubernetes","tags":[],"keywords":[],"description":"How to create and automate database replicas using the database controller","content":"You can configure a replica of a database by creating an item in the replicaSources section of the Redis Enterprise database specification. The value of replicaSourceType must be \u0026lsquo;SECRET\u0026rsquo;; replicaSourceName must be the name of a secret that contains the replica source url.\nA secret must be created using a stringData section containing the replica source URL as follows:\napiVersion: v1 kind: Secret metadata: name: my-replica-source stringData: url: replica-source-url-goes-here The replica source URL can be retrieved by going to \u0026ldquo;UI \u0026gt; database \u0026gt; configuration \u0026gt; Press the button Get Replica of source URL\u0026rdquo; in the administrative UI. But, this information can also be retrieved directly from the REST API as well.\nA replica of database CR simply uses the secret in the replicaSources section:\napiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: name-of-replica spec: redisEnterpriseCluster: name: name-of-cluster replicaSources: - replicaSourceType: SECRET replicaSourceName: my-replica-source In the above, name-of-replica database will be created as a replica of the source database as long as the source database exists on the source cluster and the secret contains the correct replica source URL for that database.\nRetrieving the replica source URL via kubectl You will need kubectl, curl, and jq installed for this procedure.\nSet your metadata:\nCLUSTER_NAME=test SOURCE_DB=db1 TARGET_DB=db2 TARGET_CLUSTER_NAME=test Retrieve the cluster authentication:\nCLUSTER_USER=`kubectl get secret/${CLUSTER_NAME} -o json | jq -r .data.username | base64 -d` CLUSTER_PASSWORD=`kubectl get secret/${CLUSTER_NAME} -o json | jq -r .data.password | base64 -d` Forward the port of the REST API service for your source cluster:\nkubectl port-forward pod/${CLUSTER_NAME}-0 9443 Request the information from the REST API:\nJQ=\u0026#39;.[] | select(.name==\u0026#34;\u0026#39; JQ+=\u0026#34;${SOURCE_DB}\u0026#34; JQ+=\u0026#39;\u0026#34;) | (\u0026#34;redis://admin:\u0026#34; + .authentication_admin_pass + \u0026#34;@\u0026#34;+.endpoints[0].dns_name+\u0026#34;:\u0026#34;+(.endpoints[0].port|tostring))\u0026#39; URI=`curl -sf -k -u \u0026#34;$CLUSTER_USER:$CLUSTER_PASSWORD\u0026#34; \u0026#34;https://localhost:9443/v1/bdbs?fields=uid,name,endpoints,authentication_admin_pass\u0026#34; | jq \u0026#34;$JQ\u0026#34; | sed \u0026#39;s/\u0026#34;//g\u0026#39;` Note: URI now contains the replica source URI.\nConstruct the secret for the replica:\ncat \u0026lt;\u0026lt; EOF \u0026gt; secret.yaml apiVersion: v1 kind: Secret metadata: name: ${SOURCE_DB}-url stringData: uri: ${URI} EOF kubectl apply -f secret.yaml Create the replica database:\ncat \u0026lt;\u0026lt; EOF \u0026gt; target.yaml apiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: ${TARGET_DB} spec: redisEnterpriseCluster: name: ${TARGET_CLUSTER_NAME} replicaSources: - replicaSourceType: SECRET replicaSourceName: ${SOURCE_DB}-url EOF kubectl apply -f target.yaml Automating the creation via a job The following procedure uses a ConfigMap and a Job to construct the replica source URL secret from the source database and configure the target database.\nThere are four parameters:\nsource - the name of the source database cluster - the name of the cluster for the source database target - the name of the target database targetCluster - the name of the cluster for the target database These parameters can be set by:\nkubectl create configmap replica-of-database-parameters \\ --from-literal=source=name-of-source \\ --from-literal=cluster=name-of-cluster \\ --from-literal=target=name-of-target \\ --from-literal=targetCluster=name-of-cluster where \u0026ldquo;name-of-\u0026hellip;\u0026rdquo; is replaced with the database source, source cluster, database target, and target cluster names.\nThe Job and ConfigMap below, when submitted, will create the secret and replica database:\napiVersion: batch/v1 kind: Job metadata: name: replica-of-database spec: backoffLimit: 4 template: spec: serviceAccountName: redis-enterprise-operator restartPolicy: Never volumes: - name: scripts configMap: name: replica-of-database containers: - name: createdb image: debian:stable-slim env: - name: MY_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: SCRIPT value: create.sh - name: SOURCE_DB valueFrom: configMapKeyRef: name: replica-of-database-parameters key: source - name: TARGET_DB valueFrom: configMapKeyRef: name: replica-of-database-parameters key: target - name: CLUSTER_SERVICE value: .svc.cluster.local - name: CLUSTER_NAME valueFrom: configMapKeyRef: name: replica-of-database-parameters key: cluster - name: CLUSTER_PORT value: \u0026#34;9443\u0026#34; - name: TARGET_CLUSTER_NAME valueFrom: configMapKeyRef: name: replica-of-database-parameters key: targetCluster volumeMounts: - mountPath: /opt/scripts/ name: scripts command: - /bin/bash - -c - | apt-get update; apt-get install -y curl jq apt-transport-https gnupg2 apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6A030B21BA07F4FB curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | tee -a /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubectl bash /opt/scripts/$SCRIPT --- apiVersion: v1 kind: ConfigMap metadata: name: replica-of-database data: create.sh: | CLUSTER_USER=`kubectl get secret/${CLUSTER_NAME} -o json | jq -r .data.username | base64 -d` CLUSTER_PASSWORD=`kubectl get secret/${CLUSTER_NAME} -o json | jq -r .data.password | base64 -d` CLUSTER_HOST=${CLUSTER_NAME}.${MY_NAMESPACE}${CLUSTER_SERVICE} JQ=\u0026#39;.[] | select(.name==\u0026#34;\u0026#39; JQ+=\u0026#34;${SOURCE_DB}\u0026#34; JQ+=\u0026#39;\u0026#34;) | (\u0026#34;redis://admin:\u0026#34; + .authentication_admin_pass + \u0026#34;@\u0026#34;+.endpoints[0].dns_name+\u0026#34;:\u0026#34;+(.endpoints[0].port|tostring))\u0026#39; URI=`curl -sf -k -u \u0026#34;$CLUSTER_USER:$CLUSTER_PASSWORD\u0026#34; \u0026#34;https://${CLUSTER_HOST}:${CLUSTER_PORT}/v1/bdbs?fields=uid,name,endpoints,authentication_admin_pass\u0026#34; | jq \u0026#34;$JQ\u0026#34; | sed \u0026#39;s/\u0026#34;//g\u0026#39;` echo \u0026#34;URL: ${URL}\u0026#34; echo \u0026#34;\u0026#34; cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/secret.yaml apiVersion: v1 kind: Secret metadata: name: ${SOURCE_DB}-url stringData: uri: ${URI} EOF cat /tmp/secret.yaml cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/target.yaml apiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: ${TARGET_DB} spec: redisEnterpriseCluster: name: ${TARGET_CLUSTER_NAME} replicaSources: - replicaSourceType: SECRET replicaSourceName: ${SOURCE_DB}-url EOF echo \u0026#34;---\u0026#34; cat /tmp/target.yaml echo \u0026#34;\u0026#34; kubectl -n ${MY_NAMESPACE} apply -f /tmp/secret.yaml kubectl -n ${MY_NAMESPACE} apply -f /tmp/target.yaml ","categories":["Platforms"]},{"uri":"/rs/clusters/monitoring/prometheus-metrics-definitions/","uriRel":"/rs/clusters/monitoring/prometheus-metrics-definitions/","title":"Metrics in Prometheus","tags":[],"keywords":[],"description":"The metrics available to Prometheus.","content":"The integration with Prometheus lets you create dashboards that highlight the metrics that are important to you.\nHere are the metrics available to Prometheus:\nDatabase metrics Metric Description bdb_avg_latency Average latency of operations on the DB (seconds); returned only when there is traffic bdb_avg_latency_max Highest value of average latency of operations on the DB (seconds); returned only when there is traffic bdb_avg_read_latency Average latency of read operations (seconds); returned only when there is traffic bdb_avg_read_latency_max Highest value of average latency of read operations (seconds); returned only when there is traffic bdb_avg_write_latency Average latency of write operations (seconds); returned only when there is traffic bdb_avg_write_latency_max Highest value of average latency of write operations (seconds); returned only when there is traffic bdb_conns Number of client connections to DB bdb_egress_bytes Rate of outgoing network traffic from the DB (bytes/sec) bdb_egress_bytes_max Highest value of rate of outgoing network traffic from the DB (bytes/sec) bdb_evicted_objects Rate of key evictions from DB (evictions/sec) bdb_evicted_objects_max Highest value of rate of key evictions from DB (evictions/sec) bdb_expired_objects Rate keys expired in DB (expirations/sec) bdb_expired_objects_max Highest value of rate keys expired in DB (expirations/sec) bdb_fork_cpu_system % cores utilization in system mode for all redis shard fork child processes of this database bdb_fork_cpu_system_max Highest value of % cores utilization in system mode for all redis shard fork child processes of this database bdb_fork_cpu_user % cores utilization in user mode for all redis shard fork child processes of this database bdb_fork_cpu_user_max Highest value of % cores utilization in user mode for all redis shard fork child processes of this database bdb_ingress_bytes Rate of incoming network traffic to DB (bytes/sec) bdb_ingress_bytes_max Highest value of rate of incoming network traffic to DB (bytes/sec) bdb_instantaneous_ops_per_sec Request rate handled by all shards of DB (ops/sec) bdb_main_thread_cpu_system % cores utilization in system mode for all redis shard main threas of this database bdb_main_thread_cpu_system_max Highest value of % cores utilization in system mode for all redis shard main threas of this database bdb_main_thread_cpu_user % cores utilization in user mode for all redis shard main threads of this database bdb_main_thread_cpu_user_max Highest value of % cores utilization in user mode for all redis shard main threads of this database bdb_mem_frag_ratio RAM fragmentation ratio (RSS / allocated RAM) bdb_mem_size_lua Redis lua scripting heap size (bytes) bdb_memory_limit Configured RAM limit for the database bdb_monitor_sessions_count Number of client connected in monitor mode to the DB bdb_no_of_keys Number of keys in DB bdb_other_req Rate of other (non read/write) requests on DB (ops/sec) bdb_other_req_max Highest value of rate of other (non read/write) requests on DB (ops/sec) bdb_other_res Rate of other (non read/write) responses on DB (ops/sec) bdb_other_res_max Highest value of rate of other (non read/write) responses on DB (ops/sec) bdb_pubsub_channels Count the pub/sub channels with subscribed clients bdb_pubsub_channels_max Highest value of count the pub/sub channels with subscribed clients bdb_pubsub_patterns Count the pub/sub patterns with subscribed clients bdb_pubsub_patterns_max Highest value of count the pub/sub patterns with subscribed clients bdb_read_hits Rate of read operations accessing an existing key (ops/sec) bdb_read_hits_max Highest value of rate of read operations accessing an existing key (ops/sec) bdb_read_misses Rate of read operations accessing a non-existing key (ops/sec) bdb_read_misses_max Highest value of rate of read operations accessing a non-existing key (ops/sec) bdb_read_req Rate of read requests on DB (ops/sec) bdb_read_req_max Highest value of rate of read requests on DB (ops/sec) bdb_read_res Rate of read responses on DB (ops/sec) bdb_read_res_max Highest value of rate of read responses on DB (ops/sec) bdb_shard_cpu_system % cores utilization in system mode for all redis shard processes of this database bdb_shard_cpu_system_max Highest value of % cores utilization in system mode for all redis shard processes of this database bdb_shard_cpu_user % cores utilization in user mode for the redis shard process bdb_shard_cpu_user_max Highest value of % cores utilization in user mode for the redis shard process bdb_total_connections_received Rate of new client connections to DB (connections/sec) bdb_total_connections_received_max Highest value of rate of new client connections to DB (connections/sec) bdb_total_req Rate of all requests on DB (ops/sec) bdb_total_req_max Highest value of rate of all requests on DB (ops/sec) bdb_total_res Rate of all responses on DB (ops/sec) bdb_total_res_max Highest value of rate of all responses on DB (ops/sec) bdb_up Database is up and running bdb_used_memory Memory used by db (in bigredis this includes flash) (bytes) bdb_write_hits Rate of write operations accessing an existing key (ops/sec) bdb_write_hits_max Highest value of rate of write operations accessing an existing key (ops/sec) bdb_write_misses Rate of write operations accessing a non-existing key (ops/sec) bdb_write_misses_max Highest value of rate of write operations accessing a non-existing key (ops/sec) bdb_write_req Rate of write requests on DB (ops/sec) bdb_write_req_max Highest value of rate of write requests on DB (ops/sec) bdb_write_res Rate of write responses on DB (ops/sec) bdb_write_res_max Highest value of rate of write responses on DB (ops/sec) no_of_expires Current number of volatile keys in the database Node metrics Metric Description node_available_flash Available flash in node (bytes) node_available_flash_no_overbooking Available flash in node (bytes), without taking into account overbooking node_available_memory Amount of free memory in node (bytes) that is available for database provisioning node_available_memory_no_overbooking Available ram in node (bytes) without taking into account overbooking node_avg_latency Average latency of requests handled by endpoints on node (seconds); returned only when there is traffic node_bigstore_free Sum of free space of back-end flash (used by flash DB\u0026rsquo;s [BigRedis]) on all cluster nodes (bytes); returned only when BigRedis is enabled node_bigstore_iops Rate of i/o operations against back-end flash for all shards which are part of a flash based DB (BigRedis) in cluster (ops/sec); returned only when BigRedis is enabled node_bigstore_kv_ops Rate of value read/write operations against back-end flash for all shards which are part of a flash based DB (BigRedis) in cluster (ops/sec); returned only when BigRedis is enabled node_bigstore_throughput Throughput i/o operations against back-end flash for all shards which are part of a flash based DB (BigRedis) in cluster (bytes/sec); returned only when BigRedis is enabled node_conns Number of clients connected to endpoints on node node_cpu_idle CPU idle time portion (0-1, multiply by 100 to get percent) node_cpu_idle_max Highest value of CPU idle time portion (0-1, multiply by 100 to get percent) node_cpu_idle_median Average value of CPU idle time portion (0-1, multiply by 100 to get percent) node_cpu_idle_min Lowest value of CPU idle time portion (0-1, multiply by 100 to get percent) node_cpu_system CPU time portion spent in kernel (0-1, multiply by 100 to get percent) node_cpu_system_max Highest value of CPU time portion spent in kernel (0-1, multiply by 100 to get percent) node_cpu_system_median Average value of CPU time portion spent in kernel (0-1, multiply by 100 to get percent) node_cpu_system_min Lowest value of CPU time portion spent in kernel (0-1, multiply by 100 to get percent) node_cpu_user CPU time portion spent by users-pace processes (0-1, multiply by 100 to get percent) node_cpu_user_max Highest value of CPU time portion spent by users-pace processes (0-1, multiply by 100 to get percent) node_cpu_user_median Average value of CPU time portion spent by users-pace processes (0-1, multiply by 100 to get percent) node_cpu_user_min Lowest value of CPU time portion spent by users-pace processes (0-1, multiply by 100 to get percent) node_cur_aof_rewrites Number of aof rewrites that are currently performed by shards on this node node_egress_bytes Rate of outgoing network traffic to node (bytes/sec) node_egress_bytes_max Highest value of rate of outgoing network traffic to node (bytes/sec) node_egress_bytes_median Average value of rate of outgoing network traffic to node (bytes/sec) node_egress_bytes_min Lowest value of rate of outgoing network traffic to node (bytes/sec) node_ephemeral_storage_avail Disk space available to RLEC processes on configured ephemeral disk (bytes) node_ephemeral_storage_free Free disk space on configured ephemeral disk (bytes) node_free_memory Free memory in node (bytes) node_ingress_bytes Rate of incoming network traffic to node (bytes/sec) node_ingress_bytes_max Highest value of rate of incoming network traffic to node (bytes/sec) node_ingress_bytes_median Average value of rate of incoming network traffic to node (bytes/sec) node_ingress_bytes_min Lowest value of rate of incoming network traffic to node (bytes/sec) node_persistent_storage_avail Disk space available to RLEC processes on configured persistent disk (bytes) node_persistent_storage_free Free disk space on configured persistent disk (bytes) node_provisional_flash Amount of flash available for new shards on this node, taking into account overbooking, max redis servers, reserved flash and provision and migration thresholds (bytes) node_provisional_flash_no_overbooking Amount of flash available for new shards on this node, without taking into account overbooking, max redis servers, reserved flash and provision and migration thresholds (bytes) node_provisional_memory Amount of RAM that is available for provisioning to databases out of the total RAM allocated for databases node_provisional_memory_no_overbooking Amount of RAM that is available for provisioning to databases out of the total RAM allocated for databases, without taking into account overbooking node_total_req Request rate handled by endpoints on node (ops/sec) node_up Node is part of the cluster and is connected Proxy metrics Metric Description listener_acc_latency Accumulative latency (sum of the latencies) of all types of commands on DB. For the average latency, divide this value by listener_total_res listener_acc_latency_max Highest value of accumulative latency of all types of commands on DB listener_acc_other_latency Accumulative latency (sum of the latencies) of commands that are type \u0026ldquo;other\u0026rdquo; on DB. For the average latency, divide this value by listener_other_res listener_acc_other_latency_max Highest value of accumulative latency of commands that are type \u0026ldquo;other\u0026rdquo; on DB listener_acc_read_latency Accumulative latency (sum of the latencies) of commands that are type \u0026ldquo;read\u0026rdquo; on DB. For the average latency, divide this value by listener_read_res listener_acc_read_latency_max Highest value of accumulative latency of commands that are type \u0026ldquo;read\u0026rdquo; on DB listener_acc_write_latency Accumulative latency (sum of the latencies) of commands that are type \u0026ldquo;write\u0026rdquo; on DB. For the average latency, divide this value by listener_write_res listener_acc_write_latency_max Highest value of accumulative latency of commands that are type \u0026ldquo;write\u0026rdquo; on DB listener_auth_cmds Number of memcached AUTH commands sent to the DB listener_auth_cmds_max Highest value of number of memcached AUTH commands sent to the DB listener_auth_errors Number of error responses to memcached AUTH commands listener_auth_errors_max Highest value of number of error responses to memcached AUTH commands listener_cmd_flush Number of memcached FLUSH_ALL commands sent to the DB listener_cmd_flush_max Highest value of number of memcached FLUSH_ALL commands sent to the DB listener_cmd_get Number of memcached GET commands sent to the DB listener_cmd_get_max Highest value of number of memcached GET commands sent to the DB listener_cmd_set Number of memcached SET commands sent to the DB listener_cmd_set_max Highest value of number of memcached SET commands sent to the DB listener_cmd_touch Number of memcached TOUCH commands sent to the DB listener_cmd_touch_max Highest value of number of memcached TOUCH commands sent to the DB listener_conns Number of clients connected to the endpoint listener_egress_bytes Rate of outgoing network traffic to the endpoint (bytes/sec) listener_egress_bytes_max Highest value of rate of outgoing network traffic to the endpoint (bytes/sec) listener_ingress_bytes Rate of incoming network traffic to the endpoint (bytes/sec) listener_ingress_bytes_max Highest value of rate of incoming network traffic to the endpoint (bytes/sec) listener_last_req_time Time of last command sent to the DB listener_last_res_time Time of last response sent from the DB listener_max_connections_exceeded Number of times the Number of clients connected to the db at the same time has exeeded the max limit listener_max_connections_exceeded_max Highest value of number of times the Number of clients connected to the db at the same time has exeeded the max limit listener_monitor_sessions_count Number of client connected in monitor mode to the endpoint listener_other_req Rate of other (non read/write) requests on the endpoint (ops/sec) listener_other_req_max Highest value of rate of other (non read/write) requests on the endpoint (ops/sec) listener_other_res Rate of other (non read/write) responses on the endpoint (ops/sec) listener_other_res_max Highest value of rate of other (non read/write) responses on the endpoint (ops/sec) listener_other_started_res Number of responses sent from the DB of type \u0026ldquo;other\u0026rdquo; listener_other_started_res_max Highest value of number of responses sent from the DB of type \u0026ldquo;other\u0026rdquo; listener_read_req Rate of read requests on the endpoint (ops/sec) listener_read_req_max Highest value of rate of read requests on the endpoint (ops/sec) listener_read_res Rate of read responses on the endpoint (ops/sec) listener_read_res_max Highest value of rate of read responses on the endpoint (ops/sec) listener_read_started_res Number of responses sent from the DB of type \u0026ldquo;read\u0026rdquo; listener_read_started_res_max Highest value of number of responses sent from the DB of type \u0026ldquo;read\u0026rdquo; listener_total_connections_received Rate of new client connections to the endpoint (connections/sec) listener_total_connections_received_max Highest value of rate of new client connections to the endpoint (connections/sec) listener_total_req Request rate handled by the endpoint (ops/sec) listener_total_req_max Highest value of rate of all requests on the endpoint (ops/sec) listener_total_res Rate of all responses on the endpoint (ops/sec) listener_total_res_max Highest value of rate of all responses on the endpoint (ops/sec) listener_total_started_res Number of responses sent from the DB of all types listener_total_started_res_max Highest value of number of responses sent from the DB of all types listener_write_req Rate of write requests on the endpoint (ops/sec) listener_write_req_max Highest value of rate of write requests on the endpoint (ops/sec) listener_write_res Rate of write responses on the endpoint (ops/sec) listener_write_res_max Highest value of rate of write responses on the endpoint (ops/sec) listener_write_started_res Number of responses sent from the DB of type \u0026ldquo;write\u0026rdquo; listener_write_started_res_max Highest value of number of responses sent from the DB of type \u0026ldquo;write\u0026rdquo; Replication metrics Metric Description bdb_replicaof_syncer_ingress_bytes Rate of compressed incoming network traffic to Replica Of DB (bytes/sec) bdb_replicaof_syncer_ingress_bytes_decompressed Rate of decompressed incoming network traffic to Replica Of DB (bytes/sec) bdb_replicaof_syncer_local_ingress_lag_time Lag time between the source and the destination for Replica Of traffic (ms) bdb_replicaof_syncer_status Syncer status for Replica Of traffic; 0 = in-sync, 1 = syncing, 2 = out of sync bdb_crdt_syncer_ingress_bytes Rate of compressed incoming network traffic to CRDB (bytes/sec) bdb_crdt_syncer_ingress_bytes_decompressed Rate of decompressed incoming network traffic to CRDB (bytes/sec) bdb_crdt_syncer_local_ingress_lag_time Lag time between the source and the destination (ms) for CRDB traffic bdb_crdt_syncer_status Syncer status for CRDB traffic; 0 = in-sync, 1 = syncing, 2 = out of sync Shard metrics Metric Description redis_active_defrag_running Automatic memory defragmentation current aggressiveness (% cpu) redis_allocator_active Total used memory including external fragmentation redis_allocator_allocated Total allocated memory redis_allocator_resident Total resident memory (RSS) redis_aof_last_cow_size Last AOFR, CopyOnWrite memory redis_aof_rewrite_in_progress The number of simultaneous AOF rewrites that are in progress redis_aof_rewrites Number of AOF rewrites this process executed redis_aof_delayed_fsync Number of times an AOF fsync caused delays in the redis main thread (inducing latency); This can indicate that the disk is slow or overloaded redis_blocked_clients Count the clients waiting on a blocking call redis_connected_clients Number of client connections to the specific shard redis_connected_slaves Number of connected slaves redis_db0_avg_ttl Average TTL of all volatile keys redis_db0_expires Total count of volatile keys redis_db0_keys Total key count redis_evicted_keys Keys evicted so far (since restart) redis_expire_cycle_cpu_milliseconds The cumulative amount of time spent on active expiry cycles redis_expired_keys Keys expired so far (since restart) redis_forwarding_state Shard forwarding state (on or off) redis_keys_trimmed The number of keys that were trimmed in the current or last resharding process redis_keyspace_read_hits Number of read operations accessing an existing keyspace redis_keyspace_read_misses Number of read operations accessing an non-existing keyspace redis_keyspace_write_hits Number of write operations accessing an existing keyspace redis_keyspace_write_misses Number of write operations accessing an non-existing keyspace redis_master_link_status Indicates if the replica is connected to its master redis_master_repl_offset Number of bytes sent to replicas by the shard; Calculate the throughput for a time period by comparing the value at different times redis_master_sync_in_progress The master shard is synchronizing (1 true redis_max_process_mem Current memory limit configured by redis_mgr according to node free memory redis_maxmemory Current memory limit configured by redis_mgr according to db memory limits redis_mem_aof_buffer Current size of AOF buffer redis_mem_clients_normal Current memory used for input and output buffers of non-replica clients redis_mem_clients_slaves Current memory used for input and output buffers of replica clients redis_mem_fragmentation_ratio Memory fragmentation ratio (1.3 means 30% overhead) redis_mem_not_counted_for_evict Portion of used_memory (in bytes) that\u0026rsquo;s not counted for eviction and OOM error redis_mem_replication_backlog Size of replication backlog redis_module_fork_in_progress A binary value that indicates if there is an active fork spawned by a module (1) or not (0) redis_process_cpu_system_seconds_total Shard Process system CPU time spent in seconds redis_process_cpu_usage_percent Shard Process cpu usage precentage redis_process_cpu_user_seconds_total Shard user CPU time spent in seconds redis_process_main_thread_cpu_system_seconds_total Shard main thread system CPU time spent in seconds redis_process_main_thread_cpu_user_seconds_total Shard main thread user CPU time spent in seconds redis_process_max_fds Shard Maximum number of open file descriptors redis_process_open_fds Shard Number of open file descriptors redis_process_resident_memory_bytes Shard Resident memory size in bytes redis_process_start_time_seconds Shard Start time of the process since unix epoch in seconds redis_process_virtual_memory_bytes Shard virtual memory in bytes redis_rdb_bgsave_in_progress Indication if bgsave is currently in progress redis_rdb_last_cow_size Last bgsave (or SYNC fork) used CopyOnWrite memory redis_rdb_saves Total count of bgsaves since process was restarted (including replica fullsync and persistence) redis_repl_touch_bytes Number of bytes sent to replicas as TOUCH commands by the shard as a result of a READ command that was processed; Calculate the throughput for a time period by comparing the value at different times redis_total_commands_processed Number of commands processed by the shard; Calculate the number of commands for a time period by comparing the value at different times redis_total_connections_received Number of connections received by the shard; Calculate the number of connections for a time period by comparing the value at different times redis_total_net_input_bytes Number of bytes received by the shard; Calculate the throughput for a time period by comparing the value at different times redis_total_net_output_bytes Number of bytes sent by the shard; Calculate the throughput for a time period by comparing the value at different times redis_up Shard is up and running redis_used_memory Memory used by shard (in bigredis this includes flash) (bytes) ","categories":["RS"]},{"uri":"/rs/security/access-control/ldap/update-database-acls/","uriRel":"/rs/security/access-control/ldap/update-database-acls/","title":"Update database ACLs","tags":[],"keywords":[],"description":"Describes how to use the admin console to update database access control lists (ACLs) to authorize access to roles authorizing LDAP user access.","content":"To grant LDAP users access to a database, assign the mapped access role to the access control list (ACL) for the database.\nFrom the admin console menu, select Databases and then select the database from the list.\nSelect the Configuration tab to display the database details.\nSelect the Edit button.\nLocate the Access Control List setting and select its Add button.\nSelect the appropriate roles and then save your changes.\nIf you assign multiple roles to an ACL and a user is authorized by more than one of these roles, their access is determined by the first “matching” rule in the list.\nIf the first rule gives them read access and the third rule authorizes write access, the user will only be able to read data.\nAs a result, we recommend ordering roles so that higher access roles appear before roles with more limited access.\nMore info Enable and configure role-based LDAP Map LDAP groups to access control roles Learn more about Redis Enterprise Software security and practices ","categories":["RS"]},{"uri":"/ri/release-notes/archive/v0.9.42/","uriRel":"/ri/release-notes/archive/v0.9.42/","title":"RDBTools v0.9.42, 22 July 2019","tags":[],"keywords":[],"description":"Bug fixes","content":"Bug fixes Error message while adding an instance mentioned db number as well. It has been removed now Streams: Adding new stream via the UI did not update the stream list Browser: Adding a new hash key with empty values would default to the values in the last added hash Browser: Scores for sorted set elements were being truncated to integers ","categories":[]},{"uri":"/ri/using-redisinsight/memory-analysis/","uriRel":"/ri/using-redisinsight/memory-analysis/","title":"Memory analysis","tags":[],"keywords":[],"description":"","content":"RedisInsight Memory analysis helps you analyze your Redis database, which can reduce memory use and improve application performance.\nAnalysis can be done in two ways:\nonline mode - In this mode, RedisInsight downloads an rdb file from your connected Redis instance and analyzes it to create a temp file with all the keys and metadata required for analysis. In case there is a master-replica connection, RedisInsight downloads the dump from the replica instead of the master in order to avoid affecting the performance of the master.\noffline mode - In this mode, RedisInsight analyzes your redis backup files. These files can either be present in your system or on s3. RedisInsight accepts a list of rdb files given to it and analyzes all the information required from these files instead of downloading it from your redis instance. In order to analyze backup files stored in s3, RedisInsight should have ReadOnly permission to your s3 bucket where the files are stored. Specify the name of the s3 bucket and the path to the rdb file.\nRunning memory analysis on an instance Navigate to Memory Analysis \u0026gt; Overview, and then click the \u0026ldquo;Analyze Now\u0026rdquo; button. You should see a dialog box with two options - Offline Analysis and Online Analysis.\nChoose the offline analysis approach if you have a RDB Backup file that you want to analyze. We can proceed with online analysis.\nMemory analysis can take several minutes, and largely depends on the size of your data.\nOnce memory analysis completes, you can see various statistics about memory consumption under Memory Analysis. The overview page gives you a high level breakup of memory usage.\nMemory overview Memory overview gives you an overview of your redis instance through graphical representation. Memory breakup by data type shows the overall size and count distribution of keys based on each data type. Expiry analysis gives a overview of how your keys are configured to expire. There could be a few keys which never expire.\nKeyspace summary Keyspace Summary identifies the top key patterns from the set of keys in decending order of memory. This helps you identify which key patterns are consuming most of your memory and what are the top keys for that pattern. You can add your own key patterns in order to identify their memory usage and the top keys for that key pattern.\nRecommendations RedisInsight provide recommendations on how you can save your memory. The recommendations are specially curated according to your Redis instance. These recommendations have been formed based on industry standards and our own experiences.\nMemory analyzer Memory Analyzer lets you search a key or key patterns and get related information regarding it with other stats. You can apply various filters and aggregations using our advance filters feature.\nHow memory analysis works Here\u0026rsquo;s a brief description of what goes on under the hood when you analyze a snapshot: When the analyze-memory button is clicked, it connects to the redis instance and takes a point-in-time snapshot of the database. This can be done in two ways:\nUsing the SYNC command. This is the preferred approach and is used if possible. Redis has a SYNC command that replicas use to synchronize with the master. Our agent pretends to be a replica and sends the SYNC command to the instance, which responds with all its data as it would to a replica trying to synchronize with it. Using the DUMP command. Cloud providers do not support the SYNC command, so that approach won\u0026rsquo;t work. But they do support the DUMP command. This command serializes the value of a key in a redis-specific format and returns it. We scan all the keys iteratively, dump the values, and concatenate them to generate an RDB. There are a couple of caveats with this method, among them being that the serialization format is opaque and non-standard and that all the keys are not dumped at the exact same time, meaning that some keys\u0026rsquo; values may have changed since the time the first key was dumped, making this not an exact point-in-time snapshot. After we have the dump, following either of the approaches mentioned above, we perform analysis on the dump, computing memory statistics and discovering key patterns. What happens here is similar to what is done by the open source redis-rdb-tools. The result of this process is an RSNAP file (stands for redis snapshot), which contains the key names, memory statistics and other generated information about the dump, but importantly, not the values of the keys themselves. The dump file never really leaves the system the agent runs on.\nAfter the RSNAP file is completely generated, it is used to generate recommendations. We have over 20 recommendations at this point which give you simple advice on how to optimize your redis database.\nSo that\u0026rsquo;s a brief look under the hood of RedisInsight. We are constantly working on improving our process and we\u0026rsquo;ve had quite a bit of back and forth about the exact mechanism of the entire process. It goes without saying that the process keeps evolving and might look different in the near future. We\u0026rsquo;ll try to keep this page updated with all significant changes, so check back here or follow our blog to stay updated.\n","categories":["RI"]},{"uri":"/ri/installing/install-ec2/","uriRel":"/ri/installing/install-ec2/","title":"Install RedisInsight on AWS EC2","tags":[],"keywords":[],"description":"","content":"This tutorial shows how to install RedisInsight on an AWS EC2 instance and manage ElastiCache Redis instances using RedisInsight. To complete this tutorial you must have access to the AWS Console and permissions to launch EC2 instances.\nStep 1: Create a new IAM Role (Optional) RedisInsight needs read-only access to S3 and ElastiCache APIs. This is an optional step.\nLogin to AWS Console, and navigate to IAM screen Create a new IAM Role Under Select type of trusted entity, choose EC2. In other words, the role is used by an EC2 instance Assign the following permissions: AmazonS3ReadOnlyAccess AmazonElastiCacheReadOnlyAccess Step 2: Launch EC2 Instance Next, launch an EC2 instance.\nNavigate to EC2 under AWS Console Click Launch Instance Choose 64 bit Amazon Linux AMI Choose at least a t2.medium instance. The size of the instance depends on the memory used by your ElastiCache instance that you want to analyze Under Configure Instance: Choose the VPC that has your ElastiCache instances Choose a subnet that has network access to your ElastiCache instances Ensure that your EC2 instance has a public IP Address Assign the IAM role that you created in Step 1 Under the storage section, ensure at least 100 GiB storage Under security group, ensure that: Incoming traffic is allowed on port 8001 Incoming traffic is allowed on port 22 only during installation Review and launch the ec2 instance Step 3: Verify Permissions and Connectivity Next, verify the EC2 instance has the required IAM permissions, and can connect to ElastiCache Redis instances.\nSSH into the newly launched EC2 instance Open a command prompt Run the command aws s3 ls. This should list S3 buckets If the aws command cannot be found, make sure your ec2 instance is based of amazon linux Next, find the hostname an ElastiCache instance you want to analyze, and run the command echo info | nc \u0026lt;redis host\u0026gt; 6379 If you see some details about the ElastiCache redis instance, you can proceed to the next step If you cannot connect to redis, you should review your VPC, subnet, and security group settings. Step 4: Install Docker on EC2 Next, install Docker on the EC2 instance. Run the following commands:\nsudo yum update -y sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user Log out and log back in again to pick up the new docker group permissions. To verify, run docker ps. You should see some output without having to run sudo Step 5: Run RedisInsight Docker Container Finally, we can now install RedisInsight. Run the following command\ndocker run -v redisinsight:/db -p 8001:8001 redislabs/redisinsight:latest This command downloads and runs the RedisInsight docker image and exposes it as a web page on port 8001.\nFind the IP Address of your EC2 instances, and a launch your browser to http://\u0026lt;EC2 IP Address\u0026gt;:8001. You should see the Databases page of RedisInsight. Accept the EULA and start using RedisInsight.\nSummary In this guide, we installed RedisInsight on an EC2 instance running Docker. As a next step, you should now add an ElastiCache Redis Instance and then run the memory analysis.\n","categories":["RI"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/accumulate/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/accumulate/","title":"Accumulate","tags":[],"keywords":[],"description":"Reduces many records in the pipe to a single record.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; accumulate​( gears.operations.AccumulateOperation\u0026lt;T,​I\u0026gt; accumulator) public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; accumulate​( I initialValue, gears.operations.AccumulateOperation\u0026lt;T,​I\u0026gt; accumulator) Accumulate is a many-to-one function that iterates through the records in the pipe and reduces them to a single record.\nYou can provide a parameter to set the initial accumulator value. Otherwise, the initial accumulator object is null.\nParameters Type parameters:\nName Description I The template type of the returned builder Function parameters:\nName Type Description accumulator AccumulateOperation\u0026lt;T,​I\u0026gt; A function with logic to update the accumulator value with each record initialValue template type I The initial value of the accumulated object Returns Returns a GearsBuilder object with a new template type.\nExamples Both of the following examples count the number of records in the pipeline.\nWithout the initialValue parameter:\nGearsBuilder.CreateGearsBuilder(reader).accumulate((a, r)-\u0026gt;{ Integer ret = null; if (a == null) { ret = 1; } else { ret = (Integer)a; } return ret + 1; }); With the initialValue parameter set to 0:\nGearsBuilder.CreateGearsBuilder(reader).accumulate(0, (a, r)-\u0026gt;{ return a + 1; }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/accumulateby/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/accumulateby/","title":"AccumulateBy","tags":[],"keywords":[],"description":"Groups records and reduces each group to a single record per group.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; accumulateBy​( gears.operations.ExtractorOperation\u0026lt;T\u0026gt; extractor, gears.operations.AccumulateByOperation\u0026lt;T,​I\u0026gt; accumulator) public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; accumulateBy​( gears.operations.ValueInitializerOperation\u0026lt;I\u0026gt; valueInitializer, gears.operations.ExtractorOperation\u0026lt;T\u0026gt; extractor, gears.operations.AccumulateByOperation\u0026lt;T,​I\u0026gt; accumulator) Iterates through the records in the pipe, groups them based on the provided extractor, and then reduces each group to a single record per group with the accumulator function.\nThe initial value of the accumulator is null unless you provide a value initializer operation as a parameter.\nParameters Type parameters:\nName Description I The template type of the returned builder Function parameters:\nName Type Description accumulator AccumulateByOperation\u0026lt;T,​I\u0026gt; A function with logic to update the accumulator value with each record extractor ExtractorOperation Extracts a specific value from each record valueInitializer ValueInitializerOperation Whenever the accumulated value is null, use this function to initialize it Returns Returns a GearsBuilder object with a new template type.\nExamples Both of the following examples count the number of unique values.\nWithout the valueInitializer parameter:\nGearsBuilder.CreateGearsBuilder(reader). accumulateBy(r-\u0026gt;{ return r.getStringVal(); },(k, a, r)-\u0026gt;{ Integer ret = null; if(a == null) { ret = 0; }else { ret = (Integer)a; } return ret + 1; }); With the valueInitializer parameter:\nGearsBuilder.CreateGearsBuilder(reader). accumulateBy(()-\u0026gt;{ return 0; },r-\u0026gt;{ return r.getStringVal(); },(k, a, r)-\u0026gt;{ return a + 1; }); ","categories":["Modules"]},{"uri":"/modules/redisjson/active-active/","uriRel":"/modules/redisjson/active-active/","title":"RedisJSON in Active-Active databases","tags":[],"keywords":[],"description":"RedisJSON support and conflict resolution rules for Active-Active databases.","content":"RedisJSON v2.2 adds support for RedisJSON in Active-Active Redis Enterprise databases.\nThe design is based on A Conflict-Free Replicated JSON Datatype by Kleppmann and Beresford, but the implementation includes some changes. Several conflict resolution rule examples were adapted from this paper as well.\nCommand differences Some RedisJSON commands work differently for Active-Active databases.\nJSON.CLEAR JSON.CLEAR resets JSON arrays and objects. It supports concurrent updates to JSON documents from different instances in an Active-Active database and allows the results to be merged.\nConflict resolution rules With Active-Active databases, it\u0026rsquo;s possible for two different instances to try to run write operations on the same data at the same time. If this happens, conflicts can arise when the replicas attempt to sync these changes with each other. Conflict resolution rules determine how the database handles conflicting operations.\nThere are two types of conflict resolution:\nMerge:\nThe operations are associative.\nMerges the results of both operations.\nWin over:\nThe operations are not associative.\nOne operation wins the conflict and sets the value.\nIgnores the losing operation.\nThe following conflict resolution rules show how Active-Active databases resolve conflicts for various RedisJSON commands.\nAssign different types to a key Conflict\nTwo instances concurrently assign values of different types to the same key within a JSON document.\nFor example:\nInstance 1 assigns an object to a key within a JSON document.\nInstance 2 assigns an array to the same key.\nResolution type\nWin over\nResolution rule\nThe instance with the smaller ID wins, so the key becomes an object in the given example.\nExample\nTime Description Instance 1 Instance 2 t1 Set the same key to an object or an array JSON.SET doc $.a \u0026lsquo;{}\u0026rsquo; JSON.SET doc $.a \u0026lsquo;[]\u0026rsquo; t2 Add data to the object and array JSON.SET doc $.a.x \u0026lsquo;“y”\u0026rsquo; Result: {\u0026ldquo;a\u0026rdquo;: {\u0026ldquo;x\u0026rdquo;: \u0026ldquo;y\u0026rdquo;}} JSON.SET doc $.a \u0026lsquo;[\u0026ldquo;z\u0026rdquo;]\u0026rsquo; Result: {“a”: [\u0026ldquo;z\u0026rdquo;]} t3 Active-Active synchronization – Sync – – Sync – t4 Instance 1 wins JSON.GET doc $ Result: {\u0026ldquo;a\u0026rdquo;: {\u0026ldquo;x\u0026rdquo;: \u0026ldquo;y\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;a\u0026rdquo;: {\u0026ldquo;x\u0026rdquo;: \u0026ldquo;y\u0026rdquo;}} Create versus create Conflict\nTwo instances concurrently use JSON.SET to assign a new JSON document to the same key.\nResolution type\nWin over\nResolution rule\nThe instance with the smaller ID wins.\nExample\nTime Description Instance 1 Instance 2 t1 Create a new JSON document JSON.SET doc $ \u0026lsquo;{\u0026ldquo;field\u0026rdquo;: \u0026ldquo;a\u0026rdquo;}\u0026rsquo; JSON.SET doc $ \u0026lsquo;{\u0026ldquo;field\u0026rdquo;: \u0026ldquo;b\u0026rdquo;}\u0026rsquo; t2 Active-Active synchronization – Sync – – Sync – t3 Instance 1 wins JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;a\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;a\u0026rdquo;} Create versus update Conflict\nInstance 1 creates a new document and assigns it to an existing key with JSON.SET.\nInstance 2 updates the existing content of the same key with JSON.SET.\nResolution type\nWin over\nResolution rule\nThe operation that creates a new document wins.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} t2 Instance 1 creates a new document; instance 2 updates the existing document JSON.SET doc $ \u0026lsquo;{\u0026ldquo;field2\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;}\u0026rsquo; JSON.SET doc $.field1 \u0026lsquo;[1, 2, 3]\u0026rsquo; t3 Active-Active synchronization – Sync – – Sync – t4 Instance 1 wins JSON.GET doc . Result: {\u0026ldquo;field2\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;} JSON.GET doc . Result: {\u0026ldquo;field2\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;} Delete versus create Conflict\nInstance 1 deletes a JSON document with JSON.DEL.\nInstance 2 uses JSON.SET to create a new JSON document and assign it to the key deleted by instance 1.\nResolution type\nWin over\nResolution rule\nDocument creation wins over deletion.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} t2 Instance 1 deletes the document; instance 2 creates a new document JSON.DEL doc JSON.SET doc $ \u0026lsquo;{\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;}\u0026rsquo; t3 Active-Active synchronization – Sync – – Sync – t4 Instance 2 wins JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value2\u0026rdquo;} Delete versus update Conflict\nInstance 1 deletes a JSON document with JSON.DEL.\nInstance 2 updates the content of the same document with JSON.SET.\nResolution type\nWin over\nResolution rule\nDocument deletion wins over updates.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field1\u0026rdquo;: \u0026ldquo;value1\u0026rdquo;} t2 Instance 1 deletes the document; instance 2 updates it JSON.DEL doc JSON.SET doc $.field1 \u0026lsquo;[1, 2, 3]\u0026rsquo; t3 Active-Active synchronization – Sync – – Sync – t4 Instance 1 wins JSON.GET doc $ Result: (nil) JSON.GET doc $ Result: (nil) Update versus update Conflict\nInstance 1 updates a field inside a JSON document with JSON.SET.\nInstance 2 updates the same field with a different value.\nResolution type\nWin over\nResolution rule\nThe instance with the smallest ID wins.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;a\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;a\u0026rdquo;} t2 Update the same field with different data JSON.SET doc $.field \u0026ldquo;b\u0026rdquo; JSON.SET doc $.field \u0026ldquo;c\u0026rdquo; t3 Active-Active synchronization – Sync – – Sync – t4 Instance 1 wins JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;b\u0026rdquo;} JSON.GET doc $ Result: {\u0026ldquo;field\u0026rdquo;: \u0026ldquo;b\u0026rdquo;} Update versus clear The version of RedisJSON prior to v2.2 has two different ways to reset the content of a JSON object:\nAssign a new empty JSON object:\nJSON.SET doc $.colors \u0026#39;{}\u0026#39; If you use this method, it cannot be merged with a concurrent update.\nFor each key, remove it with JSON.DEL:\nJSON.DEL doc $.colors.blue With this method, it can merge the reset with concurrent updates.\nAs of RedisJSON v2.2, you can use the JSON.CLEAR command to reset the JSON document without removing each key manually. This method also lets concurrent updates be merged.\nAssign an empty object Conflict\nInstance 1 adds \u0026ldquo;red\u0026rdquo; to the existing \u0026ldquo;colors\u0026rdquo; object with JSON.SET.\nInstance 2 assigns a new empty object for \u0026ldquo;colors\u0026rdquo;.\nResolution type\nWin over\nResolution rule\nDocument creation wins over the update, so the result will be an empty object.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;}} t2 Instance 1 adds a new color; instance 2 resets colors to an empty object JSON.SET doc $.colors.red ‘#ff0000’ JSON.SET doc $.colors ‘{}’ t3 Instance 2 adds a new color JSON.SET doc $.colors.green ‘#00ff00’ t4 JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;, \u0026ldquo;red\u0026rdquo;: \u0026ldquo;#ff0000\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} t5 Active-Active synchronization – Sync – – Sync – t6 Instance 2 wins JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} Use JSON.CLEAR Conflict\nInstance 1 adds \u0026ldquo;red\u0026rdquo; to the existing \u0026ldquo;colors\u0026rdquo; object with JSON.SET.\nInstance 2 clears the \u0026ldquo;colors\u0026rdquo; object with JSON.CLEAR and adds \u0026ldquo;green\u0026rdquo; to \u0026ldquo;colors\u0026rdquo;.\nResolution type\nMerge\nResolution rule\nMerges the results of all operations.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;}} t2 Instance 1 adds a new color; instance 2 resets the colors JSON.SET doc $.colors.red ‘#ff0000’ JSON.CLEAR doc $.colors t3 JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;blue\u0026rdquo;: \u0026ldquo;#0000ff\u0026rdquo;, \u0026ldquo;red\u0026rdquo;: \u0026ldquo;#ff0000\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {}} t4 Instance 2 adds a new color JSON.SET doc $.colors.green ‘#00ff00’ t5 JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} t6 Active-Active synchronization – Sync – – Sync – t7 Merges the results of both instances JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;red\u0026rdquo;: \u0026ldquo;#ff0000\u0026rdquo;, \u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} JSON.GET doc $ Result: {\u0026ldquo;colors\u0026rdquo;: {\u0026ldquo;red\u0026rdquo;: \u0026ldquo;#ff0000\u0026rdquo;, \u0026ldquo;green\u0026rdquo;: \u0026ldquo;#00ff00\u0026rdquo;}} Update versus update array Conflict\nTwo instances update the same existing array with different content.\nResolution type\nMerge\nResolution rule\nMerges the results of all operations on the array. Preserves the original element order from each instance.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: \u0026lsquo;[\u0026ldquo;a\u0026rdquo;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]\u0026rsquo; JSON.GET doc $ Result: \u0026lsquo;[\u0026ldquo;a\u0026rdquo;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]\u0026rsquo; t2 Instance 1 removes an array element; instance 2 adds one JSON.ARRPOP doc $ 1 Result: [\u0026ldquo;a\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] JSON.ARRINSERT doc $ 0 ‘“y”’ Result: [\u0026ldquo;y\u0026rdquo;, \u0026ldquo;a\u0026rdquo;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] t3 Both instances add another element to the array JSON.ARRINSERT doc $ 1 ‘“x”’ Result: [\u0026ldquo;a\u0026rdquo;, \u0026ldquo;x\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] JSON.ARRINSERT doc $ 2 ‘“z”’ Result: [\u0026ldquo;y\u0026rdquo;, \u0026ldquo;a\u0026rdquo;, \u0026ldquo;z\u0026rdquo;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] t4 Active-Active synchronization – Sync – – Sync – t5 Merge results from both instances JSON.GET doc $ Result: [\u0026ldquo;y\u0026rdquo;, \u0026ldquo;a\u0026rdquo;, \u0026ldquo;x\u0026rdquo;, \u0026ldquo;z\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] JSON.GET doc $ Result: [\u0026ldquo;y\u0026rdquo;, \u0026ldquo;a\u0026rdquo;, \u0026ldquo;x\u0026rdquo;, \u0026ldquo;z\u0026rdquo;, \u0026ldquo;c\u0026rdquo;] Update versus delete array element Conflict\nInstance 1 removes an element from a JSON array with JSON.ARRPOP.\nInstance 2 updates the same element that instance 1 removes.\nResolution type\nWin over\nResolution rule\nDeletion wins over updates.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: {“todo”: [{“title”: “buy milk”, “done”: false}]} JSON.GET doc $ Result: {“todo”: [{“title”: “buy milk”, “done”: false}]} t2 Instance 1 removes an array element; instance 2 updates the same element JSON.ARRPOP doc $.todo 0 JSON.SET doc \u0026lsquo;$.todo[0][\u0026ldquo;done\u0026rdquo;]\u0026rsquo; \u0026rsquo;true\u0026rsquo;’ t3 JSON.GET doc $ Result: {“todo”: []} JSON.GET doc $ Result: [{“title”: “buy milk”, “done”: true}]} t4 Active-Active synchronization – Sync – – Sync – t5 Instance 1 wins JSON.GET doc $ Result: doc = {“todo”: []} JSON.GET doc $ Result: doc = {“todo”: []} Update versus update object Conflict\nBoth instances update the same existing object with different content.\nResolution type\nMerge\nResolution rule\nMerges the results of all operations on the object.\nExample\nTime Description Instance 1 Instance 2 t1 The document exists on both instances JSON.GET doc $ Result: \u0026lsquo;{\u0026ldquo;grocery\u0026rdquo;: []}\u0026rsquo; JSON.GET doc $ Result: \u0026lsquo;{\u0026ldquo;grocery\u0026rdquo;: []}\u0026rsquo; t2 Add new elements to the array JSON.ARRAPPEND doc $.grocery ‘“eggs”’ JSON.ARRAPPEND doc $.grocery ‘“milk”’ t3 Add new elements to the array JSON.ARRAPPEND doc $.grocery ‘“ham”’ JSON.ARRAPPEND doc $.grocery ‘“flour”’ t4 JSON.GET doc $ Result: {\u0026ldquo;grocery\u0026rdquo;:[\u0026ldquo;eggs\u0026rdquo;, \u0026ldquo;ham\u0026rdquo;]} JSON.GET doc $ Result: {\u0026ldquo;grocery\u0026rdquo;:[\u0026ldquo;milk\u0026rdquo;, \u0026ldquo;flour\u0026rdquo;]} t5 Active-Active synchronization – Sync – – Sync – t6 Merges the results from both instances JSON.GET doc . Result: {\u0026ldquo;grocery\u0026rdquo;:[\u0026ldquo;eggs\u0026rdquo;,\u0026ldquo;ham\u0026rdquo;,\u0026ldquo;milk\u0026rdquo;, \u0026ldquo;flour\u0026rdquo;]} JSON.GET doc . Result: {\u0026ldquo;grocery\u0026rdquo;:[\u0026ldquo;eggs\u0026rdquo;,\u0026ldquo;ham\u0026rdquo;,\u0026ldquo;milk\u0026rdquo;, \u0026ldquo;flour\u0026rdquo; ]} ","categories":["Modules"]},{"uri":"/ri/using-redisinsight/","uriRel":"/ri/using-redisinsight/","title":"Administration","tags":[],"keywords":[],"description":"","content":" Add a Redis database Before using any of the tools to work with your database, you must first add the database so RedisInsight can connect to it. Each of these database configurations requires specific steps to add them to RedisInsight: Standalone Redis Redis cluster Redis Sentinel Redis with TLS authentication Elasticache Note: Currently, RedisInsight supports Redis versions 4 and newer. Add a standalone Redis database This is the simplest setup of a Redis database with just a single Redis server.\nAutomatically Discovering Databases RedisInsight lets you automatically add Redis Enterprise Software and Redis Enterprise Cloud databases. Note: For Redis Cloud, auto-discovery is supported only for Flexible or Annual subscriptions. Auto-discovery for Redis Software To automatically discover and add Redit Software databases to RedisInsight: In RedisInsight, select ADD REDIS DATABASE. Select Automatically Discover Databases. Select Redis Enterprise. Enter the connection details and then select DISCOVER DATABASES. From the list of databases, choose the databases that you want to add and then select ADD SELECTED DATABASES.\nAdding Databases Programmatically If you have a lot of Redis databases or you are using RedisInsight as part of some automated workflow, you might want to add databases programmatically. Now this is possible using our experimental REST API. Below is the documentation for the endpoints required to add databases. Note that this API should not be considered stable at this point and might change or break entirely in future releases. Do not rely on this API for production.\nAdding Databases via GET URL If you want to automate adding a Redis database without filling the database form, you might want to add database via GET URL. Below is the documentation for the URL required to add databases. Add Redis database Used to add Redis databases to RedisInsight. URL : /add/ Method : GET Query Parameters These are the required query parameters for any type of database. Parameter Type Required/Optional Description host string required Hostname of your Redis database.\nAuthenticate database users You can enforce authentication of users who share your databases by running Redisinsight with variables RIAUTHPROMPT, RIAUTHTIMER, and RILOGLEVEL. For more information on variables, see Configure RedisInsight. By setting the variables, enforce the prompt for username and password each time the database is opened and at a specific time interval while users work with the database. You can maintain multiple tabs with the same database without having to enter username and password in each one.\nProxy Trusted origins By default, RedisInsight trusts only the origin to which RedisInsight binds to. If RedisInsight is run behind a proxy, the origin where the requests come from is not trusted if the proxy\u0026rsquo;s origin is not the origin where RedisInsight binds to. While RedisInsight does start and is reachable, no operations are allowed. Because the origin of requests/proxy is always known, the trusted origins must be manually set via the RITRUSTEDORIGINS environment variable.\nPerformance Metrics RedisInsight Overview provides you the quick overview about your Redis instance through graphical representation. It displays the total memory and keys for your instance. Number of connections received, clients connected, Network input and output and various other information.\nCluster Management RedisInsight Cluster Management provides a graphical user interface (GUI) to manage your Redis Cluster. Cluster Management comes with three different views to analyze your cluster architecture. Master Layout - This view only contains information about the masters present in the Redis Cluster. The information present is - slot ranges, host, port and few metrics gathered from redis INFO Command. Master-Replica Layout - This view contains masters along with their replicas. This view contains information about slots ranges, host, port, etc for both master and replica.\nBrowser RedisInsight Browser lets you explore keys in your redis server. You can add, edit and delete a key. You can even update the key expiry and copy the key name to be used in different places of the application. Note: RedisInsight restricts visualization of keys that are greater than 2GB in size.\nCLI RedisInsight CLI lets you run commands against a redis server. You don\u0026rsquo;t need to remember the syntax - the integrated help shows you all the arguments and validates your command as you type. Key Bindings Emacs and Vim keybindings are supported. Action Emacs Vim Previous history item C-p k Next history item C-n j Move cursor to the begining C-a 0 Move cursor to the end C-e $ Move cursor to the right by one character C-f h Move cursor to the left by one character C-b l Enter insert mode - i Enter normal mode - ESC Memory analysis RedisInsight Memory analysis helps you analyze your Redis database, which can reduce memory use and improve application performance. Analysis can be done in two ways: online mode - In this mode, RedisInsight downloads an rdb file from your connected Redis instance and analyzes it to create a temp file with all the keys and metadata required for analysis. In case there is a master-replica connection, RedisInsight downloads the dump from the replica instead of the master in order to avoid affecting the performance of the master.\nProfiler RedisInsight Profiler runs Redis MONITOR command, which analyzes every command sent to the redis instance. It parses the output of the MONITOR command and generates a summarized view. All the commands sent to the redis instance are monitored for the duration of the profiling. Profiler gives information about the number of commands processed, commands/second and number of connected clients. It also gives information about top prefixes, top keys and top commands.\nSlowlog RedisInsight Slowlog is a list of slow operations for your redis instance. These can be used to troubleshoot performance issues. Each entry in the list displays the command, duration and timestamp. Any transaction that exceeds slowlog-log-slower-than microseconds are recorded up to a maximum of slowlog-max-len after which older entries are discarded. Clear Slowlog - Clear slowlog clears all the slowlog entries from your redis server.\nConfiguration RedisInsight configuration allows to update your redis instance\u0026rsquo;s config with its easy to use config editor. Each of the keys shown corresponds to an entry in the Redis configuration file. Most of the configuration settings can be applied without restarting the server. Also, it comes with an option of rewriting your current settings to your redis.conf file so that these settings remain even when server restarts. Configurations are also separated into categories like- Advanced config, Security, Lua Scripting etc.\nView Java Serialized Objects in Redis RedisInsight detects java serialized objects and converts them to a nicely formatted JSON object, along with the fully qualified class name. It doesn\u0026rsquo;t matter what you store. Whether it is a hibernate object, or a user session or a plain old java object, RedisInsight reverse-engineers and show it to you nicely. Just for fun, we tried out how such an object would look without the formatting.\nTroubleshooting RedisInsight When RedisInsight doesn\u0026rsquo;t behave as expected, use these steps to see what the problem is. For additional configuration options, such as changing the default port, go to: https://docs.redislabs.com/latest/ri/installing/configurations/ Logs To get detailed information about errors in RedisInsight, you can review the log files with the .log extension in: Docker: In the /db/ directory inside the container. Mac: In the /Users/\u0026lt;your-username\u0026gt;/.redisinsight directory. Windows: In the C:\\Users\\\u0026lt;your-username\u0026gt;\\.redisinsight directory. Linux: In the /home/\u0026lt;your-username\u0026gt;/.redisinsight directory.\n","categories":["RI"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/asyncfilter/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/asyncfilter/","title":"AsyncFilter","tags":[],"keywords":[],"description":"Asynchronously filters out records in the pipe based on a given condition.","content":"public GearsBuilder\u0026lt;T\u0026gt; asyncFilter​( gears.operations.AsyncFilterOperation\u0026lt;T\u0026gt; filter) The asyncFilter function allows you to use a GearsFuture object to asynchronously filter out records in the pipe based on a given condition.\nThe filter operation should contain a conditional statement and return a boolean for each record:\nIf true, the record will continue through the pipe. If false, it filters out the record. Parameters Name Type Description filter AsyncFilterOperation A function that checks a condition for each record in the pipe. Returns a boolean. Returns Returns a GearsBuilder object with the same template type as the input builder.\nExample GearsBuilder.CreateGearsBuilder(reader).map(r-\u0026gt;r.getKey()). asyncFilter(r-\u0026gt;{ GearsFuture\u0026lt;Boolean\u0026gt; f = new GearsFuture\u0026lt;Boolean\u0026gt;(); new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(1); f.setResult(r.equals(\u0026#34;x\u0026#34;)); } catch (Exception e) { e.printStackTrace(); }\t} }).start(); return f; }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/asyncforeach/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/asyncforeach/","title":"AsyncForeach","tags":[],"keywords":[],"description":"For each record in the pipe, asynchronously runs some operations.","content":"public GearsBuilder\u0026lt;T\u0026gt; asyncForeach​( gears.operations.AsyncForeachOperation\u0026lt;T\u0026gt; foreach) The asyncForeach function allows you to use a GearsFuture object to define a set of operations and run them asynchronously for each record in the pipe.\nParameters Name Type Description foreach AsyncForeachOperation The set of operations to run for each record Returns Returns a GearsBuilder object with a new template type.\nExample GearsBuilder.CreateGearsBuilder(reader).map(r-\u0026gt;r.getKey()). asyncForeach(r-\u0026gt;{ GearsFuture\u0026lt;Serializable\u0026gt; f = new GearsFuture\u0026lt;Serializable\u0026gt;(); new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(1); f.setResult(r); } catch (Exception e) { e.printStackTrace(); }\t} }).start(); return f; }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/asyncmap/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/asyncmap/","title":"AsyncMap","tags":[],"keywords":[],"description":"Asynchronously maps records one-to-one.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; asyncMap​( gears.operations.AsyncMapOperation\u0026lt;T,​I\u0026gt; mapper) The asyncMap function allows you to use a GearsFuture object to asynchronously map each input record in the pipe to an output record, one-to-one.\nParameters Type parameters:\nName Description I The template type of the returned builder Function parameters:\nName Type Description mapper AsyncMapOperation\u0026lt;T,​I\u0026gt; For each input record, returns a new output record Returns Returns a GearsBuilder object with a new template type.\nExample GearsBuilder.CreateGearsBuilder(reader). asyncMap(r-\u0026gt;{ GearsFuture\u0026lt;String\u0026gt; f = new GearsFuture\u0026lt;String\u0026gt;(); new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(1); f.setResult(\u0026#34;done\u0026#34;); } catch (Exception e) { e.printStackTrace(); }\t} }).start(); return f; }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/callnext/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/callnext/","title":"CallNext","tags":[],"keywords":[],"description":"Calls the next execution that overrides the command or the original command itself. A more flexible version of callNextArray.","content":"public static java.lang.Object callNext(java.lang.String... args) When you override a Redis command with the CommandOverrider, use callNext to run the next execution that overrides the command or the original command itself.\nIt is a more flexible version of callNextArray since the list of string arguments does not have to be an explicit String[] object. This allows function calls like: callNext(\u0026quot;key\u0026quot;, \u0026quot;value\u0026quot;).\nParameters Name Type Description args string Redis command arguments Returns Returns the command result. It could be a string or an array of strings, depending on the command.\nExamples Without String[]:\nGearsBuilder.callNext(\u0026#34;restaurant:1\u0026#34;, \u0026#34;reviews\u0026#34;, \u0026#34;50\u0026#34;); With String[]:\nGearsBuilder.callNext(new String[]{\u0026#34;restaurant:1\u0026#34;, \u0026#34;reviews\u0026#34;, \u0026#34;50\u0026#34;}); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/callnextarray/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/callnextarray/","title":"CallNextArray","tags":[],"keywords":[],"description":"Calls the next execution that overrides the command or the original command itself.","content":"public static native java.lang.Object callNextArray( java.lang.String[] command) When you override a Redis command with the CommandOverrider, use callNextArray to run the next execution that overrides the command or the original command itself.\nIt accepts an array of strings, which represents the command arguments.\nParameters Name Type Description args array of strings Redis command arguments Returns Returns the command result. It could be a string or an array of strings, depending on the command.\nExample GearsBuilder.callNextArray(new String[]{\u0026#34;restaurant:1\u0026#34;, \u0026#34;reviews\u0026#34;, \u0026#34;50\u0026#34;}); ","categories":["Modules"]},{"uri":"/rs/clusters/logging/rsyslog-logging/cluster-events/","uriRel":"/rs/clusters/logging/rsyslog-logging/cluster-events/","title":"Cluster alert and event logs","tags":[],"keywords":[],"description":"Logged cluster alerts and events","content":"The following cluster alerts and events can appear in syslog.\nUI alerts Logged alerts that appear in the UI\nAlert code name Alert as shown in the UI Severity Notes even_node_count True high availability requires an odd number of nodes with a minimum of three nodes true: warning\nfalse: info inconsistent_redis_sw Not all databases are running the same open source version true: warning\nfalse: info inconsistent_rl_sw Not all nodes in the cluster are running the same Redis Enterprise Cluster version true: warning\nfalse: info internal_bdb Issues with internal cluster databases true: warning\nfalse: info multiple_nodes_down Multiple cluster nodes are down - this might cause data loss true: warning\nfalse: info ram_overcommit Cluster capacity is less than total memory allocated to its databases true: error\nfalse: info too_few_nodes_for_replication Database replication requires at least two nodes in cluster true: warning\nfalse: info UI events Logged events that appear in the UI\nEvent code name Event as shown in the UI Severity Notes node_joined Node joined info node_remove_abort_completed Node removed info The remove node is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. node_remove_abort_failed Node removed error The remove node is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. node_remove_completed Node removed info The remove node is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. node_remove_failed Node removed error The remove node is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. rebalance_abort_completed Nodes rebalanced info The nodes rebalance is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. rebalance_abort_failed Nodes rebalanced error The nodes rebalance is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. rebalance_completed Nodes rebalanced info The nodes rebalance is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. rebalance_failed Nodes rebalanced error The nodes rebalance is a process that can fail and can also be cancelled. If cancelled, the cancellation process can succeed or fail. Non-UI events Logged events that do not appear in the UI\nEvent code name Severity Notes cluster_updated info Indicates that cluster settings have been updated license_added info license_deleted info license_updated info ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/collect/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/collect/","title":"Collect","tags":[],"keywords":[],"description":"Collects all records to the origin shard.","content":"public GearsBuilder\u0026lt;T\u0026gt; collect() Collects all of the records to the shard where the RedisGears job started.\nParameters None\nReturns Returns a GearsBuilder object with the same template type as the input builder.\nExample GearsBuilder.CreateGearsBuilder(reader).collect(); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/config-get/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/config-get/","title":"ConfigGet","tags":[],"keywords":[],"description":"Gets the value of a RedisGears configuration setting.","content":"public static java.lang.String configGet​(java.lang.String key) Gets the value of a RedisGears configuration setting.\nNote: You can set configuration values when you load the module or use the RG.CONFIGSET command. Parameters Name Type Description key string The configuration setting to get Returns Returns the configuration value of a RedisGears configuration setting.\nExample GearsBuilder.configGet(\u0026#34;ExecutionMaxIdleTime\u0026#34;); ","categories":["Modules"]},{"uri":"/rs/clusters/configure/","uriRel":"/rs/clusters/configure/","title":"Configure clusters","tags":[],"keywords":[],"description":"Configuration options for your Redis Enterprise cluster.","content":"You can manage your Redis Enterprise Software clusters with several different tools:\nAdmin console (the web-based user interface) Command-line tools (rladmin, redis-cli, crdb-cli) REST API Cluster settings You can view and set various cluster settings such as cluster name, email service, time zone, and license.\nCluster license keys The cluster key (or license) enables features and capacity within Redis Enterprise Software\nSynchronize cluster node clocks Sync node clocks to avoid problems with internal custer communication.\nRack-zone awareness in Redis Enterprise Software Rack-zone awareness ensures high-availability in the event of a rack or zone failure.\n","categories":["RS"]},{"uri":"/rs/security/tls/tls-protocols/","uriRel":"/rs/security/tls/tls-protocols/","title":"Configure TLS protocol","tags":[],"keywords":[],"description":"","content":"You can change TLS protocols to improve the security of your Redis Enterprise cluster and databases. The default settings are in line with industry best practices, but you can customize them to match the security policy of your organization.\nConfigure TLS protocol The communications for which you can modify TLS protocols are:\nControl plane - The TLS configuration for cluster administration. Data plane - The TLS configuration for the communication between applications and databases. Discovery service (Sentinel) - The TLS configuration for the discovery service. You can configure the TLS protocols with the rladmin commands shown here or with the REST API.\nWarning - After you set the minimum TLS version, Redis Enterprise Software does not accept communications with TLS versions older than the specified version. TLS support depends on the operating system. You cannot enable support for protocols or versions that aren\u0026rsquo;t supported by the operating system running Redis Enterprise Software. In addition, updates to the operating system or to Redis Enterprise Software can impact protocol and version support.\nTo illustrate, version 6.2.8 of Redis Enterprise Software removed support for TLS 1.0 and TLS 1.1 on Red Hat Enterprise Linux 8 (RHEL 8) because that operating system does not enable support for these versions by default.\nIf you have trouble enabling specific versions of TLS, verify that they\u0026rsquo;re supported by your operating system and that they\u0026rsquo;re configured correctly.\nNote: TLSv1.2 is generally recommended as the minimum TLS version for encrypted communications. Check with your security team to confirm which TLS protocols meet your organization\u0026rsquo;s policies. Control plane To set the minimum TLS protocol for the control plane:\nDefault TLS Protocols: TLSv1.0 Syntax: rladmin cluster config min_control_TLS_version \u0026lt;TLS_Version\u0026gt; TLS versions available: For TLSv1 - 1 For TLSv1.1 - 1.1 For TLSv1.2 - 1.2 For example:\nrladmin cluster config min_control_TLS_version 1.2 Data plane To set the minimum TLS protocol for the data path:\nDefault TLS Protocols: TLSv1.0 Syntax: rladmin cluster config min_data_TLS_version \u0026lt;TLS_Version\u0026gt; TLS versions available: For TLSv1 - 1 For TLSv1.1 - 1.1 For TLSv1.2 - 1.2 For example:\nrladmin cluster config min_data_TLS_version 1.2 Discovery service To enable TLS for the discovery service:\nDefault: Allows both TLS and non-TLS connections Syntax: rladmin cluster config sentinel_ssl_policy \u0026lt;ssl_policy\u0026gt; ssl_policy values available: allowed - Allows both TLS and non-TLS connections required - Allows only TLS connections disabled - Allows only non-TLS connections For example:\nrladmin cluster config sentinel_ssl_policy required min_data_TLS_version 1.2 For your changes to take effect on the discovery service, restart the service with this command:\nsupervisorctl restart sentinel_service ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/count/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/count/","title":"Count","tags":[],"keywords":[],"description":"Counts the number of records in the pipe.","content":"public GearsBuilder\u0026lt;java.lang.Integer\u0026gt; count() Counts the number of records in the pipe and returns the total as a single record.\nParameters None\nReturns Returns a GearsBuilder object with a new template type of Integer.\nExample GearsBuilder.CreateGearsBuilder(reader).count(); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/creategearsbuilder/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/creategearsbuilder/","title":"CreateGearsBuilder","tags":[],"keywords":[],"description":"Creates a new GearsBuilder object.","content":"public static \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; CreateGearsBuilder​( gears.readers.BaseReader\u0026lt;I\u0026gt; reader) public static \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; CreateGearsBuilder​( gears.readers.BaseReader\u0026lt;I\u0026gt; reader, java.lang.String desc) Creates a new GearsBuilder object. Use this function instead of a GearsBuilder constructor to avoid type warnings.\nParameters Type Parameters:\nName Description I The template type of the returned builder. The reader determines the type. Parameters:\nName Type Description desc string The description reader BaseReader The pipe reader Returns Returns a new GearsBuilder object.\nExample GearsBuilder.CreateGearsBuilder(reader); ","categories":["Modules"]},{"uri":"/rs/clusters/logging/rsyslog-logging/bdb-events/","uriRel":"/rs/clusters/logging/rsyslog-logging/bdb-events/","title":"Database alert and event logs","tags":[],"keywords":[],"description":"Logged database alerts and events","content":"The following database (BDB) alerts and events can appear in syslog.\nUI alerts Logged alerts that appear in the UI\nAlert code name Alert as shown in the UI Severity Notes backup_delayed Periodic backup has been delayed for longer than minutes true: warning\nfalse: info Has threshold parameter in the data section of the log entry. high_latency Latency is higher than msec true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. high_syncer_lag Replica of - sync lag is higher than seconds true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. high_throughput Throughput is higher than RPS (requests per second) true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. low_throughput Throughput is lower than RPS (requests per second) true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. ram_dataset_overhead RAM Dataset overhead in a shard has reached % of its RAM limit true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. ram_values Percent of values in a shard’s RAM is lower than % of its key count true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. shard_num_ram_values Number of values in a shard’s RAM is lower than values true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. size Dataset size has reached % of the memory limit true: warning\nfalse: info Has threshold parameter in the key/value section of the log entry. syncer_connection_error Replica of - database unable to sync with source error syncer_general_error Replica of - database unable to sync with source error Non-UI events Logged events that do not appear in the UI\nEvent code name Severity Notes authentication_err error Replica of - Error authenticating with the source database backup_failed error backup_started info backup_succeeded info bdb_created info bdb_deleted info bdb_updated info Indicates that a BDB configuration has been updated compression_unsup_err error Replica of - Compression not supported by sync destination crossslot_err error Replica of - Sharded destination does not support operation executed on source export_failed error export_started info export_succeeded info import_failed error import_started info import_succeeded info oom_err error Replica of - Replication source/target out of memory ","categories":["RS"]},{"uri":"/rc/subscriptions/delete-subscription/","uriRel":"/rc/subscriptions/delete-subscription/","title":"Delete a subscription","tags":[],"keywords":[],"description":"","content":"To delete a subscription:\nDelete all databases from the subscription.\nThe number of databases for the subscription is shown in the subscription list. You can not delete a subscription until there are zero databases in the subscription.\nView the subscription details.\nIf you have more than one subscription, select the target subscription from the subscription list.\nSelect the Overview tab.\nLocate and then select the Cancel subscription button near the bottom, right corner of the Overview details.\nSelect the Yes, cancel button to confirm your choice.\nOnce a subscription is deleted, it cannot be recovered.\nWe recommend backing up your data before removing databases or subscriptions.\n","categories":["RC"]},{"uri":"/rs/security/certificates/ocsp-stapling/","uriRel":"/rs/security/certificates/ocsp-stapling/","title":"Enable OCSP stapling","tags":[],"keywords":[],"description":"Use OCSP stapling to verify certificates maintained by a third-party CA and authenticate connection attempts between clients and servers.","content":"OCSP (Online Certificate Status Protocol) lets a client or server verify the status (GOOD, REVOKED, or UNKNOWN) of a certificate maintained by a third-party certificate authority (CA).\nTo check whether a certificate is still valid or has been revoked, a client or server can send a request to the CA\u0026rsquo;s OCSP server (also called an OCSP responder). The OCSP responder checks the certificate\u0026rsquo;s status in the CA\u0026rsquo;s certificate revocation list and sends the status back as a signed and timestamped response.\nOCSP stapling overview With OCSP enabled, the Redis Enterprise server regularly polls the CA\u0026rsquo;s OCSP responder for the certificate\u0026rsquo;s status. After it receives the response, the server caches this status until its next polling attempt.\nWhen a client tries to connect to the Redis Enterprise server, they perform a TLS handshake to authenticate the server and create a secure, encrypted connection. During the TLS handshake, OCSP stapling lets the Redis Enterprise server send (or \u0026ldquo;staple\u0026rdquo;) the cached certificate status to the client.\nIf the stapled OCSP response confirms the certificate is still valid, the TLS handshake succeeds and the client connects to the server.\nThe TLS handshake fails and the client blocks the connection to the server if the stapled OCSP response indicates either:\nThe certificate has been revoked.\nThe certificate\u0026rsquo;s status is unknown. This can happen if the OCSP responder fails to send a response.\nSet up OCSP stapling You can configure and enable OCSP stapling for your Redis Enterprise cluster with the admin console, the REST API, or rladmin.\nWhile OCSP is enabled, the server always staples the cached OCSP status when a client tries to connect. It is the client\u0026rsquo;s responsibility to use the stapled OCSP status. Some Redis clients, such as Jedis and redis-py, already support OCSP stapling, but others might require additional configuration.\nAdmin console method To set up OCSP stapling with the Redis Enterprise admin console:\nUse rladmin or the REST API to replace the proxy certificate with a certificate signed by your third-party CA.\nSign in to the Redis Enterprise admin console.\nGo to settings \u0026gt; OCSP.\nSelect the OCSP checkbox.\nSelect the Test certificate button to verify the certificate is valid. This queries the OCSP responder and caches the result.\nConfigure the following settings if you don\u0026rsquo;t want to use their default values:\nName Default value Description Query frequency 3600 The time interval in seconds between OCSP queries to the responder URI. Response timeout 1 The time interval in seconds to wait for a response before timing out. Recovery frequency 60 The time interval in seconds between retries after a failed query. Recovery maximum tries 5 The number of retries before the validation query fails and invalidates the certificate. Select Save.\nREST API method To set up OCSP stapling with the REST API:\nUse the REST API to replace the proxy certificate with a certificate signed by your third-party CA.\nTo configure and enable OCSP, send a PUT request to the /v1/ocsp endpoint and include an OCSP JSON object in the request body:\n{ \u0026#34;ocsp_functionality\u0026#34;: true, \u0026#34;query_frequency\u0026#34;: 3600, \u0026#34;response_timeout\u0026#34;: 1, \u0026#34;recovery_frequency\u0026#34;: 60, \u0026#34;recovery_max_tries\u0026#34;: 5 } rladmin method To set up OCSP stapling with the rladmin command-line utility:\nUse rladmin to replace the proxy certificate with a certificate signed by your third-party CA.\nUpdate the cluster\u0026rsquo;s OCSP settings with the rladmin cluster ocsp config command if you don\u0026rsquo;t want to use their default values.\nFor example:\nrladmin cluster ocsp config recovery_frequency set 30 Enable OCSP:\nrladmin cluster ocsp config ocsp_functionality set enabled ","categories":["RS"]},{"uri":"/rc/api/examples/dryrun-cost-estimates/","uriRel":"/rc/api/examples/dryrun-cost-estimates/","title":"Estimate cost","tags":[],"keywords":[],"description":"How to evaluate the cost of a specific subscription or database without changing existing resources.","content":"When you change your subscriptions and databases, you also change the cost of your deployment. With a dry-run request, you can evaluate the impact and obtain a new cost estimate before you deploy these changes:\nCreate subscription Create a database Update a database Defining a dry-run request API operations that support dry-run requests accept the dryRun boolean parameter in the JSON request body.\nFor example, the JSON body of a create subscription request body can include the dryRun=true parameter:\n{ \u0026#34;name\u0026#34;: \u0026#34;DryRun Cost evaluation subscription\u0026#34;, \u0026#34;dryRun\u0026#34;: true, \u0026#34;paymentMethodId\u0026#34;: 8240, \u0026#34;cloudProviders\u0026#34;: [ { \u0026#34;cloudAccountId\u0026#34;: 16424, \u0026#34;regions\u0026#34;: [ { \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;networking\u0026#34;: { \u0026#34;deploymentCIDR\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; } } ] } ], \u0026#34;databases\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;database-a\u0026#34;, \u0026#34;memoryLimitInGb\u0026#34;: 0.1 } ] } Executing a dry-run request Dry-run requests behave like regular requests except that no changes are made to existing resources. A dry-run request produces a cost evaluation report for the subscription.\nAPI Operation dryRun=false (default) dryRun=true Create subscription Create a subscription Returns a cost evaluation report of the planned subscription Create database Creates a new database in the subscription Returns a cost evaluation report for the relevant subscription Update database Changes the specified database Returns a cost evaluation report and evaluates whether the relevant subscription requires additional resources based on the database modification Example of a dry-run request \u0026amp; response This section demonstrates a complete example of a create subscription dry-run request and response.\nDry-run request body Here is an example of the pricing request body:\n{ \u0026#34;name\u0026#34;: \u0026#34;DryRun Cost evaluation subscription\u0026#34;, \u0026#34;dryRun\u0026#34;: true, \u0026#34;paymentMethodId\u0026#34;: 8240, \u0026#34;cloudProviders\u0026#34;: [ { \u0026#34;cloudAccountId\u0026#34;: 16424, \u0026#34;regions\u0026#34;: [ { \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;networking\u0026#34;: { \u0026#34;deploymentCIDR\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34; } } ] } ], \u0026#34;databases\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;database-a\u0026#34;, \u0026#34;throughputMeasurement\u0026#34;: { \u0026#34;by\u0026#34;: \u0026#34;operations-per-second\u0026#34;, \u0026#34;value\u0026#34;: 1000 }, \u0026#34;memoryLimitInGb\u0026#34;: 0.1 , \u0026#34;quantity\u0026#34;: 3 }, { \u0026#34;name\u0026#34;: \u0026#34;database-b\u0026#34;, \u0026#34;throughputMeasurement\u0026#34;: { \u0026#34;by\u0026#34;: \u0026#34;operations-per-second\u0026#34;, \u0026#34;value\u0026#34;: 1000 }, \u0026#34;replication\u0026#34;: false ,\u0026#34;memoryLimitInGb\u0026#34;: 0.1 }, { \u0026#34;name\u0026#34;: \u0026#34;database-c\u0026#34;, \u0026#34;throughputMeasurement\u0026#34;: { \u0026#34;by\u0026#34;: \u0026#34;operations-per-second\u0026#34;, \u0026#34;value\u0026#34;: 1000 }, \u0026#34;replication\u0026#34;: true ,\u0026#34;memoryLimitInGb\u0026#34;: 5 }, { \u0026#34;name\u0026#34;: \u0026#34;database-d\u0026#34;, \u0026#34;throughputMeasurement\u0026#34;: { \u0026#34;by\u0026#34;: \u0026#34;operations-per-second\u0026#34;, \u0026#34;value\u0026#34;: 10000 }, \u0026#34;replication\u0026#34;: false ,\u0026#34;memoryLimitInGb\u0026#34;: 12 }, { \u0026#34;name\u0026#34;: \u0026#34;database-e\u0026#34;, \u0026#34;throughputMeasurement\u0026#34;: { \u0026#34;by\u0026#34;: \u0026#34;operations-per-second\u0026#34;, \u0026#34;value\u0026#34;: 25000 }, \u0026#34;replication\u0026#34;: true ,\u0026#34;memoryLimitInGb\u0026#34;: 25 } ] } The create subscription request contains the \u0026quot;dryRun\u0026quot;: true parameter The databases array contains the definitions of multiple database templates, named database-a, database-b, database-c, etc. Note that the databases differ in structure (size, throughput measurement, replication, quantity, etc.) Dry-run response Here is an example of the pricing response section for the above create subscription dry-run request:\n{ \u0026#34;response\u0026#34;: { \u0026#34;resource\u0026#34;: { \u0026#34;pricing\u0026#34;: [ { \u0026#34;databaseName\u0026#34;: \u0026#34;database-a\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;typeDetails\u0026#34;: \u0026#34;micro\u0026#34;, \u0026#34;quantity\u0026#34;: 6, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;shards\u0026#34;, \u0026#34;pricePerUnit\u0026#34;: 0.027, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;pricePeriod\u0026#34;: \u0026#34;hour\u0026#34; }, { \u0026#34;databaseName\u0026#34;: \u0026#34;database-b\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;typeDetails\u0026#34;: \u0026#34;micro\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;shards\u0026#34;, \u0026#34;pricePerUnit\u0026#34;: 0.027, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;pricePeriod\u0026#34;: \u0026#34;hour\u0026#34; }, { \u0026#34;databaseName\u0026#34;: \u0026#34;database-c\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;typeDetails\u0026#34;: \u0026#34;high-throughput\u0026#34;, \u0026#34;quantity\u0026#34;: 2, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;shards\u0026#34;, \u0026#34;pricePerUnit\u0026#34;: 0.124, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;pricePeriod\u0026#34;: \u0026#34;hour\u0026#34; }, { \u0026#34;databaseName\u0026#34;: \u0026#34;database-d\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;typeDetails\u0026#34;: \u0026#34;small\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;shards\u0026#34;, \u0026#34;pricePerUnit\u0026#34;: 0.156, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;pricePeriod\u0026#34;: \u0026#34;hour\u0026#34; }, { \u0026#34;databaseName\u0026#34;: \u0026#34;database-e\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;typeDetails\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;quantity\u0026#34;: 2, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;shards\u0026#34;, \u0026#34;pricePerUnit\u0026#34;: 0.293, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;pricePeriod\u0026#34;: \u0026#34;hour\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;EBS Volume\u0026#34;, \u0026#34;quantity\u0026#34;: 345, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;GB\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;r5.xlarge\u0026#34;, \u0026#34;quantity\u0026#34;: 3, \u0026#34;quantityMeasurement\u0026#34;: \u0026#34;instances\u0026#34; } ] } } } Note: Some of the response content was omitted for brevity. The pricing array contains an element for each database, containing the database name and cost evaluation related to that database The database cost is measured in type and number of shards required for the specific database, as defined by the database template in the request. See Cloud Pricing The cost evaluation for each database is measured in quantity of the specific shard type required by the requested database. The cost per shard is defined by the fields pricePerUnit (where unit is a shard of the specific type), priceCurrency, and pricePeriod For example, to calculate the total hourly cost of a database, use the following formula: pricePerUnit * quantity The structure of the pricing response depends on the cloud account used by the request. There are two types of cloud accounts: a cloud account owned, named, and managed by the customer (AWS only) and one maintained by \u0026ldquo;Redis resources\u0026rdquo; cloud account (All cloud providers) For a customer-provided cloud account - The cost evaluation response lists the AWS resources required (storage and compute instances) without pricing data (which depends on the specific details of the customer\u0026rsquo;s AWS account) For a Redis internal cloud account (defined a cloudAccountId = 1 in the create subscription request) - The cost evaluation response includes a MinimumPrice element. This indicates the minimal hourly cost of the entire subscription. This minimum price will be charged if the sum of all shards for all the subscription\u0026rsquo;s databases is less than the specified minimum price Viewing actual subscription cost The Get subscriptions and Get subscription by id JSON response contains an element named subscriptionPricing that details the latest calculated cost of a subscription, grouped by shard type for all the databases in the subscription.\n","categories":["RC"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/execute/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/execute/","title":"Execute","tags":[],"keywords":[],"description":"Runs a Redis command. A more flexible version of executeArray.","content":"public static java.lang.Object execute​(java.lang.String... command) Runs a Redis command, similar to executeArray. However, the execute function is more flexible. Unlike executeArray, the list of string arguments does not have to be an explicit String[] object. It allows function calls like this: execute(\u0026quot;SET\u0026quot;, \u0026quot;key\u0026quot;, \u0026quot;value\u0026quot;).\nParameters Name Type Description command string A Redis command Returns Returns the command result. It could be a string or an array of strings, depending on the command.\nExamples Without String[]:\nGearsBuilder.execute(\u0026#34;SET\u0026#34;, \u0026#34;age:maximum\u0026#34;, \u0026#34;100\u0026#34;); With String[]:\nGearsBuilder.execute(new String[]{\u0026#34;SET\u0026#34;, \u0026#34;age:maximum\u0026#34;, \u0026#34;100\u0026#34;}); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/executearray/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/executearray/","title":"ExecuteArray","tags":[],"keywords":[],"description":"Runs a Redis command.","content":"public static native java.lang.Object executeArray( java.lang.String[] command) Runs a Redis command. It accepts an array of strings, which represents the command to execute.\nParameters Name Type Description command array of strings A Redis command Returns Returns the command result. It could be a string or an array of strings, depending on the command.\nExample GearsBuilder.executeArray(new String[]{\u0026#34;SET\u0026#34;, \u0026#34;age:maximum\u0026#34;, \u0026#34;100\u0026#34;}); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/filter/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/filter/","title":"Filter","tags":[],"keywords":[],"description":"Filters out records in the pipe based on a given condition.","content":"public GearsBuilder\u0026lt;T\u0026gt; filter​( gears.operations.FilterOperation\u0026lt;T\u0026gt; filter) Filters out records in the pipe based on a given condition.\nThe filter operation should contain a conditional statement and return a boolean for each record:\nIf true, the record will continue through the pipe. If false, it filters out the record. Parameters Name Type Description filter FilterOperation A function that checks a condition for each record in the pipe. Returns a boolean. Returns Returns a GearsBuilder object with the same template type as the input builder.\nExample Get all records that contain the substring \u0026ldquo;person:\u0026rdquo;:\nGearsBuilder.CreateGearsBuilder(reader). filter(r-\u0026gt;{ return r.getKey().contains(\u0026#34;person:\u0026#34;); }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/flatmap/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/flatmap/","title":"FlatMap","tags":[],"keywords":[],"description":"Maps a single input record to one or more output records.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; flatMap​( gears.operations.FlatMapOperation\u0026lt;T,​I\u0026gt; flatmapper) Maps a single input record to one or more output records.\nThe FlatMap operation must return an Iterable. RedisGears splits the elements from the Iterable object and processes them as individual records.\nParameters Type parameters:\nName Description I The template type of the returned builder object Function parameters:\nName Type Description flatmapper FlatMapOperation\u0026lt;T,​I\u0026gt; For each input record, returns one or more output records Returns Returns a GearsBuilder object with a new template type.\nExample GearsBuilder.CreateGearsBuilder(reader).flatMap(r-\u0026gt;{ return r.getListVal(); }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/foreach/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/foreach/","title":"Foreach","tags":[],"keywords":[],"description":"For each record in the pipe, runs some operations.","content":"public GearsBuilder\u0026lt;T\u0026gt; foreach​( gears.operations.ForeachOperation\u0026lt;T\u0026gt; foreach) Defines a set of operations to run for each record in the pipe.\nParameters Name Type Description foreach ForeachOperation The set of operations to run for each record Returns Returns a GearsBuilder object with a new template type.\nExample For each person hash, add a new full_name field that combines their first and last names:\nGearsBuilder.CreateGearsBuilder(reader).foreach(r-\u0026gt;{ String firstName = r.getHashVal().get(\u0026#34;first_name\u0026#34;); String lastName = r.getHashVal().get(\u0026#34;last_name\u0026#34;); r.getHashVal().put(\u0026#34;full_name\u0026#34;, firstName + lastName); }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/hashtag/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/hashtag/","title":"Hashtag","tags":[],"keywords":[],"description":"Returns a string that maps to the current shard.","content":"public static java.lang.String hashtag() Returns a string that maps to the current shard according to the cluster slot mapping.\nNote: You can use the hashtag function when you need to create a key that resides on the current shard. Parameters None\nReturns Returns a string that maps to the current shard.\nExample The following example uses the hashtag function to calculate the hslot. The string maps to the current shard.\nGearsBuilder.execute( \u0026#34;SET\u0026#34;, String.format(\u0026#34;key{%s}\u0026#34;, GearsBuilder.hashtag()), \u0026#34;1\u0026#34; ); ","categories":["Modules"]},{"uri":"/rs/references/client_references/client_java/","uriRel":"/rs/references/client_references/client_java/","title":"Redis with Java","tags":[],"keywords":[],"description":"The clients Lettuce and Jedis allow you to use Redis with Java.","content":"To use Redis with Java, you need a Java Redis client. The following sections demonstrate the use of two Java client libraries for Redis: Lettuce and Jedis. Additional Java clients for Redis can be found under the Java section of the Redis Clients page.\nLettuce Lettuce is a thread-safe Redis client that supports both synchronous and asynchronous connections.\nInstall Lettuce See Lettuce\u0026rsquo;s README file for installation instructions.\nAdd the following Maven dependency to your pom.xml file to use Lettuce:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;biz.paluch.redis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lettuce\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.Final\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; You can also download the latest Lettuce release from the GitHub repository.\nConnect to Redis The following code creates a connection to Redis using Lettuce:\nimport com.lambdaworks.redis.*; public class ConnectToRedis { public static void main(String[] args) { RedisClient redisClient = new RedisClient( RedisURI.create(\u0026#34;redis://password@host:port\u0026#34;)); RedisConnection\u0026lt;String, String\u0026gt; connection = redisClient.connect(); System.out.println(\u0026#34;Connected to Redis\u0026#34;); connection.close(); redisClient.shutdown(); } } To adapt this example to your code, replace the following values in line 7\u0026rsquo;s URI string with your database\u0026rsquo;s values:\nSet password to your database\u0026rsquo;s password or remove password@ to connect without authentication Set host to your database\u0026rsquo;s host Set port to your database\u0026rsquo;s port Lettuce is thread-safe, and the same Lettuce connection can be used from different threads. Using multiple connections is also possible.\nSpring integration If you\u0026rsquo;re using Spring, add the following XML to your bean configuration file to create a Lettuce instance:\n\u0026lt;bean id=\u0026#34;RedisClient\u0026#34; class=\u0026#34;com.lambdaworks.redis.support.RedisClientFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;uri\u0026#34; value=\u0026#34;redis://host:port\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; Then use it within your managed beans as follows:\nimport com.lambdaworks.redis.*; import org.springframework.beans.factory.annotation.Autowired; public class MySpringBean { private RedisClient redisClient; @Autowired public void setRedisClient(RedisClient redisClient) { this.redisClient = redisClient; } public String ping() { RedisConnection\u0026lt;String, String\u0026gt; connection = redisClient.connect(); String result = connection.ping(); connection.close(); return result; } } Once your standalone application exits, remember to shutdown Lettuce by using the shutdown method:\nredisClient.shutdown(); If you are using Spring and CDI, the frameworks manage the resources for you, and you do not have to close the client using the shutdown method.\nExample code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n// open a connection to Redis ... connection.set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); String value = connection.get(\u0026#34;foo\u0026#34;); System.out.println(value); Example output:\n$ java ReadWriteExample Connected to Redis bar SSL For an added security measure, you can secure the connection using SSL connections. Lettuce supports SSL connections natively.\nimport com.lambdaworks.redis.*; public class ConnectToRedisSSL { public static void main(String[] args) { RedisClient redisClient = new RedisClient( RedisURI.create(\u0026#34;rediss://password@host:port\u0026#34;)); RedisConnection\u0026lt;String, String\u0026gt; connection = redisClient.connect(); System.out.println(\u0026#34;Connected to Redis using SSL\u0026#34;); connection.close(); redisClient.shutdown(); } } Jedis Jedis is a simple and complete Java client for Redis.\nInstall Jedis See the Jedis README file for installation instructions.\nAdd the following Maven dependency to your pom.xml file to use Jedis:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.2\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;jar\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; You can also download the latest Jedis release from the GitHub repository. To build it, extract the source and run the following command:\n$ cd jedis ~/jedis$ make package Connect to Redis The following code creates a connection to Redis using Jedis:\nimport redis.clients.jedis.Jedis; public class JedisExample { public static void main(String[] args) throws Exception { Jedis jedis = new Jedis(\u0026#34;hostname\u0026#34;, port); jedis.auth(\u0026#34;password\u0026#34;); System.out.println(\u0026#34;Connected to Redis\u0026#34;); } } To adapt this example to your code, replace the following values with your database\u0026rsquo;s values:\nIn line 6, set hostname to your database\u0026rsquo;s hostname or IP address In line 6, set port to your database\u0026rsquo;s port In line 7, set password to your database\u0026rsquo;s password Example code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n// open a connection to Redis ... jedis.set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); String value = jedis.get(\u0026#34;foo\u0026#34;); System.out.println(value); Example output:\n$ java JedisExample Connected to Redis bar Connection pooling Jedis isn\u0026rsquo;t thread-safe, and the same Jedis instance shouldn\u0026rsquo;t be used from different threads. Instead, use JedisPool to handle multiple Jedis instances and connection maintenance.\nJedisPool requires Apache Commons Pool 2.3. Download it from Apache Commons or add the following Maven dependency to the pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; The following code instantiates a pool of connections:\nJedisPool pool = new JedisPool(new JedisPoolConfig(), \u0026#34;hostname\u0026#34;, port, Protocol.DEFAULT_TIMEOUT, \u0026#34;password\u0026#34;); Spring integration If you\u0026rsquo;re using Spring, add the following XML to your bean configuration file to create a JedisPool:\n\u0026lt;bean id=\u0026#34;jedisPool\u0026#34; class=\u0026#34;redis.clients.jedis.JedisPool\u0026#34;\u0026gt; \u0026lt;constructor-arg index=\u0026#34;0\u0026#34; ref=\u0026#34;jedisPoolConfig\u0026#34; /\u0026gt; \u0026lt;constructor-arg index=\u0026#34;1\u0026#34; value=\u0026#34;hostname\u0026#34; /\u0026gt; \u0026lt;constructor-arg index=\u0026#34;2\u0026#34; value=\u0026#34;port\u0026#34; /\u0026gt; \u0026lt;constructor-arg index=\u0026#34;3\u0026#34; value=\u0026#34;Protocol.DEFAULT_TIMEOUT\u0026#34; /\u0026gt; \u0026lt;constructor-arg index=\u0026#34;4\u0026#34; value=\u0026#34;password\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;jedisPoolConfig\u0026#34; class=\u0026#34;redis.clients.jedis.JedisPoolConfig\u0026#34; \u0026gt; \u0026lt;/bean\u0026gt; JedisPool is thread-safe and can be stored in a static variable and shared among threads. The following code gets a Jedis instance from the JedisPool:\nJedis redis = null; try { redis = redisPool.getResource(); return redis.get(keyName); } catch (JedisConnectionException e) { if (redis != null) { redisPool.returnBrokenResource(redis); redis = null; } throw e; } finally { if (redis != null) { redisPool.returnResource(redis); } } } Once your application exits, remember to dispose of the JedisPool by using the destroy method:\nredisPool.destroy(); SSL Jedis does not support SSL connections natively.\nFor an added security measure, you can secure the connection using stunnel or this Jedis fork, which includes SSL support.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/localaccumulateby/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/localaccumulateby/","title":"LocalAccumulateBy","tags":[],"keywords":[],"description":"Groups records and reduces each group to a single record per group locally on each shard.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; localAccumulateBy​( gears.operations.ExtractorOperation\u0026lt;T\u0026gt; extractor, gears.operations.AccumulateByOperation\u0026lt;T,​I\u0026gt; accumulator) The localAccumulateBy function is similar to accumulateBy, except it performs the operation locally on each shard without moving data between shards.\nOn each shard, it iterates through the records in the pipe, groups them based on the provided extractor, and then reduces each group to a single record per group with the accumulator function.\nThe initial value of the accumulator is null.\nParameters Type parameters:\nName Description I The template type of the returned builder Function parameters:\nName Type Description accumulator AccumulateByOperation\u0026lt;T,​I\u0026gt; A function with logic to update the accumulator value with each record extractor ExtractorOperation Extracts a specific value from each record Returns Returns a GearsBuilder object with a new template type.\nExample GearsBuilder.CreateGearsBuilder(reader). localAccumulateBy(r-\u0026gt;{ return r.getStringVal(); },(k, a, r)-\u0026gt;{ Integer ret = null; if(a == null) { ret = 0; }else { ret = (Integer)a; } return ret + 1; }); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/log/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/log/","title":"Log","tags":[],"keywords":[],"description":"Writes a log message to the Redis log file.","content":"public static void log​(java.lang.String msg) public static void log​(java.lang.String msg, LogLevel level) Writes a log message to the Redis log file. If you do not specify a LogLevel, it will default to NOTICE.\nParameters Name Type Description msg string The message to write to the log level LogLevel The log level (DEBUG, NOTICE, VERBOSE, WARNING) Returns None\nExample GearsBuilder.log( \u0026#34;Setting keys to expire after 1 month\u0026#34;, LogLevel.WARNING ); ","categories":["Modules"]},{"uri":"/rs/clusters/logging/log-security/","uriRel":"/rs/clusters/logging/log-security/","title":"Log security","tags":[],"keywords":[],"description":"","content":"Redis Enterprise comes with a set of logs on the server and available through the user interface to assist users in investigating actions taken on the server and to troubleshoot issues.\nSending logs to a remote logging server Redis Enterprise sends logs to syslog by default. You can send these logs to a remote logging server by configuring syslog.\nTo do this, modify the syslog or rsyslog configuration on your operating system to send logs in /var/opt/redislabs/log to a remote monitoring server of your choice.\nLog rotation Redis Enterprise uses the default logrotate daemon to schedule rotation of logs stored on the operating system. The configuration of log rotation may be found at /etc/logrotate.d.\nBy default the log rotation should occur on a daily basis. We recommend that you send log files to a remote logging server so that they can be more effectively maintained.\nThe below log rotation policy is enabled by default with Redis Enterprise but can be modified to meet your needs.\n/var/opt/redislabs/log/*.log { daily missingok copytruncate rotate 7 compress notifempty } Below describes what the log rotation this configuration policy puts into effect.\n/var/opt/redislabs/log/*.log - When logrotate runs it checks the files under directory /var/opt/redislabs/log/ and rotates any files that end with the extension .log.\nDaily - The interval is set to daily.\nMissingok - If there are missing logfiles don\u0026rsquo;t do anything.\nCopytruncate - Truncate the original log file to zero sizes after creating a copy.\nrotate 7 - Keep 7 log files and delete the rest.\ncompress - gzip log files.\nnotifempty - Don\u0026rsquo;t rotate the log file if it is empty\nNote: For large scale deployments, it may be nessesary to rotate logs at quicker intervals, such as hourly. This can be done through a cronjob or external vendor solutions. ","categories":["RS"]},{"uri":"/rc/api/get-started/manage-tasks/","uriRel":"/rc/api/get-started/manage-tasks/","title":"Manage API tasks","tags":[],"keywords":[],"description":"A task is an API operation that is performed asynchronously because it exceeds the time allowed for the synchronous request/response model.","content":"A task is an API operation that is performed asynchronously because it exceeds the time allowed for the synchronous request/response model.\nExamples of API operations that use tasks are:\ncreate subscription create database update database delete database All create, update, and delete API operations (POST, PUT, and DELETE) and some query operations (GET) use tasks.\nAfter you request an asynchronous operation, the operation returns a taskId that identities the specific task, and contains contextual and status data on the API operation performed by the task.\nTasks are part of the API processing and provisioning lifecycle.\nTask information When you query a task of an asynchronous API operation, the response to the request includes the task status and additional information about the task:\n{ \u0026#34;taskId\u0026#34;: \u0026#34;f3ec0e7b-0548-46e3-82f3-1977012ec738\u0026#34;, \u0026#34;commandType\u0026#34;: \u0026#34;subscriptionCreateRequest\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;received\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Task request received and is being queued for processing.\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2019-08-08T09:07:39.826Z\u0026#34;, \u0026#34;_links\u0026#34;: { \u0026#34;task\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;https://api.redislabs.com/v1/tasks/f3ec0e7b-0548-46e3-82f3-1977012ec738\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;getTaskStatusUpdates\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } } } Where:\ntaskId - The unique identifier (UUID) of the specific task commandType - The request (command) type status - The status of the task description - A description of the status timestamp - The time of the response in ISO-8601 date format and in the UTC timezone _links - URI links to resources related to the task including: A link to itself Additional links based on the context of the response Task status updates With the task ID, you can query the task status for updates and progress information. The response in the above example shows a URL with the title getTaskStatusUpdates. The URL in the href property returns updates for the specified task.\nThis request returns the updated status of the task identifier:\nGET \u0026#34;https://[host]/v1/tasks/\u0026lt;taskId\u0026gt;\u0026#34; The response to the getTaskStatusUpdates request shows:\n{ \u0026#34;taskId\u0026#34;: \u0026#34;36d4b04d-72d4-4404-8600-a223120a553e\u0026#34;, \u0026#34;commandType\u0026#34;: \u0026#34;subscriptionCreateRequest\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;processing-completed\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Request processing completed successfully and its resources are now being provisioned / de-provisioned.\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2019-08-08T06:49:15.929Z\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;resourceId\u0026#34;: 77899 }, \u0026#34;_links\u0026#34;: { \u0026#34;resource\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;https://api.redislabs.com/v1/subscriptions/77899\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;getSubscriptionInformation\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, \u0026#34;self\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;https://api.redislabs.com/v1/tasks/36d4b04d-72d4-4404-8600-a223120a553e\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } } } This response example shows:\nThe status value is \u0026quot;processing-completed\u0026quot;. The response field contains the resource identifier of the subscription resource changed by this task. The links array contains another getSubscriptionInformation URL that links to the newly created subscription. This link queries the subscription status during provisioning) Tasks list You can use the API operation GET /tasks to list the recently submitted and completed tasks for the current account.\nThis API operation returns a list of tasks for the current account, sorted by most recent status update.\nGET \u0026#34;https://$HOST/tasks\u0026#34; The result returns all the tasks submitted during the past 10 days.\n","categories":["RC"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/map/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/map/","title":"Map","tags":[],"keywords":[],"description":"Maps records one-to-one.","content":"public \u0026lt;I extends java.io.Serializable\u0026gt; GearsBuilder\u0026lt;I\u0026gt; map​( gears.operations.MapOperation\u0026lt;T,​I\u0026gt; mapper) Maps each input record in the pipe to an output record, one-to-one.\nParameters Type parameters:\nName Description I The template type of the returned builder Function parameters:\nName Type Description mapper MapOperation\u0026lt;T,​I\u0026gt; For each input record, returns a new output record Returns Returns a GearsBuilder object with a new template type.\nExample Map each record to its string value:\nGearsBuilder.CreateGearsBuilder(reader). map(r-\u0026gt;{ return r.getStringVal(); }); ","categories":["Modules"]},{"uri":"/rs/clusters/logging/rsyslog-logging/node-events/","uriRel":"/rs/clusters/logging/rsyslog-logging/node-events/","title":"Node alert and event logs","tags":[],"keywords":[],"description":"Logged node alerts and events","content":"The following node alerts and events can appear in syslog.\nUI alerts Logged alerts that appear in the UI\nAlert code name Alert as shown in the UI Severity Notes aof_slow_disk_io Redis performance is degraded as result of disk I/O limits true: error\nfalse: info cpu_utilization CPU utilization has reached % true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. ephemeral_storage Ephemeral storage has reached % of its capacity true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. failed Node failed critical free_flash Flash storage has reached % of its capacity true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. insufficient_disk_aofrw Node has insufficient disk space for AOF rewrite true: error\nfalse: info memory Node memory has reached % of its capacity true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. net_throughput Network throughput has reached MB/s true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. persistent_storage Persistent storage has reached % of its capacity true: warning\nfalse: info Has global_threshold parameter in the key/value section of the log entry. Non-UI events Logged events that do not appear in the UI\nEvent code name Severity Notes checks_error error Indicates that one or more node checks have failed node_abort_remove_request info node_remove_request info ","categories":["RS"]},{"uri":"/rs/clusters/optimize/","uriRel":"/rs/clusters/optimize/","title":"Optimize clusters","tags":[],"keywords":[],"description":"Configuration changes and information you can use to optimize your performance and memory usage.","content":"Benchmarking Redis Enterprise Use the memtier_benchmark tool to perform a performance benchmark of Redis Enterprise Software.\nDisk sizing for heavy write scenarios Determine the required persistent disk space for heavy throughput databases.\nCluster environment optimization Optimize your cluster environments for better performance.\nTurn off services to free system memory Turn off services to free memory and improve performance.\nRedis OSS Cluster API Use the Redis OSS Cluster API to improve performance and keep applications current with cluster topology changes.\nWAIT command Use the WAIT command to control the consistency and durability guarantees for the replicated and persisted database.\n","categories":["RS"]},{"uri":"/rc/security/private-service-connect/","uriRel":"/rc/security/private-service-connect/","title":"Enable Private Service Connect","tags":[],"keywords":[],"description":"Private Service Connect creates a private endpoint that allows secure connections to Redis Cloud databases without exposing your application VPC.","content":"Private Service Connect (PSC) creates a private endpoint that allows secure connections to Redis Cloud databases without exposing your application\u0026rsquo;s virtual private cloud (VPC).\nThis feature is only available for Flexible and Annual subscriptions hosted on Google Cloud Platform (GCP).\nConsiderations You can use Private Service Connect as an alternative to VPC peering, or you can enable both for your subscription.\nCompared to VPC peering, Private Service Connect:\nOnly exposes the private endpoint instead of the entire application VPC network.\nAllows producer (Redis Cloud VPC) and consumer (application VPC) CIDR ranges to overlap.\nHas slightly higher network latency than VPC peering due to load balancing requirements.\nNote: Larger clusters are more likely to experience increased latency with Private Service Connect versus VPC peering. Consider using VPC peering and Private Service Connect in parallel for the following situations:\nWhen migrating from one connectivity solution to the other.\nIf different applications need to connect to the same database but have different latency or security requirements.\nSet up Private Service Connect To set up Private Service Connect, you need to:\nConfigure Private Service Connect in the Redis Cloud admin console.\nCreate Private Service Connect endpoints in the application VPC.\nFrom the Redis Cloud admin console, review and accept the Private Service Connect endpoint connection.\nConfigure PSC First, configure Private Service Connect in Redis Cloud:\nSelect Subscriptions from the Redis Cloud admin console menu and then select your subscription from the list.\nSelect the Connectivity tab and then Private Service Connect.\nSelect the Create connection button:\nRead the Latency and cost impact message and select Accept and continue:\nFor Create connection, enter the following Endpoint details:\nSetting name Description Google Cloud project ID GCP project ID VPC name Name of the VPC that hosts your application Subnet name Name of your VPC\u0026rsquo;s subnet of IP address ranges Endpoint name Prefix used to create PSC endpoints in the consumer application VPC, so endpoint names appear in GCP as endpoint name prefix + endpoint number Continue to the Add connections step:\nSelect either Bash Shell or PowerShell and then download or copy the provided gcloud script for later:\nSelect Continue to save this endpoint configuration:\nCreate endpoints Now that you have a pending Private Service Connect entry, you need to create the endpoints in your application\u0026rsquo;s VPC:\nFollow GCP\u0026rsquo;s guide to enable Cloud DNS for your GCP project if you haven\u0026rsquo;t already.\nNote: Since it can take some time for the DNS changes to become active, we recommend you wait 10 minutes before running the gcloud script in the next steps. If you already have a copy of the gcloud script shown earlier during the Add connections step, you can continue to the next step.\nOtherwise, return to your Redis Cloud subscription\u0026rsquo;s Connectivity \u0026gt; Private Service Connect screen and select Complete setup for the pending endpoint:\nDownload or copy the script.\nUse the gcloud CLI to run the script.\nWarning - To ensure the gcloud script configures the endpoints correctly, do not make any changes to it. The gcloud script creates 40 endpoints in the consumer application VPC. Each endpoint appears in GCP as the configured endpoint name followed by the endpoint number.\nRedis Cloud displays this collection of endpoints as a single endpoint in the admin console.\nAccept PSC connection After the gcloud script finishes creating the Private Service Connect endpoints, you need to accept the connection in Redis Cloud:\nIn the Redis Cloud admin console, return to your subscription\u0026rsquo;s Connectivity \u0026gt; Private Service Connect screen.\nFind your pending endpoint connection in the list and select Accept:\nConnect to database Once your Private Service Connect endpoint is active, you can connect your application to a database:\nFrom your subscription\u0026rsquo;s Connectivity \u0026gt; Private Service Connect tab, select the Connect button for the active endpoint:\nSelect a database from the list.\nCopy the endpoint and use it in your application to connect to your database.\nDeactivate Private Service Connect To deactivate Private Service Connect for your subscription:\nSelect Subscriptions from the Redis Cloud admin console menu and then select your subscription from the list.\nSelect the Connectivity tab and then Private Service Connect.\nFor each endpoint:\nSelect the Delete PSC endpoint button:\nCopy the provided gcloud script from the Remove endpoint dialog.\nRun the gcloud script with gcloud CLI to delete the endpoint.\nAfter you remove all endpoints, select the Actions button to see a list of available actions:\nSelect Remove service and then Confirm:\nLimitations Private Service Connect has the following limitations in Redis Cloud:\nAlthough Redis Cloud supports using Private Service Connect with Enterprise clustering, you cannot use the OSS Cluster API with Private Service Connect enabled.\nPrivate Service Connect is not available for clusters with Redis versions 6.2.12 and earlier. Contact Redis support to upgrade the cluster to a compatible version or create a new subscription.\n","categories":["RC"]},{"uri":"/kubernetes/","uriRel":"/kubernetes/","title":"Redis Enterprise for Kubernetes","tags":[],"keywords":[],"description":"The Redis Enterprise operators allows you to use Redis Enterprise for Kubernetes.","content":"Kubernetes provides enterprise orchestration of containers and has been widely adopted. Redis Enterprise for Kubernetes provides a simple way to get a Redis Enterprise cluster on Kubernetes and enables more complex deployment scenarios.\nRedis Enterprise for Kubernetes architecture This section provides an overview of the architecture and considerations for Redis Enterprise for Kubernetes.\nDeployment This section lists the different ways to set up and run Redis Enterprise for Kubernetes. You can deploy on variety of Kubernetes distributions both on-prem and in the cloud via our Redis Enterprise operator for Kubernetes.\nRedis Enterprise clusters (REC) Articles to help you manage your Redis Enterprise clusters (REC).\nRedis Enterprise databases (REDB) Articles to help you manage your Redis Enterprise databases (REDBs).\nSecurity Security settings and configuration for Redis Enterprise for Kubernetes\nRedis Enterprise Software logs on Kubernetes This section provides information about how logs are stored and accessed.\nManage memory resources Settings and configuration to manage memory resources for your Redis Enterprise cluster.\nReference Reference material for the operator, cluster, and database deployment options.\nRedis Enterprise Software for Kubernetes release notes Redis Enterprise Software for Kubernetes operator release notes.\nRedis Enterprise for Kubernetes FAQs Here are some frequently asked questions about Redis Enterprise on integration platforms. What is an Operator? An operator is a Kubernetes custom controller which extends the native K8s API. Refer to the article Redis Enterprise K8s Operator-based deployments – Overview. Does Redis Enterprise operator support multiple RECs per namespace? Redis Enterprise for Kubernetes may only deploy a single Redis Enterprise cluster (REC) per namespace. Each REC can run multiple databases while maintaining high capacity and performance.\n","categories":["Platforms"]},{"uri":"/rs/databases/redis-on-flash/","uriRel":"/rs/databases/redis-on-flash/","title":"Redis on Flash","tags":[],"keywords":[],"description":"Redis on Flash enables your data to span both RAM and dedicated flash memory.","content":"Redis on Flash (RoF) offers Redis Enterprise users the unique ability to have large Redis databases but at significant cost savings. Where standard Redis databases must all be in RAM, Redis on Flash enables your databases to span both RAM and dedicated flash memory in the form of SSDs (solid state drives).\nYour data is distributed between RAM and flash memory so that your frequently used data is accessible in RAM, while the values you need less often are stored on the flash memory. Since flash memory usually comes with more storage space and less cost than RAM, this can result in significant savings for large datasets.\nJust like all-RAM databases, RoF is compatible with existing Redis applications. Databases that employ RoF are identical to all-RAM Redis Enterprise Software databases in characteristics and features.\nRedis on Flash is also supported on Redis Cloud and Redis Enterprise Software for Kubernetes.\nUse cases The benefits associated with Redis on Flash are dependent on the use case.\nRedis on Flash is ideal when your:\nworking set is significantly smaller than your dataset average key size is smaller than average value size most recent data is the most frequently used Redis on Flash is not recommended for:\nLong key names (all key names are stored in RAM) Broad access patterns (any value could be pulled into RAM) Large working sets (working set is stored in RAM) Frequently moved data (moving to and from RAM too often can impact performance) Redis on Flash is not intended to be used for persistent storage. Redis Enterprise Software database persistent and ephemeral storage should be on different disks, either local or attached.\nWhere is my data? When using Redis on Flash, RAM storage holds:\nAll keys (names) Key indexes Dictionaries Hot data (working set) All data is accessed through RAM. If a value in flash memory is accessed, it becomes part of the working set and is moved to RAM. These values are referred to as “hot data”.\nInactive or infrequently accessed data is referred to as “warm data” and stored in flash memory. When more space is needed in RAM, warm data is moved from RAM to flash storage.\nNote: When using Redis on Flash with RediSearch, it’s important to note that RediSearch indexes are also stored in RAM. RAM to Flash ratio You can easily configure or tune the ratio of RAM-to-Flash for each database independently, optimizing performance for your specific use case. This is an online operation requiring no downtime for your database.\nThe RAM size cannot be smaller than 10% or larger than 60% of the total memory. We recommend you keep at least 20% of all values in RAM.\nFlash memory Implementing Redis on Flash requires preplanning around memory and sizing. Considerations and requirements for Redis on Flash include:\nFlash memory must be locally attached (as opposed to network attached. Flash memory must be dedicated to RoF and not shared with other parts of the database, such as durability, binaries, or persistence. For the best performance, the SSDs should be NVMe based, but SATA can also be used. Note: The Redis Enterprise Software database persistent and ephemeral storage should be on different disks, either local or attached. Once these requirements are met, you can create and manage both Redis on Flash databases and all-RAM databases in the same cluster.\nWhen you begin planning the deployment of Redis on Flash in production, we recommend working closely with the Redis technical team for sizing and performance tuning.\nCloud environments When running in a cloud environment:\nFlash memory is on the ephemeral SSDs of the cloud instance. Persistent database storage needs to be network attached (for example, AWS EBS for AWS). Note: We specifically recommend \u0026ldquo;Storage Optimized I4i - High I/O Instances\u0026rdquo; because of the performance of NVMe for flash memory. On-premises environments When you begin planning the deployment of Redis on Flash in production, we recommend working closely with the Redis technical team for sizing and performance tuning.\nOn-premises environments support more deployment options than other environments such as:\nUsing Active-Active distributed databases Using supported modules RediSearch RedisJSON RedisTimeSeries RedisBloom Warning - Redis on Flash is not supported running on network attached storage (NAS), storage area network (SAN), or with local HDD drives. Next steps Redis on Flash metrics\nRedis on Flash quick start\nEphemeral and persistent storage\nHardware requirements\n","categories":["RS"]},{"uri":"/modules/redistimeseries/","uriRel":"/modules/redistimeseries/","title":"RedisTimeSeries","tags":[],"keywords":[],"description":"","content":"The RedisTimeSeries module lets you manage time series data with Redis.\nFeatures Query by start time and end-time Query by labels sets Aggregated queries (Min, Max, Avg, Sum, Range, Count, First, Last) for any time bucket Configurable max retention period Compaction/Roll-ups - automatically updated aggregated time series labels index - each key has labels which allows query by labels Memory model A time series is a linked list of memory chunks. Each chunk has a predefined size of samples. Each sample is a tuple of the time and the value of 128 bits, 64 bits for the timestamp and 64 bits for the value.\nRedisTimeSeries capabilities In RedisTimeSeries, we introduce a new data type that uses chunks of memory of fixed size for time series samples, indexed by the same Radix Tree implementation as Redis Streams. With Streams, you can create a capped stream, effectively limiting the number of messages by count. In RedisTimeSeries, you can apply a retention policy in milliseconds. This is better for time series use cases, because they are typically interested in the data during a given time window, rather than a fixed number of samples.\nDownsampling/compaction Before Downsampling After Downsampling If you want to keep all of your raw data points indefinitely, your data set grows linearly over time. However, if your use case allows you to have less fine-grained data further back in time, downsampling can be applied. This allows you to keep fewer historical data points by aggregating raw data for a given time window using a given aggregation function. RedisTimeSeries supports downsampling with the following aggregations: avg, sum, min, max, range, count, first, and last.\nSecondary indexing When using Redis’ core data structures, you can only retrieve a time series by knowing the exact key holding the time series. Unfortunately, for many time series use cases (such as root cause analysis or monitoring), your application won’t know the exact key it’s looking for. These use cases typically want to query a set of time series that relate to each other in a couple of dimensions to extract the insight you need. You could create your own secondary index with core Redis data structures to help with this, but it would come with a high development cost and require you to manage edge cases to make sure the index is correct.\nRedisTimeSeries does this indexing for you based on field value pairs called labels. You can add labels to each time series and use them to filter at query time.\nHere’s an example of creating a time series with two labels (sensor_id and area_id are the fields with values 2 and 32 respectively) and a retention window of 60,000 milliseconds:\nTS.CREATE temperature RETENTION 60000 LABELS sensor_id 2 area_id 32 Aggregation at read time When you need to query a time series, it’s cumbersome to stream all raw data points if you’re only interested in, say, an average over a given time interval. RedisTimeSeries follows the Redis philosophy to only transfer the minimum required data to ensure lowest latency.\nHere\u0026rsquo;s an example of aggregation over time buckets of 5,000 milliseconds:\n127.0.0.1:12543\u0026gt; TS.RANGE temperature:3:32 1548149180000 1548149210000 AGGREGATION avg 5000 1) 1) (integer) 1548149180000 2) \u0026#34;26.199999999999999\u0026#34; 2) 1) (integer) 1548149185000 2) \u0026#34;27.399999999999999\u0026#34; 3) 1) (integer) 1548149190000 2) \u0026#34;24.800000000000001\u0026#34; 4) 1) (integer) 1548149195000 2) \u0026#34;23.199999999999999\u0026#34; 5) 1) (integer) 1548149200000 2) \u0026#34;25.199999999999999\u0026#34; 6) 1) (integer) 1548149205000 2) \u0026#34;28\u0026#34; 7) 1) (integer) 1548149210000 2) \u0026#34;20\u0026#34; Integrations RedisTimeSeries comes with several integrations into existing time series tools. One such integration is our RedisTimeSeries adapter for Prometheus, which keeps all your monitoring metrics inside RedisTimeSeries while leveraging the entire Prometheus ecosystem.\nFurthermore, we also created direct integrations for Grafana and Telegraf. This repository contains a docker-compose setup of RedisTimeSeries, its remote write adaptor, Prometheus and Grafana. It also comes with a set of data generators and pre-built Grafana dashboards.\nTime series modelling approaches with Redis Data modelling approaches Redis Streams allows you to add several field value pairs in a message for a given timestamp. For each device, we collected 10 metrics that were modelled as 10 separate fields in a single stream message.\nFor Sorted Sets, we modeled the data in two different ways. For “Sorted Set per Device”, we concatenated the metrics and separated them out by colons, e.g. “\u0026lt;timestamp\u0026gt;:\u0026lt;metric1\u0026gt;:\u0026lt;metric2\u0026gt;: … :\u0026lt;metric10\u0026gt;”.\nOf course, this consumes less memory but needs more CPU cycles to get the correct metric at read time. It also implies that changing the number of metrics per device isn’t straightforward, which is why we also benchmarked a second Sorted Set approach. In “Sorted Set per Metric,” we kept each metric in its own Sorted Set and had 10 sorted sets per device. We logged values in the format “\u0026lt;timestamp\u0026gt;:\u0026lt;metric\u0026gt;”.\nAnother alternative approach would be to normalize the data by creating a hash with a unique key to track all measurements for a given device for a given timestamp. This key would then be the value in the sorted set. However, having to access many hashes to read a time series would come at a huge cost during read time, so we abandoned this path.\nIn RedisTimeSeries, each time series holds a single metric. We chose this design to maintain the Redis principle that a larger number of small keys is better than a fewer number of large keys.\nIt is important to note that our benchmark did not utilize RedisTimeSeries’ out-of-the-box secondary indexing capabilities. The module keeps a partial secondary index in each shard, and since the index inherits the same hash-slot of the key it indices, it is always hosted on the same shard. This approach would make the setup for native data structures even more complex to model, so for the sake of simplicity, we decided not to include it in our benchmarks. Additionally, while Redis Enterprise can use the proxy to fan out requests for commands like TS.MGET and TS.MRANGE to all the shards and aggregate the results, we chose not to exploit this advantage in the benchmark either.\nData ingestion For the data ingestion part of our benchmark, we compared the four approaches by measuring how many devices’ data we could ingest per second. Our client side had 8 worker threads with 50 connections each, and a pipeline of 50 commands per request.\nIngestion details of each approach:\nRedis Streams RedisTimeSeries Sorted Setper Device Sorted Setper Metric Command XADD TS.MADD ZADD ZADD Pipeline 50 50 50 50 Metrics per request 5000 5000 5000 500 # keys 4000 40000 4000 40000 All our ingestion operations were executed at sub-millisecond latency and, although both used the same Rax data structure, the RedisTimeSeries approach has slightly higher throughput than Redis Streams.\nEach approach yields different results, which shows the value of prototyping against specific use cases. As we see on query performance, the Sorted Set per Device comes with improved write throughput but at the expense of query performance. It’s a trade off between ingestion, query performance, and flexibility (remember the earlier data modeling remark).\nRead performance The read query we used in this benchmark queried a single time series and aggregated it in one-hour time buckets by keeping the maximum observed CPU percentage in each bucket. The time range we considered in the query was exactly one hour, so a single maximum value was returned. For RedisTimeSeries, this is out of the box functionality (as discussed earlier).\n127.0.0.1:12543\u0026gt; TS.RANGE cpu_usage_user{1340993056} 1451606390000 1451609990000 AGGREGATION max 3600000 For the Redis Streams and Sorted Sets approaches, we created the following LUA scripts. The client once again had 8 threads and 50 connections each. Since we executed the same query, only a single shard was hit, and in all four cases this shard maxed out at 100% CPU.\nThis is where you can see the real power of having dedicated data structure for a given use case with a toolbox that runs alongside it. RedisTimeSeries just blows all other approaches out of the water, and is the only one to achieve sub-millisecond response times.\nMemory utilization In both the Redis Streams and Sorted Set approaches, the samples were kept as a string, while in RedisTimeSeries it was a double. In this specific data set, we chose a CPU measurement with rounded integer values between 0-100, which thus consumes two bytes of memory as a string. In RedisTimeSeries, however, each metric had 64-bit precision.\nRedisTimeSeries can be seen to dramatically reduce the memory consumption when compared against both Sorted Set approaches. Given the unbounded nature of time series data, this is typically a critical criteria to evaluate - the overall data set size that needs to be retained in memory. Redis Streams reduces the memory consumption further but would be equal or higher than RedisTimeSeries when more digits for a higher precision would be required.\nMore info RedisTimeSeries commands RedisTimeSeries configuration RedisTimeSeries source ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/register/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/register/","title":"Register","tags":[],"keywords":[],"description":"Registers the pipeline of functions to run when certain events occur.","content":"public java.lang.String register() public java.lang.String register​(ExecutionMode mode) public java.lang.String register​( ExecutionMode mode, gears.operations.OnRegisteredOperation onRegister, gears.operations.OnUnregisteredOperation onUnregistered) Registers the pipeline of functions to run when certain events occur. The registered functions will run each time the event occurs.\nExecution modes:\nName Description ASYNC Runs asynchronously on all of the shards. ASYNC_LOCAL Runs asynchronously but only on the current shard that generated the event. SYNC Runs synchronously only on the same shard that generated the event. Note: If you call register() without specifying an execution mode, it will default to ASYNC. Parameters Name Type Description mode ExecutionMode The execution mode to use (ASYNC/ASYNC_LOCAL/SYNC) onRegister OnRegisteredOperation Register callback that will be called on each shard upon register onUnregistered OnUnregisteredOperation Unregister callback that will be called on each shard upon unregister Returns Returns a registration ID.\nExample GearsBuilder.CreateGearsBuilder(reader).register(); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/repartition/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/repartition/","title":"Repartition","tags":[],"keywords":[],"description":"Moves records between shards according to the extracted data.","content":"public GearsBuilder\u0026lt;T\u0026gt; repartition​( gears.operations.ExtractorOperation\u0026lt;T\u0026gt; extractor) Moves records between the shards. The extracted data determines the new shard location for each record.\nParameters Name Type Description extractor ExtractorOperation Extracts a specific value from each record Returns Returns a GearsBuilder object with a new template type.\nExample Repartition by value:\nGearsBuilder.CreateGearsBuilder(reader). repartition(r-\u0026gt;{ return r.getStringVal(); }); ","categories":["Modules"]},{"uri":"/rs/databases/configure/replica-ha/","uriRel":"/rs/databases/configure/replica-ha/","title":"Configure high availability for replica shards","tags":[],"keywords":[],"description":"Configure high availability for replica shards so that the cluster automatically migrates the replica shards to an available node.","content":"When you enable database replication, Redis Enterprise Software copies your data to a replica node to make your data highly available. If the replica node fails or if the primary (master) node fails and the replica is promoted to primary, the remaining primary node is a single point of failure.\nYou can configure high availability for replica shards so that the cluster automatically migrates the replica shards to an available node. This process is known as replica high availability or replica_ha (also known as slave_ha or Slave HA in the UI).\nAn available node:\nMeets replica migration requirements, such as rack-awareness. Has enough available RAM to store the replica shard. Does not also contain the master shard. In practice, replica migration creates a new replica shard and copies the data from the master shard to the new replica shard.\nFor example:\nNode:2 has a master shard and node:3 has the corresponding replica shard.\nEither:\nNode:2 fails and the replica shard on node:3 is promoted to master. Node:3 fails and the master shard is no longer replicated to the replica shard on the failed node. If replica HA is enabled, a new replica shard is created on an available node.\nThe data from the master shard is replicated to the new replica shard.\nNote: Replica HA follows all prerequisites of replica migration, such as rack-awareness. Replica HA migrates as many shards as possible based on available DRAM in the target node. When no DRAM is available, replica HA stops migrating replica shards to that node. Configuring high availability for replica shards Using rladmin or the REST API, replica HA is controlled on the database level and on the cluster level. You can enable or disable replica HA for a database or for the entire cluster.\nWhen replica HA is enabled for both the cluster and a database, replica shards for that database are automatically migrated to another node in the event of a master or replica shard failure. If replica HA is disabled at the cluster level, replica HA will not migrate replica shards even if replica HA is enabled for a database.\nNote: By default, replica HA is enabled for the cluster and disabled for each database. Note: For Active-Active databases, replica HA is enabled for the database by default to make sure that replica shards are available for Active-Active replication. To enable replica HA for a cluster using rladmin, run:\nrladmin tune cluster slave_ha enabled To disable replica HA for a specific database using rladmin, run:\nrladmin tune db \u0026lt;bdb_uid\u0026gt; slave_ha disabled Configuration options You can see the current configuration options for replica HA with:\nrladmin info cluster Grace period By default, replica HA has a 10-minute grace period after node failure and before new replica shards are created.\nTo configure this grace period from rladmin, run:\nrladmin tune cluster slave_ha_grace_period \u0026lt;time_in_seconds\u0026gt; Shard priority Replica shard migration is based on priority. When memory resources are limited, the most important replica shards are migrated first:\nslave_ha_priority - Replica shards with higher integer values are migrated before shards with lower values.\nTo assign priority to a database, run:\nrladmin tune db \u0026lt;bdb_uid\u0026gt; slave_ha_priority \u0026lt;positive integer\u0026gt; Active-Active databases - Active-Active database synchronization uses replica shards to synchronize between the replicas.\nDatabase size - It is easier and more efficient to move replica shards of smaller databases.\nDatabase UID - The replica shards of databases with a higher UID are moved first.\nCooldown periods Both the cluster and the database have cooldown periods. After node failure, the cluster cooldown period prevents another replica migration due to another node failure for any database in the cluster until the cooldown period ends (default: one hour).\nAfter a database is migrated with replica HA, it cannot go through another migration due to another node failure until the cooldown period for the database ends (Default: two hours).\nTo configure this grace period from rladmin, run:\nFor the cluster:\nrladmin tune cluster slave_ha_cooldown_period \u0026lt;time_in_seconds\u0026gt; For all databases in the cluster:\nrladmin tune cluster slave_ha_bdb_cooldown_period \u0026lt;time_in_seconds\u0026gt; Alerts The following alerts are sent during replica HA activation:\nShard migration begins after the grace period. Shard migration fails because there is no available node (sent hourly). Shard migration is delayed because of the cooldown period. ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/run/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/run/","title":"Run","tags":[],"keywords":[],"description":"Runs the pipeline of functions immediately.","content":"public void run() public void run​(boolean jsonSerialize, boolean collect) Runs the pipeline of functions immediately upon execution. It will only run once.\nParameters Name Type Description collect boolean Whether or not to collect the results from the entire cluster before returning them jsonSerialize boolean Whether or not to serialize the results to JSON before returning them Returns None\nExample GearsBuilder.CreateGearsBuilder(reader).run(); ","categories":["Modules"]},{"uri":"/rc/security/single-sign-on/saml-sso/","uriRel":"/rc/security/single-sign-on/saml-sso/","title":"SAML single sign-on","tags":[],"keywords":[],"description":"SAML single sign-on (SSO) with Redis Cloud.","content":"Redis Cloud supports both IdP-initiated and SP-initiated single sign-on (SSO) with SAML (Security Assertion Markup Language).\nYou cannot use SCIM (System for Cross-domain Identity Management) to provision Redis Cloud users. However, Redis Cloud supports just-in-time (JIT) user provisioning, which means Redis Cloud automatically creates a user account the first time a new user signs in with SAML SSO.\nSAML SSO overview When SAML SSO is enabled, the identity provider (IdP) admin handles SAML user management instead of the Redis Cloud account owner.\nAfter you activate SAML SSO for a Redis Cloud account, all existing local users for the account, except for the user that set up SAML SSO, are converted to SAML users and are required to use SAML SSO to sign in. Before they can sign in to Redis Cloud, the identity provider admin needs to set up these users on the IdP side and configure the redisAccountMapping attribute to map them to the appropriate Redis Cloud accounts and roles.\nIdP-initiated SSO With IdP-initiated single sign-on, you can select the Redis Cloud application after you sign in to your identity provider (IdP). This redirects you to the Redis Cloud admin console and signs you in to your SAML user account.\nSP-initiated SSO You can also initiate single sign-on from the Redis Cloud admin console. This process is known as service provider (SP)-initiated single sign-on.\nFrom the Redis Cloud admin console\u0026rsquo;s sign in screen, select the SSO button:\nEnter the email address associated with your SAML user account.\nSelect the Login button.\nIf you already have an active SSO session with your identity provider, this signs you in to your SAML user account.\nOtherwise, the SSO flow redirects you to your identity provider\u0026rsquo;s sign in screen.\nEnter your IdP user credentials to sign in.\nThis redirects you back to the Redis Cloud admin console and automatically signs in to your SAML user account.\nMulti-factor authentication The account owner remains a local user and should set up multi-factor authentication (MFA) to help secure their account. After SAML activation, the account owner can set up additional local bypass users with MFA enabled.\nIf MFA enforcement is enabled, note that Redis Cloud does not enforce MFA for SAML users since the identity provider handles MFA management and enforcement.\nSet up SAML SSO To set up SAML single sign-on for a Redis Cloud account:\nSet up a SAML app to integrate Redis Cloud with your identity provider.\nConfigure SAML in Redis Cloud.\nDownload service provider metadata and upload it to your identity provider.\nActivate SAML SSO.\nNote: SAML integration guides are available for several popular identity providers. You can contact Redis support to request a guide for your identity provider. Set up SAML app First, set up a SAML app to integrate Redis Cloud with your identity provider:\nSign in to your identity provider\u0026rsquo;s admin console.\nCreate or add a SAML integration app for the service provider Redis Cloud.\nSet up your SAML service provider app so the SAML assertion contains the following attributes:\nAttribute name\n(case-sensitive) Description FirstName User\u0026rsquo;s first name LastName User\u0026rsquo;s last name Email User\u0026rsquo;s email address (used as the username in the Redis Cloud console) redisAccountMapping Maps the user to multiple Redis Cloud accounts and roles (roles must be lowercase) For redisAccountMapping, you can add the same user to multiple SAML-enabled accounts with either:\nA single string that contains a comma-separated list of account/role pairs\n\u0026lt;saml2:Attribute Name=\u0026#34;redisAccountMapping\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:unspecified\u0026#34;\u0026gt; \u0026lt;saml2:AttributeValue xsi:type=\u0026#34;xs:string\u0026#34; xmlns:xs=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; 12345=owner,54321=manager \u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;/saml2:Attribute\u0026gt; Multiple strings, where each represents a single account/role pair\n\u0026lt;saml2:Attribute Name=\u0026#34;redisAccountMapping\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:unspecified\u0026#34;\u0026gt; \u0026lt;saml2:AttributeValue xsi:type=\u0026#34;xs:string\u0026#34; xmlns:xs=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; 12345=owner \u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;saml2:AttributeValue xsi:type=\u0026#34;xs:string\u0026#34; xmlns:xs=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; 54321=manager \u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;/saml2:Attribute\u0026gt; Note: To confirm the identity provider\u0026rsquo;s SAML assertions contain the required attributes, you can use a SAML-tracer web developer tool to inspect them. Set up any additional configuration required by your identity provider to ensure you can configure the redisAccountMapping attribute for SAML users.\nIf your identity provider lets you configure custom attributes with workflows or group rules, you can set up automation to configure the redisAccountMapping field automatically instead of manually.\nConfigure SAML in Redis Cloud After you set up the SAML integration app and create a SAML user in your identity provider, you need to configure the Redis Cloud account with some identity provider metadata:\nSign in to Redis Cloud with the email address associated with the SAML user you set up with your identity provider.\nSelect Access Management from the admin console menu.\nSelect Single Sign-On.\nSelect the Setup SSO button:\nYou need the following metadata values from your identity provider:\nSetting Description Issuer (IdP entity ID) The unique entity ID for the identity provider IdP server URL The identity provider\u0026rsquo;s HTTPS URL for SAML SSO Single logout URL The URL used to sign out of the identity provider and connected apps (optional) Assertion signing certificate Public SHA-256 certificate used to validate SAML assertions from the identity provider To find these metadata values, see your identity provider\u0026rsquo;s documentation.\nFrom the SAML screen of the Redis Cloud admin console, configure the Identity Provider metadata settings.\nEmail domain binding should match the email domain that SAML users will use to sign in from the Redis Cloud admin console (SP-initiated SSO).\nSelect the Enable button.\nFrom the SAML activation dialog box, select Continue.\nDownload service provider metadata Next, you need to download the service provider metadata for Redis Cloud and use it to finish configuring the SAML integration app for your identity provider:\nSelect the Download button to download the service provider metadata in XML format.\nSign in to your identity provider\u0026rsquo;s admin console.\nConfigure the Redis Cloud service provider app with the downloaded XML.\nSome identity providers let you upload the XML file directly.\nOthers require you to manually configure the service provider app with specific metadata fields, such as:\nXML attribute Value Description EntityDescriptor\u0026rsquo;s entityID https://auth.redis.com/saml2/service-provider/\u0026lt;ID\u0026gt; Unique URL that identifies the Redis Cloud service provider AssertionConsumerService\u0026rsquo;s Location https://auth.redis.com/sso/saml2/\u0026lt;ID\u0026gt; The service provider endpoint where the identity provider sends a SAML assertion that authenticates a user To use IdP-initiated SSO with certain identity providers, you also need to set the RelayState parameter to the following URL:\nhttps://app.redislabs.com/#/login/?idpId=\u0026lt;ID\u0026gt; Note: Replace \u0026lt;ID\u0026gt; so it matches the AssertionConsumerService Location URL\u0026rsquo;s ID. To learn more about how to configure service provider apps, see your identity provider\u0026rsquo;s documentation.\nActivate SAML SSO After you finish the required SAML SSO configuration between your identity provider and Redis Cloud account, you can test and activate SAML SSO.\nAll users associated with the account, excluding the local user you used to set up SAML SSO, are converted to SAML users on successful activation. They can no longer sign in with their previous sign-in method and must use SAML SSO instead. However, you can add local bypass users after SAML SSO activation to allow access to the account in case of identity provider downtime or other issues with SAML SSO.\nTo activate SAML SSO:\nSign out of any active SSO sessions with your identity provider.\nFor Activate SAML integration, select the Activate button.\nFrom the Logout notification dialog, select Continue. This redirects you to your configured identity provider\u0026rsquo;s sign-in screen.\nSign in with your identity provider.\nWhen redirected to the Redis Cloud sign-in screen, you can either:\nSign in with your local credentials as usual.\nSelect the SSO button and enter the email address associated with the SAML user configured in your identity provider:\nThis will convert your user to a SAML user in Redis Cloud, so do not use this method if you want your user account to remain a local bypass user.\nNote: If you see a SAML activation failed notification when redirected to the Redis Cloud sign-in screen, sign in with your local user credentials and review the SAML configuration for issues. After you activate SAML SSO, add a few local bypass users from the Team tab. Local bypass users should set up MFA for additional security.\nUpdate configuration If you change certain metadata or configuration settings after you set up SAML SSO, such as the assertion signing certificate, remember to do the following:\nUpdate the SAML SSO configuration with the new values.\nDownload the updated service provider metadata and use it to update the Redis Cloud service provider app.\nLink other accounts After you set up SAML SSO for one account, you can link other accounts you own to the existing SAML configuration. This lets you use the same SAML configuration for SSO across multiple accounts.\nTo link other accounts to an existing SAML SSO configuration:\nGo to Access Management \u0026gt; Single Sign-On in the Redis Cloud admin console.\nSelect the Edit button.\nFor Account linking, select the checkboxes for the other accounts you want to link to SAML SSO.\nSelect Save.\nFrom the Link accounts dialog, select Continue to enable SAML SSO for the selected accounts.\nDeactivate SAML SSO Before you can deactivate SAML SSO for an account, you must sign in to the account as a local (non-SAML) user with the owner role assigned.\nDeactivating SAML SSO for an account also removes any existing SAML-type users associated with the account.\nTo deactivate SAML SSO for a specific account:\nIn the Redis Cloud admin console, select your name to display your available accounts.\nIf the relevant account is not already selected, select it from the Switch account list.\nGo to Access Management \u0026gt; Single Sign-On.\nSelect Deactivate SAML. This only deactivates SAML SSO for the current account. Other linked accounts continue to use this SAML SSO configuration.\nSelect Deactivate to confirm deactivation:\nDeprovision SAML users It is important to deprovision Redis Cloud users that have API keys. When you revoke a user\u0026rsquo;s access to Redis Cloud through your identity provider, they cannot access the Redis Cloud admin console, but their API keys remain active.\nTo deprovision SAML users upon deletion, the identity provider admin can set up a webhook to automatically make the appropriate Cloud API requests.\nSee the Cloud API Swagger UI for more information about how to manage users with API requests.\n","categories":["RC"]},{"uri":"/kubernetes/security/","uriRel":"/kubernetes/security/","title":"Security","tags":[],"keywords":[],"description":"Security settings and configuration for Redis Enterprise for Kubernetes","content":"This section contains security settings and configuration for Redis Enterprise for Kubernetes.\nManage Redis Enterprise cluster (REC) credentials Redis Enterprise for Kubernetes uses a custom resource called RedisEnterpriseCluster to create a Redis Enterprise cluster (REC). During creation it generates random credentials for the operator to use. The credentials are saved in a Kubernetes (K8s) secret. The secret name defaults to the name of the cluster. Note: This procedure is only supported for operator versions 6.0.20-12 and above. Retrieve the current username and password The credentials can be used to access the Redis Enterprise admin console or the API.\nManage Redis Enterprise cluster (REC) certificates Install your own certificates to be used by the Redis Enterprise cluster\u0026#39;s operator.\nAdd client certificates Add client certificates to your REDB custom resource.\nEnable internode encryption Enable encryption for communication between REC nodes in your K8s cluster.\n","categories":["Platforms"]},{"uri":"/modules/redisgears/jvm/classes/gearsfuture/seterror/","uriRel":"/modules/redisgears/jvm/classes/gearsfuture/seterror/","title":"SetError","tags":[],"keywords":[],"description":"Sets an error message.","content":"public void setError​(java.lang.String error) throws java.lang.Exception Sets an error message for an asynchronous computation.\nParameters Name Type Description error string An error message Returns None\nExample GearsFuture\u0026lt;Boolean\u0026gt; f = new GearsFuture\u0026lt;Boolean\u0026gt;(); try { f.setError(\u0026#34;An error has occurred during asyncForeach\u0026#34;); } catch (Exception e) { e.printStackTrace(); } ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsfuture/setresult/","uriRel":"/modules/redisgears/jvm/classes/gearsfuture/setresult/","title":"SetResult","tags":[],"keywords":[],"description":"Sets a computation to run asynchronously.","content":"public void setResult​(I result) throws java.lang.Exception Sets a computation to run asynchronously.\nParameters Name Type Description result template type I The result of a computation Returns None\nExample GearsBuilder.CreateGearsBuilder(reader).map(r-\u0026gt;r.getKey()). asyncFilter(r-\u0026gt;{ GearsFuture\u0026lt;Boolean\u0026gt; f = new GearsFuture\u0026lt;Boolean\u0026gt;(); try { f.setResult(r.equals(\u0026#34;x\u0026#34;));\t} catch (Exception e) { e.printStackTrace(); }\treturn f; }); ","categories":["Modules"]},{"uri":"/kubernetes/memory/sizing-on-kubernetes/","uriRel":"/kubernetes/memory/sizing-on-kubernetes/","title":"Size and scale a Redis Enterprise cluster deployment on Kubernetes","tags":[],"keywords":[],"description":"This section provides information about sizing and scaling Redis Enterprise in a Kubernetes deployment.","content":"The following article reviews the mechanism and methods available for sizing and scaling a Redis Enterprise cluster deployment.\nFor minimum and recommended sizing, always follow the sizing guidelines detailed in the Redis Enterprise hardware requirements.\nSizing and scaling cluster nodes Setting the number of cluster nodes Define the number of cluster nodes in redis-enterprise-cluster.yaml file.\nspec: nodes: 3 The number of nodes in the cluster must be an uneven number equal to or greater than 3. Refer to the article Highly Available Redis for a detailed explanation on this topic.\nSet the number of cluster nodes during deployment by editing the redis-enterprise-cluster.yaml file and applying the file by running:\nkubectl apply -f redis-enterprise-cluster.yaml Scaling out To scale out a Redis Enterprise Cluster deployment, increase the number of nodes in the spec. For example, to scale the cluster out from 3 nodes to 5 nodes, edit the redis-enterprise-cluster.yaml file with the following:\nspec: nodes: 5 To apply the new cluster configuration run:\nkubectl apply -f redis-enterprise-cluster.yaml Note: Decreasing the number of nodes is not supported.\nSizing compute resources To set the compute resources required for each node, use the redisEnterpriseNodeResources spec field.\nUnder redisEnterpriseNodeResources spec, set the following fields according to the provided guidelines.\nlimits – specifies the maximum compute resources for a Redis node requests – specifies the minimum compute resources for a Redis node For example:\nredisEnterpriseNodeResources: limits: cpu: “2000m” memory: 4Gi requests: cpu: “2000m” memory: 4Gi The default values, if unspecified, are 2 cores (2000m) and 4GB (4Gi).\nSet the compute resources for cluster nodes during deployment by editing the redis-enterprise-cluster.yaml file and applying the file by running:\nkubectl apply -f redis-enterprise-cluster.yaml Scaling up node compute resources To scale up nodes in an existing Redis Enterprise Cluster deployment, adjust the cpu and memory parameters in the spec. For example, to scale nodes up to the recommended amount of compute resources, edit the redis-enterprise-cluster.yaml file with the following:\nredisEnterpriseNodeResources:\nlimits: cpu: “8000m” memory: 30Gi requests cpu: “8000m” memory: 30Gi Then, apply the file by running:\nkubectl apply -f redis-enterprise-cluster.yaml Warning - Be aware that [persistent volume size](/kubernetes/memory/persistent-volumes/ cannot be changed after deployment. When adjusting compute resources, make sure the ratio of persistent volume size and the new memory size are in accordance to the Hardware requirements article. ","categories":["Platforms"]},{"uri":"/rc/security/single-sign-on/social-login/","uriRel":"/rc/security/single-sign-on/social-login/","title":"Social login","tags":[],"keywords":[],"description":"Social login with Redis Cloud.","content":"Redis Cloud offers social login as a single sign-on (SSO) option. Social login lets you use an existing social media account to create or sign in to your Redis Cloud account.\nRedis Cloud supports the following social logins:\nGitHub Google Note: If your Google and GitHub accounts share an email address (such as Gmail), you can use either one to sign in to the same Redis Cloud account. Create a new account with social login Set up Google login Select Google from the sign in screen. Choose your preferred account from the list. Select Confirm on the Sign in with Google prompt. Set up GitHub login Select Github from the sign in screen. Select Authorize on the Authorize Redis Okta Auth Github prompt. Enter your GitHub password to Confirm access. Note: If the email address associated with your GitHub account is not public, you will see an error message that your sign in attempt failed. You need to make your email address public on GitHub before you try again. Migrate an existing account to social login If you already have a Redis Cloud account that requires an email address and password to sign in, you can migrate your existing account to use a social login associated with that same email address instead.\nWarning - Once you migrate your account to use social login, you cannot revert to your old email/password sign in method. Migrate to Google login To migrate your account to Google social login:\nSelect Google on the sign in screen. Choose your preferred account from the list. A confirmation prompt will display and warn that you cannot revert to your old sign in method if you proceed with the migration to social login. Select Confirm to continue migration. Migrate to GitHub login To migrate your account to GitHub social login:\nSelect Github on the sign in screen. Select Authorize on the Authorize Redis Okta Auth Github prompt. Enter your GitHub password to Confirm access. A confirmation prompt will display and warn that you cannot revert to your old sign in method if you proceed with the migration to social login. Select Confirm to continue migration. ","categories":["RC"]},{"uri":"/modules/install/upgrade-module/","uriRel":"/modules/install/upgrade-module/","title":"Upgrade modules","tags":[],"keywords":[],"description":"","content":"Upgrade a module in Redis Enterprise to get the latest features and fixes.\nNote: If you upgrade a single-node cluster, it does not load the new modules that are bundled with the new cluster version.\nBefore you upgrade a database with the RediSearch module enabled to Redis 5.0, you must upgrade the RediSearch module to version 1.4.2 or later.\nPrerequisites Before you upgrade a module enabled in a database, install the new version of the module on the cluster.\nUpgrade a module for a database After you install an updated module on the cluster, go to the configuration of the databases that use the module. The database configuration shows that a new version of the module is available.\nWarning - After you upgrade the module for a database, the database shards restart. This causes a short interruption in the availability of this database across the cluster. To upgrade a module enabled for a database:\nConnect to the terminal of a node in the cluster.\nRun rladmin status to list the databases on the node.\nCopy the name of the database that uses the module that you want to upgrade.\nFind the exact module name and version:\nExtract the module archive (zip) file. Open the JSON file. Find the module name and version number in the file. Here\u0026rsquo;s an example of the JSON file for the RediSearch module:\nTo see the versions of the modules on the cluster, run either:\nrladmin status modules - Shows the latest modules available on the cluster and the modules used by databases. rladmin status modules all - Shows all of the modules available on the cluster and the modules used by databases. To upgrade a database to the latest version of Redis and its modules to the latest version without changing the module arguments, run:\nrladmin upgrade db \u0026lt; database_name | database_ID \u0026gt; latest_with_modules Warning - The upgrade process does not validate the module upgrade arguments, and incorrect arguments can cause unexpected downtime. Test module upgrade commands in a test environment before you upgrade modules in production. Use keep_redis_version to upgrade the modules without upgrading the database to the latest Redis version.\nTo specify the modules to upgrade, add the following for each module:\nand module module_name \u0026lt;module_name\u0026gt; version \u0026lt;new_module_version_number\u0026gt; module_args \u0026#34;\u0026lt;module arguments\u0026gt;\u0026#34; For the module arguments, use one of the following:\nmodule_args \u0026quot;\u0026lt;module_arguments\u0026gt;\u0026quot; to replace the existing module arguments.\nmodule_args \u0026quot;\u0026quot; without arguments to remove the existing module arguments.\nmodule_args keep_args to use the existing module arguments.\nExamples Here are some module upgrade examples:\nKeep the current version of Redis and upgrade to the latest version of the enabled modules:\nrladmin upgrade db shopping_cart keep_redis_version latest_with_modules Upgrade the database to the latest Redis version and upgrade RediSearch to 1.6.7 with the specified arguments:\nrladmin upgrade db shopping_cart and module db_name shopping_cart module_name ft version 10607 module_args \u0026#34;PARTITIONS AUTO\u0026#34; Upgrade the database to the latest Redis version and upgrade RedisBloom to version 2.2.1 without arguments:\nrladmin upgrade db db:3 and module db_name shopping_cart module_name bf version 20201 module_args \u0026#34;\u0026#34; Upgrade RedisJSON to 1.0.4 with the existing arguments and RedisBloom to version 2.2.1 without arguments:\nrladmin upgrade module db_name MyDB module_name ReJSON version 10004 module_args keep_args and module db_name MyDB module_name bf version 20201 module_args \u0026#34;\u0026#34; Upgrade the database to use the latest version of Redis and use the latest version of the enabled modules:\nrladmin upgrade db shopping_cart latest_with_modules ","categories":["Modules"]},{"uri":"/rs/clusters/monitoring/uptrace-integration/","uriRel":"/rs/clusters/monitoring/uptrace-integration/","title":"Uptrace integration with Redis Enterprise Software","tags":[],"keywords":[],"description":"To collect, display, and monitor metrics data from your databases and other cluster components, you can connect Uptrace to your Redis Enterprise cluster using OpenTelemetry Collector.","content":"To collect, display, and monitor metrics data from your databases and other cluster components, you can connect Uptrace to your Redis Enterprise cluster using OpenTelemetry Collector.\nUptrace is an open source application performance monitoring (APM) tool that supports distributed tracing, metrics, and logs. You can use it to monitor applications and set up automatic alerts to receive notifications.\nWith OpenTelemetry Collector, you can receive, process, and export telemetry data to any monitoring tool. You can use it to scrape Prometheus metrics provided by Redis and then export those metrics to Uptrace.\nYou can use Uptrace to:\nCollect and display data metrics not available in the admin console. Use prebuilt dashboard templates maintained by the Uptrace community. Set up automatic alerts and receive notifications via email, Slack, Telegram, and others. Monitor your app performance and logs using tracing. Install Collector and Uptrace Because installing OpenTelemetry Collector and Uptrace can take some time, you can use the docker-compose example that also comes with Redis Enterprise cluster and AlertManager.\nAfter you download the Docker example, you can edit the following configuration files in the uptrace/example/redis-enterprise directory before you start the Docker containers:\notel-collector.yaml - Configures /etc/otelcol-contrib/config.yaml in the OpenTelemetry Collector container. uptrace.yml - Configures/etc/uptrace/uptrace.yml in the Uptrace container. You can also install OpenTelemetry and Uptrace from scratch using the following guides:\nGetting started with OpenTelemetry Collector Getting started with Uptrace After you install Uptrace, you can access the Uptrace UI at http://localhost:14318/.\nScrape Prometheus metrics Redis Enterprise cluster exposes a Prometheus scraping endpoint on http://localhost:8070/. You can scrape that endpoint by adding the following lines to the OpenTelemetry Collector config:\n# /etc/otelcol-contrib/config.yaml prometheus_simple/cluster1: collection_interval: 10s endpoint: \u0026#34;localhost:8070\u0026#34; # Redis Cluster endpoint metrics_path: \u0026#34;/\u0026#34; tls: insecure: false insecure_skip_verify: true min_version: \u0026#34;1.0\u0026#34; Next, you can export the collected metrics to Uptrace using OpenTelemetry protocol (OTLP):\n# /etc/otelcol-contrib/config.yaml receivers: otlp: protocols: grpc: http: exporters: otlp: # Uptrace is accepting metrics on this port endpoint: localhost:14317 headers: { \u0026#34;uptrace-dsn\u0026#34;: \u0026#34;http://project1_secret_token@localhost:14317/1\u0026#34; } tls: insecure: true service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp, prometheus_simple/cluster1] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Don\u0026rsquo;t forget to restart the Collector and then check logs for any errors:\ndocker-compose logs otel-collector # or sudo journalctl -u otelcol-contrib -f You can also check the full OpenTelemetry Collector config here.\nView metrics When metrics start arriving to Uptrace, you should see a couple of dashboards in the Metrics tab. In total, Uptrace should create 3 dashboards for Redis Enterprise metrics:\n\u0026ldquo;Redis: Nodes\u0026rdquo; dashboard displays a list of cluster nodes. You can select a node to view its metrics.\n\u0026ldquo;Redis: Databases\u0026rdquo; displays a list of Redis databases in all cluster nodes. To find a specific database, you can use filters or sort the table by columns.\n\u0026ldquo;Redis: Shards\u0026rdquo; contains a list of shards that you have in all cluster nodes. You can filter or sort shards and select a shard for more details.\nMonitor metrics To start monitoring metrics, you need to add some alerting rules.\nFor example, the following rule uses the group by node expression to create an alert whenever an individual Redis shard is down:\n# /etc/uptrace/uptrace.yml alerting: rules: - name: Redis shard is down metrics: - redis_up as $redis_up query: - group by cluster # monitor each cluster, - group by bdb # each database, - group by node # and each shard - $redis_up == 0 # shard should be down for 5 minutes to trigger an alert for: 5m # only monitor Uptrace projects with these ids projects: [1] You can also create queries with more complex expressions.\nFor example, the following rules create an alert when the keyspace hit rate is lower than 75% or memory fragmentation is too high:\n# /etc/uptrace/uptrace.yml alerting: rules: - name: Redis read hit rate \u0026lt; 75% metrics: - redis_keyspace_read_hits as $hits - redis_keyspace_read_misses as $misses query: - group by cluster - group by bdb - group by node - $hits / ($hits + $misses) \u0026lt; 0.75 for: 5m projects: [1] - name: Memory fragmentation is too high metrics: - redis_used_memory as $mem_used - redis_mem_fragmentation_ratio as $fragmentation query: - group by cluster - group by bdb - group by node - $mem_used \u0026gt; 32mb and $fragmentation \u0026gt; 3 for: 5m projects: [1] You can learn more about the query language here.\nSend notifications Uptrace does not manage notifications by itself and instead provides an integration with AlertManager.\nAlertManager handles alerts sent by client applications such as Uptrace and takes care of deduplicating, grouping, and routing notifications to configured receivers via email, Slack, Telegram, and many others.\nTo connect Uptrace to AlertManager and enable alert notifications, add the AlertManager API endpoint to uptrace.yml:\n# /etc/uptrace/uptrace.yml ## ## AlertManager client configuration. ## See https://uptrace.dev/get/alerting.html for details. ## ## Note that this is NOT an AlertManager config and you need to configure AlertManager separately. ## See https://prometheus.io/docs/alerting/latest/configuration/ for details. ## alertmanager_client: # AlertManager API endpoints that Uptrace uses to manage alerts. urls: - \u0026#34;http://localhost:9093/api/v2/alerts\u0026#34; ","categories":["RS"]},{"uri":"/rs/clusters/logging/rsyslog-logging/user-events/","uriRel":"/rs/clusters/logging/rsyslog-logging/user-events/","title":"User event logs","tags":[],"keywords":[],"description":"Logged user events","content":"The following user events can appear in syslog.\nNon-UI events Logged events that do not appear in the UI\nEvent code name Severity Notes user_created info user_deleted info user_updated info Indicates that a user configuration has been updated ","categories":["RS"]},{"uri":"/ri/release-notes/archive/v0.9.41/","uriRel":"/ri/release-notes/archive/v0.9.41/","title":"RDBTools v0.9.41, 4 July 2019","tags":[],"keywords":[],"description":"Added lazy loading for large JSON objects in the Browser tool; assorted bug fixes.","content":"Features Lazy loading for large JSONs in the Browser tool Bug fixes Client list: polling not being cleaned up properly when user navigates away from Client list tool Streams: Hide add entry button when no stream is selected. Streams: Added option to delete a stream Browser: Display message when searching members of hashes, sets, etc. doesn\u0026rsquo;t return any matches CLI: Visual bug Browser: Remove key from key list after it has been deleted Browser: Add newly added key to the key list Fixed visual inconsistencies ","categories":[]},{"uri":"/rc/security/","uriRel":"/rc/security/","title":"Security","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Cloud provides a number of ways to secure subscriptions and databases.\nAs a Redis Cloud user, here are a few things to consider when thinking about security:\nThe admin console Your databases The Redis Cloud API Before digging into all the details, you should review our shared responsibility model for security.\nAdmin console security The admin console is the web application you use to manage your Redis Cloud deployments.\nSecure access to the admin console by:\nAssigning appropriate roles to team members with access\nEnabling enabling multi-factor authentication\nDatabase security You have several options when it comes to securing your Redis Cloud databases. These include:\nencryption at rest role-based access control network security TLS network security using VPC peering and CIDR whitelist API security The Redis Enterprise Cloud API allows you to programmatically administer your subscriptions and database deployments. This API is disabled by default. When you enable the API, you can then manage the API keys for all owners of your Redis Cloud account. For an overview of the security features of the API, see the API authentication documentation.\n","categories":["RC"]},{"uri":"/ri/release-notes/archive/v0.9.40.1/","uriRel":"/ri/release-notes/archive/v0.9.40.1/","title":"RDBTools v0.9.40.1, June 2019","tags":[],"keywords":[],"description":"Added wildcard support to Profile tool search.  Multiple bug fixes handling JSON object.  Fixed a crash in Configuration tool.","content":"Bug fixes Fixed multiple issues in RedisJSON view in the Browser tool Fixed styling of JSON Rename ReJSON to JSON on Add Key Delete button is not shown on closed node in JSON Float strings were being considered as JSON strings and the user was not allowed to edit it (JSON object strings cannot be edited) Fixed a crash in the Configuration tool due to a JS bug * literal was not supported in the search bar in the Profiling tool ","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.40/","uriRel":"/ri/release-notes/archive/v0.9.40/","title":"RDBTools v0.9.40, 17 May 2019","tags":[],"keywords":[],"description":"RediSearch support.  Instance IDs are now UUIDs.  Fixes to Streams tool and Browser search.","content":"Features New RediSearch tool, which allows executing and visualising RediSearch queries. This is a preview feature, alpha quality. Not exposed to end users. Instance IDs are now UUIDs. This is a security improvement over using serial numbers Bug fixes Streams Tool: The time slider was not handing empty streams properly. The timestamps now show seconds as well, and in the same format as the slider Avoiding extra API call when a key is not selected Browser search bar was not responding if all keys in the redis database are loaded. ","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.39/","uriRel":"/ri/release-notes/archive/v0.9.39/","title":"RDBTools v0.9.39, 8 May 2019","tags":[],"keywords":[],"description":"RedisGraph support (alpha0).  Browser now supports RedisJSON. Improved bulk operations.","content":"New features RedisGraph tool A basic UI for running RedisGraph queries with a historic view of queries.\nNote: The RedisGraph tool is an alpha feature at this point and will only be enabled for a subset of early users. RedisJSON support in Browser Now, if the size of the JSON value exceeds a threshold, it will not be displayed in the browser, instead the user can download the entire JSON value directly as a file.\nBulk Operations Improvements\nA history of executed jobs is now available in the Bulk Operations tool. The internal implementation has been changed to be more reliable and accurate, especially when dealing with binary data. ","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.38/","uriRel":"/ri/release-notes/archive/v0.9.38/","title":"RDBTools v0.9.38, 3 May 2019","tags":[],"keywords":[],"description":"RedisJSON support.","content":"Features Added support for viewing and editing RedisJSON values in the Browser tool Added support for using RedisJSON commands (JSON.*) in the CLI tool ","categories":[]},{"uri":"/rs/security/access-control/ldap/migrate-to-role-based-ldap/","uriRel":"/rs/security/access-control/ldap/migrate-to-role-based-ldap/","title":"Migrate to role-based LDAP","tags":[],"keywords":[],"description":"Describes how to migrate existing cluster-based LDAP deployments to role-based LDAP.","content":"Redis Enterprise Software supports LDAP through a role-based mechanism, first introduced in v6.0.20.\nEarlier versions of Redis Enterprise Software supported a cluster-based mechanism; however, that mechanism was removed in v6.2.12.\nIf you\u0026rsquo;re using the cluster-based mechanism to enable LDAP authentication, you need to migrate to the role-based mechanism before upgrading to Redis Enterprise Software v6.2.12 or later.\nMigration checklist This checklist covers the basic process:\nIdentify accounts per app on the customer end.\nCreate or identify an LDAP user account on the server that is responsible for LDAP authentication and authorization.\nCreate or identify an LDAP group that contains the app team members.\nVerify or configure the Redis Enterprise ACLs.\nConfigure each database ACL.\nRemove the earlier \u0026ldquo;external\u0026rdquo; (LDAP) users from Redis Enterprise.\nUse Settings \u0026gt; LDAP to enable role-based LDAP.\nMap your LDAP groups to access control roles.\nTest application connectivity using the LDAP credentials of an app team member.\n(Recommended) Turn off default access for the database to avoid anonymous client connections.\nBecause deployments and requirements vary, you’ll likely need to adjust these guidelines.\nTest LDAP access To test your LDAP integration, you can:\nConnect with redis-cli and use the AUTH command to test LDAP username/password credentials.\nSign in to the admin console using LDAP credentials authorized for admin access.\nUse RedisInsight to access a database using authorized LDAP credentials.\nUse the REST API to connect using authorized LDAP credentials.\nMore info Enable and configure role-based LDAP Map LDAP groups to access control roles Update database ACLs to authorize LDAP access Learn more about Redis Enterprise Software security and practices ","categories":["RS"]},{"uri":"/ri/release-notes/archive/v0.9.37/","uriRel":"/ri/release-notes/archive/v0.9.37/","title":"RDBTools v0.9.37, 10 April 2019","tags":[],"keywords":[],"description":"Added bulk operation support.  Support Redis Enterprise Software. Bug fixes.","content":"New features Added two new bulk operations: Backup and Restore. These were previously part of the browser tool. Bulk operation now supports cluster instances (including Backup and Restore) Added support for Redis Enterprise (both standalone and clustered) Bug fixes Cached Redis connections now expire after a specific idle time Fixed UI bugs in the memory analysis dialog Fixed connection error handling in Client List tool Fixed cluster and sentinel badge not appearing for cluster instances in the main instances page. ","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.36/","uriRel":"/ri/release-notes/archive/v0.9.36/","title":"RDBTools v0.9.36, March 2019","tags":[],"keywords":[],"description":"Added Streams tool.  Updated Configuration tool to batch updates.  Fixed ElastiCache auto-discovery.","content":"New features Added the Streams tool The new Streams tool has landed in this release, a specialized browser tool to work specifically with the new Stream data type added in Redis 5.0. Batched updates and multi-node updates in Configuration tool New it\u0026rsquo;s possible to group multiple configuration changes together and push them in one batch. Also, the batch of changes can be pushed to multiple nodes in the cluster, making it super easy to change configration in cluster nodes. Bug fixes Fixed ElastiCache auto-discovery ","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.35/","uriRel":"/ri/release-notes/archive/v0.9.35/","title":"RDBTools v0.9.35, February 2019","tags":[],"keywords":[],"description":"Memory analysis: added eviction reports and other bug fixes.  RBAC UI improvements.","content":"Memory analysis bug fixes Fixed manual exit() call. Added proper error handling. Fixed RDB parsing bug Added support for streams Memory analysis eviction reports Added reports on keys based on LFU/LRU data extracted from rdb. Role based access control UI improvements New permissions are created automatically on instance or user addition. Replaced multiselect widget on Role admin panel page with checkboxed for permission fields. ","categories":[]},{"uri":"/kubernetes/release-notes/k8s-6-2-18-41-jan-2023/","uriRel":"/kubernetes/release-notes/k8s-6-2-18-41-jan-2023/","title":"Redis Enterprise for Kubernetes release notes 6.2.18-41 (Jan 2023)","tags":[],"keywords":[],"description":"This is a maintenance release for 6.2.18 and includes support for Redis Enterprise 6.2.18-72.","content":"Overview This is a maintenance release of Redis Enterprise for Kubernetes 6.2.18-41 that adds supports for Redis Enterprise 6.2.18-72.\nNew images and fixes are listed below. Refer to 6.2.18-41 (Dec 2022 for compatibility notes and known limitations.\nImages DockerHub images are available at docker.io/.\nRedis Enterprise: redislabs/redis:6.2.18-72 Operator: redislabs/operator:6.2.18-41 Services Rigger: redislabs/k8s-controller:6.2.18-41 OpenShift images OLM operator version: v6.2.18-41c\nRedis Enterprise: registry.connect.redhat.com/redislabs/redis-enterprise:6.2.18-72.rhel8-openshift (or registry.connect.redhat.com/redislabs/redis-enterprise:6.2.18-72.rhel7-openshift if upgrading from RHEL 7) Operator: registry.connect.redhat.com/redislabs/redis-enterprise-operator:6.2.18-41 Services Rigger: registry.connect.redhat.com/redislabs/services-manager:6.2.18-41 Feature enhancements Upgraded to support Redis Enterprise 6.2.18-72 Compatibility notes See Redis Enterprise for Kubernetes release notes 6.2.18-41 (Dec 2022).\nKnown limitations See Redis Enterprise for Kubernetes release notes 6.2.18-41 (Dec 2022).\n","categories":["Platforms"]},{"uri":"/ri/release-notes/archive/v0.9.34.2/","uriRel":"/ri/release-notes/archive/v0.9.34.2/","title":"RDBTools v0.9.34.2, 11 February 2019","tags":[],"keywords":[],"description":"Bug fixes","content":"Fixes Fixed a long-existing caching bug Compress static files Committed previously missed migration files for RBAC models ","categories":[]},{"uri":"/kubernetes/release-notes/k8s-6-2-18-41/","uriRel":"/kubernetes/release-notes/k8s-6-2-18-41/","title":"Redis Enterprise for Kubernetes release notes 6.2.18-41 (Dec 2022)","tags":[],"keywords":[],"description":"This is a maintenance release for 6.2.18 and includes bug fixes.","content":"Overview The Redis Enterprise K8s 6.2.18-41 is a maintenance release which supports the Redis Enterprise Software release 6.2.18 and contains bug fixes.\nThe key bug fixes and known limitations are described below.\nImages Redis Enterprise: redislabs/redis:6.2.18-65 Operator: redislabs/operator:6.2.18-41 Services Rigger: redislabs/k8s-controller:6.2.18-41 OpenShift images OLM operator version: v6.2.18-41a\nRedis Enterprise: registry.connect.redhat.com/redislabs/redis-enterprise:6.2.18-65.rhel8-openshift (or redislabs/redis-enterprise:6.2.18-65.rhel7-openshift if upgrading from RHEL 7) Operator: registry.connect.redhat.com/redislabs/redis-enterprise-operator:6.2.18-41 Services Rigger: registry.connect.redhat.com/redislabs/services-manager:6.2.18-41 Bug fixes Fixed issues with incorrect image digest (OLM/OpenShift) (RED-88863) Fixed upgrade issue for using NGINX-based ingress with Active-Active databases. (RED-88882) Upgraded to Redis Software 6.2.18-65 (RED-88985) Fixed upgrade issue with OLM (RED-89170) Removed non-core Python dependencies from the log collector (RED-90108) Fixed issues with running log collector on OLM (RED-90129 Compatibility notes Below is a table showing supported distributions at the time of this release. See Supported Kubernetes distributions for the current list of supported distributions.\nKubernetes version 1.21 1.22 1.23 1.24 1.25 Community Kubernetes supported supported supported supported Amazon EKS deprecated supported supported Azure AKS supported supported supported Google GKE deprecated supported supported supported supported Rancher 2.6 supported supported supported supported VMware TKG 1.6 supported supported OpenShift version 4.8 4.9 4.10 4.11 deprecated supported supported VMware TKGI version 1.12 1.13 1.14 1.15 supported supported supported * Support added in this release\nKnown limitations Long cluster names cause routes to be rejected (RED-25871)\nA cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542)\nA cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805)\nWhen a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300)\nSTS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462)\nDNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233)\nKubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884)\nIn Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825)\nIn OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254)\nWhen REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192)\nWhen a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nHashicorp Vault integration - no support for Gesher (RED-55080)\nThere is no workaround at this time.\nREC might report error states on initial startup (RED-61707)\nThere is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132)\nThe workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515)\nThe workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351)\nContact support if your deployment is impacted.\n","categories":["Platforms"]},{"uri":"/ri/installing/configurations/","uriRel":"/ri/installing/configurations/","title":"Configure RedisInsight","tags":[],"keywords":[],"description":"","content":"You can configure RedisInsight with system environment variables.\nTo configure RedisInsight with environment variables:\nFollow the instructions to set environment variables for your operating system:\nMac Windows Linux Docker Set the environment variables.\nRestart RedisInsight.\nRedisInsight environment variables The following environment variables can be set to configure RedisInsight:\nRIPORT Description: Which port should RedisInsight listen on.\nType: Number\nDefault: 8001\nRIHOST Description: Which host should RedisInsight bind to.\nType: String\nDefault: \u0026quot;0.0.0.0\u0026quot; on Docker and \u0026quot;127.0.0.1\u0026quot; on Windows, Mac, and Linux.\nRIHOMEDIR Description: Sets the storage directory where RedisInsight stores application data (such as local database, log files and snapshot files).\nType: String\nDefault: \u0026quot;~/.redisinsight\u0026quot; on desktop, \u0026quot;/db\u0026quot; on docker.\nRILOGDIR Description: Sets the logging storage directory where RedisInsight stores application logs.\nType: String\nDefault: \u0026quot;~/.redisinsight\u0026quot; on desktop, \u0026quot;/db\u0026quot; on docker.\nRILOGLEVEL Description: Configures the log level of the application. Possible values are - \u0026quot;DEBUG\u0026quot;, \u0026quot;INFO\u0026quot;, \u0026quot;WARNING\u0026quot;, \u0026quot;ERROR\u0026quot; and \u0026quot;CRITICAL\u0026quot;.\nType: String\nDefault: \u0026quot;WARNING\u0026quot;\nRITRUSTEDORIGINS Description: Configures the trusted origins of the application.\nType: String\nDefault: \u0026quot;\u0026quot;\nExamples: \u0026quot;https://my-website.com,https://my-another-website.com,http://my-third-website.com\u0026quot;\nRIPROXYENABLE Description: Enables Subpath Proxy for the application.\nType: Bool\nDefault: False\nRIPROXYPATH Description: Configures Subpath Proxy path for the application.\nType: String\nDefault: \u0026quot;\u0026quot;\nExamples: \u0026quot;/redisinsight\u0026quot;, \u0026quot;/myapp\u0026quot;\nRIPROXYPREFIX Description: Sets the Subpath proxy prefix HTTP header field name for the application. The application uses the value from this HTTP header key as proxy subpath.\nType: String\nDefault: \u0026quot;X-Forwarded-Prefix\u0026quot;\nExamples: \u0026quot;X-Forwarded-Prefix\u0026quot;, \u0026quot;X-Forwarded-Path\u0026quot;\nRIAUTHPROMPT Description: Enables authentication prompt that asks for authentication before opening an instance or when the user is idle.\nType: Bool\nDefault: false\nRIAUTHTIMER Description: Idle timer value for authentication prompt, in minutes.\nType: Bool\nDefault: 30\nREDISINSIGHT_PORT (DEPRECATED) Description: Which port should RedisInsight listen on.\nType: Number\nDefault: 8001\nDeprecated in: v1.9.0\nREDISINSIGHT_HOST (DEPRECATED) Description: Which host should RedisInsight bind to.\nType: String\nDefault: \u0026quot;0.0.0.0\u0026quot;\nDeprecated in: v1.9.0\nREDISINSIGHT_HOME_DIR (DEPRECATED) Description: Sets the storage directory where RedisInsight stores application data (such as local database, log files and snapshot files).\nType: String\nDefault: \u0026quot;~/.redisinsight\u0026quot; on desktop, \u0026quot;/db\u0026quot; on docker.\nDeprecated in: v1.9.0\nLOG_DIR (DEPRECATED) Description: Sets the logging storage directory where RedisInsight stores application logs.\nType: String\nDefault: \u0026quot;~/.redisinsight\u0026quot; on desktop, \u0026quot;/db\u0026quot; on docker.\n","categories":["RI"]},{"uri":"/ri/release-notes/archive/v0.9.34.1/","uriRel":"/ri/release-notes/archive/v0.9.34.1/","title":"RDBTools v0.9.34.1, 7 February 2019","tags":[],"keywords":[],"description":"Bug fixes.","content":"Bug fixes Added special handling for desktop mode in RBAC Enable GA only in production Removed full server reloads from the application Improved logging for memory analysis ","categories":[]},{"uri":"/ri/using-redisinsight/profiler/","uriRel":"/ri/using-redisinsight/profiler/","title":"Profiler","tags":[],"keywords":[],"description":"","content":"RedisInsight Profiler runs Redis MONITOR command, which analyzes every command sent to the redis instance. It parses the output of the MONITOR command and generates a summarized view. All the commands sent to the redis instance are monitored for the duration of the profiling.\nProfiler gives information about the number of commands processed, commands/second and number of connected clients. It also gives information about top prefixes, top keys and top commands.\nStart profiling - Starts the profiling.\nStop Profiling - Stops the profiler i.e. the monitor command.\nNote: Running monitor command is dangerous to the performance of your production server, hence the profiler is run for a maximum time of 5 minutes, if the user has not stopped it in between. This is to avoid overload on the server. ","categories":["RI"]},{"uri":"/kubernetes/release-notes/k8s-6-2-18-3/","uriRel":"/kubernetes/release-notes/k8s-6-2-18-3/","title":"Redis Enterprise for Kubernetes release notes 6.2.18-3 (Nov 2022)","tags":[],"keywords":[],"description":"Support added for additional distributions, as well as feature improvements and bug fixes.","content":"Overview The Redis Enterprise K8s 6.2.18-3 supports the Redis Enterprise Software release 6.2.18 and includes feature improvements and bug fixes.\nThe key bug fixes, new features, and known limitations are described below.\nWarning - If you use NGINX as an ingress controller for Redis Enterprise, do not upgrade to the 6.2.18-3 release. Skip this release and upgrade to version 6.2.18-41 instead. Images This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.18-58 or redislabs/redis:6.2.18-58.rhel8-openshift (or redislabs/redis:6.2.18-58.rhel7-openshift if upgrading from RHEL 7) Operator: redislabs/operator:6.2.18-3 Services Rigger: redislabs/k8s-controller:6.2.18-3 or redislabs/services-manager:6.2.18-3 (on the Red Hat registry) New features Support for Redis on Flash (RoF) (RED-78613) Feature improvements The podSecurityPolicyName field in RedisEnterpriseCluster resources is now deprecated for Kubernetes versions 1.24 or earlier, and invalid for 1.25 and later. Customers are advised to switch to using the PodSecurityAdmission or alternative methods to enforce pod security (RED-81921).\nAdded support for VMware Tanzu Kubernetes Grid (TKG), in addition to Tanzu Kubernetes Grid Integration Edition (TKGI) that was previously and is still supported (RED-65630).\nAdded support for PEM encryption through the Redis Enterprise cluster API(RED-78613).\nHardened security context constraints to align with standards for OpenShift 4.11 (RED-83215).\nChanged log collector default to avoid collection of non-Redis Enterprise logs and items (RED-83216).\nAllowed configuration of the Redis Enterprise cluster (REC) service type (RED-84644).\nBug fixes Allow any ingress class name annotation when using NGINX ingress controller. This is no longer required to be exactly nginx (RED-79205). Fixed log collector handling of namespace parameter on Windows (RED-83532). Fixed issue with updating credentials on Openshift when accessing the cluster externally with routes (RED-73251, RED-75329). API changes The following fields were added to the Redis Enterprise cluster (REC) API:\nAdded .services.apiService.type to allow configuration of the API service type. Made .redisOnFlashSpec available by default. Made .ocspConfiguration available by default for configuring OCSP stapling. Made .encryptPkeys available by default for configuring PEM encryption. The following fields were added to the Redis Enterprise database (REDB) API:\n.isRoF and .rofRamSize added to support the Redis on Flash feature. Compatibility notes Below is a table showing supported distributions at the time of this release. See Supported Kubernetes distributions for the current list of supported distributions.\nKubernetes version 1.21 1.22 1.23 1.24 1.25 Community Kubernetes supported supported supported supported Amazon EKS deprecated supported supported Azure AKS supported supported supported Google GKE deprecated supported supported supported supported* Rancher 2.6 supported supported supported supported* VMware TKG 1.6 supported* supported* OpenShift version 4.8 4.9 4.10 4.11 deprecated supported supported* VMware TKGI version 1.12 1.13 1.14 1.15 supported supported supported* * Support added in this release\nKnown limitations Long cluster names cause routes to be rejected (RED-25871)\nA cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542)\nA cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805)\nWhen a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300)\nSTS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462)\nDNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233)\nKubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884)\nIn Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825)\nIn OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254)\nWhen REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192)\nWhen a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nHashicorp Vault integration - no support for Gesher (RED-55080)\nThere is no workaround at this time.\nREC might report error states on initial startup (RED-61707)\nThere is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132)\nThe workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515)\nThe workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351)\nContact support if your deployment is impacted.\n","categories":["Platforms"]},{"uri":"/rs/security/access-control/active-active/","uriRel":"/rs/security/access-control/active-active/","title":"Access control for Active-Active databases","tags":[],"keywords":[],"description":"Access control for Active-Active databases.","content":"Users, roles, and Redis ACLs Users, roles, and Redis ACLs are cluster-level entities. Therefore they are applied per local participating cluster and Active-Active database instance, and they are not replicated or propagated to the other participating clusters and instances. ACLs will be enforced according to the instance the client is connected to. The Active-Active replication mechanism will propagate all the effects of the operation.\nUpdate admin credentials Active-Active databases use administrator credentials to manage operations.\nTo update the administrator user password on a cluster with Active-Active databases:\nFrom the user management page, update the administrator user password on the clusters you want to update.\nFor each participating cluster and each Active-Active database, update the admin user credentials to match the changes in step 1.\nWarning - Do not perform any management operations on the databases until these steps are complete. ","categories":["RS"]},{"uri":"/rc/api/get-started/process-lifecycle/","uriRel":"/rc/api/get-started/process-lifecycle/","title":"The API request lifecycle","tags":[],"keywords":[],"description":"API requests follow specific lifecycle phases and states, based on the complexity and length of execution of the operation.","content":"Flexible and Annual Redis Enterprise Cloud subscriptions can leverage a RESTful API that permits operations against a variety of resources, including servers, services, and related infrastructure.\nNote: The REST API is not supported for Fixed or Free subscriptions. Once it\u0026rsquo;s enabled, you can use the REST API to create, update, and delete subscriptions, databases, and other entities.\nAPI operations run asynchronously, which means that provisioning occurs in the background. When you submit a request, a background process starts working on it. The response object includes an ID that lets you determine the status of the background process as it performs its work.\nFor operations that do not create or modify resources (such as most GET operations), the API is sychronous; that is, the response object reports the results of the request.\nAsynchronous operations have two main phases: processing and provisioning. A resource is not available until both phases are complete.\nTask processing During this phase, the request is received, evaluated, planned, and executed.\nUse tasks to track requests Many operations are asychronous, including CREATE, UPDATE, and DELETE operations. The response objects for such operations provide a taskId identifier that lets you track the progress of the underlying operation.\nYou can query the taskId to track the state of a specific task:\nGET \u0026#34;https://[host]/v1/tasks/\u0026lt;taskId\u0026gt;\u0026#34; You can also query the state of all active tasks or recently completed tasks in your account:\nGET \u0026#34;https://[host]/v1/tasks\u0026#34; Task process states During the processing of a request, the task moves through these states:\nreceived - Request is received and awaits processing. processing-in-progress - A dedicated worker is processing the request. processing-completed - Request processing succeeded and the request is being provisioned (or de-provisioned, depending on the specific request). A response segment is included with the task status JSON response. The response includes a resourceId for each resource that the request creates, such as Subscription or Database ID. processing-error - Request processing failed. A detailed cause or reason is included in the task status JSON response. Note: A task that reaches the received state cannot be cancelled and it will await completion (i.e. processing and provisioning). If you wish to undo an operation that was performed by a task, perform a compensating action (for example: delete a subscription that was created unintentionally) Task provisioning phase When the processing phase succeeds and the task is in the processing-completed state, the provisioning phase starts. During the provisioning phase, the API orchestrates all of the infrastructure, resources, and dependencies required by the request.\nNote: The term \u0026ldquo;provisioning\u0026rdquo; refers to all infrastructure changes required in order to apply the request. This includes provisioning new or additional infrastructure. The provisioning phase may require several minutes to complete. You can query the resource identifier to track the progress of the provisioning phase.\nFor example, when you provision a new subscription, use this API call to query the status of the subscription:\nGET \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscription-id\u0026gt;\u0026#34; Where the \u0026lt;subscription-id\u0026gt; is the resource ID that you receive when the task is in the processing-completed state.\nProvisioning state values During the provisioning of a resource (such as a subscription, database, or cloud account) the resource transitions through these states:\npending - Provisioning is in progress. active - Provisioning completed successfully. deleting - De-provisioning and deletion is in progress. error - An error occurred during the provisioning phase, including the details of the error. Process limitations The following limitations apply to asynchronous operations:\nFor each account, only one operation is processed concurrently. When multiple tasks are sent for the same account, they will be received and processed one after the other.\nThe provisioning phase can be performed in parallel except:\nSubscription creation, update, and deletion: You cannot change (make non-active) more than three subscriptions at the same time. Database creation in an existing subscription: This can cause the subscription state to change from active to pending during database provisioning in cases such as database sizing that requires cluster resizing or updating cluster metadata. For example:\nConcurrently sending multiple \u0026ldquo;create database\u0026rdquo; tasks will cause each task to be in the received state, awaiting processing. When the first task starts processing it will be moved to the processing-in-progress state. When that first task is completed (either processing-completed or processing-error), the second task will start processing, and so on. Typically, the processing phase is much faster than the provisioning phase, and multiple tasks will be in provisioned concurrently. If the creation of the database requires an update to the subscription, the subscription state is set to pending. When you create multiple databases one after the other, we recommend that you check the subscription state after the processing phase of each database create request. If the subscription is in pending state you must wait for the subscription changes to complete and the subscription state to return to active. ","categories":["RC"]},{"uri":"/rc/api/examples/audit-system-logs/","uriRel":"/rc/api/examples/audit-system-logs/","title":"Audit using Service Log","tags":[],"keywords":[],"description":"Use the service log to track and audit actions performed in the account","content":"Service logs collect and report actions performed on various entities in your Redis Enterprise Cloud subscription. These entities include the account itself, users, API Keys, subscriptions, databases, accounts, payment methods, and more. For each entity, various lifecycle events are logged in the system log.\nTo view the log, sign in to the Redis Cloud admin console and then select Logs from the main menu.\nTo learn more, see System logs.\nSystem log REST API The REST API operation for querying the system service log is GET /logs.\nFor example, the following request returns the latest 100 system log entries, in descending order:\nGET \u0026#34;https://[host]/v1/logs?limit=100\u0026amp;offset=0\u0026#34; The /logs API operation accepts the following parameters:\noffset - The starting point for the results. The default value of 0 starts with the latest log entry. A value of 11 skips the first 10 entries and retrieves entries starting with the 11 and older. limit - The maximum number of entries to return per request. The default value is 100. The system log reports activity for the entire account. It reports log entries for all types of entities, including subscriptions, databases, and so on.\nRequest results An API system log request results in data that includes an entries array. The entries are sorted by system log entry ID in descending order and include the following properties:\nid - A unique identifier for each system log entry.\ntime - The system log entry timestamp, defined in ISO-8601 date format and in the UTC timezone (for example: 2019-03-15T14:26:02Z).\noriginator - The name of the user who performed the action described by the system log entry.\napiKeyName - The name of the API key used to perform the action described by the system log entry. This field only appears if the action was performed through the API. If the operation was performed through the Redis Enterprise Cloud admin console, this property is omitted.\nresource - The name of the entity associated with the logged action (for example, database name). This property is omitted if it is not applicable to the specific log entry.\ntype - The category associated with the action log entry.\ndescription - The detailed description of the action in the log entry.\n","categories":["RC"]},{"uri":"/rs/security/certificates/","uriRel":"/rs/security/certificates/","title":"Certificates","tags":[],"keywords":[],"description":"An overview of certificates in Redis Enterprise Software.","content":"Redis Enterprise Software uses self-signed certificates by default to ensure that the product is secure. If using a self-signed certificate is not the right solution for you, you can import a certificate signed by a certificate authority of your choice.\nHere\u0026rsquo;s the list of self-signed certificates that create secure, encrypted connections to your Redis Enterprise cluster:\nCertificate name Description api Encrypts REST API requests and responses. cm Secures connections to the Redis Enterprise admin console. metrics_exporter Sends Redis Enterprise metrics to external monitoring tools over a secure connection. proxy Creates secure, encrypted connections between clients and databases. syncer For Active-Active or Replica Of databases, encrypts data during the synchronization of participating clusters. These self-signed certificates are generated on the first node of each Redis Enterprise Software installation and are copied to all other nodes added to the cluster.\nWhen you use the default self-signed certificates and you connect to the admin console over a web browser, you\u0026rsquo;ll see an untrusted connection notification.\nDepending on your browser, you can allow the connection for each session or add an exception to trust the certificate for all future sessions.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/","uriRel":"/modules/redisgears/jvm/classes/","title":"RedisGears Java classes and functions","tags":[],"keywords":[],"description":"","content":"The RedisGears JVM plugin provides a set of classes and functions for you to use in your Java code.\nClasses Class Description GearsBuilder Creates a RedisGears pipeline of operations to transform data. GearsFuture Allows asynchronous processing of records. Readers Extracts data from the database and creates records to pass through a RedisGears pipeline. ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/readers/commandoverrider/","uriRel":"/modules/redisgears/jvm/classes/readers/commandoverrider/","title":"CommandOverrider","tags":[],"keywords":[],"description":"Override a Redis command.","content":"The CommandOverrider allows you to override and customize Redis commands.\nPass the CommandOverrider to the GearsBuilder.CreateGearsBuilder() function in your Java code. Call the register() function. Run RG.JEXECUTE to register your code. Note: If you register code that uses CommandOverrider, its reader value is \u0026quot;CommandReader\u0026quot; when you run the RG.DUMPREGISTRATIONS command, not \u0026quot;CommandOverrider\u0026quot;. Parameters Name Type Description command string The command to override prefix string Only override the command for keys that start with this string Output records Outputs a record with the command\u0026rsquo;s name and arguments.\nExample The following example shows how to override the HSET command so that it also adds a last_modified timestamp for \u0026ldquo;user:\u0026rdquo; hashes.\nAdd some user data as a hash:\nredis\u0026gt; HSET user:1 name \u0026#34;morgan\u0026#34; posts 201 (integer) 2 Example code:\n// Create the reader that will pass data to the pipe CommandOverrider overrider = new CommandOverrider(); // Override the HSET command overrider.setCommand(\u0026#34;HSET\u0026#34;); // Only override HSET for keys that start with \u0026#34;user:\u0026#34; overrider.setPrefix(\u0026#34;user:\u0026#34;); // Create the data pipe builder GearsBuilder.CreateGearsBuilder(overrider).map(r-\u0026gt; { // Extract the key from the command arguments String keyName = new String((byte[]) r[1], StandardCharsets.UTF_8); // Add a last_modified timestamp to the user\u0026#39;s profile GearsBuilder.execute(\u0026#34;HSET\u0026#34;, keyName, \u0026#34;last_modified\u0026#34;, Long.toString(System.currentTimeMillis())); // Get the original HSET arguments ArrayList\u0026lt;String\u0026gt; commandArray = new ArrayList\u0026lt;String\u0026gt;(); for (int i=1; i \u0026lt; r.length; i++) { commandArray.add(new String((byte[]) r[i], StandardCharsets.UTF_8)); } // Run the original HSET command GearsBuilder.callNext(commandArray.toArray(new String[0])); return \u0026#34;OK\u0026#34;; }).register(); After you register the previous code with the RG.JEXECUTE command, try to update the user\u0026rsquo;s data with HSET to test it:\nredis\u0026gt; HSET user:1 posts 234 \u0026#34;OK\u0026#34; Now the user\u0026rsquo;s profile should have the updated value for posts and a last_modified timestamp:\nredis\u0026gt; HGETALL user:1 1) \u0026#34;name\u0026#34; 2) \u0026#34;morgan\u0026#34; 3) \u0026#34;posts\u0026#34; 4) \u0026#34;234\u0026#34; 5) \u0026#34;last_modified\u0026#34; 6) \u0026#34;1643237927663\u0026#34; ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/readers/commandreader/","uriRel":"/modules/redisgears/jvm/classes/readers/commandreader/","title":"CommandReader","tags":[],"keywords":[],"description":"Run RedisGears functions on command.","content":"The CommandReader allows you to run RedisGears functions on command when you:\nPass the CommandReader to the GearsBuilder.CreateGearsBuilder() function in your Java code.\nCall the register() function.\nRun RG.JEXECUTE to register your code.\nUse RG.TRIGGER to run your code on command:\nRG.TRIGGER \u0026lt;Trigger name\u0026gt; [arg1 arg2 ...] Parameters Name Type Description trigger string The command name that triggers the registered RedisGears functions to run Output records Outputs a record with the command trigger\u0026rsquo;s name and arguments.\nExample The following example shows how to create a custom command to update an item\u0026rsquo;s stock. It also adds a timestamp to track when the last restock occurred.\nAdd a hash to the database that represents an inventory item:\nredis\u0026gt; HSET inventory:headphones:1 color \u0026#34;blue\u0026#34; stock 5 price 30.00 (integer) 3 Example code:\n// Create the reader that will pass data to the pipe CommandReader reader = new CommandReader(); // Set the name of the custom command reader.setTrigger(\u0026#34;Restock\u0026#34;); // Create the data pipe builder GearsBuilder.CreateGearsBuilder(reader).map(r-\u0026gt; { // Parse the command arguments to get the key name and new stock value String itemKey = new String((byte[]) r[1], StandardCharsets.UTF_8); String newStock = new String((byte[]) r[2], StandardCharsets.UTF_8); // Update the item\u0026#39;s stock and add a timestamp GearsBuilder.execute(\u0026#34;HSET\u0026#34;, itemKey , \u0026#34;stock\u0026#34;, newStock, \u0026#34;last_restocked\u0026#34;, Long.toString(System.currentTimeMillis())); return \u0026#34;OK restocked \u0026#34; + itemKey; }).register(); After you register the previous code with the RG.JEXECUTE command, run RG.TRIGGER to test it:\nredis\u0026gt; RG.TRIGGER Restock inventory:headphones:1 20 1) \u0026#34;OK restocked inventory:headphones:1\u0026#34; The item now has the updated value for stock and a last_restocked timestamp:\nredis\u0026gt; HGETALL inventory:headphones:1 1) \u0026#34;color\u0026#34; 2) \u0026#34;blue\u0026#34; 3) \u0026#34;stock\u0026#34; 4) \u0026#34;20\u0026#34; 5) \u0026#34;price\u0026#34; 6) \u0026#34;30.00\u0026#34; 7) \u0026#34;last_restocked\u0026#34; 8) \u0026#34;1643232394078\u0026#34; ","categories":["Modules"]},{"uri":"/rs/security/tls/ciphers/","uriRel":"/rs/security/tls/ciphers/","title":"Configure cipher suites","tags":[],"keywords":[],"description":"Shows how to configure cipher suites.","content":"Ciphers are algorithms that help secure connections between clients and servers. You can change the ciphers to improve the security of your Redis Enterprise cluster and databases. The default settings are in line with industry best practices, but you can customize them to match the security policy of your organization.\nDefault cipher:\nHIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH Configure cipher suites The communications for which you can modify ciphers are:\nControl plane - The TLS configuration for cluster administration. Data plane - The TLS configuration for the communication between applications and databases. Discovery service (Sentinel) - The TLS configuration for the discovery service. You can configure ciphers with the rladmin commands shown here or with the REST API. Note that configuring cipher suites overwrites existing ciphers rather than appending new ciphers to the list.\nWhen you modify your cipher suites, make sure:\nThe configured TLS version matches the required cipher suites. The certificates in use are properly signed to support the required cipher suites. Note: Redis Enterprise Software doesn\u0026rsquo;t support static Diffie–Hellman key exchange ciphers.\nIt does support Ephemeral Diffie–Hellman key exchange ciphers on RHEL8 and Bionic OS.\nControl plane 6.0.8 or earlier See the example below to configure cipher suites for the control plane:\nrladmin cluster config cipher_suites ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305 6.0.12 or later Control plane cipher suites use the BoringSSL library format for TLS connections to the admin console. See the BoringSSL documentation for a full list of available BoringSSL configurations.\nTo configure the cipher suites for cluster communications, use the following rladmin command syntax:\nrladmin cluster config cipher_suites \u0026lt;BoringSSL cipher list\u0026gt; See the example below to configure cipher suites for the control plane:\nrladmin cluster config cipher_suites ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305 Data plane 6.0.20 or later Data plane cipher suites use the OpenSSL library format. See the OpenSSL documentation for a list of available OpenSSL configurations.\nTo configure the cipher suites for communications between applications and databases, use the following rladmin command syntax:\nrladmin cluster config data_cipher_list \u0026lt;OpenSSL cipher list\u0026gt; See the example below to configure cipher suites for the data plane:\nrladmin cluster config data_cipher_list AES128-SHA:AES256-SHA Discovery service 6.0.20 or later Sentinel service cipher suites use the golang.org OpenSSL format for discovery service TLS connections. See their documentation for a list of available configurations.\nTo configure the discovery service cipher suites, use the following rladmin command syntax:\nrladmin cluster config sentinel_cipher_suites \u0026lt;golang cipher list\u0026gt; See the example below to configure cipher suites for the sentinel service:\nrladmin cluster config sentinel_cipher_suites TLS_RSA_WITH_AES_128_CBC_SHA:TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 ","categories":["RS"]},{"uri":"/rs/security/access-control/manage-users/default-user/","uriRel":"/rs/security/access-control/manage-users/default-user/","title":"Deactivate default user","tags":[],"keywords":[],"description":"Deactivate a database&#39;s default user.","content":"When you provision a database, the default user will be enabled. This allows for backwards compatibility with versions of Redis before Redis 6.\nTo deactivate the default user:\nSelect the configuration tab. Clear the checkbox for Default database access. Select Save. Note: We recommend that you deactivate the default user when using ACLs with your database and backwards compatibility is not required. ","categories":["RS"]},{"uri":"/rs/databases/durability-ha/","uriRel":"/rs/databases/durability-ha/","title":"Durability and high availability","tags":[],"keywords":[],"description":"Overview of Redis Enterprise durability features such as replication, clustering, and rack-zone awareness.","content":"Redis Enterprise Software comes with several features that make your data more durable and accessible. The following features can help protect your data in cases of failures or outages and help keep your data available when you need it.\nReplication When you replicate your database, each database instance (shard) is copied one or more times. Your database will have one primary shard and one or more replica shards. When a primary shard fails, Redis Enterprise automatically promotes a replica shard to primary.\nClustering Clustering (or sharding) breaks your database into individual instances (shards) and spreads them across several nodes. Clustering lets you add resources to your cluster to scale your database and prevents node failures from causing availability loss.\nDatabase persistence Database persistence gives your database durability against process or server failures by saving data to disk at set intervals.\nActive-Active geo-distributed replication Active-Active Redis Enterprise databases distribute your replicated data across multiple nodes and availability zones. This increases the durability of your database by reducing the likelihood of data or availability loss. It also reduces data access latency.\nRack-zone awareness Rack-zone awareness maps each node in your Redis Enterprise cluster to a physical rack or logical zone. The cluster uses this information to distribute primary shards and their replica shards in different racks or zones. This ensures data availability if a rack or zone fails.\nDiscovery service The discovery service provides an IP-based connection management service used when connecting to Redis Enterprise Software databases. It lets your application discover which node hosts the database endpoint. The discovery service API complies with the Redis Sentinel API.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/gearsbuilder/","uriRel":"/modules/redisgears/jvm/classes/gearsbuilder/","title":"GearsBuilder","tags":[],"keywords":[],"description":"Creates a RedisGears pipeline of operations to transform data.","content":"The GearsBuilder class allows you to create a pipeline of RedisGears functions that transform data.\nIt requires a reader to supply data to the pipe.\nTo create a GearsBuilder object, follow this example code:\nBaseReader reader = ...; // Initialize the reader builder = GearsBuilder.CreateGearsBuilder(reader); Functions Function Description accumulate Reduces many records in the pipe to a single record. accumulateBy Groups records and reduces each group to a single record per group. asyncFilter Asynchronously filters out records in the pipe based on a given condition. asyncForeach For each record in the pipe, asynchronously runs some operations. asyncMap Asynchronously maps records one-to-one. callNext Calls the next execution that overrides the command or the original command itself. A more flexible version of callNextArray. callNextArray Calls the next execution that overrides the command or the original command itself. collect Collects all records to the origin shard. configGet Gets the value of a RedisGears configuration setting. count Counts the number of records in the pipe. CreateGearsBuilder Creates a new GearsBuilder object. execute Runs a Redis command. A more flexible version of executeArray. executeArray Runs a Redis command. filter Filters out records in the pipe based on a given condition. flatMap Maps a single input record to one or more output records. foreach For each record in the pipe, runs some operations. hashtag Returns a string that maps to the current shard. localAccumulateBy Groups records and reduces each group to a single record per group locally on each shard. log Writes a log message to the Redis log file. map Maps records one-to-one. register Registers the pipeline of functions to run when certain events occur. repartition Moves records between shards according to the extracted data. run Runs the pipeline of functions immediately. ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/gearsfuture/","uriRel":"/modules/redisgears/jvm/classes/gearsfuture/","title":"GearsFuture","tags":[],"keywords":[],"description":"Allows asynchronous processing of records.","content":"The GearsFuture class allows asynchronous processing of records in another thread.\nYou can use a GearsFuture object with the following GearsBuilder functions:\nasyncFilter asyncForeach asyncMap Functions Function Description setError Sets an error message. setResult Sets a computation to run asynchronously. ","categories":["Modules"]},{"uri":"/modules/redisgears/installing-redisgears/","uriRel":"/modules/redisgears/installing-redisgears/","title":"Install RedisGears","tags":[],"keywords":[],"description":"","content":"Before you can use RedisGears, you have to install the RedisGears module on your Redis Enterprise cluster.\nMinimum requirements Redis Enterprise 6.0.12 or later The cluster is setup and all of the nodes are joined to the cluster Install RedisGears If your cluster uses Redis Enterprise v6.0.12 or later and has internet access, you only need to download the RedisGears package. It automatically fetches dependencies like the Python and JVM plugins during online installation.\nOffline installation requires you to manually upload dependencies to the master node.\nInstall RedisGears and dependencies Download the RedisGears package from the Redis Enterprise download center.\nNote: For offline installation of RedisGears v1.2 and later, you also need to download the RedisGears Dependencies packages for both Python and Java. For RedisGears v1.0, you only need the Python dependency package. Upload the RedisGears package to a node in the cluster.\nFor offline installation only, copy the dependencies to the following directory on the master node: $modulesdatadir/rg/\u0026lt;version-integer\u0026gt;/deps/\n$ cp redisgears-jvm.\u0026lt;OS\u0026gt;.\u0026lt;version\u0026gt;.tgz $modulesdatadir/rg/\u0026lt;version-integer\u0026gt;/deps/ Replace these fields with your own values:\n\u0026lt;OS\u0026gt;: the operating system running Redis Enterprise\n\u0026lt;version\u0026gt;: the RedisGears version (x.y.z)\n\u0026lt;version-integer\u0026gt;: the RedisGears version as an integer, calculated as (x*10000 + y*100 + z)\nFor example, the \u0026lt;version-integer\u0026gt; for RedisGears version 1.2.5 is 10205.\nNote: Skip this step unless your cluster does not have internet access. Add RedisGears to the cluster with a POST request to the master node\u0026rsquo;s /v2/modules REST API endpoint:\nPOST https://[host][:port]/v2/modules {\u0026#34;module=@/tmp/redisgears.\u0026lt;OS\u0026gt;.\u0026lt;version\u0026gt;.zip\u0026#34;} Here, the module parameter specifies the full path of the module package and must be submitted as form-data. In addition, the package must be available and accessible to the server processing the request.\nAfter the install is complete, RedisGears will appear in the list of available modules on the settings and create database pages of the Redis Enterprise admin console.\nEnable RedisGears for a database After installation, create a new database and enable RedisGears:\nWith Python\nWith the JVM\nUninstall RedisGears To uninstall RedisGears, make a DELETE request to the /v2/modules REST API endpoint.\n","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/readers/javareader/","uriRel":"/modules/redisgears/jvm/classes/readers/javareader/","title":"JavaReader","tags":[],"keywords":[],"description":"A template for creating custom readers.","content":"The JavaReader is an abstract class that allows you to create a custom reader in Java.\nCreate a custom reader To create a custom reader:\nExtend the JavaReader class Override the iterator() function Custom reader example The implementation of the KeysOnlyReader class shows how to create a custom reader with JavaReader:\nimport java.util.Iterator; import gears.GearsBuilder; /** * A reader that only reads key names from the key space * */ public class KeysOnlyReader extends JavaReader\u0026lt;String\u0026gt; { /** * */ private static final long serialVersionUID = 1L; private String scanSize; private String pattern; /** * Create a new KeysOnlyReader reader * @param scanSize - the size to use with the scan command * @param pattern - the pattern of the keys to read */ public KeysOnlyReader(int scanSize, String pattern) { this.scanSize = Integer.toString(scanSize); this.pattern = pattern; } /** * Create a new KeysOnlyReader reader with default pattern (*) and default * scan size (10000) */ public KeysOnlyReader() { this(10000, \u0026#34;*\u0026#34;); } @Override public Iterator\u0026lt;String\u0026gt; iterator() { return new Iterator\u0026lt;String\u0026gt;() { String cursor = \u0026#34;0\u0026#34;; int currIndex = 0; Object[] keys = null; boolean isDone = false; String nextKey = null; private String innerNext() { while(!isDone) { if(keys == null) { Object[] res = (Object[]) GearsBuilder.execute(\u0026#34;scan\u0026#34;, cursor == null ? \u0026#34;0\u0026#34; : cursor, \u0026#34;MATCH\u0026#34;, pattern, \u0026#34;COUNT\u0026#34;, scanSize); keys = (Object[])res[1]; cursor = (String)res[0]; currIndex = 0; } if(currIndex \u0026lt; keys.length) { return (String) keys[currIndex++]; } keys = null; if(cursor.charAt(0) == \u0026#39;0\u0026#39;) { isDone = true; } } return null; } @Override public boolean hasNext() { if(nextKey == null) { nextKey = innerNext(); } return !isDone; } @Override public String next() { String temp = nextKey != null ? nextKey : innerNext(); nextKey = innerNext(); return temp; } }; } } ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/readers/keysonlyreader/","uriRel":"/modules/redisgears/jvm/classes/readers/keysonlyreader/","title":"KeysOnlyReader","tags":[],"keywords":[],"description":"Gets key names from a database.","content":"The KeysOnlyReader only extracts the key names from a database.\nConstructors You can use one of these constructors to create a new KeysOnlyReader object:\npublic KeysOnlyReader() public KeysOnlyReader(int scanSize, String pattern) Parameters Name Type Default value Description pattern string \u0026ldquo;*\u0026rdquo; (match all keys) Get all keys that match this pattern scanSize integer 10000 The scan command\u0026rsquo;s size limit Output records Each output record is a string that represents the key\u0026rsquo;s name.\nExamples Get all keys in the database:\nKeysOnlyReader reader = new KeysOnlyReader(); Only get keys that start with \u0026ldquo;user:\u0026rdquo;:\nKeysOnlyReader reader = new KeysOnlyReader(1000, \u0026#34;user:*\u0026#34;); ","categories":["Modules"]},{"uri":"/modules/redisgears/jvm/classes/readers/keysreader/","uriRel":"/modules/redisgears/jvm/classes/readers/keysreader/","title":"KeysReader","tags":[],"keywords":[],"description":"Gets keys and their values from a database.","content":"Creates records from the keys and values stored in a Redis database.\nNote: Currently only supports string and hash data types. For other data types, it will only extract the key name. Constructors You can use one of these constructors to create a new KeysReader object:\npublic KeysReader() public KeysReader(String pattern) public KeysReader(String prefix, boolean readValues) public KeysReader(String pattern, boolean noScan, boolean readValues) public KeysReader(String prefix, boolean readValues, String[] eventTypes, String[] keyTypes) public KeysReader(String pattern, boolean noScan, boolean readValues, String[] eventTypes, String[] keyTypes) Parameters Name Type Default value Description commands array of strings null The commands that this reader is registered on eventTypes array of strings null The event types to register on (usually the command name) keyTypes array of strings null The key types to register on noScan boolean false Whether or not to scan the key space or just read the pattern as is pattern/prefix string \u0026ldquo;*\u0026rdquo; (match all keys) The reader will get all keys that match this pattern readValues boolean true Whether or not to read the keys\u0026rsquo; values Output records Creates a KeysReaderRecord for each matching key in the database.\nName Type Description key string The name of the key type long The core Redis type: \u0026lsquo;string\u0026rsquo;, \u0026lsquo;hash\u0026rsquo;, \u0026rsquo;list\u0026rsquo;, \u0026lsquo;set\u0026rsquo;, \u0026lsquo;zset\u0026rsquo;, or \u0026lsquo;stream\u0026rsquo; event string The event that triggered the execution (null if using the run function) stringVal string The key\u0026rsquo;s value for string data types hashVal Map\u0026lt;String,String\u0026gt; The key\u0026rsquo;s value for hash data types listVal List The key\u0026rsquo;s value for list data types setVal Set The key\u0026rsquo;s value for set data types Examples Here\u0026rsquo;s a basic example of a KeysReader that creates records for all keys in the database:\nKeysReader reader = new KeysReader(); In the following example, the KeysReader creates records for all keys in the database that start with \u0026ldquo;person:\u0026rdquo;. When registered, it only runs for hashes after HSET and DEL events occur.\nString[] eventTypes = {\u0026#34;HSET\u0026#34;, \u0026#34;DEL\u0026#34;}; String[] keyTypes = {\u0026#34;HASH\u0026#34;}; KeysReader reader = new KeysReader(\u0026#34;person:*\u0026#34;, false, true, eventTypes, keyTypes); ","categories":["Modules"]},{"uri":"/kubernetes/logs/","uriRel":"/kubernetes/logs/","title":"Redis Enterprise Software logs on Kubernetes","tags":[],"keywords":[],"description":"This section provides information about how logs are stored and accessed.","content":"Logs Each redis-enterprise container stores its logs under /var/opt/redislabs/log. When using persistent storage this path is automatically mounted to the redis-enterprise-storage volume. This volume can easily be accessed by a sidecar, i.e. a container residing on the same pod.\nFor example, in the REC (Redis Enterprise Cluster) spec you can add a sidecar container, such as a busybox, and mount the logs to there:\nsideContainersSpec: - name: busybox image: busybox args: - /bin/sh - -c - while true; do echo \u0026#34;hello\u0026#34;; sleep 1; done volumeMounts: - name: redis-enterprise-storage mountPath: /home/logs subPath: logs Now the logs can be accessed from in the sidecar. For example by running\nkubectl exec -it \u0026lt;pod-name\u0026gt; -c busybox tail home/logs/supervisord.log\nThe sidecar container is user determined and can be used to format, process and share logs in a specified format and protocol.\n","categories":["Platforms"]},{"uri":"/rs/clusters/maintenance-mode/","uriRel":"/rs/clusters/maintenance-mode/","title":"Maintenance mode for cluster nodes","tags":[],"keywords":[],"description":"Prepare a cluster node for maintenance.","content":"Use maintenance mode to prevent data loss during hardware or operating system maintenance on Redis Enterprise servers. When maintenance mode is on, all shards move off of the node under maintenance and migrate to another available node.\nActivate maintenance mode When you activate maintenance mode, Redis Enterprise does the following:\nChecks whether a shut down of the node will cause quorum loss. If so, maintenance mode will not turn on.\nMaintenance mode does not protect against quorum loss. If you activate maintenance mode for the majority of nodes in a cluster and restart them simultaneously, quorum is lost, which can lead to data loss.\nTakes a snapshot of the node configuration in order to record the shard and endpoint configuration of the node.\nMarks the node as a quorum node to prevent shards and endpoints from migrating to it.\nAt this point, rladmin status displays the node\u0026rsquo;s shards field in yellow, which indicates that shards cannot migrate to the node.\nMigrates shards and binds endpoints to other nodes, when space is available.\nMaintenance mode does not demote a master node by default. The cluster elects a new master node when the original master node restarts.\nAdd the demote_node option to the rladmin command to demote a master node when you activate maintenance mode.\nTo activate maintenance mode for a node, run the following command:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode on You can start server maintenance if:\nAll shards and endpoints have moved to other nodes\nEnough nodes are still online to maintain quorum\nPrevent replica shard migration If you do not have enough resources available to move all of the shards to other nodes, you can turn maintenance mode on without migrating the replica shards.\nIf you prevent replica shard migration, the shards remain on the node during maintenance.\nIf the maintenance node fails in this case, the master shards do not have replica shards to maintain data redundancy and high availability.\nTo activate maintenance mode without replica shard migration, run:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode on keep_slave_shards Demote a master node If maintenance might affect connectivity to the master node, you can demote the master node when you activate maintenance mode. This lets the cluster elect a new master node.\nTo demote a master node when activating maintenance mode, run:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode on demote_node Verify maintenance mode activation To verify maintenance mode for a node, use rladmin status and review the node\u0026rsquo;s shards field. If that value is displayed in yellow (shown earlier), then the node is in maintenance mode.\nAvoid activating maintenance mode when it is already active. Maintenance mode activations stack. If you activate maintenance mode for a node that is already in maintenance mode, you will have to deactivate maintenance mode twice in order to restore full functionality.\nDeactivate maintenance mode When you deactivate maintenance mode, Redis Enterprise:\nLoads a specified snapshot or defaults to the latest snapshot.\nUnmarks the node as a quorum node to allow shards and endpoints to migrate to the node.\nRestores the shards and endpoints that were in the node at the time of the snapshot.\nDeletes the snapshot.\nTo deactivate maintenance mode after server maintenance, run:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode off By default, a snapshot is required to deactivate maintenance mode. If the snapshot cannot be restored, deactivation is cancelled and the node remains in maintenance mode. In such events, it may be necessary to reset node status.\nSpecify a snapshot Redis Enterprise saves a snapshot of the node configuration every time you turn on maintenance mode. If multiple snapshots exist, you can restore a specific snapshot when you turn maintenance mode off.\nTo get a list of available snapshots, run:\nrladmin node \u0026lt;node_id\u0026gt; snapshot list To specify a snapshot when you turn maintenance mode off, run:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode off snapshot_name \u0026lt;snapshot_name\u0026gt; Note: If an error occurs when you turn on maintenance mode, the snapshot is not deleted. When you rerun the command, use the snapshot from the initial attempt since it contains the original state of the node. Skip shard restoration You can prevent the migrated shards and endpoints from returning to the original node after you turn off maintenance mode.\nTo turn maintenance mode off and skip shard restoration, run:\nrladmin node \u0026lt;node_id\u0026gt; maintenance_mode off skip_shards_restore Reset node status In extreme cases, you may need to reset a node\u0026rsquo;s status. Run the following commands to do so:\n$ rladmin tune node \u0026lt;node_id\u0026gt; max_listeners 100 $ rladmin tune node \u0026lt;node_id\u0026gt; quorum_only disabled Use these commands with caution. For best results, contact Support before running these commands.\nCluster status example This example shows how the output of rladmin status changes when you turn on maintenance mode for a node.\nThe cluster status before turning on maintenance mode:\nredislabs@rp1_node1:/opt$ rladmin status CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS *node:1 master 172.17.0.2 rp1_node1 2/100 node:2 slave 172.17.0.4 rp3_node1 2/100 node:3 slave 172.17.0.3 rp2_node1 0/100 The cluster status after turning on maintenance mode:\nredislabs@rp1_node1:/opt$ rladmin node 2 maintenance_mode on Performing maintenance_on action on node:2: 0% created snapshot NodeSnapshot\u0026lt;name=maintenance_mode_2019-03-14_09-50-59,time=None,node_uid=2\u0026gt; node:2 will not accept any more shards Performing maintenance_on action on node:2: 100% OK redislabs@rp1_node1:/opt$ rladmin status CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS *node:1 master 172.17.0.2 rp1_node1 2/100 node:2 slave 172.17.0.4 rp3_node1 0/0 node:3 slave 172.17.0.3 rp2_node1 2/100 After turning on maintenance mode for node 2, Redis Enterprise saves a snapshot of its configuration and then moves its shards and endpoints to node 3.\nNow node 2 has 0/0 shards because shards cannot migrate to it while it is in maintenance mode.\nMaintenance mode REST API You can also turn maintenance mode on or off via REST API requests to POST /nodes/{node_uid}/actions/{action}.\nActivate maintenance mode (REST API) Use POST /nodes/{node_uid}/actions/maintenance_on to activate maintenance mode:\nPOST https://[host][:port]/v1/nodes/\u0026lt;node_id\u0026gt;/actions/maintenance_on \u0026#39;{\u0026#34;keep_slave_shards\u0026#34;:true}\u0026#39; The keep_slave_shards boolean flag prevents replica shard migration when set to true.\nThe maintenance_on request returns a JSON response body:\n{ \u0026#34;status\u0026#34;:\u0026#34;queued\u0026#34;, \u0026#34;task_id\u0026#34;:\u0026#34;\u0026lt;task-id-guid\u0026gt;\u0026#34; } Deactivate maintenance mode (REST API) Use POST /nodes/{node_uid}/actions/maintenance_off deactivate maintenance mode:\nPOST https://[host][:port]/v1/nodes/\u0026lt;node_id\u0026gt;/actions/maintenance_off \u0026#39;{\u0026#34;skip_shards_restore\u0026#34;:false}\u0026#39; The skip_shards_restore boolean flag allows the maintenance_off action to skip shard restoration when set to true.\nThe maintenance_off request returns a JSON response body:\n{ \u0026#34;status\u0026#34;:\u0026#34;queued\u0026#34;, \u0026#34;task_id\u0026#34;:\u0026#34;\u0026lt;task-id-guid\u0026gt;\u0026#34; } Track action status You can send a request to GET /nodes/{node_uid}/actions/{action} to track the status of the maintenance_on and maintenance_off actions.\nThis request returns the status of the maintenance_on action:\nGET https://\u0026lt;hostname\u0026gt;:9443/v1/nodes/\u0026lt;node_id\u0026gt;/actions/maintenance_on The response body:\n{ \u0026#34;status\u0026#34;:\u0026#34;completed\u0026#34;, \u0026#34;task_id\u0026#34;:\u0026#34;38c7405b-26a7-4379-b84c-cab4b3db706d\u0026#34; } ","categories":["RS"]},{"uri":"/kubernetes/deployment/openshift/openshift-cli/","uriRel":"/kubernetes/deployment/openshift/openshift-cli/","title":"Deployment with OpenShift CLI for Redis Enterprise for Kubernetes","tags":[],"keywords":[],"description":"Redis Enterprise for Kubernetes and cluster can be installed via CLI tools OpenShift","content":"These are the steps required to set up a Redis Enterprise Software cluster with OpenShift.\nPrerequisites OpenShift cluster installed, with at least three nodes (each meeting the minimum requirements for a development installation) Note: If you are running an OpenShift 3 version, use the bundle.yaml file located in the openshift_3_x folder in the redis-enterprise-k8s-docs repo. This folder also contains the custom resource definitions (CRDs) compatible with OpenShift 3.x. kubectl tool installed at version 1.9 or higher OpenShift CLI installed Deploy the operator Create a new project.\noc new-project \u0026lt;your-project-name\u0026gt; Verify that you are using the newly created project, run:\noc project \u0026lt;your-project-name\u0026gt; Get deployment files by cloning the redis-enterprise-k8s-docs repository.\ngit clone https://github.com/RedisLabs/redis-enterprise-k8s-docs Apply the file scc.yaml file.\nThe scc (Security Context Constraint) yaml defines security context constraints for the cluster for our project. We strongly recommend that you not change anything in this yaml file.\noc apply -f openshift/scc.yaml You should receive the following response:\nsecuritycontextconstraints.security.openshift.io \u0026#34;redis-enterprise-scc\u0026#34; configured Provide the operator permissions for the pods.\noc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:\u0026lt;my-project\u0026gt;:redis-enterprise-operator oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:\u0026lt;my-project\u0026gt;:\u0026lt;rec\u0026gt; You can see the name of your project with the oc project command to replace \u0026lt;my-project\u0026gt; in the command above. Replace rec with the name of your Redis Enterprise cluster, if different.\nDeploy the OpenShift operator bundle.\nIf you are running on OpenShift 3.x, use the openshift.bundle.yaml file in the openshift_3_x folder.\noc apply -f openshift.bundle.yaml Warning - Changes to the openshift.bundle.yaml file can cause unexpected results. Verify that your redis-enterprise-operator deployment is running, run:\noc get deployment A typical response will look like this:\nNAME READY UP-TO-DATE AVAILABLE AGE redis-enterprise-operator 1/1 1 1 0m36s Create your Redis Enterprise cluster (REC) custom resource Apply the RedisEnterpriseCluster resource file (rec_rhel.yaml).\nYou can rename the file to \u0026lt;your_cluster_name\u0026gt;.yaml, but it is not required (the examples below will use \u0026lt;rec_rhel\u0026gt;.yaml). Options for Redis Enterprise clusters has more info about the REC custom resource, or see the Redis Enterprise cluster API for a full list of options.\nNote: Each Redis Enterprise cluster must have at least 3 nodes. Single-node RECs are not supported. Apply the custom resource file to create your Redis Enterprise cluster.\noc apply -f \u0026lt;rec_rhel\u0026gt;.yaml The operator typically creates the REC within a few minutes.\nCheck the cluster status\nkubectl get pod You should receive a response similar to the following:\n| NAME | READY | STATUS | RESTARTS | AGE | | -------------------------------- | ----- | ------- | -------- | --- | | rec-name-0 | 2/2 | Running | 0 | 1m | | rec-name-1 | 2/2 | Running | 0 | 1m | | rec-name-2 | 2/2 | Running | 0 | 1m | | rec-name-controller-x-x | 1/1 | Running | 0 | 1m | | Redis-enterprise-operator-x-x | 1/1 | Running | 0 | 5m | Configure the admission controller Verify the secret has been created. The operator creates a Kubernetes secret for the admission controller during deployment.\nkubectl get secret admission-tls The response will be similar to this:\nNAME TYPE DATA AGE admission-tls Opaque 2 2m43s Save the automatically generated certificate to a local environment variable.\nCERT=`kubectl get secret admission-tls -o jsonpath=\u0026#39;{.data.cert}\u0026#39;` Create a patch file for the Kubernetes webhook, using your own values for the namespace and webhook name.\nsed \u0026#39;\u0026lt;your_namespace\u0026gt;\u0026#39; admission/webhook.yaml | kubectl create -f - cat \u0026gt; modified-webhook.yaml \u0026lt;\u0026lt;EOF webhooks: - name: \u0026lt;your.admission.webhook\u0026gt; clientConfig: caBundle: $CERT admissionReviewVersions: [\u0026#34;v1beta1\u0026#34;] EOF Patch the validating webhook with the certificate.\nkubectl patch ValidatingWebhookConfiguration redb-admission --patch \u0026#34;$(cat modified-webhook.yaml)\u0026#34; Limit the webhook to relevant namespaces If not limited, the webhook will intercept requests from all namespaces. If you have several REC objects in your Kubernetes cluster, you need to limit the webhook to the relevant namespaces. If you aren\u0026rsquo;t using multiple namespaces, you can skip this step.\nView your namespace YAML file to verify your namespace is labeled and the label is unique to this namespace (see example below).\napiVersion: v1 kind: Namespace metadata: labels: namespace-name: staging name: staging Patch the webhook spec with the namespaceSelector field.\ncat \u0026gt; modified-webhook.yaml \u0026lt;\u0026lt;EOF webhooks: - name: redb.admission.redislabs namespaceSelector: matchLabels: namespace-name: staging EOF Apply the patch.\nkubectl patch ValidatingWebhookConfiguration redb-admission --patch \u0026#34;$(cat modified-webhook.yaml)\u0026#34; Verify the admission controller installation Apply an invalid resource (provided below).\nThis should force the admission controller to reject it. If it applies successfully, the admission controller is not installed correctly.\n$ kubectl apply -f - \u0026lt;\u0026lt; EOF apiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: redis-enterprise-database spec: evictionPolicy: illegal EOF You should see an error from the admission controller webhook redb.admission.redislabs.\nError from server: error when creating \u0026#34;STDIN\u0026#34;: admission webhook \u0026#34;redb.admission.redislabs\u0026#34; denied the request: eviction_policy: u\u0026#39;illegal\u0026#39; is not one of [u\u0026#39;volatile-lru\u0026#39;, u\u0026#39;volatile-ttl\u0026#39;, u\u0026#39;volatile-random\u0026#39;, u\u0026#39;allkeys-lru\u0026#39;, u\u0026#39;allkeys-random\u0026#39;, u\u0026#39;noeviction\u0026#39;, u\u0026#39;volatile-lfu\u0026#39;, u\u0026#39;allkeys-lfu\u0026#39;] Create a Redis Enterprise database (REDB) custom resource The operator uses the instructions in the REDB custom resources to manage databases on the Redis Enterprise cluster.\nCreate a RedisEnterpriseDatabase custom resource.\nThe following example creates a database for testing purposes. For production databases, see creating a database and database options.\nExample:\ncat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/redis-enterprise-database.yml apiVersion: app.redislabs.com/v1alpha1 kind: RedisEnterpriseDatabase metadata: name: redis-enterprise-database spec: memorySize: 100MB EOF Apply the newly created REDB resource\noc apply -f /tmp/redis-enterprise-database.yml More info Options for Redis Enterprise clusters (REC) Redis Enterprise cluster API Options for Redis Enterprise databases (REDB) Redis Enterprise database API ","categories":["Platforms"]},{"uri":"/rs/references/rest-api/permissions/","uriRel":"/rs/references/rest-api/permissions/","title":"Permissions","tags":[],"keywords":[],"description":"Documents the permissions used with Redis Enterprise Software REST API calls.","content":"Some Redis Enterprise REST API requests may require the user to have specific permissions.\nAdministrators can assign a predefined role to a user via the admin console or a PUT /users/{uid} API request in order to grant necessary permissions to them.\nRoles Each user in the cluster has an assigned role, which defines the permissions granted to the user.\nAvailable roles include:\ndb_viewer: Can view database info. db_member: Can create or modify databases and view their info. cluster_viewer: Can view cluster and database info. cluster_member: Can modify the cluster and databases and view their info. admin: Can view and modify all elements of the cluster. Permissions list for each role Role Permissions \u0026lt;none\u0026gt; No permissions. admin add_cluster_module, cancel_cluster_action, cancel_node_action, config_ldap, config_ocsp, create_bdb, create_crdb, create_ldap_mapping, create_new_user, create_redis_acl, create_role, delete_bdb, delete_cluster_module, delete_crdb, delete_ldap_mapping, delete_redis_acl, delete_role, delete_user, edit_bdb_module, flush_crdb, install_new_license, migrate_shard, purge_instance, reset_bdb_current_backup_status, reset_bdb_current_export_status, reset_bdb_current_import_status, start_bdb_export, start_bdb_import, start_cluster_action, start_node_action, test_ocsp_status, update_bdb, update_bdb_alerts, update_bdb_with_action, update_cluster, update_crdb, update_ldap_mapping, update_node, update_proxy, update_redis_acl, update_role, update_user, view_all_bdb_stats, view_all_bdbs_alerts, view_all_bdbs_info, view_all_ldap_mappings_info, view_all_nodes_alerts, view_all_nodes_checks, view_all_nodes_info, view_all_nodes_stats, view_all_proxies_info, view_all_redis_acls_info, view_all_roles_info, view_all_shard_stats, view_all_users_info, view_bdb_alerts, view_bdb_info, view_bdb_stats, view_cluster_alerts, view_cluster_info, view_cluster_keys, view_cluster_modules, view_cluster_stats, view_crdb, view_crdb_list, view_endpoint_stats, view_ldap_config, view_ldap_mapping_info, view_license, view_logged_events, view_node_alerts, view_node_check, view_node_info, view_node_stats, view_ocsp_config, view_ocsp_status, view_proxy_info, view_redis_acl_info, view_redis_pass, view_role_info, view_shard_stats, view_status_of_all_node_actions, view_status_of_cluster_action, view_status_of_node_action, view_user_info cluster_member create_bdb, create_crdb, delete_bdb, delete_crdb, edit_bdb_module, flush_crdb, migrate_shard, purge_instance, reset_bdb_current_backup_status, reset_bdb_current_export_status, reset_bdb_current_import_status, start_bdb_export, start_bdb_import, update_bdb, update_bdb_alerts, update_bdb_with_action, update_crdb, view_all_bdb_stats, view_all_bdbs_alerts, view_all_bdbs_info, view_all_nodes_alerts, view_all_nodes_checks, view_all_nodes_info, view_all_nodes_stats, view_all_proxies_info, view_all_redis_acls_info, view_all_roles_info, view_all_shard_stats, view_bdb_alerts, view_bdb_info, view_bdb_stats, view_cluster_alerts, view_cluster_info, view_cluster_keys, view_cluster_modules, view_cluster_stats, view_crdb, view_crdb_list, view_endpoint_stats, view_license, view_logged_events, view_node_alerts, view_node_check, view_node_info, view_node_stats, view_proxy_info, view_redis_acl_info, view_redis_pass, view_role_info, view_shard_stats, view_status_of_all_node_actions, view_status_of_cluster_action, view_status_of_node_action cluster_viewer view_all_bdb_stats, view_all_bdbs_alerts, view_all_bdbs_info, view_all_nodes_alerts, view_all_nodes_checks, view_all_nodes_info, view_all_nodes_stats, view_all_proxies_info, view_all_redis_acls_info, view_all_roles_info, view_all_shard_stats, view_bdb_alerts, view_bdb_info, view_bdb_stats, view_cluster_alerts, view_cluster_info, view_cluster_modules, view_cluster_stats, view_crdb, view_crdb_list, view_endpoint_stats, view_license, view_logged_events, view_node_alerts, view_node_check, view_node_info, view_node_stats, view_proxy_info, view_redis_acl_info, view_role_info, view_shard_stats, view_status_of_all_node_actions, view_status_of_cluster_action, view_status_of_node_action db_member create_bdb, create_crdb, delete_bdb, delete_crdb, edit_bdb_module, flush_crdb, migrate_shard, purge_instance, reset_bdb_current_backup_status, reset_bdb_current_export_status, reset_bdb_current_import_status, start_bdb_export, start_bdb_import, update_bdb, update_bdb_alerts, update_bdb_with_action, update_crdb, view_all_bdb_stats, view_all_bdbs_alerts, view_all_bdbs_info, view_all_nodes_alerts, view_all_nodes_checks, view_all_nodes_info, view_all_nodes_stats, view_all_proxies_info, view_all_redis_acls_info, view_all_roles_info, view_all_shard_stats, view_bdb_alerts, view_bdb_info, view_bdb_stats, view_cluster_alerts, view_cluster_info, view_cluster_modules, view_cluster_stats, view_crdb, view_crdb_list, view_endpoint_stats, view_license, view_logged_events, view_node_alerts, view_node_check, view_node_info, view_node_stats, view_proxy_info, view_redis_acl_info, view_redis_pass, view_role_info, view_shard_stats, view_status_of_all_node_actions, view_status_of_cluster_action, view_status_of_node_action db_viewer view_all_bdb_stats, view_all_bdbs_alerts, view_all_bdbs_info, view_all_nodes_alerts, view_all_nodes_checks, view_all_nodes_info, view_all_nodes_stats, view_all_proxies_info, view_all_redis_acls_info, view_all_roles_info, view_all_shard_stats, view_bdb_alerts, view_bdb_info, view_bdb_stats, view_cluster_alerts, view_cluster_info, view_cluster_modules, view_cluster_stats, view_crdb, view_crdb_list, view_endpoint_stats, view_license, view_node_alerts, view_node_check, view_node_info, view_node_stats, view_proxy_info, view_redis_acl_info, view_role_info, view_shard_stats, view_status_of_all_node_actions, view_status_of_cluster_action, view_status_of_node_action Roles list per permission Permission Roles add_cluster_module admin cancel_cluster_action admin cancel_node_action admin config_ldap admin config_ocsp admin create_bdb admin\ncluster_member\ndb_member create_crdb admin\ncluster_member\ndb_member create_ldap_mapping admin create_new_user admin create_redis_acl admin create_role admin delete_bdb admin\ncluster_member\ndb_member delete_cluster_module admin delete_crdb admin\ncluster_member\ndb_member delete_ldap_mapping admin delete_redis_acl admin delete_role admin delete_user admin edit_bdb_module admin\ncluster_member\ndb_member flush_crdb admin\ncluster_member\ndb_member install_new_license admin migrate_shard admin\ncluster_member\ndb_member purge_instance admin\ncluster_member\ndb_member reset_bdb_current_backup_status admin\ncluster_member\ndb_member reset_bdb_current_export_status admin\ncluster_member\ndb_member reset_bdb_current_import_status admin\ncluster_member\ndb_member start_bdb_export admin\ncluster_member\ndb_member start_bdb_import admin\ncluster_member\ndb_member start_cluster_action admin start_node_action admin test_ocsp_status admin update_bdb admin\ncluster_member\ndb_member update_bdb_alerts admin\ncluster_member\ndb_member update_bdb_with_action admin\ncluster_member\ndb_member update_cluster admin update_crdb admin\ncluster_member\ndb_member update_ldap_mapping admin update_node admin update_proxy admin update_redis_acl admin update_role admin update_user admin view_all_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_bdbs_alerts admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_bdbs_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_ldap_mappings_info admin view_all_nodes_alerts admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_nodes_checks admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_nodes_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_nodes_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_proxies_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_redis_acls_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_roles_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_shard_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_all_users_info admin view_bdb_alerts admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_bdb_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_cluster_alerts admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_cluster_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_cluster_keys admin\ncluster_member view_cluster_modules admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_cluster_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_crdb admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_crdb_list admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_endpoint_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_ldap_config admin view_ldap_mapping_info admin view_license admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_logged_events admin\ncluster_member\ncluster_viewer\ndb_member view_node_alerts admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_node_check admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_node_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_node_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_ocsp_config admin view_ocsp_status admin view_proxy_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_redis_acl_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_redis_pass admin\ncluster_member\ndb_member view_role_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_shard_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_status_of_all_node_actions admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_status_of_cluster_action admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_status_of_node_action admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer view_user_info admin ","categories":["RS"]},{"uri":"/rs/references/client_references/client_php/","uriRel":"/rs/references/client_references/client_php/","title":"Redis with PHP (Predis)","tags":[],"keywords":[],"description":"The Predis client allows you to use Redis with PHP.","content":"To use Redis with PHP, you need a PHP Redis client.\nHere, we show Predis, a flexible and feature-complete Redis client library for PHP version 5.3 and later.\nOther Redis clients are available for PHP; see the PHP section of the Redis Clients page.\nInstall Predis See the How to install and use Predis section of the Predis client\u0026rsquo;s README file for installation instructions.\nThe recommended method to install Predis is to use Composer and install it from Packagist or the dedicated PEAR channel.\nYou can also download the latest Predis release from the GitHub repository.\nConnect to Redis The following code creates a connection to Redis using Predis:\n\u0026lt;?php require \u0026#34;predis/autoload.php\u0026#34;; Predis\\Autoloader::register(); $redis = new Predis\\Client(array( \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;tcp\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;hostname\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; port, \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;password\u0026#34;)); echo \u0026#34;Connected to Redis\u0026#34;; ?\u0026gt; Note: If you aren\u0026rsquo;t autoloading PHP dependencies, use require to load Predis and then call its register method, as shown here. To learn more, see Loading the library. To adapt this example to your code, replace the following values with your database\u0026rsquo;s values:\nIn line 8, set host to your database\u0026rsquo;s hostname or IP address In line 9, set port to your database\u0026rsquo;s port In line 10, set password to your database\u0026rsquo;s password Example code for Redis commands Once connected to Redis, you can read and write data. The following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n// open a connection to Redis ... $redis-\u0026gt;set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); $value = $redis-\u0026gt;get(\u0026#34;foo\u0026#34;); var_dump($value); Example output:\n$ php predis_example.php Connected to Redis string(3) \u0026#34;bar\u0026#34; Persistent connections Predis supports the use of persistent connections, which are recommended to minimize connection management overhead.\nTo enable persistent connections, use the persistent connection attribute, as shown in the following code snippet:\n$redis = new Predis\\Client(array( \u0026#34;scheme\u0026#34; =\u0026gt; \u0026#34;tcp\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;hostname\u0026#34;, \u0026#34;port\u0026#34; =\u0026gt; port, \u0026#34;password\u0026#34; =\u0026gt; \u0026#34;password\u0026#34;, \u0026#34;persistent\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;)); Encrypted connections As of v1.1.0, you can encrypt connections by specifying tls or rediss as the value of the scheme attribute or as part of the URI.\nIn addition, you can configure connection parameters, as shown here:\n// Named array of connection parameters: $client = new Predis\\Client([ \u0026#39;scheme\u0026#39; =\u0026gt; \u0026#39;tls\u0026#39;, \u0026#39;ssl\u0026#39; =\u0026gt; [\u0026#39;cafile\u0026#39; =\u0026gt; \u0026#39;private.pem\u0026#39;, \u0026#39;verify_peer\u0026#39; =\u0026gt; true], ]); // Same set of parameters, but using an URI string: $client = new Predis\\Client(\u0026#39;tls://127.0.0.1?ssl[cafile]=private.pem\u0026amp;ssl[verify_peer]=1\u0026#39;); To learn more, see Connecting to Redis.\nWhen using earlier versions of Predis (which is not recommended), you can use stunnel or this Predis fork that has been added with SSL support.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/readers/","uriRel":"/modules/redisgears/jvm/classes/readers/","title":"RedisGears readers","tags":[],"keywords":[],"description":"Extracts data from the database and creates records to pass through a RedisGears pipeline.","content":"A reader extracts data from the database and creates records.\nThe GearsBuilder.CreateGearsBuilder(reader) function takes a reader as a parameter and passes the generated records through a pipeline of RedisGears functions.\nClasses Class Description CommandOverrider Override a Redis command. CommandReader Run RedisGears functions on command. JavaReader A template for creating custom readers. KeysOnlyReader Gets key names from a database. KeysReader Gets keys and their values from a database. ShardsIDReader Gets the shard ID for each shard in a database. StreamReader Reads Redis stream data. ","categories":["Modules"]},{"uri":"/modules/redisbloom/","uriRel":"/modules/redisbloom/","title":"RedisBloom","tags":[],"keywords":[],"description":"","content":"A Bloom filter is a probabilistic data structure which provides an efficient way to verify that an entry is certainly not in a set. This makes it especially ideal when trying to search for items on expensive-to-access resources (such as over a network or disk): If I have a large on-disk database and I want to know if the key foo exists in it, I can query the Bloom filter first, which tells me with a certainty whether it potentially exists (and then the disk lookup can continue) or whether it does not exist, and in this case I can forego the expensive disk lookup and simply send a negative reply up the stack.\nWhile it\u0026rsquo;s possible to use other data structures (such as a hash table) to perform this, Bloom filters are also especially useful in that they occupy very little space per element, typically counted in the number of bits (not bytes!). There exists a percentage of false positives (which is controllable), but for an initial test of whether a key exists in a set, they provide excellent speed and most importantly excellent space efficiency.\nBloom filters are used in a wide variety of applications such as ad serving - making sure a user doesn\u0026rsquo;t see an ad too often; likewise in content recommendation systems - ensuring recommendations don\u0026rsquo;t appear too often, in databases - quickly checking if an entry exists in a table before accessing it on disk, and so on.\nHow Bloom filters work Most of the literature on Bloom filter uses highly symbolic and/or mathematical descriptions to describe it. If you\u0026rsquo;re mathematically challenged like yours truly, you might find my explanation more useful.\nA Bloom filter is an array of many bits. When an element is \u0026lsquo;added\u0026rsquo; to a bloom filter, the element is hashed. Then bit[hashval % nbits] is set to 1. This looks fairly similar to how buckets in a hash table are mapped. To check if an item is present or not, the hash is computed and the filter sees if the corresponding bit is set or not.\nOf course, this is subject to collisions. If a collision happens, the filter returns a false positive - indicating that the entry is indeed found (note that a bloom filter never returns a false negative, that is, claim that something does not exist when it fact it is present).\nIn order to reduce the risk of collisions, an entry may use more than one bit: the entry is hashed bits_per_element (bpe) times with a different seed for each iteration resulting in a different hash value, and for each hash value, the corresponding hash % nbits bit is set. To check if an entry exists, the candidate key is also hashed bpe times, and if any corresponding bit is unset, then it can be determined with certainty that the item does not exist.\nThe actual value of bpe is determined at the time the filter is created. Generally the more bits per element, the lower the likelihood of false positives.\nIn the example above, all three bits would need to be set in order for the filter to return a positive result.\nAnother value affecting the accuracy of a Bloom filter is its fill ratio, or how many bits in the filter are actually set. If a filter has a vast majority of bits set, the likelihood of any specific lookup returning false is decreased, and thus the possibility of the filter returning false positives is increased.\nScalable Bloom filters Typically Bloom filters must be created with a foreknowledge of how many entries they contain. The bpe number needs to be fixed, and likewise, the width of the bit array is also fixed. Unlike hash tables, Bloom filters cannot be \u0026ldquo;rebalanced\u0026rdquo; because there is no way to know which entries are part of the filter (the filter can only determine whether a given entry is not present, but does not actually store the entries which are present).\nIn order to allow Bloom filters to \u0026lsquo;scale\u0026rsquo; and be able to accommodate more elements than they\u0026rsquo;ve been designed to, they may be stacked. Once a single Bloom filter reaches capacity, a new one is created atop it. Typically the new filter has greater capacity than the previous one in order to reduce the likelihood of needing to stack yet another filter.\nIn a stackable (scalable) Bloom filter, checking for membership now involves inspecting each layer for presence. Adding new items now involves checking that it does not exist beforehand, and adding it to the current filter. Hashes still only need to be computed once, however.\nWhen creating a Bloom filter - even a scalable one, it\u0026rsquo;s important to have a good idea of how many items it is expected to contain. A filter whose initial layer can only contain a small number of elements will degrade performance significantly because it will take more layers to reach a larger capacity.\nMore info RedisBloom commands RedisBloom configuration RedisBloom source ","categories":["Modules"]},{"uri":"/rs/security/","uriRel":"/rs/security/","title":"Security","tags":[],"keywords":[],"description":"","content":"Security is an important part of any production system. This section describes the security features and settings available in Redis Enterprise.\nArchitecture security When deploying Redis Enterprise Software to production, we recommend the following practices:\nDeploy Redis Enterprise inside a trusted network: Redis Enterprise is database software and should be deployed on a trusted network not accessible to the public internet. Deploying Redis Enterprise in a trusted network reduces the likelihood that someone can obtain unauthorized access to your data or the ability to manage your database configuration.\nImplement anti-virus exclusions: To ensure that anti-virus solutions that scan files or intercept processes to protect memory do not interfere with Redis Enterprise software, customers should ensure that anti-virus exclusions are implemented across all nodes in their Redis Enterprise cluster in a consistent policy. This helps ensure that anti-virus software does not impact the availibility of your Redis Enterprise cluster.\nIf you are replacing your existing antivirus solution or installing/supporting Redis Enterprise, make sure that the below paths are excluded:\nNote: For antivirus solutions that intercept processes, binary files may have to be excluded directly depending on the requirements of your anti-virus vendor. Path Description /opt/redislabs Main installation directory for all Redis Enterprise Software binaries /opt/redislabs/bin Binaries for all the utilities for command line access and managements such as \u0026ldquo;rladmin\u0026rdquo; or \u0026ldquo;redis-cli\u0026rdquo; /opt/redislabs/config System configuration files /opt/redislabs/lib System library files /opt/redislabs/sbin System binaries for tweaking provisioning Send logs to a remote logging server: Redis Enterprise is configured to send logs by default to syslog. To send these logs to a remote logging server you must configure syslog based the requirements of the remote logging server vendor. Remote logging helps ensure that the logs are not deleted so that you can rotate the logs so that your server disk does not fill up.\nDeploy clusters with an odd number of 3 or more nodes - Redis is an available and partition tolerant database. We recommend that Redis Enterprise be deployed in a cluster of an odd number of 3 or more nodes so that you are able to successfully failover in the event of a failure.\nReboot nodes in a sequence rather that all at once: Customers will frequently maintain reboot schedules. There are cases, however, where our customers have rebooted too many servers at once, causing a quorum failure and resulting in loss of availability of the database. We recommend that rebooting be done in a phased manner so that quorum is not lost. For example, to maintain quorum in a 3 node cluster, at least 2 nodes must be up at all times. Only one server should be rebooted at any given time to maintain quorum.\nImplement client-side encryption: Client-side encryption, or the practice of encrypting data within an application before storing it in a database, such as Redis, is the most widely adopted method to achieve encryption in memory. Redis is an in-memory database and stores data in-memory. If you require encryption in memory, better known as encryption in use, then client side encryption may be the right solution for you. Please be aware that when implementing solutions using client-side encryption database functions that need to operate on data — such as simple searching functions, comparisons, and incremental operations — don’t work with client-side encryption.\nDatabase Security Redis Enterprise offers several database security controls to help protect your data against unauthorized access and to improve the operational security of your databse. The following section details configurable security controls availible for implementation.\nImplement role-based access for users: With role-based access control (RBAC), you can manage ACLs for the entire cluster. You can reuse ACL templates across users, accounts, and multiple databases to precisely scale complex security configurations with a few simple clicks. RBAC lets you set permissions for your databases and for the Redis Enterprise management console itself, providing a complete security-management solution for your cluster.\nPrevent database users from logging into the admin console: Redis Enterprise allows users to be provisioned with both control plane access and access to the database. In some senarios this may be helpful for administrative users, but for applications we recommend that you disable their access to the control plane.\nUse strong Redis passwords: A frequent recommendation in the security industry is to use strong passwords to authenticate users. This helps to prevent brute force password guessing attacks against your database. Its important to check that your password aligns with your organizations security policy.\nDisable the default user: Redis Enterprise comes with a \u0026ldquo;default\u0026rdquo; user for backwards compatibility with applications designed with versions of Redis prior to Redis Enterprise 6. The default user is turned on by default. This allows you to access the database without specifying a username and only using a shared secret. For applications designed to use access control lists, we recommend that you disable the default user.\nEnable client certificate authentication: To prevent unauthorized access to your data, Redis Enterprise databases support the TLS protocol, which includes authentication and encryption. Client certificate authentication can be used to ensure only authorized hosts can access the database.\nInstall trusted certificates: Redis implements self-signed certificates for the database proxy and replication service, but many organizations prefer to use their own certificates.\nConfigure Transport Layer Security (TLS): Similar to the control plane, you can also configure TLS protocols to help support your security and compliance needs.\nConfigure and verify database backups: Implementing a disaster recovery strategy is an important part of data security. Redis Enterprise supports database backups to many destinations.\nLDAP authentication If your organization uses the Lightweight Directory Access Protocol (LDAP), we recommend enabling Redis Software support for role-based LDAP authentication.\n","categories":["RS"]},{"uri":"/rs/databases/configure/shard-placement/","uriRel":"/rs/databases/configure/shard-placement/","title":"Configure shard placement","tags":[],"keywords":[],"description":"Configure shard placement to improve performance.","content":"In Redis Enterprise Software , the location of master and replica shards on the cluster nodes can impact the database and node performance. Master shards and their corresponding replica shards are always placed on separate nodes for data resiliency. The shard placement policy helps to maintain optimal performance and resiliency.\nIn addition to the shard placement policy, considerations that determine shard placement are:\nSeparation of master and replica shards Available persistence and Redis on Flash (RoF) storage Rack awareness Memory available to host the database when fully populated The shard placement policies are:\ndense - Place as many shards as possible on the smallest number of nodes to reduce the latency between the proxy and the database shards; Recommended for Redis on RAM databases to optimize memory resources sparse - Spread the shards across as many nodes in the cluster as possible to spread the traffic across cluster nodes; Recommended for Redis on Flash databases to optimize disk resources When you create a Redis Enterprise Software cluster, the default shard placement policy (dense) is assigned to all databases that you create on the cluster.\nYou can:\nChange the default shard placement policy for the cluster to sparse so that the cluster applies that policy to all databases that you create Change the shard placement policy for each database after the database is created Default shard placement policy When you create a new cluster, the cluster configuration has a dense default shard placement policy. When you create a database, this default policy is applied to the new database.\nTo see the current default shard placement policy, run rladmin info cluster:\nTo change the default shard placement policy so that new databases are created with the sparse shard placement policy, run:\nrladmin tune cluster default_shards_placement [ dense | sparse ] Shard placement policy for a database To see the shard placement policy for a database in rladmin status.\nTo change the shard placement policy for a database, run:\nrladmin placement db [ database name | database ID ] [ dense | sparse ] ","categories":["RS"]},{"uri":"/modules/redisgears/jvm/classes/readers/shardsidreader/","uriRel":"/modules/redisgears/jvm/classes/readers/shardsidreader/","title":"ShardsIDReader","tags":[],"keywords":[],"description":"Gets the shard ID for each shard in a database.","content":"The ShardsIDReader creates a single record on each shard that represents the current shard\u0026rsquo;s ID.\nUse this reader when you want to run an operation on each shard in the database.\nParameters None\nOutput records Creates a record for each shard with the shard\u0026rsquo;s cluster identifier.\nExample ShardsIDReader reader = new ShardsIDReader(); ","categories":["Modules"]},{"uri":"/rc/security/single-sign-on/","uriRel":"/rc/security/single-sign-on/","title":"Single sign-on","tags":[],"keywords":[],"description":"Single sign-on (SSO) integration with Redis Cloud.","content":"As an alternative to the traditional email/password account authentication method, you can create a Redis Cloud account and sign in to the admin console with single sign-on (SSO). This lets you use one set of credentials, managed by your identity provider, to access multiple websites.\nSupported SSO methods SAML SSO Social login ","categories":["RC"]},{"uri":"/modules/redisgears/jvm/classes/readers/streamreader/","uriRel":"/modules/redisgears/jvm/classes/readers/streamreader/","title":"StreamReader","tags":[],"keywords":[],"description":"Reads Redis stream data.","content":"Reads Redis stream data.\nParameters Name Type Default value Description batchSize integer 1 The number of new messages that will cause the functions to run duration integer 0 How many seconds to wait before execution, regardless of batch size failurePolicy FailurePolicy FailurePolicy.CONTINUE How to handle execution failure (CONTINUE/ABORT/RETRY) failureRetryInterval integer 5000 The number of seconds to wait before retrying pattern string \u0026ldquo;*\u0026rdquo; (match all keys) The pattern of keys that store streams startId string \u0026ldquo;0-0\u0026rdquo; Start reading from this stream ID trimStream boolean true Whether or not to trim the stream Output records Creates a record for each message in the input stream.\nEach record is a HashMap\u0026lt;String, Object\u0026gt; with the following fields:\nName Type Description id string The message\u0026rsquo;s ID key string The stream key name value HashMap\u0026lt;String, byte[]\u0026gt; The message\u0026rsquo;s data Examples The following example creates a StreamReader with default values:\nStreamReader reader = new StreamReader(); To change the parameter values for a StreamReader, use their setter methods:\nStreamReader reader = new StreamReader(); // Get streams for keys that match \u0026#34;weather\u0026#34; reader.setPattern(\u0026#34;weather\u0026#34;); // Run RedisGears functions after every 10 messages reader.setBatchSize(10); ","categories":["Modules"]},{"uri":"/rs/installing-upgrading/upgrading/","uriRel":"/rs/installing-upgrading/upgrading/","title":"Upgrade an existing Redis Enterprise Software deployment","tags":[],"keywords":[],"description":"","content":"To upgrade Redis Enterprise Software, you:\nUpgrade the software on all nodes of the cluster.\n(Optional) Upgrade each database in the cluster.\nYou don\u0026rsquo;t have to upgrade the databases in your cluster; however, new features and important fixes might not be enabled until you do so.\nDefault Redis database versions When you upgrade an existing database or create a new one, it uses the default Redis version (default_redis_version) unless you specify the database version explicitly with redis_version in the REST API or rladmin upgrade db.\nRedis Enterprise Software v6.x includes two Redis database versions: 6.0 and 6.2. The default Redis database version differs between Redis Enterprise releases as follows:\nRedis\nEnterprise Bundled Redis\nDB versions Default DB version\n(upgraded/new databases) 6.2.x 6.0, 6.2 6.0 6.4.2 6.0, 6.2 6.2 v6.2.x: default_redis_version is 6.0.\nSetting redis_upgrade_policy to major enforces this default. However, if you change redis_upgrade_policy to latest, this enforces 6.2 as the default.\nThe upgrade policy is only relevant for Redis Enterprise Software versions 6.2.4 through 6.2.18. For more information about upgrade policies, see the 6.2 version of this document.\nv6.4.2: default_redis_version is 6.2.\nBoth major and latest upgrade policies use this new default.\nYou can override the default version with rladmin tune cluster; however, this might limit future upgrade options:\nrladmin tune cluster default_redis_version 6.0 Supported upgrade paths The following upgrade paths are supported:\nCurrentcluster version Upgrade tocluster version 6.2.x 6.4.2 6.0.x 6.4.2\n6.2.x 5.6 6.0.x Upgrade a cluster Upgrade prerequisites Before upgrading a cluster:\nVerify that you meet the upgrade path requirements for your desired cluster version and review the relevant release notes for any preparation instructions.\nIdentify the cluster master node and upgrade that node first.\nUse the rladmin status nodes command or send a GET /nodes/status request to the REST API to identify the master node.\nCluster upgrade process Starting with the master node, follow these steps for every node in the cluster. (We recommend upgrading each node separately to ensure cluster availability.)\nDownload the Redis Enterprise Software installation package to the machine running the node.\nFor help, see Download the installation package\nUntar the installation package. Note that neither the installation path nor the user can be changed during the upgrade.\nVerify node operation with the following commands:\nrlcheck rladmin status extra all Run the install command:\nsudo ./install.sh -y The installation script automatically recognizes the upgrade and responds accordingly.\nThe upgrade replaces all node processes, which might briefly interrupt any active connections.\nVerify node operation by repeating the commands from Step 3.\nIf the admin console was open in a web browser during the upgrade, refresh the browser to reload the console.\nWhen each node is upgraded, the cluster is fully upgraded.\nUpgrade a database Upgrade prerequisites Before upgrading a database:\nReview the relevant release notes for any preparation instructions.\nVerify that the database version meets the minimums specified earlier.\nTo determine the database version:\nUse the admin console to open the Configuration tab for the database.\nUse the rladmin status all command to display configuration details. (An indicator appears in the command output when the database compatibility version is out-of-date.)\nVerify the cluster is fully upgraded and operational.\nUse the admin console to display the Configuration tab for the cluster. The tab displays the cluster version information and the Redis database compatibility version.\nTo avoid data loss during the upgrade, take care to back up the data.\nYou can export the data to an external location, enable replication, or enable persistence.\nWhen choosing how to back up data, keep the following in mind:\nTo reduce downtime when replication is enabled, a failover is performed before restarting the master database.\nWhen persistence is enabled without replication, the database is unavailable during restart because the data is restored from the persistence file. AOF persistence restoration is slower than snapshot restoration.\nDatabase upgrade process To upgrade a database:\nVerify that the redis_upgrade_policy is set according to your preferences.\n(Optional) Back up the database to minimize data loss.\nUse rladmin to upgrade the database:\nrladmin upgrade db \u0026lt;database name | database ID\u0026gt; This restarts the database. No data is lost.\nCheck the Redis database compatibility version for the database to confirm the upgrade.\nTo do so:\nUse the admin console to open the Compatibility tab for the database; the Redis version displays the Redis database compatibility version.\nUse rladmin status databases extra all to display a list of the databases in your cluster and their current Redis database compatibility version.\nVerify that the Redis version is set to the expected value.\nWhen you upgrade an Active-Active (CRDB) database, you can also upgrade:\nProtocol version - Starting with version 5.4.2, a new CRDB protocol version helps support Active-Active features.\nThe CRDB protocol is backward-compatible, which means v5.4.2 CRDB instances can understand write-operations from instances using the the earlier CRDB protocol. However, earlier CRDB instances (those using the older protocol cannot) understand write-operations from instances using the newer protocol version.\nOnce you upgrade the CRDB protocol on one instance, non-upgraded instances cannot receive write updates from the upgraded instance.\nThe upgraded instance receives updates from upgraded and non-upgraded instances.\nWhen upgraded to the latest protocol version, upgraded instances automatically receives any missing write-operations.\nGuidelines:\nUpgrade all instances of a specific CRDB within a reasonable time frame to avoid temporary inconsistencies between the instances.\nMake sure that you upgrade all instances of a specific CRDB before you do global operations on the CRDB, such as removing instances and adding new instances.\nAs of v6.0.20, protocol version 0 is deprecated; support will be removed in a future version.\nTo avoid upgrade failures, update all Active-Active databases to the latest protocol version before upgrading Redis Enterprise Software to v6.0.20 or later.\nFeature version - Starting with version 5.6.0, a new feature version (also called a feature set version) helps support new Active-Active features.\nWhen you update the feature version for an Active-Active database, the feature version is updated for all of the database instances.\nGuidelines:\nAs of v6.0.20, feature version 0 is deprecated; support will be removed in a future version.\nTo avoid upgrade failures, update all Active-Active databases to the latest protocol version before upgrading Redis Enterprise Software to v6.0.20 or later.\nTo upgrade a CRDB instance:\nUpgrade Redis Enterprise Software on each node in the clusters where the CRDB instances are located.\nTo see the status of your CRDB instances, run: rladmin status\nThe statuses of the CRDB instances on the node can indicate:\nOLD REDIS VERSION OLD CRDB PROTOCOL VERSION OLD CRBD FEATURESET VERSION To upgrade each CRDB instance, including the Redis version and CRDB protocol version, run:\nrladmin upgrade db \u0026lt;database_name | database_ID\u0026gt; If the protocol version is old, read the warning message carefully and confirm.\nThe CRDB instance uses the new Redis version and CRDB protocol version.\nUse the keep_crdt_protocol_version option to upgrade the database feature version without upgrading the CRDB protocol version.\nIf you use this option, make sure that you upgrade the CRDB protocol soon after with the rladmin upgrade db command.\nYou must upgrade the CRDB protocol before you update the CRDB feature set version.\nIf the feature set version is old, you must upgrade all of the CRDB instances. Then, to update the feature set for each active-active database, run:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; --featureset-version yes You can retrieve the \u0026lt;CRDB-GUID\u0026gt; with the following command:\ncrdb-cli crdb list Look for the fully qualified domain name (CLUSTER-FDQN) of your cluster and use the associated GUID:\nCRDB-GUID NAME REPL-ID CLUSTER-FQDN 700140c5-478e-49d7-ad3c-64d517ddc486 aatest 1 aatest1.example.com 700140c5-478e-49d7-ad3c-64d517ddc486 aatest 2 aatest2.example.com ","categories":["RS"]},{"uri":"/kubernetes/release-notes/k8s-6-2-12-1/","uriRel":"/kubernetes/release-notes/k8s-6-2-12-1/","title":"Redis Enterprise for Kubernetes release notes 6.2.12-1 (Sept 2022)","tags":[],"keywords":[],"description":"Support added for additional distributions, as well as feature improvements and bug fixes.","content":"Overview The Redis Enterprise K8s 6.2.12-1 supports the Redis Enterprise Software release 6.2.12 and includes feature improvements and bug fixes.\nThe key bug fixes and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.12-82 or redislabs/redis:6.2.12-82.rhel8-openshift Operator: redislabs/operator:6.2.12-1 Services Rigger: redislabs/k8s-controller:6.2.12-1 or redislabs/services-manager:6.2.12-1 (on the Red Hat registry) Feature improvements Redis Enterprise Software 6.2.12 support (RED-83829) Added support for annotations on services created by Redis Enterprise (RED-56245) Support for additional builds of same Redis Software version with same operator version. The list of supported builds will be published. (RED-78757) Bug fixes Fixed Golang related vulnerabilities (RED-79205) Log collector creating larger packages (RED-79650) Log collector crashes when Redis Enterprise cluster is not running (RED-79996) Redis Enterprise Software pods termination process could lead, in some circumstances, to pod shutdown without proper failover of its master shards. This has been fixed so that the failover attempts are retried indefinitely within the grace period, which by default is 1 year. The grace period is controllable via the redisEnterpriseTerminationGracePeriodSeconds parameter in the REC (but note that shutting down a pod without the proper failovers may lead to data loss).(RED-75388) API changes The Redis Enterprise cluster podSecurityPolicy is deprecated. This is still supported but will be removed when all K8s versions supporting the feature are removed.\nCompatibility notes Below is a table showing supported distributions at the time of this release. See Supported Kubernetes distributions for the current list of supported distributions.\nKubernetes version 1.20 1.21 1.22 1.23 1.24 Community Kubernetes supported supported supported Amazon EKS deprecated supported supported* Azure AKS supported supported supported* Google GKE deprecated supported supported supported* Rancher 2.6 supported supported supported* OpenShift version 4.7 4.8 4.9 4.10 4.11 deprecated deprecated supported supported supported* VMware TKGI version 1.11 1.12 1.13 1.14 1.15 deprecated deprecated supported* supported* * Support added in this release\nSupport added Azure AKS 1.24 Amazon EKS 1.23 Google GKE 1.24 OpenShift 4.11 Rancher 1.23 VMware TKGI 1.14 Deprecated Amazon EKS 1.21 Google GKE 1.21 OpenShift 4.7 OpenShift 4.8 VMware TKGI 1.11 VMware TKGI 1.12 No longer supported Community Kubernetes 1.21 Amazon EKS 1.19, 1.20 Azure AKS 1.21 Google GKE 1.19, 1.20 Rancher 2.6 1.19, 1.20 VMware TKGI 1.10 Known limitations Long cluster names cause routes to be rejected (RED-25871)\nA cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542)\nA cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805)\nWhen a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300)\nSTS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002)\nThe redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579)\nOpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462)\nDNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233)\nKubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884)\nIn Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825)\nIn OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254)\nWhen REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192)\nWhen a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921)\nIn K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080)\nThere is no workaround at this time.\nREC might report error states on initial startup (RED-61707)\nThere is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132)\nThe workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515)\nThe workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351)\nContact support if your deployment is impacted.\nProcedure to update credentials might be problematic on OpenShift when accessing the cluster externally using routes (RS issue)(RED-73251)(RED-75329) To workaround this, access the API from within the K8s cluster.\nOn Windows, log_collector doesn\u0026rsquo;t recognize the namespace given with the -n flag (RED-83532)\nTo workaround this, use a different operating system.\nActive-Active database creation will fail if the ingress class annotation is not exactly \u0026ldquo;nginx\u0026rdquo; when using Nginx ingress controller (RED-83070)\n","categories":["Platforms"]},{"uri":"/ri/release-notes/archive/v0.9.34/","uriRel":"/ri/release-notes/archive/v0.9.34/","title":"RDBTools v0.9.34.0, 4 February 2019","tags":[],"keywords":[],"description":"Added support for RBAC permissions.","content":"Role-based access control support When RBAC is enabled, admin users can control the permissions for other users. There are instance-specific permissions and global permissions. Instance permissions can be broadly classified into read and write permissions for each tool.\n","categories":[]},{"uri":"/ri/release-notes/archive/v0.9.33/","uriRel":"/ri/release-notes/archive/v0.9.33/","title":"RDBTools v0.9.34.0, 23 January 2019","tags":[],"keywords":[],"description":"Improve cluster support; minor bug fixes.","content":"Cluster support improvements Ask for seed nodes when adding a cluster Added 2 cluster management alerts: Seed nodes don\u0026rsquo;t agree on cluster configuration All seed nodes are down Fixed \u0026ldquo;flickering\u0026rdquo; issue after a replica node is deleted. Minor bug fixes Skip already processed key in rename bulk ops CSS fix for update version dialog Handle case of unlimited instances for adding sentinel masters ","categories":[]},{"uri":"/kubernetes/release-notes/k8s-6-2-10-45-2022-07/","uriRel":"/kubernetes/release-notes/k8s-6-2-10-45-2022-07/","title":"Redis Enterprise for Kubernetes release notes 6.2.10-45 (July 2022)","tags":[],"keywords":[],"description":"Support added for additional distributions as well as some feature improvements.","content":"Overview The Redis Enterprise K8s 6.2.10-45 supports the Redis Enterprise Software release 6.2.10 and includes feature improvements and bug fixes.\nThe key bug fixes and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.10-129 or redislabs/redis:6.2.10-129.rhel8-openshift (or redislabs/redis:6.2.10-129.rhel7-openshift if upgrading from a RHEL7 cluster) Operator: redislabs/operator:6.2.10-45 Services Rigger: redislabs/k8s-controller:6.2.10-45 or redislabs/services-manager:6.2.10-45 (on the Red Hat registry) Feature improvements OpenShift OperatorLifecycleManager support on restricted networks (RED-72968) log_collector script uses oc command with automatic detection of OpenShift (RED-73215) Operator uses policy/v1 for PodDisruptionBudget(RED-78564) Added support for Kubernetes distributions (see Compatibility notes below) Fixed bugs Upgrade failures when RHEL7 was used (RED-77890) Log collector failures when Python2 was used (RED-73403) API changes The digestHash optional field added to imageSpec fields in the REC. This field should be used in disconnected environments using the OperatorLifecycleManager.\nCompatibility notes Below is a table showing supported distributions at the time of this release. See Supported Kubernetes distributions for the current list of supported distributions.\nKubernetes version 1.19 1.20 1.21 1.22 1.23 1.24 Community Kubernetes deprecated supported supported supported* Amazon EKS deprecated deprecated supported supported* Azure AKS deprecated supported supported Google GKE deprecated deprecated supported supported supported* Rancher 2.6 deprecated deprecated supported supported OpenShift version 4.6 4.7 4.8 4.9 4.10 deprecated deprecated supported supported VMware TKGI version 1.10 1.11 1.12 1.13 deprecated deprecated supported* supported* * Support added in this release\nSupport added K8s community version 1.24 Deprecated OpenShift 4.7-4.8 Kubernetes 1.20 Rancher 2.6 for K8s 1.19-1.20 TKGI 1.10-11 No longer supported OpenShift 4.6 (previously deprecated) Kubernetes 1.18-1.19 (previously deprecated) Rancher 2.6 for K8s 1.18 (previously deprecated) AKS 1.20-1.21 (previously deprecated) EKS 1.18-1.19 (previously deprecated) GKE 1.19 (previously deprecated) Known limitations Long cluster names cause routes to be rejected (RED-25871)\nA cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542)\nA cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805)\nWhen a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300)\nSTS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002)\nThe redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579)\nOpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462)\nDNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233)\nKubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884)\nIn Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825)\nIn OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254)\nWhen REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192)\nWhen a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921)\nIn K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080)\nThere is no workaround at this time.\nREC might report error states on initial startup (RED-61707)\nThere is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132)\nThe workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515)\nThe workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351)\nContact support if your deployment is impacted.\nProcedure to update credentials might be problematic on OpenShift when accessing the cluster externally using routes (RS issue)(RED-73251)(RED-75329) To workaround this, access the API from within the K8s cluster.\n","categories":["Platforms"]},{"uri":"/ri/release-notes/archive/v0.9.32/","uriRel":"/ri/release-notes/archive/v0.9.32/","title":"RDBTools v0.9.32, 7 Jan 2019","tags":[],"keywords":[],"description":"Updates to support Free plan subscriptions and Improved support for ElastiCache.","content":"Free plan Free for a lifetime with usage limit applied on Memory Analyzer, Browser, Cluster Management, and Bulk Operation. Disabled SSL support in the free plan. Disabled S3 support for offline analysis from the free plan. Improved Elasticache support Improved error handling in auto-discovery of elasticache. Removed AWS Access and Secret Key option for security reason now only IAM role option is available now. ","categories":[]},{"uri":"/kubernetes/release-notes/k8s-6-2-10-34-2022-05/","uriRel":"/kubernetes/release-notes/k8s-6-2-10-34-2022-05/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.10-34 (May 2022)","tags":[],"keywords":[],"description":"Support for REBD specific upgrade policies, memcached type REDBs, and RHEL8 for OpenShift, as well as feature improvements and bug fixes.","content":"Overview The Redis Enterprise K8s 6.2.10-34 supports the Redis Enterprise Software release 6.2.10 and includes feature improvements and bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nWarning - Do not upgrade to this 6.2.10-34 release if you are an OpenShift customer and also use modules. Instead, use the 6.2.10-45 release.\nThere was a change in 6.2.10-34 to a new RHEL 8 base image for the Redis Server image. Due to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules.\nPlease contact support if you have further questions.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.10-107 or redislabs/redis:6.2.10-107.rhel8-openshift Operator: redislabs/operator:6.2.10-34 Services Rigger: redislabs/k8s-controller:6.2.10-34 or redislabs/services-manager:6.2.10-34 (on the Red Hat registry) New features Support database upgrade policy (major/latest) for REDB resources (RED-71028) Support for memcached type databases for REDB (RED-70284)(RED-75269) Use RHEL8 base images for OpenShift deployments (RED-72374) Feature improvements OpenShift 4.10 support (RED-73966) Allow setting host time zone on running containers (RED-56810) AKS 1.23 support (RED-73965) EKS 1.22 support (RED-73972) Fixed bugs Outdated SCC YAML file (RED-72026) (RED-73341) Admission container startup failure (RED-72081) Admission container restarts due to race condition with config map creation (RED-72268) Incorrect REDB status report during cluster recovery (RED-72944) Invalid REDB spec not always rejected by admission controller (RED-73145) Compatibility notes Below is a table showing supported distributions at the time of this release. See Supported Kubernetes distributions for the current list of supported distributions.\nKubernetes version 1.19 1.20 1.21 1.22 1.23 Community Kubernetes deprecated deprecated supported supported supported* Amazon EKS supported supported supported Azure AKS supported supported supported* Google GKE supported supported supported supported Rancher 2.6 supported supported supported supported OpenShift version 4.6 4.7 4.8 4.9 4.10 deprecated deprecated supported supported supported* VMware TKGI version 1.10 1.11 1.12 1.13 supported supported supported* * Support added in most recent release\nNow supported OpenShift 4.10 is now supported kOps (Community Kubernetes) 1.23 is now supported AKS 1.23 is now supported EKS 1.22 is now supported Deprecated OpenShift 4.6-4.7 is deprecated kOps (Community Kubernetes) 1.18-1.20 are deprecated GKE 1.19 is deprecated Rancher 2.6 - K8s 1.18 is deprecated AKS 1.20-1.21 are deprecated EKS 1.18-1.19 are deprecated No longer supported Rancher version 2.5 (previously deprecated) is no longer supported (not supported by SUSE) OpenShift version 3.11 (previously deprecated) is no longer supported. Known limitations Warning Openshift customers using modules cannot use 6.2.10-34 version Warning - Do not upgrade to this 6.2.10-34 release if you are an OpenShift customer and also use modules.\nThere was a change in 6.2.10-34 to a new RHEL 8 base image for the Redis Server image. Due to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules.\nThis message will be updated as remediation plans and new steps are available to address this situation. Please contact support if you have further questions.\nLong cluster names cause routes to be rejected (RED-25871)\nA cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542)\nA cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805)\nWhen a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300)\nSTS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002)\nThe redis-enterprise-operator role is missing permission on replica sets.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462)\nDNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233)\nKubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884)\nIn Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825)\nIn OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254)\nWhen REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192)\nWhen a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921)\nIn K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080)\nThere is no workaround at this time.\nREC might report error states on initial startup (RED-61707)\nThere is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132)\nThe workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515)\nThe workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351)\nContact support if your deployment is impacted.\nProcedure to update credentials might be problematic on OpenShift when accessing the cluster externally using routes (RS issue)(RED-73251)(RED-75329) To workaround this, access the API from within the K8s cluster.\n","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/k8s-6-2-10-4-2022-03-copy/","uriRel":"/kubernetes/release-notes/k8s-6-2-10-4-2022-03-copy/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.10-4 (March 2022)","tags":[],"keywords":[],"description":"Added support for RS 6.2.10 as well as feature improvements and bug fixes.","content":"Overview The Redis Enterprise K8s 6.2.10-4 supports the Redis Enterprise Software release 6.2.10 and includes feature improvements and bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.10-90 or redislabs/redis:6.2.10-90.rhel7-openshift Operator: redislabs/operator:6.2.10-4 Services Rigger: redislabs/k8s-controller:6.2.10-4 or redislabs/services-manager:6.2.10-4 (on the Red Hat registry) Feature improvements REDB status - bundled Redis versions for cluster shown in status (RED-44074) OLM (OperatorHub) - example REC contains image references to Red Hat servers (RED-44439) REDB status - added information about database backups (RED-45577) Added dedicated metrics exporter service (service name ending in -prom). Using the service is recommended for gathering Prometheus data about the cluster. Using the service with the UI service is not recommended (RED-61807) REDB - added support for shard_placement RS 6.2.10 support (RED-69142) REDB - control autoUpgrade (RED-71157) Fixed bugs Cluster no longer refuses wrong sized volume to create clusters (error shown instead) (RED-61284)\nResolved high CPU usage when many pods are running within the namespace (RED-69682)\nTKGI - support 1.11/1.12 (RED-70579)\nFixed rack awareness label in documentation (RED-70622)\nFixed crash in admission container when admission service is missing (RED-70678)\nServices rigger deployment generation no longer increases without changes (RED-70835)\nKnown limitations Long cluster names cause routes to be rejected (RED-25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579) OpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time.\nREC might report error states on initial startup (RED-61707) There is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132) The workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name (RED-69515) The workaround is to use the newer (current) revision of the quick start document available online.\nautoUpgrade set to true by operator might cause unexpected bdb upgrades when redisUpgradePolicy is set to true (RED-72351) Contact support if your deployment is impacted.\nCompatibility Notes See Supported Kubernetes distributions for the full list of supported distributions.\nNow supported TKGI 1.11 (K8s 1.20) is supported Rancher version 2.6 is supported Deprecated Rancher version 2.5 support is deprecated No longer supported Rancher version 2.5 / K8s 1.17 (previously deprecated) is no longer supported (not supported by SUSE) ","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/k8s-6-2-8-15-2022-01/","uriRel":"/kubernetes/release-notes/k8s-6-2-8-15-2022-01/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.8-15 (January 2022)","tags":[],"keywords":[],"description":"Maintenance release with bug fixes","content":"Overview The Redis Enterprise K8s 6.2.8-15 is a maintenance release for the Redis Enterprise Software release 6.2.8 and includes bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.8-64 or redislabs/redis:6.2.8-64.rhel7-openshift Operator: redislabs/operator:6.2.8-15 Services Rigger: redislabs/k8s-controller:6.2.8-15 or redislabs/services-manager:6.2.8-15 (on the Red Hat registry) Fixed bugs Upgrading with the bundle using kubectl apply -f fails giving error (RED-69570):\nThe CustomResourceDefinition \u0026#34;redisenterpriseclusters.app.redislabs.com\u0026#34; is invalid: spec.preserveUnknownFields: Invalid value: true: must be false in order to use defaults in the schema. Removed unneeded certificates from the Redis Enterprise Software container (RED-69661, RED-60086)\nKnown limitations Long cluster names cause routes to be rejected (RED-25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579) OpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nREC might report error states on initial startup (RED-61707) There is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132) The workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name The workaround is to use the newer (current) revision of the quick start document available online.\nCompatibility Notes See Supported Kubernetes distributions for the full list of supported distributions.\n","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/k8s-6-2-8-11-2022-01/","uriRel":"/kubernetes/release-notes/k8s-6-2-8-11-2022-01/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.8-11 (January 2022)","tags":[],"keywords":[],"description":"Support for Istio as ingress controller, K8s 1.22 (AKS, kOps, GKE), OpenShift 4.9","content":"Overview The Redis Enterprise K8s 6.2.8-11 release provides support for the Redis Enterprise Software release 6.2.8 and includes several enhancements and bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.8-53 or redislabs/redis:6.2.8-53.rhel7-openshift Operator: redislabs/operator:6.2.8-11 Services Rigger: redislabs/k8s-controller:6.2.8-11 or redislabs/services-manager:6.2.8-11 (on the Red Hat registry) Feature improvements Istio gateway/virtual services are supported as ingress controllers. Note that for Active-Active databases, the operator doesn\u0026rsquo;t create ingress rules, and those should be manually configured. The version of Istio that was tested is Istio 1.12.0 (RED-64020) Support for K8s 1.22 (AKS, kOps, GKE) and OpenShift 4.9 (RED-64016) Support for pod termination grace period customization in the REC (advanced use case) (RED-67217) Improved security granularity of SCC configuration steps in documentation (RED-67321) Changed behavior when two databases with the same name are created on the cluster. The operator avoids creating a service for them to prevent possible corruption (RED-64535) Improved documentation about changing cluster credentials when using Hashicorp Vault (RED-65304) Fixed bugs Upgraded Go dependencies marked as vulnerable (RED-63858, RED-68651) Avoided flooding operator logs with deprecation notices on K8s 1.21 (RED-67544) Fixed log collector utility issues running on Microsoft Windows (RED-67682) Fixed excessive updates to the RS cluster when using Windows line endings for TLS certificates (RED-67874) Known limitations Upgrading with the bundle using kubectl apply -f fails (RED-69515) Upgrading with the bundle using kubectl apply -f fails, giving the following error:\nThe CustomResourceDefinition \u0026quot;redisenterpriseclusters.app.redislabs.com\u0026quot; is invalid: spec.preserveUnknownFields: Invalid value: true: must be false in order to use defaults in the schema\nWorkaround: Before the upgrade, set the \u0026lsquo;spec.preserveUnknownFields` value to false on the REC custom resource. You can use the following command:\nkubectl patch crd redisenterpriseclusters.app.redislabs.com -p $\u0026#39;spec:\\n preserveUnknownFields: false\u0026#39; Long cluster names cause routes to be rejected (RED-25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579) OpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions cannot support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nREC might report error states on initial startup (RED-61707) There is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132) The workaround for this issue is to make sure you use integer values for the PVC size.\nFollowing old revision of quick start guide causes issues creating an REDB due to unrecognized memory field name The workaround is to use the newer (current) revision of the quick start document available online.\nCompatibility Notes See Supported Kubernetes distributions for the full list of supported distributions.\nNow supported This release adds support for the following:\nK8s 1.22 for GKE, AKS, and kOps OpenShift 4.9 (uses K8s 1.22) Deprecated Rancher 2.5/K8s 1.17 support is deprecated No longer supported This release removes support for the following:\nRancher 2.4 (previously deprecated) ","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/k8s-6-2-8-2-2021-11/","uriRel":"/kubernetes/release-notes/k8s-6-2-8-2-2021-11/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.8-2 (November 2021)","tags":[],"keywords":[],"description":"Support for RS 6.2.8, certificate management, and Redis upgrade policy.","content":"Overview The Redis Enterprise K8s 6.2.8-2 release provides support for the Redis Enterprise Software release 6.2.8 and includes several enhancements and bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.8-41 or redislabs/redis:6.2.8-41.rhel7-openshift Operator: redislabs/operator:6.2.8-2 Services Rigger: redislabs/k8s-controller:6.2.8-2 or redislabs/services-manager:6.2.8-2 (on the Red Hat registry) New features Certificate management capabilities through the Redis Enterprise cluster API (RED-61176) Feature improvements Support for managing database ports through the REDB API (RED-48286) Use explicit object instead of wildcard for roles (RED-38013) Advanced use case support for custom attributes for REC stateful set and rigger pods (RED-56502, RED-62550, RED-63426) Services rigger/K8s controller image changed to ubi-minimal lightweight base image (RED-61866) Support for the Redis upgrade policy setting through the REC API (RED-63000) Fixed bugs Fixed issue of log rotation not working on OpenShift (RED-64726) Known limitations Long cluster names cause routes to be rejected (RED-25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on node failure.\nRole missing on replica sets (RED-39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579) OpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions can not support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nREC might report error states on initial startup (RED-61707) There is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132) The workaround for this issue is to make sure you use integer values for the PVC size.\nCompatibility Notes See Supported Kubernetes distributions for the full list of supported distributions.\nNo longer supported kOps K8s versions 1.16 and 1.17 (previously deprecated) are no longer supported AKS K8s version 1.18 (previously deprecated) is no longer supported GKE K8s version 1.18 (previously deprecated) is no longer supported VMWare TKGI version 1.7 (K8s 1.16) (previously deprecated) is no longer supported VMWare TKGI version 1.8 (K8s 1.17) (previously deprecated) is no longer supported Deprecation notice Rancher version 2.4 support is deprecated ","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/k8s-6-2-4-1-2021-09/","uriRel":"/kubernetes/release-notes/k8s-6-2-4-1-2021-09/","title":"Redis Enterprise for Kubernetes Release Notes 6.2.4-1 (September 2021)","tags":[],"keywords":[],"description":"Support for RS 6.2.4 and OpenShift 4.8. Support for K8s 1.19 (EKS and AKS), K8s 1.20 (Rancher, EKS, AKS), K8s 1.21 (GKE and kOps).","content":"Overview The Redis Enterprise K8s 6.2.4-1 release provides support for the Redis Enterprise Software release 6.2.4 and includes several enhancements and bug fixes.\nThe key new features, bug fixes, and known limitations are described below.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.2.4-55 or redislabs/redis:6.2.4-55.rhel7-openshift Operator: redislabs/operator:6.2.4-1 Services Rigger: redislabs/k8s-controller:6.2.4-1 or redislabs/services-manager:6.2.4-1 (on the Red Hat registry) New features Internode encryption configuration through K8s custom resources (RED-59699, RED-60318) Feature improvements Support for addition attribute in REDB secret containing comma separated list of service names (RED-48469) Support OpenShift 4.8 (K8s 1.21) (RED-59424) Support K8s 1.21 - GKE (RED-59048) Support K8s 1.21 - kOps (RED-59047) Support K8s 1.19-1.21 - EKS (RED-60287) Support K8s 1.19, 1.20 - AKS (RED-59050) Support K8s 1.20 - Rancher (RED-59049) Fixed bugs Fixed issue with RS pods not recovering from container failures (RED-53042) Fixed rare problem of Redis Enterprise pod restarting too early while statefulSet was rolling out, causing quorum loss (RED-53042) Improved Github public documentation around using the admission controller with multiple namespaces (RED-59915) Fixed integration issues with HashiCorp Vault enterprise namespaces and custom auth paths (RED-61273) Known limitations Applying bundle.yaml generated a warning When applying bundle.yaml on K8s clusters 1.20 and above, the following warning is generated:\nWarning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition The warning can be safely ignored. A future release will leverage the updated API version for CustomResourceDefinition, once support for K8s 1.22 is introduced.\nEnabling the Kubernetes webhook generates a warning When installing and configuring the admission controller webhook (step 5), the following warning is generated:\nWarning: admissionregistration.k8s.io/v1beta1 ValidatingWebhookConfiguration is deprecated in v1.16+, unavailable in v1.22+; use admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration The warning can be safely ignored. A future release will leverage the updated API version for ValidatingWebhookConfiguration, once support for K8s 1.22 is introduced.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nLong cluster names cause routes to be rejected (RED-25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or fewer.\nCluster CR (REC) errors are not reported after invalid updates (RED-25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED-32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED-39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on the node fails.\nRole missing on replica sets (RED-39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED-38579) OpenShift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED-37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED-37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED-36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED-39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) are bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED-47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED-47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED-51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions can not support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nREC might report error states on initial startup (RED-61707) There is no workaround at this time except to ignore the errors.\nPVC size issues when using decimal value in spec (RED-62132) The workaround for this issue is to make sure you use integer values for the PVC size.\nLong REC names or namespace names cause failures (RED-62222) The combined length of the REC name and namespace name must be equal to or less than 45 characters.\nCompatibility Notes See Supported Kubernetes distributions for the full list of supported distributions.\nNow supported OpenShift 4.8 GKE K8s version 1.21 kOps K8s version 1.21 EKS K8s versions 1.19-1.21 AKS K8s versions 1.19-1.20 Rancher K8s version 1.20 No longer supported GKE K8s version 1.17 (previously deprecated) kOps K8s version 1.15 (previously deprecated) Deprecation notice kOps 1.16 and 1.17 are deprecated VMWare TKGI 1.7 (K8s 1.16), VMWare TKGI 1.8 (K8s 1.17) are deprecated (no longer supported by VMWare) Openshift 3.11 (K8s 1.11) is now deprecated. Redis will continue to support existing deployments for the lifetime of Openshift 3.11, but new deployments are strongly discouraged. ","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-20-12/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-20-12/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.20-12 (July 2021)","tags":[],"keywords":[],"description":"Support for RS 6.0.20-97, EKS, Hashicorp Vault, and added feature support for OpenShift OLM.","content":"Overview The Redis Enterprise K8s 6.0.20-12 release is a major release on top of 6.0.20-4 providing support for the Redis Enterprise Software release 6.0.20-97 and includes several enhancements and bug fixes.\nThis release of the operator provides:\nNew features Various bug fixes Images This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.0.20-97 or redislabs/redis:6.0.20-97.rhel7-openshift` Operator and Bootstrapper: redislabs/operator:6.0.20-12 Services Rigger: redislabs/k8s-controller:6.0.20-12 or redislabs/services-manager:6.0.20-12 (on the Red Hat registry) New features EKS support Feature parity for Openshift OLM; added support for: REDB admission Hashicorp Vault integration Hashicorp Vault integration is now GA Moved Operator environment variables to configmap Important fixes Admission controller deployment combined with operator deployment to simplify deployment (RED-52701) Additional verbosity in the kubectl get redb command output (RED-55042) Support for Redis Enterprise license storage within secrets; Vault and K8s secrets (RED-55587) Support for manual procedure to replace the REC credentials (RED-56529, RED-56530) Fixed cluster recovery issue - recovery process was not starting (RED-55500) When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete (RED-33713) Master pod is not always labeled in Rancher (fixed in 6.0.8 but was not documented) (RED-42896) Known limitations Hashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nNodes down indefinitely after the redis-enterprise-node container of a REC pod is restarted (53042) In some cases where the Redis Enterprise Cluster container in the Redis Enterprise Cluster(REC) pod is restarted, the REC node remains down. Workaround: restart the pod, while ensuring the majority of REC nodes are available.\nCrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod\u0026rsquo;s status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually. The recovery process will then continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or fewer.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nMaster pod label in Rancher (RED42896) The master pod is not always labeled in Rancher.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions can not support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nCompatibility Notes EKS is now supported (K8s 1.18) OpenShift 4.4 (previously deprecated) is no longer supported GKE K8s versions 1.15, 1.16 (previously deprecated) is no longer supported VMWare TKGI 1.10 (K8s 1.19) is now supported Deprecation notice GKE K8s version 1.17 (no longer supported by Google) is deprecated kOps 1.15 is deprecated VMWare TKGI 1.7 (K8s 1.16), VMWare TKGI 1.8 (K8s 1.17) are deprecated (no longer supported by VMWare) ","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-20-4/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-20-4/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.20-4 (May 2021)","tags":[],"keywords":[],"description":"Support for 6.0.20-69, OpenShift 4.7, and K8s 1.20. Hashicorp Vault integration with REC and REDB secrets.","content":"The Redis Enterprise K8s 6.0.20-4 release is a major release on top of 6.0.8-20 providing support for the Redis Enterprise Software release 6.0.20-69 and includes several enhancements and bug fixes.\nOverview This release of the operator provides:\nNew features Various bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.0.20-69 or redislabs/redis:6.0.20-69.rhel7-openshift Operator and Bootstrapper: redislabs/operator:6.0.20-4 Services Rigger: redislabs/k8s-controller:6.0.20-4 or redislabs/services-manager:6.0.20-4 (on the Red Hat registry) New features Support for Openshift 4.7 Support for Kubernetes 1.20 New preview features Hashicorp Vault integration - REC secret Hashicorp Vault integration - REDB secrets Important fixes Fixed upgrade issue with custom container repositories specifying port numbers (RED-53192) REDB controller no longer performs reconciliation until Redis Enterprise software version complies with operator (RED-53194) Removed unused node.js package from Services Rigger image (RED-53536) Fixed operator crash on change of uiServiceType (RED-54621) Avoid excessive logging within RS pod (envoy_access.log) (RED-55525) Known limitations OpenShift 4.7 - v6.0.20-4 cannot be deployed by OLM 6.0.20-4 does not appear in OLM. Workaround - deploy manually. A future maintenance release will address this.\nHashicorp Vault integration - no support for Gesher (RED-55080) There is no workaround at this time\nNodes down indefinitely after the redis-enterprise-node container of a REC pod is restarted (53042) In some cases where the Redis Enterprise Cluster container in the Redis Enterprise Cluster(REC) pod is restarted, the REC node remains down. Workaround: restart the pod, while ensuring the majority of REC nodes are available.\nCrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod\u0026rsquo;s status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually. The recovery process will then continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or fewer.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nMaster pod label in Rancher (RED42896) The master pod is not always labeled in Rancher.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions can not support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nCompatability Notes OpenShift 4.7 and Rancher/kOps 1.20 are now supported OpenShift 4.1, 4.2, 4.3 (previously deprecated) are no longer supported GKE K8s version 1.14 (previously deprecated) is no longer supported kOps (upstream K8s) 1.13, 1.14 (previously deprecated) are no longer supported Deprecation notice OpenShift 4.4 (no longer supported by Red Hat) is deprecated GKE K8s versions 1.15, 1.16 (no longer supported by Google) are deprecated kOps (upstream K8s) 1.15 is deprecated ","categories":["Platforms"]},{"uri":"/ri/using-redisinsight/slowlog/","uriRel":"/ri/using-redisinsight/slowlog/","title":"Slowlog","tags":[],"keywords":[],"description":"","content":"RedisInsight Slowlog is a list of slow operations for your redis instance. These can be used to troubleshoot performance issues. Each entry in the list displays the command, duration and timestamp. Any transaction that exceeds slowlog-log-slower-than microseconds are recorded up to a maximum of slowlog-max-len after which older entries are discarded.\nClear Slowlog - Clear slowlog clears all the slowlog entries from your redis server.\n","categories":["RI"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-12-5/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-12-5/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.12-5 (February 2021)","tags":[],"keywords":[],"description":"Support for RS 6.0.12-57, Amazon Kubernetes Service (AKS), and role permissions on custom resources.","content":"The Redis Enterprise K8s 6.0.12-5 release is a major release on top of 6.0.8-20 providing support for the Redis Enterprise Software release 6.0.12-57 and includes several enhancements and bug fixes.\nOverview This release of the operator provides:\nNew features Various bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.0.12-57 or redislabs/redis:6.0.12-57.rhel7-openshift Operator and Bootstrapper: redislabs/operator:6.0.12-5 Services Rigger: redislabs/k8s-controller:6.0.12-5 or redislabs/services-manager:6.0.12-5 (Red Hat registry) New features Azure Kubernetes Service (AKS) is now supported. (RED40323) Database custom resources now support roles permissions (role to Redis ACL bindings). (RED49780) Feature Improvements The license information has been added to the REC status. (RED43078) kubectl get rec now displays more information (i.e., added \u0026ldquo;NODES\u0026rdquo;, \u0026ldquo;VERSION\u0026rdquo;, \u0026ldquo;STATE\u0026rdquo;, \u0026ldquo;SPEC STATUS\u0026rdquo;). (RED46428) The extraLabels in the cluster spec now applies to the PV and PVC at creation. (RED48694) Added Rancher 2.5 support. (RED50211) Added K8s (Kops) 1.19 support. (RED50211) Added OpenShift 4.6 support. (RED50495) Database resources can now disable the default database user. (RED50215) REC credentials no longer use environment variables in pods. (RED47969) Important fixes Fixed an issue where pods are stuck terminating during teardown. (RED44726) Fixed an issue where pods are stuck terminating during cluster recovery. (RED43846) Changed the operator logs to use human-readable dates and times. (RED39026) Fixed the display of resources in the OLM (OpenShift). (RED48116) Fixed backup configuration issue for GCS without a subdir. (RED49299) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod\u0026rsquo;s status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually. The recovery process will then continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or fewer.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nMaster pod label in Rancher (RED42896) The master pod is not always labeled in Rancher.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nDeleting an OpenShift project with an REC deployed may hang (RED47192) When a REC cluster is deployed in a project (namespace) and has REDB resources, the REDB resources must be deleted first before the REC can be deleted. As such, until the REDB resources are deleted, the project deletion will hang. The fix is to delete the REDB resources first and the REC second. Afterwards, you may delete the project.\nREC extraLabels are not applied to PVCs on K8s versions 1.15 or older (RED51921) In K8s 1.15 or older, the PVC labels come from the match selectors and not the PVC templates. As such, these versions can not support PVC labels. If this feature is required, the only fix is to upgrade the K8s cluster to a newer version.\nCompatibility Notes OpenShift 4.6 and Rancher/kOps 1.19 are now supported. Rancher 2.5 is now supported. AKS (K8s 1.18) is now supported. Deprecation notice OpenShift 4.1, 4.2, and 4.3 are now deprecated. GKE K8s version 1.14 is deprecated. kOps 1.13 and 1.14 are deprecated. ","categories":["Platforms"]},{"uri":"/rs/references/client_references/client_c/","uriRel":"/rs/references/client_references/client_c/","title":"Redis with C","tags":[],"keywords":[],"description":"The hiredis client lets you use C to connect to Redis databases.","content":"To use Redis with C, you need a C Redis client library. Here, you can learn how to use hiredis to connect to a Redis database from an application written in C.\nAdditional C client libraries are available. To learn more, see the C section of the Redis Clients page.\nInstall hiredis Download the latest hiredis release from the GitHub repository.\nConnect to Redis The following code creates a connection to Redis using the hiredis synchronous API:\n#include \u0026#34;hiredis.h\u0026#34; redisContext *c = redisConnect(\u0026#34;hostname\u0026#34;, port); if (c != NULL \u0026amp;\u0026amp; c-\u0026gt;err) { printf(\u0026#34;Error: %s\\n\u0026#34;, c-\u0026gt;errstr); // handle error } else { printf(\u0026#34;Connected to Redis\\n\u0026#34;); } redisReply *reply; reply = redisCommand(c, \u0026#34;AUTH password\u0026#34;); freeReplyObject(reply); ... redisFree(c); To adapt this example to your code, replace the following values with your database\u0026rsquo;s values:\nIn line 1, set the hostname of redisConnect to your database\u0026rsquo;s hostname or IP address In line 1, set the port of redisConnect to your database\u0026rsquo;s port In line 6, replace \u0026ldquo;password\u0026rdquo; with your database\u0026rsquo;s password Example code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n// open a connection to Redis ... redisReply *reply; reply = redisCommand(c,\u0026#34;SET %s %s\u0026#34;,\u0026#34;foo\u0026#34;,\u0026#34;bar\u0026#34;); freeReplyObject(reply); reply = redisCommand(c,\u0026#34;GET %s\u0026#34;,\u0026#34;foo\u0026#34;); printf(\u0026#34;%s\\n\u0026#34;,reply-\u0026gt;str); freeReplyObject(reply); Example output:\n$ gcc example_hiredis.c -o example_hiredis $ ./example_hiredis Connected to Redis bar SSL The hiredis client supports SSL natively as of version 1.0.0. For older versions of hiredis, you can use stunnel to secure the connection.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/recipes/write-behind/","uriRel":"/modules/redisgears/jvm/recipes/write-behind/","title":"Write-behind, write-through, and read-through caching","tags":[],"keywords":[],"description":"Write-behind, write-through, and read-through caching between Redis and other databases (SQL or NoSQL).","content":"The rghibernate recipe uses RedisGears functions and the Hibernate framework to implement write-behind, write-through, and read-through caching.\nThese caching strategies allow applications to simply connect to a Redis cache layer instead of an underlying database. Whenever the application updates data in the cache, Redis also syncs the data in the backend database.\nThe underlying database could be an SQL database like MySQL, so you will need to provide an XML file that tells rghibernate how to map data between Redis and the other database.\nDifferences from RGSync RGSync:\nPython-based recipe Programmable rghibernate:\nJava-based recipe Uses the Hibernate framework Configurable rather than programmable Set up write-behind and read-through To set up write-behind caching, first build an rghibernate JAR and register it with RedisGears.\nThen, register the following configuration files:\nConnector XML: Tells Redis how to connect to the underlying database.\nMapping XML: Shows how to map data between the two databases, such as mapping Redis hashes to MySQL tables.\nRegister rghibernate JAR Download the rghibernate JAR from the download center.\nUpload the JAR to a Redis node.\nRegister rghibernate with RedisGears:\n$ redis-cli -x RG.JEXECUTE com.redislabs.WriteBehind \u0026lt; {filepath}/rghibernate-0.1.1-jar-with-dependencies.jar Configure database connection Create a connector XML file with the configuration to connect Redis to an underlying database.\nUpload the file to a Redis node.\nRegister the connector configuration:\n\u0026gt; redis-cli -x RG.TRIGGER SYNC.REGISTERCONNECTOR mysql 1000 10 5 \u0026lt; src/test/resources/mysql_hibernate.cfg.xml 1) \u0026#34;OK\u0026#34; Configure data mapping Create a mapping XML file that defines how Redis maps data to an underlying database.\nUpload the file to a Redis node.\nRegister the mapping configuration for write-behind:\n\u0026gt; redis-cli -x RG.TRIGGER SYNC.REGISTERSOURCE StudentWrite mysql WriteBehind \u0026lt; src/test/resources/Student.hbm.xml 1) \u0026#34;OK\u0026#34; Register the same mapping configuration for read-through:\n\u0026gt; redis-cli -x RG.TRIGGER SYNC.REGISTERSOURCE StudentRead mysql ReadThrough 0 \u0026lt; src/test/resources/Student.hbm.xml 1) \u0026#34;OK\u0026#34; Example configuration Here are some example configuration files for connecting to databases and mapping data.\nConnector XML This configuration file contains connection details for an underlying MySQL database:\n\u0026lt;!DOCTYPE hibernate-configuration PUBLIC \u0026#34;-//Hibernate/Hibernate Configuration DTD 3.0//EN\u0026#34; \u0026#34;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\u0026#34;\u0026gt; \u0026lt;hibernate-configuration\u0026gt; \u0026lt;session-factory\u0026gt; \u0026lt;!-- JDBC Database connection settings --\u0026gt; \u0026lt;property name=\u0026#34;connection.driver_class\u0026#34;\u0026gt;org.mariadb.jdbc.Driver\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;connection.url\u0026#34;\u0026gt;jdbc:mysql://localhost:3306/test?allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;connection.username\u0026#34;\u0026gt;user\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;connection.password\u0026#34;\u0026gt;pass\u0026lt;/property\u0026gt; \u0026lt;!-- JDBC connection pool settings ... using built-in test pool --\u0026gt; \u0026lt;property name=\u0026#34;connection.pool_size\u0026#34;\u0026gt;1\u0026lt;/property\u0026gt; \u0026lt;!-- Echo the SQL to stdout --\u0026gt; \u0026lt;property name=\u0026#34;show_sql\u0026#34;\u0026gt;false\u0026lt;/property\u0026gt; \u0026lt;!-- Set the current session context --\u0026gt; \u0026lt;property name=\u0026#34;current_session_context_class\u0026#34;\u0026gt;thread\u0026lt;/property\u0026gt; \u0026lt;!-- Drop and re-create the database schema on startup --\u0026gt; \u0026lt;property name=\u0026#34;hbm2ddl.auto\u0026#34;\u0026gt;update\u0026lt;/property\u0026gt; \u0026lt;!-- dbcp connection pool configuration --\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dbcp.initialSize\u0026#34;\u0026gt;5\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dbcp.maxTotal\u0026#34;\u0026gt;20\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dbcp.maxIdle\u0026#34;\u0026gt;10\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dbcp.minIdle\u0026#34;\u0026gt;5\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dbcp.maxWaitMillis\u0026#34;\u0026gt;-1\u0026lt;/property\u0026gt; \u0026lt;/session-factory\u0026gt; \u0026lt;/hibernate-configuration\u0026gt; Mapping XML The following XML maps Redis hashes, which represent students, to a MySQL table:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;hibernate-mapping xmlns=\u0026#34;http://www.hibernate.org/xsd/hibernate-mapping\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.hibernate.org/xsd/hibernate-mapping http://www.hibernate.org/xsd/hibernate-mapping/hibernate-mapping-4.0.xsd\u0026#34;\u0026gt; \u0026lt;class entity-name=\u0026#34;Student\u0026#34; table=\u0026#34;student\u0026#34;\u0026gt; \u0026lt;tuplizer entity-mode=\u0026#34;dynamic-map\u0026#34; class=\u0026#34;org.hibernate.tuple.entity.DynamicMapEntityTuplizer\u0026#34;/\u0026gt; \u0026lt;id name=\u0026#34;id\u0026#34; type=\u0026#34;integer\u0026#34; length=\u0026#34;50\u0026#34; column=\u0026#34;id\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;firstName\u0026#34; column=\u0026#34;first_name\u0026#34; type=\u0026#34;string\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;lastName\u0026#34; column=\u0026#34;last_name\u0026#34; type=\u0026#34;string\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;email\u0026#34; column=\u0026#34;email\u0026#34; type=\u0026#34;string\u0026#34; not-null=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;age\u0026#34; column=\u0026#34;age\u0026#34; type=\u0026#34;integer\u0026#34;/\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/hibernate-mapping\u0026gt; Commands Run rghibernate commands with RG.TRIGGER:\nredis-cli RG.TRIGGER SYNC.\u0026lt;COMMAND\u0026gt; To pass a file to a command like SYNC.REGISTERCONNECTOR or SYNC.REGISTERSOURCE, use the redis-cli -x option:\nredis-cli -x RG.TRIGGER SYNC.{COMMAND} {arg1 arg2 ...} \u0026lt; {file} Command list Command Description SYNC.REGISTERCONNECTOR Register a new connector SYNC.UNREGISTERCONNECTOR Unregister a connector (cannot have sources attached) SYNC.REGISTERSOURCE Extra configuration based on policy SYNC.UNREGISTERSOURCE Unregister a source SYNC.INFO SOURCES Dump all sources SYNC.INFO CONNECTORS Dump all connectors SYNC.REGISTERCONNECTOR $ redis-cli -x RG.TRIGGER SYNC.REGISTERCONNECTOR \\ {connector name} {batch size} {timeout} {retry interval} \\ \u0026lt; {connector xml} Name Description connector name Name to give to your connector batch size The size of the data sent to the backend in batches timeout After this timeout, sends data to the backend even if the batch size was not reached retry interval Retry interval on error connector xml Hibernate XML definition of the connector SYNC.REGISTERSOURCE redis-cli -x RG.TRIGGER SYNC.REGISTERSOURCE \\ {source name} {connector name} {policy} \\ \u0026lt; {mapping xml} Name Description source name Name to give to your source connector name Connector to send the data to policy WriteBehind/WriteThrough/ReadThrough: • On WriteThrough, an extra argument is WriteTimeout • On ReadThrough, an extra argument is expire (0 for no expire) mapping xml Hibernate XML definition of the mapping ","categories":["Modules"]},{"uri":"/rs/databases/active-active/causal-consistency/","uriRel":"/rs/databases/active-active/causal-consistency/","title":"Enable causal consistency","tags":[],"keywords":[],"description":"Enable causal consistency  in an Active-Active database.","content":"When you enable causal consistency in Active-Active databases, the order of operations on a specific key are maintained across all Active-Active database instances.\nFor example, if operations A and B were applied on the same key and the effect of A was observed by the instance that initiated B before B was applied to the key. All instances of an Active-Active databases would then observe the effect of A before observing the effect of B. This way, any causal relationship between operations on the same key is also observed and maintained by every replica.\nEnable causal consistency When you create an Active-Active database, causal consistency is set as:\nOnce enabled, additional operations to enable or disable can only be performed using the REST API or the crdb-cli tool. In this case, the updated Active-Active database behavior happens only for commands and operations received after the change.\nCausal consistency side effects When the causal consistency option is enabled, each instance maintains the order of operations it received from another instance and relays that information to all other N-2 instances, where N represents the number of instances used by the Active-Active database.\nAs a result, network traffic is increased by a factor of (N-2). The memory consumed by each instance and overall performance are also impacted when causal consistency is activated.\n","categories":["RS"]},{"uri":"/rs/databases/memory-performance/","uriRel":"/rs/databases/memory-performance/","title":"Memory and performance","tags":[],"keywords":[],"description":"Learn more about managing your memory and optimizing performance for your database.","content":"Redis Enterprise Software has multiple mechanisms in its architecture to help optimize storage and performance.\nMemory limits Database memory limits define the maximum size your database can reach across all database replicas and shards on the cluster. Your memory limit will also determine the number of shards you\u0026rsquo;ll need.\nBesides your dataset, the memory limit must also account for replication, Active-Active overhead, and module overhead, and a number of other factors. These can significantly increase your database size, sometimes increasing it by four times or more.\nFor more information on memory limits, see Database memory limits.\nEviction policies When a database exceeds its memory limit, eviction policies determine which data is removed. The eviction policy removes keys based on frequency of use, how recently used, randomly, expiration date, or a combination of these factors. The policy can also be set to noeviction to return a memory limit error when trying to insert more data.\nThe default eviction policy for databases is volatile-lru which evicts the least recently used keys out of all keys with the expire field set. The default for Active-Active databases is noeviction.\nFor more information, see eviction policies.\nDatabase persistence Both RAM memory and flash memory are at risk of data loss if a server or process fails. Persisting your data to disk helps protect it against loss in those situations. You can configure persistence at the time of database creation, or by editing the database’s configuration.\nThere are two main types of persistence strategies in Redis Enterprise Software: append-only files (AoF) and snapshots.\nAppend-only files (AoF) keep a record of data changes and writes each change to the end of a file, allowing you to recover the dataset by replaying the writes in the append-only log.\nSnapshots capture all the data as it exists in one moment in time and writes it to disk, allowing you to recover the entire dataset as it existed at that moment in time.\nFor more info on data persistence see Database persistence with Redis Enterprise Software or Durable Redis.\nRedis on Flash (RoF) By default, Redis Enterprise Software stores your data entirely in RAM for improved performance. The Redis on Flash feature enables your data to span both RAM and SSD storage (flash memory). Keys are always stored in RAM, but Redis on Flash manages the location of their values. Frequently used (hot) values are stored on RAM, but infrequently used (warm) values are moved to flash memory. This saves on expensive RAM space, which give you comparable performance at a lower cost for large datasets.\nFor more info, see Redis on Flash.\nShard placement The location of the primary and replica shards on the cluster nodes can impact your database performance. Primary shards and their corresponding replica shards are always placed on separate nodes for data resiliency and high availability. The shard placement policy helps to maintain optimal performance and resiliency.\nRedis Enterprise Software has two shard placement policies available:\ndense: puts as many shards as possible on the smallest number of nodes sparse: spread the shards across as many nodes as possible For more info about the shard placement policy, see Shard placement policy\nMetrics From the Redis Enterprise Software admin console, you can monitor the performance of your clusters, nodes, databases, and shards with real-time metrics. You can also enable alerts for node, cluster, or database events such as high memory usage or throughput.\nWith the Redis Enterprise Software API, you can also integrate Redis Enterprise metrics into other monitoring environments, such as Prometheus.\nFor more info about monitoring with Redis Enterprise Software, see Monitoring with metrics and alerts, and Memory statistics.\nScaling databases Each Redis Enterprise cluster can contain multiple databases. In Redis, databases represent data that belong to a single application, tenant, or microservice. Redis Enterprise is built to scale to 100s of databases per cluster to provide flexible and efficient multi-tenancy models.\nEach database can contain few or many Redis shards. Sharding is transparent to Redis applications. Master shards in the database process data operations for a given subset of keys. The number of shards per database is configurable and depend on the throughput needs of the applications. Databases in Redis Enterprise can be resharded into more Redis shards to scale throughput while maintaining sub-millisecond latencies. Resharding is performed without downtime.\nFigure 2 Redis Enterprise places master node (M) and replicas (R) in separate nodes, racks and zones and use in-memory replication to protect data against failures.\nIn Redis Enterprise, each database has a quota of RAM. The quota cannot exceed the limits of the RAM available on the node. However, with Redis Enterprise Flash, RAM is extended to the local flash drive (SATA, NVMe SSDs etc). The total quota of the database can take advantage of both RAM and Flash drive. The administrator can choose the RAM vs Flash ratio and adjust that anytime in the lifetime of the database without downtime.\nWith Redis on Flash, instead of storing all keys and data for a given shard in RAM, less frequently accessed values are pushed to flash. If applications need to access a value that is in flash, Redis Enterprise automatically brings the value into RAM. Depending on the flash hardware in use, applications experience slightly higher latency when bringing values back into RAM from flash. However subsequent accesses to the same value is fast, once the value is in RAM.\n","categories":["RS"]},{"uri":"/kubernetes/deployment/openshift/openshift-operatorhub/","uriRel":"/kubernetes/deployment/openshift/openshift-operatorhub/","title":"Deploy Redis Enterprise with OpenShift OperatorHub","tags":[],"keywords":[],"description":"OpenShift provides the OperatorHub where you can install the Redis Enterprise operator from the administrator user interface.","content":"Redis Enterprise for Kubernetes can be deployed and administered entirely from the OpenShift command-line interface (CLI). For those who prefer a user interface (UI), OpenShift also provides the OperatorHub, a web console interface where you can install operators and create custom resources.\nFor details on the OperatorHub, see the OpenShift documentation.\nPrepare the cluster The Redis Enterprise pods must run in OpenShift with privileges set in a Security Context Constraint. This grants the pod various rights, such as the ability to change system limits or run as a particular user.\nWarning - You must install the security context constraint for the operator (scc.yaml) before it can create any clusters. You only need to install the SCC once, but you must not delete it.\nSelect the project you\u0026rsquo;ll be using or create a new project.\nDownload scc.yaml.\nApply the file to install the security context constraint.\noc apply -f scc.yaml After the install, the OperatorHub automatically uses the constraint for Redis Enterprise node pods.\nNote: Known Limitation - The automatic use of the security constraint is limited. The Redis Enterprise must be named rec for the constraint to be used automatically. We recommend you use the cluster name rec when deploying with the OperatorHub.\nIf you require a different name, you must grant the SCC to the project namespace (e.g., my-project) as in OpenShift 3.x:\noc adm policy add-scc-to-group redis-enterprise-scc system:serviceaccounts:my-project Install the Redis Enterprise operator Select Operators-\u0026gt;OperatorHub.\nSearch for \u0026ldquo;Redis Enterprise\u0026rdquo; in the search dialog and select the Redis Enterprise Operator provided by Redis marked as Certified. Note: Custom catalogs are not supported. On the Install Operator page, specify the namespace for the operator.\nOnly one namespace per operator is supported.\nUpdate the channel with the version you\u0026rsquo;re installing.\nFor more information about specific versions, see the release notes.\nChoose an approval strategy.\nWe recommend Manual for production systems to ensure the operator is only upgraded by approval.\nSelect Install and approve the install plan.\nYou can monitor the subscription status in Operators-\u0026gt;Installed Operators.\nCreate Redis Enterprise custom resources The Installed Operators-\u0026gt;Operator details page shows the provided APIs: RedisEnterpriseCluster and RedisEnterpriseDatabase. You can select Create instance to create custom resources using the OperatorHub interface.\nUse the YAML view to create a custom resource file or let OperatorHub generate the YAML file for you by specifying your configuration options in the form view.\nFor more information on creating and maintaining Redis Enterprise custom resources, see Redis Enterprise clusters (REC) and Redis Enterprise databases (REDB).\n","categories":["Platforms"]},{"uri":"/modules/install/packaging-modules/","uriRel":"/modules/install/packaging-modules/","title":"Package modules","tags":[],"keywords":[],"description":"","content":"In addition to the modules packaged and certified by Redis, there are many custom modules that are compatible with Redis Enterprise.\nTo use custom modules with a Redis Enterprise database, you need to package them with the RAMP (Redis Automatic Module Packaging) utility before you install them on the cluster.\nWarning - Redis does not officially support third-party modules or databases created with them. Prerequisites Install the ramp-packer utility:\npip install ramp-packer Package custom modules Before you can install and enable a custom module in a new database, you need to download, compile, and package it with the RAMP utility.\nDownload the module Download the module source code:\ngit clone https://github.com/account/myModule.git Compile the module Compile the module with the following command:\ncd myModule/;make Package the module with RAMP $ ramp pack \u0026lt;PATH_TO_myModule.so\u0026gt; -a \u0026#34;Your Name\u0026#34; \\ -e \u0026#34;yourname@emailaddress.com\u0026#34; -A \u0026#34;x86_64\u0026#34; -d \u0026#34;My Module\u0026#34; \\ -h \u0026#34;https://www.mymodule.com/\u0026#34; -l \u0026#34;LicenseType\u0026#34; -r \u0026#34;4.0.2\u0026#34; See the RAMP README for more information about RAMP\u0026rsquo;s command-line options.\nNext steps Install the custom module on the cluster. Create a database and enable the custom module. ","categories":["Modules"]},{"uri":"/rs/clusters/configure/rack-zone-awareness/","uriRel":"/rs/clusters/configure/rack-zone-awareness/","title":"Rack-zone awareness in Redis Enterprise Software","tags":[],"keywords":[],"description":"Rack-zone awareness ensures high-availability in the event of a rack or zone failure.","content":"Rack-zone awareness is a Redis Enterprise feature that helps to ensure high-availability in the event of a rack or zone failure.\nWhen you enable rack-zone awareness in a Redis Enterprise Software cluster, you assign a rack-zone ID to each node. This ID is used to map the node to a physical rack or logical zone. The cluster can then ensure that master shards, corresponding replica shards, and associated endpoints are placed on nodes in different racks/zones.\nIn the event of a rack or zone failure, the replicas and endpoints in the remaining racks/zones are promoted. This ensures high availability when a rack or zone fails.\nThere is no limitation on the number of rack-zones per cluster; each node can belong to a different rack, or multiple nodes can belong to the same rack.\nRack-zone awareness affects various cluster, node and database-related actions, such as node rebalancing, node removal, node replacement, shard and endpoint migration, and database failover.\nCluster and node configuration To enable rack-zone awareness, you need to configure it at the cluster, node, and database levels.\nFirst, enable rack-zone awareness when you initially create the cluster.\nNow, every time you add a new node to the cluster, define a rack-zone ID for the node.\nThe rack-zone ID must comply with the following rules:\nMaximum length of 63 characters. Characters consist of letters, digits, or hyphens (\u0026rsquo;-\u0026rsquo;). ID starts with a letter and ends with a letter or a digit. Note: Rack-zone IDs are case-insensitive (uppercase and lowercase letter are treated as the same). Node layout Recall that the recommended minimum number of nodes in a RS deployment is three. For high availability, these three nodes must be distributed across three distinct racks or zones.\nWhen using availability zones, note that all three zones should exist within the same region to avoid potential latency issues.\nKeep in mind that one of the nodes in your cluster can be a quorum-only node, assuming compute resources are limited. What this means is that the minimum rack-zone aware RS deployment will consist of two data nodes and one quorum-only node, where each of these nodes is situated is a distinct rack or zone.\nDatabase configuration Once the cluster has been configured to support rack-zone awareness, you can create a rack-zone aware database.\nRack-zone awareness is relevant only for databases that have replication enabled (i.e., databases with replica shards). Once you enable replication for a database, you may also enable rack-zone awareness.\nShard placement without rack-zone awareness Note that even in the case of a database with rack-zone awareness disabled, the cluster will still ensure that master and replica shards are placed on distinct nodes.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/recipes/","uriRel":"/modules/redisgears/jvm/recipes/","title":"Java-based RedisGears recipes","tags":[],"keywords":[],"description":"","content":"A RedisGears recipe is a collection of RedisGears functions, often designed to perform a specific, complex task.\nRecipe Description Caching strategies Write-behind, write-through, and read-through caching between Redis and other databases (SQL or NoSQL). ","categories":["Modules"]},{"uri":"/modules/redisgears/python/recipes/","uriRel":"/modules/redisgears/python/recipes/","title":"Python-based RedisGears recipes","tags":[],"keywords":[],"description":"","content":"A RedisGears recipe is a collection of RedisGears functions, often designed to perform a specific, complex task.\nRecipe Description Write-behind caching Write-behind and write-through caching between Redis and other databases (SQL or NoSQL). ","categories":["Modules"]},{"uri":"/rs/clusters/cluster-recovery/","uriRel":"/rs/clusters/cluster-recovery/","title":"Recover a failed cluster","tags":[],"keywords":[],"description":"How to use the cluster configuration file and database data to recover a failed cluster.","content":"When a Redis Enterprise Software cluster fails, you must use the cluster configuration file and database data to recover the cluster.\nNote: For cluster recovery in a Kubernetes deployment, go to: Recover a Redis Enterprise cluster on Kubernetes. Cluster failure can be caused by:\nA hardware or software failure that causes the cluster to be unresponsive to client requests or administrative actions. More than half of the cluster nodes lose connection with the cluster, resulting in quorum loss. To recover a cluster and re-create it as it was before the failure you must restore the cluster configuration (ccs-redis.rdb) to the cluster nodes. To restore the data that was in the databases to databases in the new cluster you must restore the database persistence files (backup, AOF, or snapshot files) to the databases. These files are stored in the persistent storage location.\nThe cluster recovery process includes:\nInstall RS on the nodes of the new cluster. Mount the persistent storage with the recovery files from the original cluster to the nodes of the new cluster. Recover the cluster configuration on the first node in the new cluster. Join the remaining nodes to the new cluster. Recover the databases. Prerequisites We recommend that you recover the cluster to clean nodes. If you use the original nodes, make sure there are no Redis processes running on any nodes in the new cluster. We recommend that you use clean persistent storage drives for the new cluster. If you use the original storage drives, make sure that you backup the files on the original storage drives to a safe location. Identify the cluster configuration file that you want to use as the configuration for the recovered cluster. The cluster configuration file is /css/ccs-redis.rdb on the persistent storage for each node. Recovering the cluster (Optional) If you want to recover the cluster to the original cluster nodes, uninstall RS from the nodes.\nInstall RS on the new cluster nodes.\nDo not configure the cluster nodes (rladmin cluster create in the CLI or Setup in the admin console).\nThe new servers must have the same basic hardware and software configuration as the original servers, including:\nThe same number of nodes At least the same amount of memory The same RS version The same installation user and paths Note: The cluster recovery can fail if these requirements are not met. Mount the persistent storage drives with the recovery files to the new nodes. These drives must contain the cluster configuration backup files and database persistence files.\nNote: Make sure that the user redislabs has permissions to access the storage location of the configuration and persistence files on each of the nodes. If you use local persistent storage, place all of the recovery files on each of the cluster nodes.\nTo recover the cluster configuration from the original cluster to the first node in the new cluster, from the rladmin command-line interface (CLI):\ncluster recover filename [ \u0026lt;persistent_path\u0026gt; | \u0026lt;ephemeral_path\u0026gt; ]\u0026lt;filename\u0026gt; node_uid \u0026lt;node_uid\u0026gt; rack_id \u0026lt;rack_id\u0026gt; Command syntax \u0026lt;filename\u0026gt; - The full path of the old cluster configuration file in the persistent storage. The cluster configuration file is /css/ccs-redis.rdb.\n\u0026lt;node_uid\u0026gt; - The id of the node, in this case 1.\n\u0026lt;persistent_path\u0026gt; (optional) - The location of the persistent storage in the new node.\n\u0026lt;ephemeral_path\u0026gt; (optional) - The location of the ephemeral storage in the new node.\n\u0026lt;rack_id\u0026gt; (optional) - If rack-zone awareness was enabled in the cluster, you can use this parameter to override the rack ID value that was set for the node with ID 1 with a new rack ID. Otherwise, the node gets the same rack ID as the original node.\nFor example:\nrladmin cluster recover filename /tmp/persist/ccs/ccs-redis.rdb node_uid 1 rack_id 5 When the recovery command succeeds, this node is configured as the node from the old cluster that has ID 1.\nTo join the remaining servers to the new cluster, from the rladmin CLI of each new node run:\ncluster join [ nodes \u0026lt;cluster_member_ip_address\u0026gt; | name \u0026lt;cluster_FQDN\u0026gt; ] username \u0026lt;username\u0026gt; password \u0026lt;password\u0026gt; replace_node \u0026lt;node_id\u0026gt; Command syntax nodes - The IP address of a node in the cluster that this node is joining.\nname - The FQDN name of the cluster this node is joining.\nusername - The email address of the cluster administrator.\npassword - The password of the cluster administrator.\nreplace_node - The ID of the node that this node replaces from the old cluster.\npersistent_path (optional) - The location of the persistent storage in the new node.\nephemeral_path (optional) - The location of the ephemeral storage in the new node.\nrack_id (optional) - If rack-zone awareness was enabled in the cluster, use this parameter to set the rack ID to be the same as the rack ID of the old node. You can also change the value of the rack ID by providing a different value and using the override_rack_id flag.\nFor example:\nrladmin cluster join nodes 10.142.0.4 username admin@example.com password mysecret replace_node 2 You can run the rladmin status command to verify that the recovered nodes are now active, and that the databases are pending recovery.\nNote: Make sure that you update your DNS records with the IP addresses of the new nodes. After the cluster is recovered, you must recover the databases.\n","categories":["RS"]},{"uri":"/modules/","uriRel":"/modules/","title":"Redis Stack and modules","tags":[],"keywords":[],"description":"","content":"Redis develops several modules that extend the core Redis feature set. Some of the features these modules provide include querying, indexing and full-text search, JSON support, and probabalistic data structures.\nRedis Stack lets you install and leverage modules quickly; it enables access to multiple modules in a single Redis database.\nRedis Enterprise Software and Redis Enterprise Cloud support all capabilities of Redis Stack.\nEach module includes a quick start guide.\nRediSearch RediSearch is a source-available full-text and secondary index engine for Redis.\nIntroduction Release notes Sizing calculator Documentation RedisJSON RedisJSON implements ECMA-404 The JSON Data Interchange Standard as a native data type. It allows storing, updating, and fetching JSON values from Redis keys (documents).\nIntroduction Release notes Quick start Documentation RedisGraph RedisGraph is the first queryable property graph database to use sparse matrices to represent the adjacency matrix in graphs and linear algebra to query the graph.\nIntroduction Release notes Sizing calculator Documentation RedisTimeSeries RedisTimeSeries adds a time series data structure to Redis.\nIntroduction Release notes Sizing calculator Documentation RedisBloom RedisBloom provides four data types: a scalable Bloom filter, a cuckoo filter, a count-min-sketch, and a top-k.\nIntroduction Release notes Quick start Documentation RedisGears RedisGears is a serverless engine for transaction, batch, and event-driven data processing in Redis. (Redis Enterprise Software only)\nIntroduction Release notes Quick start Documentation RedisAI RedisAI is a Redis module for executing deep learning/machine learning models and managing their data. (Redis Enterprise Software only)\nIntroduction Release notes Quick start Documentation ","categories":["Modules"]},{"uri":"/modules/redisgears/","uriRel":"/modules/redisgears/","title":"RedisGears","tags":[],"keywords":[],"description":"","content":"What is RedisGears? RedisGears is an engine for data processing in Redis. RedisGears supports batch and event-driven processing for Redis data. To use RedisGears, you write functions that describe how your data should be processed. You then submit this code to your Redis deployment for remote execution.\nSupported languages As of RedisGears v1.2, you can enable a plugin to select which programming language to use. It currently supports code written in either Python or Java.\nPrior to v1.2, RedisGears only supported Python. However, an internal C API exists and can be used by other Redis modules. Support for other languages is being planned.\nGetting started with RedisGears RedisGears is implemented by a Redis module. To use RedisGears, you\u0026rsquo;ll need to make sure that your Redis deployment has the module installed. Redis Enterprise Software supports the module natively.\nIf you\u0026rsquo;re running open source Redis, you\u0026rsquo;ll also need to install the RedisGears module before using it.\nTo get started with RedisGears, see the quick start tutorial for Python or Java.\nTo learn more about the RedisGears API and understand how it works under the hood, see the RedisGears docs.\nWrite-behind caching patterns Redis users typically implement caching by using the look-aside pattern. However, with RedisGears, you can implement write-behind caching strategies as well.\nRedis publishes RedisGears recipes to support write-behind. You can learn how to use these recipes in our write-behind caching guides for Python and Java.\nMore info RedisGears introduction RedisGears source ","categories":["Modules"]},{"uri":"/ri/release-notes/","uriRel":"/ri/release-notes/","title":"RedisInsight release notes","tags":[],"keywords":[],"description":"","content":"Here are the most recent changes for RedisInsight:\nVersion (Date) Release notes v2.16.0 (Dec 2022) RedisInsight v2.16.0, December 2022 v2.14.0 (Nov 2022) RedisInsight v2.14.0, November 2022 v2.18.0 (Jan 2023) RedisInsight v2.18.0, January 2023 v2.12.0 (Oct 2022) RedisInsight v2.12.0, October 2022 v2.10.0 (Sept 2022) RedisInsight v2.10.0, September 2022 v2.8.0 (Aug 2022) RedisInsight v2.8.0, August 2022 v1.13 (Aug 2022) RedisInsight v1.13, Aug 2022 v2.6.0 (July 2022) RedisInsight v2.6.0, July 2022 v2.4.0 (June 2022) RedisInsight v2.4.0, June 2022 v2.2.0 (May 2022) RedisInsight v2.2.0, May 2022 v1.12 (May 2022) RedisInsight v1.12, May 2022 v2.0 (Nov 2021) RedisInsight v2.0, Nov 2021 v1.11 (Oct 2021) RedisInsight v1.11, Oct 2021 v1.10 (Mar 2021) RedisInsight v1.10, March 2021 v1.9 (Jan 2021) RedisInsight v1.9, January 2021 v1.8 (Nov 2020) RedisInsight v1.8, November 2020 v1.7 (Sep 2020) RedisInsight v1.7, September 2020 v1.6 (June 2020) RedisInsight v1.6, June 2020 v1.5 (May 2020) RedisInsight v1.5, May 2020 v1.4 (Apr 2020) RedisInsight v1.4, April 2020 v1.3 (Mar 2020) RedisInsight v1.3, March 2020 v1.2 (Jan 2020) RedisInsight v1.2, January 2020 v1.1 (Dec 2019) RedisInsight v1.1, December 2019 v1.0 (Nov 2019) RedisInsight v1.0, November 2019 RDBTools (2019) RDBTools releases, 2019 ","categories":["RI"]},{"uri":"/rs/security/tls/","uriRel":"/rs/security/tls/","title":"Transport Layer Security (TLS)","tags":[],"keywords":[],"description":"An overview of Transport Layer Security (TLS).","content":"Transport Layer Security (TLS), a successor to SSL, ensures the privacy of data sent between applications and Redis databases. TLS also secures connections between Redis Enterprise Software nodes.\nYou can use TLS authentication for the following types of communication:\nCommunication from clients (applications) to your database Communication from your database to other clusters for replication using Replica Of Communication to and from your database to other clusters for synchronization using Active-Active Protocols and ciphers TLS protocols and ciphers define the overall suite of algorithms that clients are able to connect to the servers with.\nYou can change the TLS protocols and ciphers to improve the security of your Redis Enterprise cluster and databases. The default settings are in line with industry best practices, but you can customize them to match the security policy of your organization.\nClient-side encryption Client-side encryption may be used to help encrypt data through its lifecycle. This comes with some limitations. Operations that must operate on the data, such as increments, comparisons, and searches will not function properly. Client-side encryption is used to help protect data in use.\nYou can write client-side encryption logic directly in your own application or use functions built into clients such as the Java Lettuce cipher codec.\n","categories":["RS"]},{"uri":"/rs/installing-upgrading/uninstalling/","uriRel":"/rs/installing-upgrading/uninstalling/","title":"Uninstall Redis Enterprise Software","tags":[],"keywords":[],"description":"","content":"Use the rl_uninstall script to uninstall Redis Enterprise Software and remove its files.\nsudo ./rl_uninstall.sh By default, the script is located in /opt/redislabs/bin.\n","categories":["RS"]},{"uri":"/rc/databases/usage-reports/","uriRel":"/rc/databases/usage-reports/","title":"Usage reports","tags":[],"keywords":[],"description":"","content":"The Usage Report shows the daily memory usage and shard usage of all databases from the subscriptions associated with your account.\nTo filter the data, you can:\nSelect a month and year from the View Statement For list to view the daily memory usage during a specific month. Select a subscription from the Subscription list to view the daily memory usage of a specific subscription associated with your account. Select a database from the Database list to view the daily memory usage of a specific database. You can also hold the pointer over each bar in the graph to view the precise memory usage on that day.\n","categories":["RC"]},{"uri":"/rc/api/examples/view-account-information/","uriRel":"/rc/api/examples/view-account-information/","title":"View account information","tags":[],"keywords":[],"description":"Get initial information on account parameters","content":"The root API operation returns information about the current account, user, and API Key (as identified by the set of API Keys provided in the API request).\nGET \u0026#34;https://[host]/v1/subscriptions/\u0026lt;subscription_id\u0026gt;/databases/\u0026lt;database_id\u0026gt;/metrics?metricSpan=1hour\u0026#34; Here is an example of the API operation response:\n{ \u0026#34;account\u0026#34;: { \u0026#34;id\u0026#34;: 654321, \u0026#34;name\u0026#34;: \u0026#34;Redis\u0026#34;, \u0026#34;createdTimestamp\u0026#34;: \u0026#34;2018-12-23T15:15:31Z\u0026#34;, \u0026#34;updatedTimestamp\u0026#34;: \u0026#34;2019-07-04T12:22:04Z\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;jay-doe-api-key-6\u0026#34;, \u0026#34;accountId\u0026#34;: 654321, \u0026#34;accountName\u0026#34;: \u0026#34;Redis account for Jay Doe\u0026#34;, \u0026#34;allowedSourceIps\u0026#34;: [ \u0026#34;192.0.2.0/24\u0026#34; ], \u0026#34;createdTimestamp\u0026#34;: \u0026#34;2019-06-06T07:41:14Z\u0026#34;, \u0026#34;owner\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Jay Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;jay.doe@redislabs.com\u0026#34; }, \u0026#34;httpSourceIp\u0026#34;: \u0026#34;192.0.2.0\u0026#34; } } } The above example contains:\nhttpSourceIp: The public source IP of the current user allowedSourceIps: Restricts the current API Key access to requests originating from IP addresses that reside within the specified IP range ","categories":["RC"]},{"uri":"/modules/redisgears/python/recipes/write-behind/","uriRel":"/modules/redisgears/python/recipes/write-behind/","title":"Write-behind caching","tags":[],"keywords":[],"description":"Write-behind and write-through caching between Redis and other databases (SQL or NoSQL).","content":"Write-behind is a caching strategy in which the cache layer itself connects to the backing database. This means that your applications need only ever connect to your cache layer, and the cache then reads from or updates the backing database as needed. Redis currently supports write-behind caching in Redis Enterprise Software.\nHere\u0026rsquo;s how these caching patterns work:\nYour application uses the cache for reads and writes. The cache syncs any changed data to the backing database asynchronously. Install the write-behind recipe The write-behind recipe comes with two types of dependencies:\nDrivers to connect to the backend database Python libraries for the RedisGears functions In most cases all of these can be provisioned to RedisGears before the functions are uploaded. However, root access for the driver on the host is required in some cases, for example with Oracle drivers.\nInstall Oracle driver (optional) If you want to do write-behind with an Oracle database:\nDownload the Oracle driver. On each RS node in the cluster, follow the installation instructions on the download page for the Oracle driver. Import requirements Install Python 3.\nTo install the gears-cli, run:\npip install gears-cli Download rgsync offline package.\nImport the requirements:\n# gears-cli import-requirements \\ --host HOST [ --port PORT ] --password PASSWORD \\ --requirements-path rgsync-99.99.99.linux-bionic-x64.zip Note: You can be more efficient and import only the requirements you need, but rgsync is always required and can be combined with one or more of these packages according to your backend database:\nredisgears-requirement-v1-snowflake-sqlalchemy-linux-\u0026lt;os\u0026gt;-x64.zip redisgears-requirement-v1-PyMySQL-linux-\u0026lt;os\u0026gt;-x64.zip redisgears-requirement-v1-cx-Oracle-linux-\u0026lt;os\u0026gt;-x64.zip redisgears-requirement-v1-cassandra-driver-linux-\u0026lt;os\u0026gt;-x64.zip This list can be different or more extensive in newer versions. The module.json file in the module package lists the dependencies for the module.\nFrom the CLI of a node or of a client that is connected to the database, check that the requirements were imported successfully with:\nredis-cli RG.PYDUMPREQS To connect to the database from a client, run:\nredis-cli -h \u0026lt;FQDN_of_node\u0026gt; -p \u0026lt;host\u0026gt; [-a \u0026lt;password\u0026gt;] This command returns a list of all available requirements.\nRegister the functions The following is a RedisGears recipe that shows how to use the write-behind pattern to map data from Redis hashes to MySQL tables.\nThe recipe maps all Redis hashes with the prefix person:\u0026lt;id\u0026gt; to the MySQL table persons, with \u0026lt;id\u0026gt; being the primary key and mapped to the person_id column. Similarly, it maps all hashes with the prefix car:\u0026lt;id\u0026gt; to the cars table.\nfrom rgsync import RGWriteBehind from rgsync.Connectors import MySqlConnector, MySqlConnection \u0026#39;\u0026#39;\u0026#39; Create MySQL connection object \u0026#39;\u0026#39;\u0026#39; connection = MySqlConnection(\u0026#39;demouser\u0026#39;, \u0026#39;Password123!\u0026#39;, \u0026#39;localhost:3306/test\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Create MySQL persons connector persons - MySQL table to put the data person_id - primary key \u0026#39;\u0026#39;\u0026#39; personConnector = MySqlConnector(connection, \u0026#39;persons\u0026#39;, \u0026#39;person_id\u0026#39;) personsMappings = { \u0026#39;first_name\u0026#39;:\u0026#39;first\u0026#39;, \u0026#39;last_name\u0026#39;:\u0026#39;last\u0026#39;, \u0026#39;age\u0026#39;:\u0026#39;age\u0026#39; } RGWriteBehind(GB, keysPrefix=\u0026#39;person\u0026#39;, mappings=personsMappings, connector=personConnector, name=\u0026#39;PersonsWriteBehind\u0026#39;, version=\u0026#39;99.99.99\u0026#39;) \u0026#39;\u0026#39;\u0026#39; Create MySQL car connector cars - MySQL table to put the data car_id - primary key \u0026#39;\u0026#39;\u0026#39; carConnector = MySqlConnector(connection, \u0026#39;cars\u0026#39;, \u0026#39;car_id\u0026#39;) carsMappings = { \u0026#39;id\u0026#39;:\u0026#39;id\u0026#39;, \u0026#39;color\u0026#39;:\u0026#39;color\u0026#39; } RGWriteBehind(GB, keysPrefix=\u0026#39;cars\u0026#39;, mappings=carsMappings, connector=carConnector, name=\u0026#39;CarsWriteBehind\u0026#39;, version=\u0026#39;99.99.99\u0026#39;) Go to the rgsync website to get the replication options and the configuration options for the database and mapping.\nCreate a Python file with the configuration mapping according to your specific needs.\nRun gears-cli with your custom file:\ngears-cli run --host \u0026lt;host\u0026gt; --port \u0026lt;post\u0026gt; --password \u0026lt;password\u0026gt; \u0026lt;yourfile\u0026gt;.py Write-behind caching with RedisGears RGSync is a RedisGears recipe that uses the write-behind pattern to synchronize Redis data structures to a backing data store (Oracle, MySQL, Cassandra, and Snowflake are currently supported).\nHere\u0026rsquo;s how you can create a write-behind function using the RGSync recipe against MySQL. This function will synchronize a set of fields in a Redis hash to the columns in a MySQL row. The key name for the Redis hash must have the form {name}:{id}, where id is the primary key for the row we want to keep in sync with. So, for example, suppose our hashes look like this:\nredis.cloud:6379\u0026gt; HSET person:5 first_name Marek last_name Michalski age 27 (integer) 2 The data in this hash would correspond to a row with ID of 5. But we need to define the mapping from our hash field to our columns. In our function, we do this using a Python dictionary:\npersonsMappings = { \u0026#39;first_name\u0026#39;:\u0026#39;first\u0026#39;, \u0026#39;last_name\u0026#39;:\u0026#39;last\u0026#39;, \u0026#39;age\u0026#39;:\u0026#39;age\u0026#39; } This is saying that the last_name field in our hash should be synchronized with the \u0026rsquo;last\u0026rsquo; field in our row. So, now we\u0026rsquo;re ready to write our RedisGears function.\nFirst, load the necessary libraries and connect to MySQL:\nfrom rgsync import RGWriteBehind from rgsync.Connectors import MySqlConnector, MySqlConnection connection = MySqlConnection(\u0026#39;myapplication\u0026#39;, \u0026#39;Password123!\u0026#39;, \u0026#39;localhost:3306/test\u0026#39;) Next, create a connector object, specifying the table name and the name of the column storing the primary key:\npersonConnector = MySqlConnector(mySqlConnection, \u0026#39;persons\u0026#39;, \u0026#39;id\u0026#39;) After that, define your field-column mappings:\npersonsMappings = { \u0026#39;first_name\u0026#39;:\u0026#39;first\u0026#39;, \u0026#39;last_name\u0026#39;:\u0026#39;last\u0026#39;, \u0026#39;age\u0026#39;:\u0026#39;age\u0026#39; } Finally, use these initialized objects to create an RGWRiteBehind object:\nRGWriteBehind(GB, name=\u0026#39;PersonsWriteBehind\u0026#39;, version=\u0026#39;1\u0026#39;, connector=personsConnector, keysPrefix=\u0026#39;person\u0026#39;, mappings=personsMappings) Notice here that you can version this function registration with the version argument, as newer versions with the same name will replace the old one.\nIf you\u0026rsquo;ve placed all of this code into a file (example.py), then you can load it into your Redis deployment using the gears-cli tool:\ngears-cli --host \u0026lt;host\u0026gt; --port \u0026lt;post\u0026gt; --password \u0026lt;password\u0026gt; example.py REQUIREMENTS rgsync Secret management You may not want to store database credentials in your RedisGears functions. To avoid this, you can pass these credentials as module parameters and then reference them from your functions.\nThe code below shows how to reference these credentials when creating a backing database connection.\ndef User(): return configGet(\u0026#39;MySqlUser\u0026#39;) def Password(): return configGet(\u0026#39;MySqlPassword\u0026#39;) def DB(): return configGet(\u0026#39;MySqlDB\u0026#39;) connection = MySqlConnection(User, Password, DB) Notice that for each credential, we define a Python function that returns the specified module parameter. We then provide each function reference when we instantiate MySqlConnection.\nThis code references three parameters: MySqlUser, MySqlPassword, and MySqlDB. In Redis Enterprise, you can set these parameters using the rladmin tool. The command to set these parameters takes the following form:\nrladmin\u0026gt; tune db [DB-NAME] module_name rg module_config_params \u0026#34;[PARAM-NAME] [PARAM-VALUE]\u0026#34; To set the MySqlPassword parameter to \u0026ldquo;Password123!\u0026rdquo; on a database named \u0026ldquo;user-api\u0026rdquo;, you would run the this rladmin command:\nrladmin\u0026gt; tune db user-api module_name rg module_config_params \u0026#34;MySqlPassword Password123!\u0026#34; Once a connection is successfully established, RedisGears will not attempt to reconnect until a disconnect occurs. This means that updates to the parameters that store the secrets will not take effect immediately, but they will be used for all subsequent connection attempts.\n","categories":["Modules"]},{"uri":"/modules/redisgears/python/","uriRel":"/modules/redisgears/python/","title":"RedisGears Python plugin","tags":[],"keywords":[],"description":"The RedisGears Python plugin allows you to run RedisGears functions with Python.","content":"With the RedisGears Python plugin, you can write RedisGears functions in Python and run them on a Redis Enterprise cluster.\nThe Python plugin allows both batch processing and event-driven processing.\nBefore you can run RedisGears with Python, you will need to install the RedisGears module and the Python plugin on your Redis Enterprise cluster and enable them for your database.\nOnce you have written your code, upload it to a node on your Redis Enterprise cluster. Use the RG.PYEXECUTE command with the redis-cli command-line tool to run your code.\nMore info RedisGears Python quick start RedisGears Python operations RedisGears recipes ","categories":["Modules"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-8-20/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-8-20/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.8-20 (December 2020)","tags":[],"keywords":[],"description":"Support for RS 6.0.8, consumer namespaces, Gesher admission controller proxy, and custom resource validation via schema.","content":"The Redis Enterprise K8s 6.0.8-20 release is a major release on top of 6.0.8-1 providing support for the Redis Enterprise Software release 6.0.8-30 and includes several enhancements and bug fixes.\nOverview This release of the operator provides:\nNew features Various bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages This release includes the following container images:\nRedis Enterprise: redislabs/redis:6.0.8-30 or redislabs/redis:6.0.8-30.rhel7-openshift Operator and Bootstrapper: redislabs/operator:6.0.8-20 Services Rigger: redislabs/k8s-controller:6.0.8-20 or redislabs/services-manager:6.0.8-20 (Red Hat registry) New features You can now create database custom resources (REDB CRs) in consumer namespaces that are separate from the operator and cluster namespace. You configured the operator deployment to watch specific namespaces for these REDB CRs. The Gesher admission control proxy is now certified by Red Hat. REDB CRs no longer require a Redis Enterprise cluster name. The name will default to the cluster in the context of the operator. REC and REDB CRs are now validated via a schema. Important fixes The database controller (REDB) no longer generates the following error message: “failed to update database status\u0026quot; Issues with configuring replica-of through the database controller (REDB) and TLS have been fixed. (RED48285) A timeout issue with rlutil upgrade was fixed. (RED48700) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod\u0026rsquo;s status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually. The recovery process will then continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration because the host part of the domain name will exceed 63 characters. The workaround is to limit cluster name to 20 characters or fewer.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state, the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe does not mark a node as \u0026ldquo;not ready\u0026rdquo; when running rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 does not support DockerHub private registries. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nMaster pod label in Rancher (RED42896) The master pod is not always labeled in Rancher.\nREC clusters fail to start on Kubernetes clusters with unsynchronized clocks (RED47254) When REC clusters are deployed on Kubernetes clusters with unsynchronized clocks, the REC cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\n","categories":["Platforms"]},{"uri":"/rs/release-notes/rs-6-4-2/","uriRel":"/rs/release-notes/rs-6-4-2/","title":"Redis Enterprise Software release notes 6.4.2 (February 2023)","tags":[],"keywords":[],"description":"Pub/sub ACLs &amp; default permissions. Validate client certificates by subject attributes.","content":"​​Redis Enterprise Software version 6.4.2 is now available!\nThe following table shows the MD5 checksums for the available packages:\nPackage MD5 checksum (6.4.2 February release) Ubuntu 16 b0dbecaa974ca08245dda55d53b6fe9b Ubuntu 18 a5192e8b0734db80d6b7c2b98a170c58 RedHat Enterprise Linux (RHEL) 7Oracle Enterprise Linux (OL) 7 c1537855dcfe7a7cedf9031ce01e2b9b RedHat Enterprise Linux (RHEL) 8Oracle Enterprise Linux (OL) 8 Rocky Enterprise Linux a24dc749d6dcb5df2162d7a41791c7aa This version offers:\nExtended validation of client certificates via mTLS (mutual TLS) full subject support\nSupport for default restrictive permissions when using publish/subscribe commands and ACLs (access control lists)\nEnhanced TLS performance when Redis returns large arrays in responses\nCompatibility with open source Redis 6.2.6\nAdditional enhancements and bug fixes\nFeatures and enhancements Validate client certificates by subject attributes You can now validate client certificates by their Subject attributes. When a client attempts to connect to a database, Redis Enterprise Software compares the values of the client certificate subject attributes to the subject values allowed by the database. Clients can connect to the database only if the subject values match. This gives more flexibility in controlling which clients can access which databases.\nSee Enable TLS for more information.\nDefault pub/sub ACL permissions Redis is continuously enhancing its ACL (access control list) functionality and coverage. Redis version 6.2 enhances ACLs to allow and disallow pub/sub channels.\nPart of protecting pub/sub channels requires changing the default access from permissive to restrictive, which blocks all pub/sub channels unless specifically permitted by an ACL rule. To allow this transition across all databases in the cluster, Redis Enterprise Software 6.4.2 provides a new configuration option acl-pubsub-default that sets the cluster-wide default for all channels to either permitted or restricted.\nThe 6.4.2 installation-provided value of acl-pubsub-default is permissive (allchannels) to comply with earlier Redis versions. After you upgrade all databases in the cluster to Redis DB version 6.2 (or later in future versions), you can use rladmin or the REST API to change the value to restrictive (resetchannels).\nTo allow certain users to access specific pub/sub channels, define the appropriate ACL. Redis Enterprise Software 6.4.2 enhances the admin console (UI), CLI, and REST API to support pub/sub channel ACL definitions.\nIf you use ACLs and pub/sub channels, we recommend you review your databases and ACL settings and plan to change your cluster to restricted mode. This will help you prepare for future Redis Enterprise Software releases that use restrictive resetchannels as the new default for acl-pubsub-default.\nVersion changes Breaking changes REST API: the authorized_names field of the BDB object is deprecated. Use the new authorized_subjects field instead. New default Redis DB version Both Redis Enterprise Software versions 6.2.x and 6.4.x package two Redis DB versions: Redis DB 6.0 and Redis DB 6.2. Until now, the default Redis DB version for creating new databases and upgrading existing databases was 6.0 (enforced by the redis_upgrade_policy parameter).\nTo allow customers more flexibility in future upgrades, starting with Redis Enterprise Software 6.4.2, the default Redis DB version for new and upgraded databases is now 6.2 for all upgrade policies (redis_upgrade_policy=major and redis_upgrade_policy=latest).\nRedis\nEnterprise Bundled Redis\nDB versions Default DB version\n(upgraded/new databases) 6.2.x 6.0, 6.2 6.0 6.4.2 6.0, 6.2 6.2 You can override the default version with rladmin; however, we recommend that you don\u0026rsquo;t change this setting.\nKnown operating system limitations RHEL 8 Due to module binary differences between RHEL 7 and RHEL 8, you cannot upgrade RHEL 7 clusters to RHEL 8 when they host databases using modules. Instead, you need to create a new cluster on RHEL 8 and then migrate existing data from your RHEL 7 cluster. This does not apply to clusters that do not use modules.\nDeprecations Ubuntu 16.04 Ubuntu 16 support is considered deprecated and will be removed in a future release. Ubuntu 16.04 LTS (Xenial) has reached the end of its free initial five-year security maintenance period as of April 30, 2021.\nActive-Active database persistence The snapshot option for Active-Active database persistence is deprecated. We advise customers running Active-Active databases, configured with snapshot data persistence, to reconfigure their data persistence mode to use the AOF (Append Only File) option with the following command:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB_GUID\u0026gt; \\ --default-db-config \u0026#39;{\u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;aof_policy\u0026#34;:\u0026#34;appendfsync-every-sec\u0026#34;}\u0026#39; TLS 1.0 and TLS 1.1 TLS 1.0 and TLS 1.1 connections are considered deprecated in favor of TLS 1.2 or later. Please verify that all clients, apps, and connections support TLS 1.2. Support for the earlier protocols will be removed in a future release. Certain operating systems, such as RHEL 8, have already removed support for the earlier protocols. Redis Enterprise Software cannot support connection protocols that are not supported by the underlying operating system.\n3DES encryption cipher The 3DES encryption cipher is considered deprecated in favor of stronger ciphers like AES. Please verify that all clients, apps, and connections support the AES cipher. Support for 3DES will be removed in a future release. Certain operating systems, such as RHEL 8, have already removed support for 3DES. Redis Enterprise Software cannot support cipher suites that are not supported by the underlying operating system.\nRedis modules Redis Enterprise Software v6.4.2 includes the following Redis modules:\nRediSearch v2.4.16\nRedisJSON v2.2.0\nRedisBloom v2.2.18\nRedisGraph v2.8.20\nRedisTimeSeries v1.6.17\nSee Upgrade modules to learn how to upgrade a module for a database.\nAdditional enhancements Installations, upgrades, and troubleshooting Added the ability for install.sh to run even if “upgrade mode” is already enabled to allow reruns in case of a previous run failure (RS77319)\nAdded log messages to the redis_mgr process (RS77891), the job_scheduler process (RS82673), and the install.sh script (RS82673)\nImproved rladmin error messages for certificate validation (RS79933)\nAdded internode encryption ports to command-line utility rlcheck validation (RS68965)\nAdded an alert to notify when a node operation (such as maintenance mode) failed, aborted, or was canceled. The alert is enabled by default (RS76089)\nResolved issues RS72866 - Improved performance for client connections which use TLS\nRS78241 - Fixed shard placement to always respect rack-zone restrictions and avoid a state where a primary (master) and replica are on the same rack, even if temporarily\nRS78144 - Removed the dependency on system-wide ldconfig so non-interactive processes will use their own dynamic libraries without impacting external services\nRS78028 - Fixed race condition during rolling upgrade that might result in shards repeatedly restarting\nRS77964 - Fixed module deletion to remove the old directory with the module\nRS75259 - Fixed node to prevent using plain text communication instead of TLS after losing connectivity\nRS69616 - Fixed validation for internode communication ports\nRS83535 - Fixed sentinel_service to start on RHEL 8 with DISA STIG profile\nRS87191 - Fixed a cross slot error when using Redis on Flash with Replica Of, in case a key on the source database swapped from RAM to flash and expired while it was also part of Lua script execution\nSecurity Open source Redis security fixes compatibility As part of Redis\u0026rsquo;s commitment to security, Redis Enterprise Software implements the latest security fixes available with open source Redis. The following open source Redis CVEs do not affect Redis Enterprise:\nCVE-2021-32625 — Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 — Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 — Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 — Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 — Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nRedis Enterprise has already included the fixes for the relevant CVEs. Some CVEs announced for open source Redis do not affect Redis Enterprise due to different and additional functionality available in Redis Enterprise that is not available in open source Redis.\n","categories":["RS"]},{"uri":"/rs/release-notes/rs-6-2-18/","uriRel":"/rs/release-notes/rs-6-2-18/","title":"Redis Enterprise Software release notes 6.2.18 (September 2022)","tags":[],"keywords":[],"description":"Database auditing.  Private key encryption.  Active-Active database support for MEMORY USAGE command.  Improvements to crdb-cli","content":"Redis Enterprise Software version 6.2.18 is now available!\nThis version of Redis Enterprise Software offers:\nRedisJSON on Active-Active General Availability Database connection auditing Private key encryption Active-Active support for memory usage command crdb-cli improvements Compatibility with open source Redis v6.2.6 Additional enhancements and fixes The following table shows the MD5 checksums for the available packages:\nPackage MD5 Checksum (6.2.18-70 January release) Ubuntu 16 69d2d2c71232adb15cebf29308ac54da Ubuntu 18 22e0637107a32ccb96a704abe9650adf RedHat Enterprise Linux (RHEL) 7Oracle Enterprise Linux (OL) 7 e14fcf6973418602f2b64a55a0bc8374 RedHat Enterprise Linux (RHEL) 8Oracle Enterprise Linux (OL) 8 Rocky Enterprise Linux f61d0d8f0bb5ad90a470482d6575eb27 Features and enhancements General Availability of Active-Active databases with RedisJSON Active-Active databases now support Index, query, and full-text search of nested JSON documents when combining RedisJSON with RediSearch.\nDatabase connection auditing You can now audit database connection and authentication events to track and troubleshoot activity. Administrators can now use third-party systems to track and analyze connection events in realtime.\nPrivate key encryption When enabled, private key encryption encrypts private keys stored in the cluster configuration store (CCS). Private keys are encrypted using a secure, self-contained internal process. Databases must be at least version 6.2.2 or later to use this feature.\nActive-Active support for MEMORY USAGE command Redis Enterprise Active-Active databases now support the MEMORY USAGE command, which simplifies troubleshooting and lets applications detect anomalous behavior and dangerous trends. To learn more, see Active-Active with RedisJSON.\nMEMORY USAGE reports the RAM memory use, in bytes, of a key and its value. The result includes the memory allocated for the data value and the administrative overhead associated with the key.\ncrdb-cli improvements The crdb-cli utility used to manage Active-Active databases now allows greater visibility into management operations to make it easier to investigate and troubleshoot problems. You can now use:\ncrdb-cli task list to retrieve details about current and previous tasks for all Active-Active databases on a cluster\ncrdb-cli task --task-id \u0026lt;task-id\u0026gt; to get detailed information about a specific task.\nTo learn more, see crdb-cli\nVersion changes Breaking changes RS84006 - When using the REST API to create a database, this specific set of conditions can lead to an error:\nsharding is enabled oss_sharding and implicit_shard_key are both deactivated A shard_key_regex is not defined If this occurs, the database endpoint returns (error) ERR key \u0026quot;test\u0026quot; does not match any rule.\nTo address this issue, do one of the following:\nExplicitly define shard_key_regex Enable oss_sharding or implicit_shard_key (enabling both also works) Prerequisites and notes You can upgrade to v6.2.18 from Redis Enterprise Software v6.0 and later.\nRefer to v6.2.4 release notes for important notes regarding changes made to the upgrade policy and how those changes might impact your experience.\nUpgrades from versions earlier than v6.0 are not supported.\nIf you plan to upgrade your cluster to RHEL 8, see the v6.2.8 release notes for known limitations.\nDeprecation notice ####Active-Active database persistence The snapshot option for data persistence on Active-Active databases will be deprecated in a future version of Redis Enterprise Software. If you have an Active-Active database using snapshot persistence, we strongly encourage you to switch to AOF persistence. Use crdb-cli to do so:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB_GUID\u0026gt; --default-db-config \u0026#39;{\u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;aof_policy\u0026#34;:\u0026#34;appendfsync-every-sec\u0026#34;}\u0026#39; TLS 1.0 and TLS 1.1 TLS 1.0 and TLS 1.1 connections are considered deprecated in favor of TLS 1.2 or later.\nPlease verify that all clients, apps, and connections support TLS 1.2. Support for the earlier protocols will be removed in a future release.\nCertain operating systems, such as RHEL 8, have already removed support for the earlier protocols. Redis Enterprise Software cannot support connection protocols that are not supported by the underlying operating system.\n3DES Encryption Cipher The 3DES encryption cipher is considered deprecated in favor of stronger ciphers like AES.\nPlease verify that all clients, apps, and connections support the AES cipher. Support for 3DES will be removed in a future release.\nCertain operating systems, such as RHEL 8, have already removed support for 3DES. Redis Enterprise Software cannot support cipher suites that are not supported by the underlying operating system.\nProduct lifecycle updates Redis Enterprise Software v6.0.x reached end-of-life (EOL) on May 31, 2022.\nRedis Enterprise Software 6.2.x EOL has been extended by six (6) months. The new EOL date is August 31, 2023.\nTo learn more, see the Redis Enterprise Software product lifecycle, which details the release number and the end-of-life schedule for Redis Enterprise Software.\nFor Redis modules information and lifecycle, see Module lifecycle.\nRedis modules Redis Enterprise Software v6.2.18-49 (October release) includes the following Redis modules:\nRediSearch v2.4.11 RedisJSON v2.2.0 RedisBloom v2.2.18 RedisGraph v2.8.17 RedisTimeSeries v1.6.17 Redis Enterprise Software v6.2.18-58 (November release) includes newer versions of the following Redis modules:\nRediSearch v2.4.14 RedisGraph v2.8.19 Redis Enterprise Software v6.2.18-65 (December release) includes newer versions of the following Redis modules:\nRediSearch v2.4.16 RedisGraph v2.8.20 For help upgrading a module, see Add a module to a cluster.\nAdditional enhancements Added support for the MODULE LIST command on Active-Active databases\nEnhanced validity checks of the input parameters of the CRDB-CLI tool\nThe Syncer lag calculation has been improved and fixes multiple miscalculations\nSupport package includes additional information for Active-Active databases\nAdded the ability to retrieve the syncer mode (centralized / distributed) by running\nrladmin info db [{db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt;}]\nTo learn more, see Distributed sychronization\nEnhancements added in 6.2.18-49 (October release) The /nodes/status REST API endpoint indicates whether a node is the primary. (RS82566) Added metrics to track certificates expiration time (RS56040) Enhancements added in 6.2.18-58 (November release) Added support to the UI to work with custom REST API port (RS84428) Added info level print to redis_mgr for troubleshooting (RS85385) Enhancements added in 6.2.18-65 (December release) When setting up an alert email server in the UI: added a “To” field to the “test email server” function (RS86119) Added a new flag to skip cluster resource validation when performing import (RS88086) Enhanced error message information for connection handling Added logging details to “insufficient_resources” planner error Enhancements added in 6.2.18-70 (January release) Added a new script to generate new self-signed certificates for renewal. For details, see Certificates Added rpm -q command output to the logs (RS82673) Added logs for server operations: start, stop, restart (RS87775) Added alert_mgr to the services you can deactivate with rladmin (RS89572) Resolved issues RS64002 - Fixes email alerts for LDAP mappings. RS79519 - fixes a bug that prevented Administrators from editing an Active-Active database using the API with username instead of email as their identifier. RS78039 - fixes a bug that caused the sync between two shards to pause when resetting a shard\u0026rsquo;s data which can be when performing full sync or importing an RDB. RS73454 - Updates internal timeouts to enable faster resharding. RS75783 - Fixes failover due to false identification of dead nodes when master node goes down. RS75206, RS52686 - Fixes backup_interval_offset in case where the user chose an offset that is higher than backup_interval; Fixes the UI from resetting backup_interval_offset after manual DB configuration. RS75176 - Fixes rare case of stuck state machine during “maintenance off”. RS57200 - Add an IP address to \u0026ldquo;failed_authentication_attempt\u0026rdquo; errors. RS56615 - Changed rladmin tune db db_name max_aof_load_time to receive the value in seconds; Added max_aof_load_time option to rladmin help tune. RS54745 - Fixes the Rest API to reject BDB creation using negative integers as a uid. RS46092 - Fixes rlcheck failure when somaxconn policy is a value other than 1024. RS68965, RS80615 - Adds internode encryption ports 3340-3344 to rlcheck connectivity. RS63302 - Adds umask validation for root user when installing. RS46947 - Fixes removal of old installations in install.sh. Resolved issues in 6.2.18-58 (November release) RS85369 - Fixes umask validation during installation to allow for temporary umask change RS77339 - Fixes the “FROM” email addresses of email alerts RS86005 - Fixes \u0026lsquo;bdbs/int:uid/actions/recover\u0026rsquo; API to return the bdb status ans not the recovery_plan Resolved issues in 6.2.18-65 (December release) RS87191 - Fixes syncer stop due to cross slot violation error in Redis on Flash Resolved issues in 6.2.18-70 (January release) RS61062 - Remove leading colon added to PATH variable Known limitations RS81463 A shard might crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nRS54131 Returning +OK reply from the QUIT command on a TLS-enabled database\nSecurity Open Source Redis Security fixes compatibility As part of Redis\u0026rsquo;s commitment to security, Redis Enterprise Software implements the latest security fixes available with open source Redis. The following Open Source Redis CVEs do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16) security fixes for recent CVEs. Redis Enterprise has already included the fixes for the relevant CVEs. Some CVEs announced for Open Source Redis do not affect Redis Enterprise due to different and additional functionality available in Redis Enterprise that is not available in Open Source Redis.\n","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-8-1/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-8-1/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.8-1 (October 2020)","tags":[],"keywords":[],"description":"Support for RS 6.0.8-28, OpenShift 4.5, K8s 1.18, and Gesher admission controller proxy.","content":"The Redis Enterprise K8s 6.0.8-1 release is a major release on top of 6.0.6-24 providing support for the latest Redis Enterprise Software release 6.0.8-28 and includes several enhancements (including OpenShift 4.5 and Kubernetes 1.18 support) and bug fixes.\nOverview This release of the operator provides:\nNew features New support K8s distributions and platforms Various bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages Redis Enterprise - redislabs/redis:6.0.8-28 or redislabs/redis:6.0.8-28.rhel7-openshift Operator - redislabs/operator:6.0.8-1 Services Rigger - redislabs/k8s-controller:6.0.8-1 or redislabs/services-manager:6.0.8-1 (Red Hat registry) New features Redis Modules can now be configured in the database custom resource. Support was added for OpenShift 4.5 Support was added for Kubernetes 1.18 Added support for the Gesher admission control proxy to provide an administrator the ability to setup delegation to avoid the need for administrator intervention on every namespaced deployed operator. Important fixes Added the missing Services Rigger health check (RED47062) Fixed failures when updating the ui service type (RED45771) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually and recovery process will continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration as the host part of the domain name exceeds 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe doesn\u0026rsquo;t mark a node as not ready when rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 doesn\u0026rsquo;t support dockerhub private registry. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nMaster pod label in Rancher (RED42896) Master pod is not always labeled in Rancher.\nCluster fail to start for clusters with unsynchronized clocks (RED47254) When REC clusters are deployed on clusters with unsynchronized clocks, the cluster does not start correctly. The fix is to use NTP to synchronize the underlying K8s nodes.\nErrors in operator log for REDB status (RED44919) Benign errors are reported in the operator log when using database controller (REDB) (e.g., “failed to update database status\u0026quot;. These errors can be ignored.\nCompatibility Notes Support for OpenShift 4.5 was added, Support for Kubernetes 1.18 was added, Support for the previous deprecated Kubernetes 1.11 and 1.12 has been removed. ","categories":["Platforms"]},{"uri":"/rs/release-notes/rs-6-2-12/","uriRel":"/rs/release-notes/rs-6-2-12/","title":"Redis Enterprise Software release notes 6.2.12 (August 2022)","tags":[],"keywords":[],"description":"OCSP Support.  Password &amp; session configuration changes.  RHEL 8.6 support.","content":"Redis Enterprise Software version 6.2.12 is now available!\nThis version of Redis Enterprise Software offers:\nOCSP stapling of the server proxy certificate Password and session configuration settings via the admin console Compatibility with open source Redis v6.2.6 Support for Red Hat Enterprise Linux (RHEL) v8.6 Additional enhancements and fixes The following table shows the MD5 checksums for the available packages.\nPackage MD5 Checksum Ubuntu 16 e702c906f200940e06ef031e6b8006d9 Ubuntu 18 7ea70067e8828b59336380df087fe03d RedHat Enterprise Linux (RHEL) 7Oracle Enterprise Linux (OL) 7 8ffda6186f70354b9d10c1ce43938c3c RedHat Enterprise Linux (RHEL) 8Oracle Enterprise Linux (OL) 8 334fe7979a7376b28fcf48913403bfb7 Features and enhancements Server side OCSP Stapling\nOnline Certificate Status Protocol (OCSP) helps verify the status of certificates managed by a third-party certificate authority (CA). It tells you whether certificates are valid, revoked, or unknown.\nRedis Enterprise Software v6.2.12 implements OCSP stapling, which allows clients to validate the status of a server proxy certificate. When OCSP is enabled, the Redis Enterprise server regularly polls the CA OCSP responder to determine a certificate\u0026rsquo;s status. The response is cached until the next polling attempt; the cached value is served to clients during the TLS handshake.\nTo learn more, see Enable OCSP stapling.\nSession and security attributes in the admin console\nYou can now use the admin console to configure password complexity rules, user login lockout, and session timeout.\nMount point import enhancement\nWhen importing data, Redis Enterprise copies files to a temporary directory on the node. For mount point import sources only, Redis Enterprise now reads files directly from the mount point. Because this import method does not copy files to a temporary directory, nodes do not require extra disk space. This new behavior is enabled by default and does not require configuration.\nVersion changes Prerequisites and notes You can upgrade to v6.2.12 from Redis Enterprise Software v6.0 and later.\nRefer to v6.2.4 release notes for important notes regarding changes made to the upgrade policy and how those changes might impct your experience.\nUpgrades from versions earlier than v6.0 are not supported.\nIf you are using the earlier cluster-based LDAP mechanism, you must migrate to the role-based mechanism before upgrading to v6.2.12. For details, see Migrate to role-based LDAP.\nIf you plan to upgrade your cluster to RHEL 8, see the v6.2.8 release notes for known limitations.\nFuture deprecation notice TLS 1.0 and TLS 1.1 TLS 1.0 and TLS 1.1 connections are considered deprecated in favor of TLS 1.2 or later.\nPlease verify that all clients, apps, and connections support TLS 1.2. Support for the earlier protocols will be removed in a future release.\nCertain operating systems, such as RHEL 8, have already removed support for the earlier protocols. Redis Enterprise Software cannot support connection protocols that are not supported by the underlying operating system.\nProduct lifecycle updates Redis Enterprise Software v6.0.x will reach end of life (EOL) on May 31, 2022.\nTo learn more, see the Redis Enterprise Software product lifecycle, which details the release number and the end-of-life schedule for Redis Enterprise Software.\nFor Redis modules information and lifecycle, see Module lifecycle.\nRedis modules Redis Enterprise Software v6.2.12 includes the following Redis modules:\nRediSearch v2.4.9 RedisJSON v2.0.11 RedisBloom v2.2.17 RedisGraph v2.8.15 RedisTimeSeries v1.6.16 For help upgrading a module, see Add a module to a cluster.\nInterface enhancements Allow creation and editing of sharded databases with a single shard. This lets you prepare for future scaling by ensuring your apps work in clustering mode, regardless of the actual number of configured shards Enhanced the slowlog in the UI to display unprintable characters in RESTORE commands Added support for custom paths when bootstrapping the cluster via the UI Improve readability of geo-distributed log entries in the UI Additional enhancements Added support for the MODULE LIST command on Active-Active databases Enhanced validity checks of the input parameters of the CRDB-CLI tool Resolved issues RS38320 A failed task leaves the nodes list outdated in the UI RS73768, RS72082 Increased certificate rotation timeout to allow it to finish RS72466, RS68668 Fix false positive alerts for certification expiration RS69256 Change pre-bootstrap default TLS version to 1.2 RS67133 Fixed replication for command RESTOREMODAUX RS66468 Fixed “Test regex keys” in the UI RS58156 Fixed the installation to abort and alert when encountering issues in NTP RS67935 Fixed releasing 30MB of memory when deleting an Active-Active database RS64276 Fixed high memory consumption of the DMC output buffers when running CLIENT LIST Known limitations RS81463 A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nRS54131 Returning +OK reply from the QUIT command on a TLS enabled database\nSecurity Open Source Redis Security fixes compatibility As part of Redis commitment to security, Redis Enterprise Software implements the latest security fixes available with open source Redis. The following Open Source Redis CVEs do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16) security fixes for recent CVEs. Redis Enterprise has already included the fixes for the relevant CVEs. Some CVEs announced for Open Source Redis do not affect Redis Enterprise due to different and additional functionality available in Redis Enterprise that is not available in Open Source Redis.\n","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-24/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-24/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.6-24 (August 2020)","tags":[],"keywords":[],"description":"Various bug fixes.","content":"The Redis Enterprise K8s 6.0.6-24 release is a maintenance release on top of 6.0.6-23 providing support for the latest Redis Enterprise Software release 6.0.6-39 and includes several bug fixes.\nOverview This release of the operator provides:\nVarious bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages Redis Enterprise - redislabs/redis:6.0.6-39 or redislabs/redis:6.0.6-39.rhel7-openshift Operator - redislabs/operator:6.0.6-24 Services Rigger - redislabs/k8s-controller:6.0.6-24 or redislabs/services-manager:6.0.6-24 (on the RedHat registry) Important fixes A fix for database observability where after 24 hours after creation or update, the controller was unable to observe the database (RED46149) A fix for a log collector crash on Windows when pods were not running (RED45477) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The workaround is to delete the crashing pods manually and recovery process will continue.\nLong cluster names cause routes to be rejected (RED25871) A cluster name longer than 20 characters will result in a rejected route configuration as the host part of the domain name exceeds 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) When a cluster is in an unreachable state the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) STS Readiness probe doesn\u0026rsquo;t mark a node as not ready when rladmin status on the node fails.\nRole missing on replica sets (RED39002) The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) Openshift 3.11 doesn\u0026rsquo;t support dockerhub private registry. This is a known OpenShift issue.\nInternal DNS and Kubernetes DNS may have conflicts (RED37462) DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for Kubernetes DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) Kubernetes-based 5.4.10 deployments seem to negatively impact existing 5.4.6 deployments that share a Kubernetes cluster.\nNode CPU usage is reported instead of pod CPU usage (RED36884) In Kubernetes, the node CPU usage we report on is the usage of the Kubernetes worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed with the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nUpdating UI service in Rancher (RED45771) Updating the UI service type may fail in Rancher. When this happens, delete the service manually and the operator will recreate it correctly.\nMaster pod label in Rancher (RED42896) Master pod is not always labeled in Rancher.\nDeprecation notice Support for K8s version 1.11 and 1.12 is deprecated (excludes Openshift 3.11, which continues to be supported). Openshift 4.1 and 4.2 are deprecated (already End Of Life by Red Hat).\n","categories":["Platforms"]},{"uri":"/rs/release-notes/rs-6-2-10-february-2022/","uriRel":"/rs/release-notes/rs-6-2-10-february-2022/","title":"Redis Enterprise Software Release Notes 6.2.10 (February 2022)","tags":[],"keywords":[],"description":"Python 3 support.  RHEL 8.5 support.","content":"Redis Enterprise Software version 6.2.10 is now available!\nThe following table shows the MD5 checksums for the available packages.\nPackage MD5 Checksum Ubuntu 16 531cea69a58fbc1125bc5f76ba01da7f Ubuntu 18 ec9ac6e0111dc85605d3b98e83f50150 RedHat Enterprise Linux (RHEL) 7Oracle Enterprise Linux (OL) 7 2f7572caab9600417ef8b4ee474d6768 RedHat Enterprise Linux (RHEL) 8Oracle Enterprise Linux (OL) 8 377a539ee050515e1e0640dec1e04129 K8s Ubuntu 099192416a70a12790535bdcd78a6e87 K8s RHEL f267abe81770ddf36f022232f4c2cb2e Features and enhancements Upgrade the Redis Enterprise infrastructure to Python v3.9.\nRed Hat Enterprise Linux (RHEL) v8.5 and Red Hat Enterprise Linux (RHEL) v8.6 is now a supported platform.\nOracle Linux v8 is now a supported platform.\nCompatibility with open source Redis 6.2.5.\nCompatibility with the security fixes of the latest open source Redis 6.2.6.\nEnhancements and bug fixes.\nVersion changes Prerequisites and notes You can upgrade to v6.2.10 from Redis Enterprise Software v6.0 and later.\nRefer to v6.2.4 release notes for important notes regarding changes made to the upgrade.\nUpgrades from versions earlier than v6.0 are not supported.\nIf you plan to upgrade your cluster to RHEL 8, refer to v6.2.8 release notes for known limitations.\nIf you are using Active-Active or Active-Passive (ReplicaOf) databases and experience synchronization issues as a result of the upgrade, see RS67434 details in Resolved issues for help resolving the problem.\nProduct lifecycle updates Redis Enterprise Software v6.0.x will reach end of life (EOF) on May 31, 2022.\nTo learn more, see the Redis Enterprise Software product lifecycle, which details the release number and the end-of-life schedule for Redis Enterprise Software.\nFor Redis modules information and lifecycle, see Module lifecycle.\nRedis modules Redis Enterprise Software v6.2.10 includes the following Redis modules:\nRediSearch v2.2.6 RedisJSON v2.0.6 RedisBloom v2.2.9 RedisGraph v2.4.12 RedisTimeSeries v1.4.13 Starting with Redis Enterprise Software v6.2.10 build 121, the included modules versions are:\nRediSearch v2.4.6 RedisJSON v2.0.8 RedisBloom v2.2.14 RedisGraph v2.8.12 RedisTimeSeries v1.6.9 For help upgrading a module, see Add a module to a cluster.\nInterface enhancements When choosing RedisJSON, the user interface (UI) now suggests RedisSearch as well. To learn more, see the RedisJSON preview announcement, which details the benefits of combining RedisJSON and RediSearch. Adds the ability to sort the columns of the node list (RS48256). When creating a new geo-distributed (Active-Active) database, an endpoint port is no longer required. The system assigns one if none if provided (RS27632). Additional enhancements Added an option to run a connectivity health check for the management layer of Active-Active databases. Run the following REST API command:\nGET https:/[host][:port]/v1/crdbs/\u0026lt;crdb_guid\u0026gt;/health_report Added TLS handshake error messages to the DMC proxy log (RS59346).\nResolved issues RS58219 - Fixes a UI error message that showed a path instead of a relevant error message.\nRS44958 - Fixes incorrect description for the graph \u0026ldquo;incoming traffic\u0026rdquo; in Active-Active (geo-distributed) database UI Metrics.\nRS66280 - Fixes the lexicographic SORT command on Active-Active databases (e.g. SORT mylist ALPHA). The SORT command should only run on keys mapped to the same slot.\nRS64575 - Fixes a bug in the replication between primary and replica shards of a destination Active-active database in the scenario of using Replica-Of from a single to an Active-Active database, where the syncer process went down during the full sync.\nRS65370 - Adds logic to remove old syncer entries in the cluster configuration during upgrades.\nRS67434 - Version 6.2.10 fixes the mTLS handshake between the syncer process and the proxy (DMC), where the proxy presented a leaf certificate without its full chain to the syncer. After upgrading to 6.2.10, syncer connections using invalid certificates will break the synchronization between Active-Active instances or deployments using Replica Of when TLS is enabled. To ensure certificates are valid before upgrading do the following:\nFor Active-Active databases, run the following command from one of the clusters:\ncrdb-cli crdb update --crdb-guid \u0026lt;CRDB-GUID\u0026gt; --force\nFor Active-Passive (Replica Of) databases: use the admin console to verify that the destination syncer has the correct certificate for the source proxy (DMC). For details, see Configure TLS for Replica Of.\nIssues resolved in build 96 RS67133 - An issue in Redis Enterprise Software affected replication in replica databases using RedisGraph, RediSearch, and RedisGears in specific scenarios. The problem appeared when importing an RDB file or while synchronizing target Active-Passive (ReplicaOf) databases.\nThis issue is fixed in Redis Enterprise Software v6.2.10-96 and RedisGraph v2.8.11. We recommend upgrading to these versions at your earliest opportunity. (Failure to upgrade can lead to data loss.)\nOnce the upgrades are complete, secondary shards might need to be restarted. You can use rlutil to restart secondary shards:\nrlutil redis_restart redis=\u0026lt;shard-id1\u0026gt;,\u0026lt;shard-id2\u0026gt;,... Issues resolved in build 100 RS74171 - A new command was added as part of Redis 6.2: XAUTOCLAIM. When used in an Active-Active configuration, this command may cause Redis shards to crash, potentially resulting in data loss. The issue is fixed in Redis Enterprise Software version 6.2.12. Additionally, we recommend enabling AOF persistence for all Active-Active configurations. Issues resolved in build 121 RS68668, RS72082 - Improvements for internode encryption certification rotation RS72304 - Avoid starting a master shard when both master and replica shards crash and the replica did not finish recovery RS74469 - Fix for some Redis Active-Active + Redis Streams scenarios that could lead to shard crash during backup; failure to backup Issues resolved in build 129 RS77003 - Add grace time to job scheduler to allow certificate rotation in case of failure due to scheduling conflicts. RS71112 - Update validation during db configuration to not fail due to ports associated with nodes that are no longer in the cluster. This was done to allow db configuration during adding and removing nodes as part of load balancing. RS78486 - Fix known issue from 6.2.10 build 100 - When using rladmin tune db to change the replica buffer size, the command appears to succeed, but the change does not take effect. Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nRS78364 - When using rladmin tune db to change the replica buffer size, the command appears to succeed, but the change does not take effect. This issue was introduced in build 100; it will be fixed in a future build of Redis Enterprise Software v6.2.10 and in the next release (v6.2.12).\nRS63258 - Redis Enterprise Software is not currently supported on RHEL 8 with FIPS enabled.\nFIPS changes system-generated keys, which can limit secure access to the cluster or the admin console via port 8443.\nRS63375 - RHEL 7 clusters cannot be directly upgraded to RHEL 8 when hosting databases using modules.\nDue to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules. Instead, you need to create a new cluster on RHEL 8 and then migrate existing data from your RHEL 7 cluster. This does not apply to clusters that do not use modules.\nAll known limitations listed in the v6.2.4 release notes have been addressed.\nSecurity Open Source Redis Security fixes compatibility As part of Redis commitment to security, Redis Enterprise Software implements the latest security fixes available with open source Redis. The following Open Source Redis CVE’s do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)security fixes for recent CVE’s. Redis Enterprise has already included the fixes for the relevant CVE’s. Some CVE’s announced for Open Source Redis do not affect Redis Enterprise due to different and additional functionality available in Redis Enterprise that is not available in Open Source Redis.\n","categories":["RS"]},{"uri":"/rs/release-notes/rs-6-2-8-october-2021/","uriRel":"/rs/release-notes/rs-6-2-8-october-2021/","title":"Redis Enterprise Software Release Notes 6.2.8 (October 2021)","tags":[],"keywords":[],"description":"RHEL 8 support. Set backup start time.","content":"Redis Enterprise Software version 6.2.8 is now available!\nFeatures and enhancements This version features:\nSupport for Red Hat Linux Edition (RHEL) 8 You can now set the start time for 12- and 24-hour backups Compatibility with version of open source Redis 6.2.3 (starting with Redis Enterprise Software v6.2.4) Compatibility with the security fixes of the latest open source Redis 6.2.6 Enhancements and bug fixes Version changes Prerequisites and notes You can upgrade to v6.2.8 from Redis Enterprise Software v6.0 and later.\nRefer to the v6.2.4 release notes for important notes regarding the upgrade process.\nWhen upgrading a cluster from Redis Enterprise 6.0.8 and earlier to 6.2.8 only, the DMC proxy might crash when proxy certificates contain additional text as comments. Redis removes these comments during upgrade, but a change to the v6.2.8 internal upgrade action sequence might cause this problem.\nIf you plan to upgrade from a pre-6.0.8 release to 6.2.8, check whether your proxy certificate includes additional comments and manually remove them. The change was reverted in 6.2.10.\nUpgrades from versions earlier than v6.0 are not supported.\nProduct lifecycle updates As of 31 October 2021, Redis Enterprise Software v5.6.0 is end of life (EOF).\nTo learn more, see the Redis Enterprise Software product lifecycle, which details the release number and the end-of-life schedule for Redis Enterprise Software.\nRedis Enterprise modules have individual release numbers and lifecycles.\nRedis modules Redis Enterprise Software v6.2.8 includes the following Redis modules:\nRediSearch v2.0.11 RedisJSON v1.0.8 RedisBloom v2.2.6 RedisGraph v2.4.7 RedisTimeSeries v1.4.10 To learn more, see Upgrade the module for a database.\nResolved issues User interface fixes RS58804 - Display an error message in case of a login attempt with an LDAP user RS56680 - Notify that SASLAUTHD should be disabled prior to enabling LDAP RS55844 - Use the correct password and mask it on LDAP password update RS60877 - Fixed reset of Active-Active database compression level, in cases where the compression level wasn’t set to default, when changing any other configuration via the DB configuration page RS43999 - Fixed UI database configuration to allow changes when SFTP SSH key is customized RS59861 - Fixed the UI to display an explanation error message when password complexity does not meet requirements RS57734 - Fixed inaccessible UI after cluster upgrade due to missing certificate RS43041 - Mask secret keys for backup destination for view and edit in the UI Additional fixes RS60068 / RS59146 - Fixed unresolved endpoint due to PDNS issues RS52812 - Expand API wrapper to return API 405 errors as JSON/XML RS57666 - Fixed false shard migration message when the shard fails to bind the port RS57444, RS55294, RS4903 - Fixed false “backup finished successfully” message when the backup failed due to restricted access to the backup destination Fixes on build #53 RS67829 - Fixed a bug that caused Modules\u0026rsquo; auxiliary field not to get replicated between the primary and the replica shards. Applicable for RediSearch, RedisGraph and RedisGears and happening only at following scenarios: - (A) On the destination databases of a Replica Of upon a full sync operation - (B) Upon import operation Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nRS63258 - Redis Enterprise Software 6.2.8 is not supported on RHEL 8 with FIPS enabled.\nFIPS changes system-generated keys, which can limit secure access to the cluster or the admin console via port 8443.\nRS63375 - RHEL 7 clusters cannot be directly upgraded to RHEL 8 when hosting databases using modules.\nDue to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules. Instead, you need to create a new cluster on RHEL 8 and then migrate existing data from your RHEL 7 cluster. This does not apply to clusters that do not use modules.\nAll known limitations from v6.2.4 have been fixed.\nKnown issues A new command was added as part of Redis 6.2: XAUTOCLAIM. When used in an Active-Active configuration, this command may cause Redis shards to crash, potentially resulting in data loss. The issue is fixed in Redis Enterprise Software version 6.2.12. Additionally, we recommend enabling AOF persistence for all Active-Active configurations.\nSecurity Open source Redis security fix compatibility As part of its commitment to security, Redis Enterprise Software implements the latest security fixes available with open source Redis.\nThe following open source Redis CVEs do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nSome CVEs announced for Open Source Redis do not affect Redis Enterprise due to functionality that is either different from (or not available in) open source Redis.\n","categories":["RS"]},{"uri":"/modules/redisgears/jvm/","uriRel":"/modules/redisgears/jvm/","title":"RedisGears JVM plugin","tags":[],"keywords":[],"description":"The RedisGears JVM plugin allows you to run RedisGears functions in the Java virtual machine.","content":"With the RedisGears JVM plugin, you can write RedisGears functions in Java and run them on a Redis Enterprise cluster. It currently supports JVM version 11.\nSimilar to the Python plugin, the JVM plugin allows both batch processing and event-driven processing.\nBefore you can run RedisGears with Java, you will need to install the RedisGears module and the JVM plugin on your Redis Enterprise cluster and enable them for your database.\nOnce you have written your code, compile and package it into a JAR file and upload it to a node on your Redis Enterprise cluster. Use the RG.JEXECUTE command with the redis-cli command-line tool to run your code.\nMore info RedisGears JVM quick start RedisGears Java classes and functions RedisGears recipes ","categories":["Modules"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-23/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-23/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.6-23 (August 2020)","tags":[],"keywords":[],"description":"Support for Redis Enterprise Software 6.0.6-39, Rancher support, new database backup and alert options.","content":"The Redis Enterprise K8s 6.0.6-23 release is a major release on top of 6.0.6-11 providing support for the latest Redis Enterprise Software release 6.0.6-39 and includes several enhancements (including Rancher support) and bug fixes.\nOverview This release of the operator provides:\nSupport for the Redis Enterprise Software release 6.0.6-39 Support for Rancher Backup options in the database custom resource and controller Alert option in the database custom resource and controller UBI images Various other enhancements and bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages Redis Enterprise - redislabs/redis:6.0.6-39 or redislabs/redis:6.0.6-39.rhel7-openshift Operator - redislabs/operator:6.0.6-23 Services Rigger - redislabs/k8s-controller:6.0.6-23 or redislabs/services-manager:6.0.6-23 (on the RedHat registry) New features Red Hat UBI base images (RED29651) - The services rigger and operator images are now based on Red Hat UBI base images. Also, the same images are now used in both OpenShift and non-OpenShift environments.\nRancher support (RED37918) - The operator is now supported on Rancher version (v2.4.5). Note that since this release of the operator doesn\u0026rsquo;t support K8s 1.18, it requires one of the other supported upstread version in this Rancher release, i.e. 1.15.12, 1.16.13 or 1.17.9.\nDatabase Replica Of support (RED40160) - Support for Replica Of was added to the DB controller.\nDatabase backup configuration (RED40165) - Support for backup configuration was added to the DB controller spec.\nAlert configuration (RED40166) - Support for alert configuration was added to the DB controller spec.\nDatabase TLS configuration (RED41758) - Support for TLS authentication configuration was added to the DB controller spec.\nOpenShift 4.4 support (RED41352) - The operator is now supported on Openshift 4.4.\nDB controller resources via the OLM (RED41755) - Support for configuration of DB controller resources was added to the OLM (preview channel).\nImportant fixes Openshift OLM upgrade support was fixed. Also, installing past versions was added through dedicated channels. (RED44130) Fixed log_collector failures when pods were not scheduled. (RED45347) Fixed wrong handling of the operator environment variables. Specifically, enabling/disabling the database controller now works correctly. (RED45351) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) - When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The solution is to delete the crashing pods manually and recovery process will continue.\nLong cluster names cause routes to be rejected (RED25871) - A cluster name longer than 20 characters will result in a rejected route configuration as the host part of the domain name exceeds 63 characters. The workaround is to limit cluster name to 20 characters or less.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) - A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) - When a cluster is in an unreachable state the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) - STS Readiness probe doesn\u0026rsquo;t mark a node as not ready when rladmin status on the node fails\nRole missing on replica sets (RED39002) - The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) - Openshift 3.11 doesn\u0026rsquo;t support dockerhub private registry. This is a known OpenShift issue.\nInternal DNS and K8s DNS may have conflicts (RED37462) - DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for K8s DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) - K8S-based 5.4.10 clusters seem to negatively impact existing 5.4.6\nNode CPU usage is reported instead of pod CPU usage (RED36884) - In Kubernetes, the node CPU usage we report on is the usage of the K8S worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) - In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\nUpdating UI service in Rancher (RED45771) - Updating the UI service type may fail in Rancher. When this happens, delete the service manually and the operator will recreate it correctly.\nMaster pod label in Rancher (RED42896) - Master pod is not always labeled in Rancher.\nDeprecation notice Support for Kubernetes version 1.11 and 1.12 is deprecated (excludes Openshift 3.11, which continues to be supported).\nCompatibility updates Added support for Rancher Added support for Openshift 4.4 ","categories":["Platforms"]},{"uri":"/rs/release-notes/rs-6-2-4-august-2021/","uriRel":"/rs/release-notes/rs-6-2-4-august-2021/","title":"Redis Enterprise Software Release Notes 6.2.4 (August 2021)","tags":[],"keywords":[],"description":"Internode encryption. Nginx replaced by envoy.  New upgrade policies/behavior.","content":"Redis Enterprise Software version 6.2.4 is now available!\nThis version offers:\nEncryption of all communications within cluster nodes Security enhancements Bug fixes Compatibility with the latest version of open source Redis 6.2.3 Version changes Prerequisites and notes You can upgrade to v6.2.4 from Redis Enterprise Software v6.0 and later.\nKeep the following in mind:\nUpgrades from versions earlier than v6.0 are not supported\nThe new internode encryption feature requires port 3342 to be open on all machines in the cluster.\nIn v6.0.20, Redis Enterprise Software replaced Nginx with envoy to improve internal security and communication. As of v6.2.4, Nginx is no longer provided with Redis Enterprise Software.\nDatabase upgrade default changes The default behavior of the upgrade db command has changed. It is now controlled by a new cluster policy (redis_upgrade_policy), which defines the policy for creating new databases and upgrading existing databases. The policy supports the following values:\nWhen set to major, the policy allows databases to be created or updated to versions of Redis compatible with open source Redis major releases. This allows for longer upgrade cycles by supporting Redis versions across multiple Redis Enterprise Software releases.\nThis is the default value for Redis Enterprise Software.\nWhen set to latest, the policy creates new databases and upgrades existing ones to be compatible with the latest (most recent) version of open source Redis, which was the default behavior of earlier versions of Redis Enterprise Software. This is no longer the default behavior.\nSetting the upgrade policy to latest ensures that the most recent Redis features are available to new databases and ones that are upgraded. It also requires more frequent upgrades, as open source Redis is updated more frequently than Redis Enterprise Software.\nThe Redis Enterprise Software 6.2.4 package includes compatibility with the most recent major Redis release (v6.0) and the latest (most recent) update to Redis (v6.2.3).\nBy default, compatibility with v6.0 will be installed. To change this, use rladmin to set the upgrade policy and the default Redis version:\ntune cluster redis_upgrade_policy latest tune cluster default_redis_version 6.2 To learn more, see the upgrade instructions.\nProduct lifecycle updates Redis Enterprise Software v5.6.0 will reach end of life (EOF) on October 31, 2021.\nTo learn more, see the Redis Enterprise Software product lifecycle, which details the release number and the end-of-life schedule for Redis Enterprise Software.\nRedis Enterprise modules have individual release numbers and lifecycles.\nDeprecation notices In v6.0.20, the SASL-based LDAP mechanism was deprecated in favor of a new RBAC-based approach. As of v6.2.12, support for the older mechanism has been removed.\nFor help migrating to the LDAP-based mechanism, see Migrate to role-based LDAP.\nOpenStack Object Storage (\u0026ldquo;Swift\u0026rdquo;) has reached end-of-life. Consequently, you can no longer use ObjectStack Swift as a target for database backup or export operations.\nFeatures and enhancements Internode encryption Internode encryption (INE) encrypts all communication between nodes in a cluster; it is available for the control plane and the data plane. Control plane internode encryption Control plane internode encryption encrypts all management communication within a cluster. It is enabled by default for all new clusters and upgraded clusters.\nData plane internode encryption Data plane internode encryption encrypts communication between nodes within a cluster, such as database replication between nodes.\nData plane internode encryption is available for new or fully upgraded clusters. It is not enabled by default.\nYou can enable data plane internode encryption by:\nSetting the cluster policy to enable data plane internode encryption by default for new databases\nrladmin tune cluster data_internode_encryption enabled Enabling it for individual existing databases\nrladmin tune db \u0026lt;db:id | name\u0026gt; data_internode_encryption enabled Internal certificate management Internode encryption relies on internal certificates signed by a unique, private CA certificate created for your deployment. The private CA generates and signs leaf certificates for internode encryption only. It\u0026rsquo;s generated when you install or upgrade to Redis Enterprise 6.2.4. It\u0026rsquo;s used only within the cluster and is not exposed outside of the cluster.\nThe leaf certificates expire regularly; they\u0026rsquo;re automatically rotated before expiration and alerts are issued as needed.\nOpen source Redis compatibility Redis 6.2 introduced new commands, feature improvements, and security fixes; it addresses many customer requests.\nRedis Enterprise Software supports all new commands, except RESET and FAILOVER. (Redis Enterprise takes a different approach to connectivity; it also separates control plane operations from data plane operations.)\nTo learn more, see Redis Enterprise Software compatibility with open source.\nRedis modules Redis Enterprise Software v6.2.4 includes the following Redis modules:\nRediSearch v2.0.11 RedisJSON v1.0.8 RedisBloom v2.2.6 RedisGraph v2.4.7 RedisTimeSeries v1.4.10 Internode encryption for modules To utilize data plane encryption for existing databases with modules, update the module to the latest version prior to enabling data plane encryption.\nFor help, see Upgrade the module for a database.\nModule-related enhancements Added the capability to update current module arguments for an existing database. In earlier versions, you could do this only when upgrading a module. To learn more, see rladmin upgrade.\nResolved issues RS39954 - Changed the UI status indication for the default user from Active/Inactive to Enabled/Disabled\nRS42626 - Increased the max length for modules commands from 23 characters to 64 characters\nRS54732 - Fixed incorrect reporting of number database connections, which caused the number of connections to be reported as a 20 digit number\nRS52265 - Fixed excessive log lines reporting when an Active-Active database is on featureset 0. We recommend upgrading the featureset version to the latest\nRS56122 - Fixed a bug that was causing AOF files to grow when the replicas of two Active-Active databases became disconnected during full synchronization\nRS58184 - Fixed a bug when trying to create an Active-Active database with expired syncer certificates; participating clusters were creating replicas even though the create operation failed.\nRS48988 - Add the username description in the log upon an unauthorized REST API request\nKnown issues A new command was added as part of Redis 6.2: XAUTOCLAIM. When used in an Active-Active configuration, this command may cause Redis shards to crash, potentially resulting in data loss. The issue is fixed in Redis Enterprise Software version 6.2.12. Additionally, we recommend enabling AOF persistence for all Active-Active configurations.\nRS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory. Security The following Open Source Redis CVE\u0026rsquo;s do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\n","categories":["RS"]},{"uri":"/rs/release-notes/rs-6-0-20-april-2021/","uriRel":"/rs/release-notes/rs-6-0-20-april-2021/","title":"Redis Enterprise Software Release Notes 6.0.20 (April 2021)","tags":[],"keywords":[],"description":"Role-based LDAP integration.  Enhanced client mutual authentication.  Active-Active improvements for eviction policies, migration, and the BITFIELD data type.","content":"Redis Enterprise Software version 6.0.20 is now available! This version includes the following new features and improvements:\nA new integration for LDAP authentication and authorization into RS role-based access controls (RBAC). You can now use LDAP to authorize access to the admin console and to authorize database access.\nAn enhanced clients mutual authentication mechanism, adding the ability to authenticate client connections using a Certificate Authority (CA).\nSupport of Redis eviction policies on Active-Active Redis databases.\nA new migration process for Active-Active Redis database using the Active-Passive (Replica Of) mechanism.\nSupport for the BITFIELD data type on Active-Active Redis databases.\nAnd other functional and stability improvements.\nVersion information Upgrade instructions Follow these instructions for upgrading to Redis Software 6.0.20 from Redis Software 5.6.0 and above.\nNote that upgrades from earlier Redis Software versions are not supported. For Active-Active deployments, this release requires that you upgrade the CRDB featureset version.\nUpgrades of Active-Active databases to Redis Software 6.0.20, will require all their instances to run with protocol version 1 and featureset version 1 or above. Active-Active databases running on protocol version 0 and/or featureset version 0 will block the upgrade.\nProduct lifecycle information End of Life (EOL) for Redis Enterprise Software 6.0 and earlier versions, can be found here.\nEOL for Redis modules can be found here.\nDeprecation Notice Upgrades to the next Redis Software will be enabled from version 6.0 and above.\nSupport for the SASL-based LDAP mechanism was deprecated in v6.0.20. As of v6.2.12, support has been removed and the feature is obsolete.\nStarting with Redis Software version 6.0.12, Envoy replaces Nginx for internal cluster administration. Support for Nginx is considered deprecated, it will be removed in a future version.\nNew Features New LDAP integration Redis Enterprise Software integrates Lightweight Directory Access Protocol (LDAP) authentication and authorization into its role-based access controls (RBAC). You can now use LDAP to authorize access to the admin console and to manage database access.\nClients Mutual TLS authentication using a Certificate Authority (CA) Redis Enterprise Software adds the ability to use a Certificate Authority (CA) for client authentications, allowing clients to rotate their certificates without the need to load new certificates to your database.\nRedis eviction policies on Active-Active Redis databases All Redis eviction policies are now supported on Active-Active Redis databases. You can create new Active-Active databases or edit existing ones using the UI console to enable it.\nNote that eviction is not supported yet for Active-Active Redis databases running with Redis on Flash (Rof). Migration to an Active-Active Redis database Redis Enterprise Software adds the ability to easily migrate your Redis database to an Active-Active Redis database using the Active-Passive (Replica Of) mechanism.\nBITFIELD on Active-Active Redis databases Redis Enterprise Software adds the ability to use the BITFIELD data type on Active-Active Redis databases. Please read more about developing for Active-Active with BITFIELD to understand the conflict resolution and limitations.\nRedis modules The following GA releases of Redis modules are bundled with Redis Software 6.0.20: (Please read the below updates for 6.0.20-97)\nRediSearch, version 2.0.6\nRedisJSON, version 1.0.7\nRedisGraph, version 2.2.14\nRedisTimeSeries, version 1.4.8\nRedisBloom, version 2.2.4\nTo use the updated modules with a database, you must upgrade the module on the database.\nAdditional capabilities Redis Software 6.0.20 includes open source Redis 6.0.9. For more information about Redis 6.0.9, check out the release notes.\nRedis Software 6.0.20 adds new rladmin commands for setting Ciphers suites and minimal TLS version for:\nControl plane: setting Envoy\nData plane: setting the Proxy for clients connections\nSentinel discovery service\nStarting with Redis Software 6.0.20, new clusters will be set with minimal TLS version v1.2\nAll known bugs around setting ciphers were fixed. To learn more, see Configure cipher suites.\nStarting with Redis Software 6.0.20, the syncer process was improved to automatically recover and resume synchronisation after reaching out-of-memory.\nEnvoy updated and verified with multiple security headers.\nStarting with Redis Software 6.0.20:\nThe replication backlog size of new databases is allocated dynamically according to shard size.\nThe Active-Active replication backlog size of new Active-Active databases is allocated dynamically according to shard size.\nImportant fixes RS50905, RS54809, 54940 - Fix in Redis preventing missing process PID\nRS53639 - Fix to avoid stuck state machine when assigning incorrect Redis ACL with the allkeys alias or ~* and also with ~\u0026lt;somekey\u0026gt;\nRS47983 - Fixed dependencies with installation using custom directories\nRS54382 - Fixed missing API documentation\nRS52433, RS53417, RS42195, RS42194, RS30526, RS46821, RS48928 - Fixed security headers for Envoy\nStarting 6.0.20-69 RS55504 - Fixed issue that caused RediSearch cursor to break. Starting 6.0.20-97 RS57659 - Fixed force removing an Active-Active instance which is hosted on an inaccessible cluster RS57315 - Fixed creation and editing of an Active-Active database with LDAP users of the new RBAC LDAP integration. This applies to UI access and REST API and does not apply to LDAP users for database access via Redis Clients RS57073 - Fixed a bug caused in the shard migration process which could leave unattended shards on the node RS56508 - Fixed backwards compatibility of client certificate when upgrading from earlier versions and using a certificate chain with Extended Key Usage extension being set to “TLS Web Server Authentication” instead of “TLS Web Client Authentication” RS49289 - Fixed updating the log rotation config file according to the custom config path that was set during the installation The bundled RedisGraph module was upgraded to v2.4.6 The bundled RedisTimeSeries module was upgraded to v1.4.9 The bundled RediSearch module was upgraded to v2.0.8 Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nRS59983 - Clients may get disconnected by the proxy when one client sends an UNSUBSCRIBE command without being subscribed to any channel and disconnect before the response returns back from the server (from the proxy). RS60068 - The pdns might not resolve the master node after master node change. Restarting the pdns service is required in this case. RS61114 - Active-Active synchronization will fail in the following scenario: a new syncer connection is established AND a partial sync (psync) was initiated AND a cron job runs before the first ACK of the psync was received. RS55504 - Bug RS6.0.20-66 (Build #66) causes RediSearch cursor to break. Please upgrade to a higher build when running with RediSearch. Upgrade Redis Software 5.4.2 introduced new Active-Active Redis Database capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis Database is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the Active-Active Redis Database protocol.\nWhen you upgrade an Active-Active Redis with active AOF from Redis Software 5.4.2 or earlier to version Redis Software 5.4.4 or later:\nIf replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade.\nIf replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade.\nNode upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For help with this issue, contact Support.\nStarting from Redis Software 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version.\nRedis commands The capability of disabling specific Redis commands does not work on commands specific to Redis modules.\nCLIENT UNBLOCK command is not supported in RS 5.4 and above\nStarting from RS 5.4.2 and after upgrading the CRDB, TYPE commands for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard).\nSecurity As part of Redis commitment to security, the following Open Source Redis CVE\u0026rsquo;s have been addressed in Redis Enterprise 6.0.20:\nCVE-2021-32626 - Lua scripts can overflow the heap-based Lua stack. This has been addressed in Redis Enterprise 6.0.20-62\nCVE-2021-32627 - Integer overflow issue with Streams. This has been addressed in Redis Enterprise 6.0.20-1\nCVE-2021-32628 - Vulnerability in handling large ziplists. This has been addressed in Redis Enterprise 6.0.20-1\nCVE-2021-32687 - Integer overflow issue with intsets. This has been addressed in Redis Enterprise 6.0.20-89\nThe following Open Source Redis CVE\u0026rsquo;s do not affect Redis Enterprise:\nCVE-2021-32625 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis since Redis Enterprise does not implement LCS. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.4, Redis 6.0.14)\nCVE-2021-32672 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the LUA debugger is unsupported in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32675 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proxy in Redis Enterprise does not forward unauthenticated requests. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-32762 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the memory allocator used in Redis Enterprise is not vulnerable. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\nCVE-2021-41099 - Redis Enterprise is not impacted by the CVE that was found and fixed in open source Redis because the proto-max-bulk-len CONFIG is blocked in Redis Enterprise. Additional information about the open source Redis fix is on the Redis GitHub page (Redis 6.2.6, Redis 6.0.16)\n","categories":["RS"]},{"uri":"/rs/release-notes/rs-6-0-12-january-2021/","uriRel":"/rs/release-notes/rs-6-0-12-january-2021/","title":"Redis Enterprise Software Release Notes 6.0.12 (January 2021)","tags":[],"keywords":[],"description":"Distribute synchronization across nodes for Active-Active and Active-Passive databases. Disable internal services to free memory. User accounts support password rotation. Module depdencies automatically installed. Syncer process recovery.","content":"Redis Enterprise Software (RS) 6.0.12 is now available! This version includes the following new features and improvements:\nSynchronization can now be distributed across the nodes of Active-Active or Active-Passive databases You can disable several internal RS services to free up more memory User accounts can have multiple passwords to allow for password rotation Dependencies are automatically installed when you add modules to a cluster Envoy replaces NGINX for internal cluster administration Automatic recovery of the syncer processs from out-of-memory (preview mode) And other functional and stability improvements.\nVersion information Upgrade instructions Follow these instructions for upgrading to RS 6.0.12 from RS 5.4.0 and above. For Active-Active deployments, this release requires that you upgrade the CRDB featureset version. Product lifecycle information End of Life (EOL) for Redis Enterprise Software 6.0 and previous RS versions, can be found here. EOL for Redis modules can be found here. Deprecation Notice Support for RS 5.4.X ended on December 31, 2020. Support for Red Hat Enterprise Linux 6 and Oracle Linux 6 and Ubuntu 14.04 (Trusty) operating systems platforms ended on November 30, 2020. This is the last RS version that supports direct upgrades from versions below 5.6.0. This is the last RS version that supports Active-Active protocol version below 1 and featureset version below 1. New Features Distributed Syncer The syncer process now supports running in a distributed mode. This option can improve the latency for Active-Active databases with a very high throughput profile. You can configure a replicated database to use distributed synchronization so that any available proxy endpoint can manage synchronization traffic.\nDisabling RS services to free memory Redis Software users can now use the REST API to disable the following services:\ncm_server mdns_server pdns_server stats_archiver saslauthd crdb_coordinator crdb_worker Once disabled, services are not monitored and controlled by the supervisord.\nWarning - This feature can cause unintended results if the cluster relies on the disabled services. To make sure you understand the impact of disabled services, test the system in a lab environment before you deploy in production. Support for multiple passwords For users of Redis 6 and RS 6.0 and above, you can now add more security to your password management by maintaining multiple passwords for a user to allow seamless password rotation.\nAs of RS 6.0, you can assign specific data access permissions (Redis ACLs) and cluster administration permissions to users. Password rotation is especially helpful so that you can do a rolling update of the passwords in the application clients that connect to the Redis databases.\nIn this version, you can only configure multiple passwords using the REST API.\nRedis Modules dependencies management RedisGears GA and RedisAI GA require Redis Software to fetch and manage external dependencies. Modules declare dependencies at release time in their ramp file.\nIn this version of RS, these dependencies are installed by Redis Software when the module is added to the cluster. The master node downloads the required dependencies and prompts the other nodes to copy the dependencies from the master node. When all dependency requirements are satisfied, the module installation is complete. New nodes to the cluster also automatically install the dependencies.\nSyncer automatic recovery from out-of-memory (Preview mode) For Active-Active databases, the syncer process gracefully recovers from an out of memory (OOM) state.\nAlthough Active-Active synchronization is bi-directional, in each direction we can define a source instance and a destination instance. When a destination instance (that is, the instance running the syncer process) gets OOM, the syncer process attempts to automatically recover the data synchronization when memory becomes available.\nThis is a configurable option and currently under preview mode. This behavior will be GA and set as default in the next RS version.\nTo enable the syncer automatic recovery, do these steps on each participating cluster:\nUpgrade the featureset version to 3.\nEnable the syncer automatic recovery using the REST API:\ncurl -v -k -u \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt; -X PUT -H “Content-Type: application/json” -d ‘{crdt_syncer_auto_oom_unlatch”:true}’ http://\u0026lt;cluster_address\u0026gt;:8080/v1/bdbs/\u0026lt;database_ID\u0026gt; The syncer process restarts to with automatic recovery on.\nRedis modules The following GA releases of Redis modules are bundled with RS 6.0.12:\nRediSearch, version 2.0.6 RedisJSON, version 1.0.4 RedisGraph, version 2.2.11 RedisTimeSeries, version 1.4.7 RedisBloom, version 2.2.4 To use the updated modules with a database, you must upgrade the module on the database.\nAdditional capabilities RS 6.0.12 includes open source Redis 6.0.6. For more information about Redis 6.0.6, check out the release notes. The bundled Nginx version was updated from version 1.16.0 to 1.18.0. The crdb-cli syntax to remove an instance is changed from remove-instance [--ordered|--unordered] to remove-instance [--force|--no-force]. Important fixes RS45627, RS47382 - Fixed bugs causing clients to disconnect when using XREAD and XREADGROUP commands in blocking mode on other clients’ connections. RS44656 - Fixed a bug causing TLS mode for clients connections to toggle between ‘all communication’ to ‘for crdb communication only’ when performing a global configuration change. RS42587 - Fixed a bug in the web UI console if the FQDN of the cluster is a substring of the FQDN of a participating cluster RS49404 - Fixed a bug in for upgrades with custom directories that prevent users from creating databases via the web UI console. RS43961 - bigkeys command fixed to handle non-printable key names RS45707 - Fixed a bug that caused RCP (Redis Cloud Pro) databases to reject connections while resharding the database. RS51144 - Fixed a bug in the syncer process that was stopping synchronization between all instances in some scenarios of network disconnection of one or more participating clusters. with 6.0.12-58:\nRS50865 - Fixed a bug causing rladmin change master node to fail when performed after a prior successful master change. RS51359 - Fixed a memory leak on replica shards in Active-Active databases with replication and AOF for persistence. RS52363 - Updated PUB/SUB max message value size from 64KB to 512MB Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nUpgrade RS 5.4.2 introduced new Active-Active Redis Database capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis Database is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the Active-Active Redis Database protocol. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or earlier to version RS 5.4.4 or later: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Support. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis modules. CLIENT UNBLOCK command is not supported in RS 5.4 and above Starting from RS 5.4.2 and after upgrading the CRDB, TYPE commands for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-11/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-11/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.6-11 (July 2020)","tags":[],"keywords":[],"description":"Maintenance release; support RS 6.0.6-39 and various bug fixes.","content":"The Redis Enterprise K8s 6.0.6-11 release is a maintenance release on top of 6.0.6-6 providing support for the latest Redis Enterprise Software release 6.0.6-39 and includes several bug fixes.\nOverview This release of the operator provides:\nSupport for the Redis Enterprise Software release 6.0.6-39 Various bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nImages Redis Enterprise - redislabs/redis:6.0.6-39 or redislabs/redis:6.0.6-39.rhel7-openshift Operator - redislabs/operator:6.0.6-11 or redislabs/operator:6.0.6-11.rhel7 Services Rigger - redislabs/k8s-controller:6.0.6-11 or redislabs/k8s-controller:6.0.6-11.rhel7 Important fixes The upgrade process may have failed in certain situations (i.e., with CRDB databases). We now ensure that rlutil runs at bootstrap to complete the upgrade process. (RED43635) The example and default custom resource for the REC in the OLM now correctly uses \u0026rsquo;nodes\u0026rsquo; (RED43847) Fixes for security vulnerabilities in the server rigger image: upgraded to httpd 2.4.42, Kubernetes Python Client 8.0.1, and removed localhost private key. (RED42495) A fix for an internal logging issue that caused errors to incorrectly show up on the operator log when databases are created even though the creation succeeded. (RED43336) Known limitations CrashLoopBackOff causes cluster recovery to be incomplete (RED33713) - When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The solution is to delete the crashing pods manually and recovery process will continue.\nLong cluster names cause routes to be rejected (RED25871) - A cluster name longer than 20 characters will result in a rejected route configuration as the host part of the domain name exceeds 63 characters. The workaround is to limit cluster name to 20 characters or less.\nNo cleanup of database services on failures (RED25825) - The service broker doesn\u0026rsquo;t clean up database service bindings when there are failures. The workaround is to manually remove service bindings.\nServer broker errors with two service naming schemes (RED25547) - The service broker deployment results in an error when two types of service naming schemes are set. You must choose one of the methods if both are set - redis-port is the recommended default.\nCluster CR (REC) errors are not reported after invalid updates (RED25542) - A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence.\nAn unreachable cluster has status running (RED32805) - When a cluster is in an unreachable state the state is still running instead of being reported as an error.\nReadiness probe incorrect on failures (RED39300) - STS Readiness probe doesn\u0026rsquo;t mark a node as not ready when rladmin status on the node fails\nRole missing on replica sets (RED39002) - The redis-enterprise-operator role is missing permission on replica sets.\nPrivate registries are not supported on OpenShift 3.11 (RED38579) - Openshift 3.11 doesn\u0026rsquo;t support dockerhub private registry. This is a known OpenShift issue.\nInternal DNS and K8s DNS may have conflicts (RED37462) - DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster nodes for K8s DNS names.\n5.4.10 negatively impacts 5.4.6 (RED37233) - K8S-based 5.4.10 clusters seem to negatively affect existing 5.4.6\nNode CPU usage is reported instead of pod CPU usage (RED36884) - In Kubernetes, the node CPU usage we report on is the usage of the K8S worker node hosting the REC pod.\nClusters must be named \u0026ldquo;rec\u0026rdquo; in OLM-based deployments (RED39825) - In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). The workaround is to name the cluster \u0026ldquo;rec\u0026rdquo;.\n","categories":["Platforms"]},{"uri":"/rs/release-notes/rs-6-0-8-september-2020/","uriRel":"/rs/release-notes/rs-6-0-8-september-2020/","title":"Redis Enterprise Software Release Notes 6.0.8 (September 2020)","tags":[],"keywords":[],"description":"RediSearch 2.0 support. Improved rladmin support for module upgrades.","content":"Redis Enterprise Software (RS) 6.0.8 is now available! This version includes the new RediSearch 2.0 module, open source Redis 6.0.5, changes the rladmin tool for upgrading modules, and includes bug fixes.\nVersion information Upgrade instructions Follow these instructions for upgrading to RS 6.0.8 from RS 5.4.0 and above. For Active-Active deployments, this release requires that you upgrade the CRDB featureset version.\nEnd of life End of Life (EOL) for Redis Enterprise Software 6.0 and previous RS versions, can be found here. EOL for Redis Modules can be found here.\nSupport for Red Hat Enterprise Linux 6 and Oracle Linux 6 operating systems platforms will end on November 30, 2020. Support for Ubuntu 14.04 (Trusty Tahr) operating systems platforms will end on November 30, 2020. New features Open source Redis 6 RS 6.0 includes open source Redis 6.0.5. For more information about Redis 6.0.5, check out the release notes.\nUpgrading Redis modules via rladmin The rladmin CLI introduces several updates to the commands for upgrading modules. It is now easier to upgrade your modules to the latest module version. Find out more here.\nRedis modules The following GA releases of Redis Modules are bundled in RS 6.0:\nRediSearch, version 2.0 (updated) RedisJSON, version 1.0.4 RedisGraph, version 2.0.19 (updated) RedisTimeSeries, version 1.2.7 (updated) RedisBloom, version 2.2.4 (updated) To use the updated modules with a database, you must upgrade the module on the database.\nAdditional capabilities Shard level metrics have been added to the metrics_exporter and are now available from Prometheus. You can find all of the metrics here.\nRS DEB packages (for Ubuntu) and RPM packages (for RHEL) are now signed with a GPG key so customers can verify that the package is authentic and has not been tampered with. You can access the GPG on the installaion page.\nThe crdb-cli history log is now being added to support packages.\nImportant fixes RS33193 - Improved log files handling in the proxy for large files. RS43572 - Fixed a bug causing the UI to fail when enabling SMTP STARTLS. RS46062 - Fixed missing metrics of Active-Active databases in Grafana. RS44758 - Fixed non responding button for saving a new user via the UI. With build 6.0.8-32: RS45627, RS47382 - Fixed bugs causing clients to disconnect when using XREAD and XREADGROUP commands in blocking mode on other clients’ connections. Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nActive-Active databases RS44656 - A bug causing TLS mode for clients connections to toggle between ‘all communication’ to ‘for crdb communication only’ when performing a global configuration change. TBD RS51359 - Active-Active databases, using replication and Append Only File (AOF) for Database persistence, are suffering from memory leaks on replica shards, causing them to grow bigger than the master shards. Customers are advised to upgrade to RS 6.0.12 TBD. Meanwhile you can use snapshots for database persistence or restart the replica shards TBD. Upgrade RS 5.4.2 introduced new Active-Active Redis Database capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis Database is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the Active-Active Redis Database protocol. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or lower to version RS 5.4.2 or higher: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Support. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. Starting from RS 5.4.2 and after you upgrade an Active-Active database, TYPE commands for string data-type in Active-Active databases return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/ri/using-redisinsight/configuration/","uriRel":"/ri/using-redisinsight/configuration/","title":"Configuration","tags":[],"keywords":[],"description":"","content":"RedisInsight configuration allows to update your redis instance\u0026rsquo;s config with its easy to use config editor. Each of the keys shown corresponds to an entry in the Redis configuration file. Most of the configuration settings can be applied without restarting the server. Also, it comes with an option of rewriting your current settings to your redis.conf file so that these settings remain even when server restarts.\nConfigurations are also separated into categories like- Advanced config, Security, Lua Scripting etc. in order to make the config editing easier.\n","categories":["RI"]},{"uri":"/rs/release-notes/rs-6-0-may-2020/","uriRel":"/rs/release-notes/rs-6-0-may-2020/","title":"Redis Enterprise Software Release Notes 6.0 (May 2020)","tags":[],"keywords":[],"description":"ACL and RBAC improvements for database access.  Active-Active databases support Redis Streams.","content":"Redis Enterprise Software (RS) 6.0 is now available! This new version bundles open-source Redis 6, implements enhanced Access Control List (ACL) capabilities using Role-Based Access Control (RBAC) for database access, and adds the support of Redis Streams on Active-Active databases.\nVersion information Upgrade instructions Follow these instructions for upgrading to RS 6.0 from RS 5.4.0 and above. For Active-Active deployments, this release requires that you upgrade the CRDB featureset version.\nEnd of life End of Life (EOL) for Redis Enterprise Software 6.0 and previous RS versions, can be found here. EOL for Redis Modules can be found here.\nSupport for Red Hat Enterprise Linux 6 and Oracle Linux 6 operating systems platforms will end on November 30, 2020. Support for Ubuntu 14.04 (Trusty Tahr) operating systems platforms will end on November 30, 2020. New features Open source Redis 6 RS 6.0 bundles latest open source Redis 6. For more information, check out the Diving into Redis 6 article.\nAccess control list (ACL) Based on OSS Redis 6, RS 6.0 offers the ability to manage and control connections to your databases using users and their data access permissions in terms of commands they can execute and keys they can access.\nIn OSS Redis, the ACLs are managed separately per user for each database. In Redis Enterprise Software, Redis ACLs are managed for the databases at the cluster. For more information, check out the Redis Enterprise Software user management documentation.\nRole-based access control (RBAC) RS 6.0 leverages Redis ACLs to implement role-based access control that easily scale and manage data access permissions. Using roles minimizes the overhead involved in managing a cluster with many databases, multiple users, and various access control lists. For more information, check out the Redis Enterprise Software user management documentation.\nActive-Active support for Redis Streams RS 6.0 adds support for Redis Streams on Active-Active geo-distributed databases using conflict-free replicated data type (CRDT). You can now use all Redis Streams commands including consumer groups on Active-Active databases. To enable it, upgrade your Active-Active database featureset version to the latest (featureset version = 2) as part of the upgrade process. For more information, check out Redis Streams on Active Active databases.\nRedis modules The following GA releases of Redis Modules are bundled in RS 6.0:\nRedisBloom, version 2.2.2 (updated) RedisGraph, version 2.0.11 (updated) RedisJson, version 1.0.4 RediSearch, version 1.6.12 (updated) RedisTimeSeries, version 1.2.5 (updated) To use the updated modules with a database, you must upgrade the module on the database.\nAdditional capabilities Added the ability to configure a storage service that uses the S3 protocol. Configuration for backup location and for import and export locations of RDB files is possible. The storage service must have a valid SSL certificate. To connect to an S3-compatible storage location, run: rladmin cluster config s3_url \u0026lt;url\u0026gt;\nThe crdb-cli tool was updated so it is now displaying the Active-Active database’s featureset version and the protocol version per instance. For example:\n$ crdb-cli crdb list --verbose CRDB-GUID NAME REPL-ID\tPROTOCOL FEATURESET DB-ID CLUSTER-FQDN 969122be-...\tloremipsum1 1\t0 0\tbdb:1 cluster1.local 969122be-...\tloremipsum1 2\t0 0\tbdb:1 cluster2.local Added REST API and rladmin commands to modify the timeout for automatically disconnecting an inactive admin console session.\nUsing the rladmin run: rladmin cluster config cm_session_timeout_minutes \u0026lt;int_value\u0026gt;\nUsing the REST API:\ncurl --request PUT \\ --url https://localhost:9443/v1/cluster \\ --header \u0026#39;content-type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;cm_session_timeout_minutes\u0026#34;: 10 }\u0026#39; Added no_of_expires metrics for database metrics and for shard metrics. You can access this metric from the REST API:\nno_of_expires shows the current number of volatile keys in the database. expired_objects shows the rate of keys expired in DB (expirations/sec). Added the ability to customize the welcome message on the login page in the admin console console.\nImportant fixes RS39121, RS35335 - Optimized the XREAD and XREADGROUP commands so when using them in a non blocking fashion (without BLOCK keyword) they use the shared connection instead of a dedicated connection. RS26448 - Fixed ‘ram overhead’ metric calculation for ROF databases using AOF. RS31190 - Fixed a bug that causes databases with OSS Cluster API enabled to override the ‘preferred IP type’ attribute from ‘external’ to ‘internal’ (the default value). RS34009 - Updated the modules loading procedure for clusters with FIPS compliance enabled. RS38233 - Improved the Redis cleanup job handling the persistent directory RS39228 - Fixed a bug in the WAIT command that in some cases was released after a longer period than requested. RS39749 - Fixed a bug that blocked eviction while LUA scripts were in progress. RS43996 - Fixed a bug when aborting an upgrade to RS 6.0. Known limitations -RS81463 - A shard may crash when resharding an Active-Active database with Redis on Flash (RoF). Specifically, the shard will crash when volatile keys or Active-Active tombstone keys reside in Flash memory.\nActive-Active databases RS51359 - Active-Active databases, using replication and Append Only File (AOF) for database persistence, are suffering from memory leaks on replica shards, causing them to grow bigger than the master shards. Customers are advised to upgrade to RS 6.0.12 TBD. Meanwhile you can use snapshots for database persistence or restart the replica shards TBD. Upgrade RS 5.4.2 introduced new Active-Active Redis Database capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis Database is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the Active-Active Redis Database protocol. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or lower to version RS 5.4.4 or higher: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Dynatrace agent installed on the cluster nodes can hamper the working on Envoy process leading to failure of UI and REST API. Prior upgrading we recommend removing Dynatrace completely or try upgrading to newer versions. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. Starting from RS 5.4.2 and after you upgrade an Active-Active database, TYPE commands for string data-type in Active-Active databases return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-6/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-6-0-6-6/","title":"Redis Enterprise for Kubernetes Release Notes 6.0.6-6 (June 2020)","tags":[],"keywords":[],"description":"Support for RS 6.0.6, new database and admission controllers, various improvements and bug fixes.","content":"Redis Enterprise for Kubernetes 6.0.6-6 is now available. This release includes Redis Enterprise (RS) version 6.0 and introduces new features, improvements, and bug fixes.\nOverview This release of the operator provides:\nSupport for the Redis Enterprise Software release 6.0.6 Includes the new database and admission controllers Various improvements and bug fixes To upgrade your deployment to this latest release, see \u0026ldquo;Upgrade a Redis Enterprise cluster (REC) on Kubernetes\u0026rdquo;.\nNote: If you are running Active-Active (CRBD) databases on a previous release, do not upgrade to 6.0.6-6 at this time. There is an issue with the upgrade process that is currently being investigated (RED43635). For more information and support please contact Redis support. Images Redis Enterprise - redislabs/redis:6.0.6-6 or redislabs/redis:6.0.6-6.rhel7-openshift Operator - redislabs/operator:6.0.6-6 or redislabs/operator:6.0.6-6.rhel7 Services Rigger - redislabs/k8s-controller:6.0.6-6 or redislabs/k8s-controller:6.0.6-6.rhel7 New features and improvements Database controller - A new database controller in the operator provides the ability to create and manage databases on a Redis Enterprise cluster via a custom resource (RED36516).\nAdmission controller - A new admission controller in the operator provides validation of database custom resources (RED36458).\nPod tolerations - Support for specifying Redis Enterprise cluster node pod tolerations of node taints has been added to the cluster CR (see podTolerations) (RED33069).\nPod annotations - Support for specifying Redis Enterprise cluster node pod annotations has been added to the cluster CR (see podAnnotations) (RED35613).\nKubernetes versions - Support for Kubernetes 1.17 was added and versions 1.9 and 1.10 (previously deprecated) are no longer supported (RED41049).\nImproved OLM Experience - The overall user experience and documentation in the OLM (OperatorHub) has been improved (RED37008).\nResource limits - Resource limits have been added to the recommended operator configuration (RED39572).\nLoadBalancer service type added - The LoadBalancer value has been added to the databaseServiceType option in servicesRiggerSpec (RED43215):\nservicesRiggerSpec: databaseServiceType: LoadBalancer Important fixes Service creation failure causes cluster setup failure (RED37197) UI service update failure (RED37198) Error shown in OLM deployment: \u0026ldquo;The field status.state is invalid\u0026rdquo; (RED40278) OLM: StatefulSet not listed as an object owned by the Redis Enterprise Cluster (RED39296) Setting extraLabels in the cluster CR did not label pods on OpenShift (RED39763) log_collector failed to get the pods logs when a namespace wasn\u0026rsquo;t given (RED39292) Role and RoleBinding created or updated using an existing ServiceAccount in REC spec (RED42912) Known limitations CrashLoopBackOff pod status and cluster recovery - When a pod status is CrashLoopBackOff and we run the cluster recovery, the process will not complete. The solution is to delete the crashing pods manually and recovery process will continue (RED33713).\nActive-Active (CRDB) - limitation on cluster name length - A cluster name longer than 20 characters will result in a rejected route configuration as the host part of the domain name exceeds 63 characters. Cluster names must be limited to 20 characters or less (RED25871).\nActive-Active (CRDB) service broker cleanup - The service broker doesn\u0026rsquo;t clean up database service bindings in case of failures. These bindings must be removed manually (RED25825).\nService broker deployment error - The service broker deployment results in an error when two types of service naming schemes are set. Choosing one of the methods will resolve this error (redis-port is the recommended default) (RED25547).\nCluster spec invalid errors not reported - A cluster CR specification error is not reported if two or more invalid CR resources are updated in sequence (RED25542).\nUnreachable cluster does not produce an error - When a cluster is in an unreachable state the state is still running instead of being reported as an error (RED32805).\nReadiness probe ignores rladmin failure - STS Readiness probe doesn\u0026rsquo;t mark a node as not ready when rladmin status nodes fails (RED39300).\nMissing permission for role - The redis-enterprise-operator role is missing permission on replicasets (RED39002).\nOpenshift 3.11 doesn\u0026rsquo;t support DockerHub private registry - Openshift 3.11 doesn\u0026rsquo;t support DockerHub private registry. This is a known OpenShift issue and not addressable by Redis Enterprise for Kubernetes (RED38579).\nPossible DNS conflicts within cluster nodes - DNS conflicts are possible between the cluster mdns_server and the K8s DNS. This only impacts DNS resolution from within cluster node and while using the full fqdn *.cluster.local (RED37462).\nCoexistence of 5.4.10 and 5.4.6 clusters - K8s clusters with Redis Enterprise 5.4.6 clusters are negatively affected by installing a Redis Enterprise 5.4.10 cluster due to changes in CRD (CustomeResourceDefinition) (RED37233).\nRedis Enterprise CPU utilization metric reports at K8s node level rather than at pod level - In Kubernetes, the node CPU usage we report on is the usage of the K8S worker node hosting the REC pod (RED-36884). Pod resource utilization should be measured by K8s-native means rather than through the application.\nCluster name is limited on OpenShift via OLM - In OLM-deployed operators, the deployment of the cluster will fail if the name is not \u0026ldquo;rec\u0026rdquo;. When the operator is deployed via the OLM, the security context constraints (scc) is bound to a specific service account name (i.e., \u0026ldquo;rec\u0026rdquo;). Naming the cluster \u0026ldquo;rec\u0026rdquo; resolves the issue (RED39825).\nComing Soon The following lists features, fixes and changes the Redis team is currently investing in:\nRedis Enterprise and Kubernetes Container Artifacts - tarting from the next release of Redis Enterprise for K8s, new container artifacts will be published using different base images:\nRedis Enterprise - A UBI (RHEL 7) base image will replace the Ubuntu and the RHEL7 base images Operator - An Image built from scratch containing the Golang executable will replace the Ubuntu and the RHEL7 base images Services Rigger - A UBI (RHEL 7) base image will replace the Ubuntu and the RHEL7 base images Deprecation notice - he service broker solution is deprecated and will not be supported starting from the next release of the Redis Enterprise Operator for Kubernetes.\nAdditional Redis Enterprise Database configuration options in the Database Controller - We\u0026rsquo;re currently investing in the following additional capabilities of the Database Controller:\nSupport for loading database modules Support for setting up Alerts, expressed as K8s events Support for configuring database backup options Support for Kubernetes 1.18 Support for Rancher K8s distribution ","categories":["Platforms"]},{"uri":"/rs/installing-upgrading/configuring/","uriRel":"/rs/installing-upgrading/configuring/","title":"Additional configuration","tags":[],"keywords":[],"description":"","content":"This section describes additional configuration options for Redis Enterprise Software installation.\nConfigure AWS EC2 instances for Redis Enterprise Software There are some special considerations for installing and running Redis Enterprise Software on Amazon Elastic Cloud Compute (EC2) instances. These include: Storage considerations Instance types Security group configuration Storage considerations AWS EC2 instances are ephemeral, but your persistent database storage should not be. If you require a persistent storage location for your database, the storage must be located outside of the instance. Therefore, when you set up an instance make sure that it has a properly sized EBS backed volume connected.\nConfigure CentOS/RHEL firewall CentOS/RHEL7 distributions have, by default, a restrictive firewall mechanism based on firewalld that in turn configures the standard iptables system. The default configuration assigns the network interfaces to the public zone and blocks all ports, except 22 (SSH). Redis Enterprise Software (RS) installation on CentOS/RHEL 7 automatically creates two firewalld system services: A service named redislabs, which includes all ports and protocols needed for communications between cluster nodes. A service named redislabs-clients, which includes the ports and protocols needed for communications external to the cluster.\nConfigure Swap for Linux Swap space is used by the Linux OS to help manage memory (pages) by copying pages from RAM to disk and the OS is configured by default to be fairly aggressive. For Redis Enterprise Software (RS) with the way it utilizes and manages memory, it is best to eliminate the likelihood of the OS swapping. If you would like to understand why, please read more on memory limits for best functionality and performance.\nChange socket file locations There are two default locations for the socket files in Redis Enterprise Software (RS): /tmp - In clean installations of RS version lower than 5.2.2 /var/opt/redislabs/run - In clean installations of RS version 5.2.2 and higher We made this change because some customers have maintenance procedures that delete the /tmp directory. When you upgrade from a RS version lower than 5.2.2 to 5.2.2 and higher, the socket files are not moved to the new location by default.\n","categories":["RS"]},{"uri":"/rs/databases/active-active/synchronization-mode/","uriRel":"/rs/databases/active-active/synchronization-mode/","title":"Configure distributed synchronization","tags":[],"keywords":[],"description":"How to configure distributed synchronization so that any available proxy endpoint can manage synchronization traffic.","content":"Replicated databases, such as Replica Of and Active-Active databases, use proxy endpoints to synchronize database changes with the databases on other participating clusters.\nTo improve the throughput and lower the latency for synchronization traffic, you can configure a replicated database to use distributed synchronization where any available proxy endpoint can manage synchronization traffic.\nEvery database by default has one proxy endpoint that manages client and synchronization communication with the database shards, and that proxy endpoint is used for database synchronization. This is called centralized synchronization.\nTo prepare a database to use distributed synchronization you must first make sure that the database proxy policy is defined so that either each node has a proxy endpoint or each primary (master) shard has a proxy endpoint. After you have multiple proxies for the database, you can configure the database synchronization to use distributed synchronization.\nConfigure distributed synchronization To configure distributed synchronization:\nTo check the proxy policy for the database, run: rladmin status\nThe output of the status command shows the list of endpoints on the cluster and the proxy policy for the endpoint.\nENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:1 db endpoint:1:1 node:1 all-master-shards No If the proxy policy (also known as a role) is single, configure the policy to all-nodes or all-master-shards according to your needs with the command:\nrladmin bind db \u0026lt;db_name\u0026gt; endpoint \u0026lt;endpoint id\u0026gt; policy \u0026lt;all-master-shards|all-nodes\u0026gt; To configure the database to use distributed synchronization, run:\ntune db \u0026lt;db_name\u0026gt; syncer_mode distributed To change back to centralized synchronization, run:\ntune db \u0026lt;db_name\u0026gt; syncer_mode centralized Verify database synchronization Use rladmin to verify a database synchronization role:\nrladmin info db \u0026lt;db_name\u0026gt; The current database role is reported as the syncer_mode value:\n$ rladmin info db \u0026lt;db_name\u0026gt; db:1 [\u0026lt;db_name\u0026gt;]: // (Other settings removed) syncer_mode: centralized ","categories":["RS"]},{"uri":"/ri/faqs/","uriRel":"/ri/faqs/","title":"FAQs","tags":[],"keywords":[],"description":"Answers frequently asked questions about RedisInsight.","content":"Here are some frequently asked questions about RedisInsight.\nGeneral What is RedisInsight? RedisInsight is a browser based graphical user interface (GUI) for Redis databases.\nWith it, you can:\nBrowse\nView real time metrics from redis Create tabular views from your redis keys and export data in different formats Perform CRUD (create, read, update, and delete) operations using the web based command-line interface (CLI). Visualize and update data from Streams, RedisGraph, RediSearch and RedisTimeSeries. Analyse\nAnalyze memory used by redis by keys or key patterns, by expiry, by data types or or the internal encoding Filter keys by number of elements or by memory, and identify top keys Profile redis to list keys or commands redis is executing Bulk Actions\nConfigure\nView and modify redis configuration View list of clients and stop specific clients What problem does RedisInsight solve? RedisInsight lets you reduce memory used by Redis, which usually translates to a proportional cost reduction. It also helps you identify common latency issues and perform routine administrative tasks on your Redis server.\nHow does RedisInsight compare with redis-rdb-tools? We open sourced redis-rdb-tools in 2012 as a way to analyze the RDB file, and we continue to support the open source version. The open source version is meant to be a library and command line utility. It is a fundamental building block for RedisInsight. RedisInsight is free to use but is closed source.\nWhat versions of Redis does RedisInsight support? Currently, RedisInsight supports single node redis instances. Redis Cluster support is experimental, but not all features work. If you want to use RedisInsight for Redis Cluster, send us an email and we can try to accommodate your use case.\nDo you support Redis Enterprise? Yes, RedisInsight is fully compatible with Redis Enterprise Software and Redis Enterprise Cloud.\nWhat cloud providers do you support? We support Redis Enterprise Cloud, AWS Elasticache, and Azure Redis Cache. That said, RedisInsight should work with any cloud provider as long as you run it on a host that has network access to your cloud based Redis server.\nSend us an email if you would like to use RedisInsight on a cloud provider that we haven\u0026rsquo;t listed.\nMemory analysis How long does the memory analysis take? This depends on the size of your database and the host server. In general, expect analysis to require 30 seconds per GB of RAM anaylyzed.\nWhat is the online mode of memory analysis? In online mode, RedisInsight connects to your Redis server, downloads the entire data set, and then analyzes it in a background process.\nData is downloaded in one of two ways:\nIf the server allows the SYNC command, we use it to a database RDB file. If the server blocks SYNC, we use SCAN + DUMP in an loop. What is the overhead of online memory analysis on redis server? The overhead is minimal. If SYNC command is supported, the overhead is the same as connecting a replica for a short duration. If SYNC command is disabled, we run the SCAN command followed by DUMP in a pipeline. Neither approach blocks your redis server.\nWhat is offline mode of memory analysis? In the offline mode, RedisInsight downloads a RDB file from a specified S3 bucket you specify and then analyzes it in a background process.\nWhat is the overhead of offline memory analysis on redis server? Zero overhead, since we do not need to connect to your Redis server\nHow is memory used by a key calculated? We look at the internal structures Redis needs to allocate to store the key and determine the memory consumption. We also account for allocator overheads and differences between Redis versions.\nHow accurate is the memory analysis? The memory used by a key is based on heuristics and is usually within 10% of the actual consumption.\nWhat are key patterns? Key pattern is a grouping of related keys, for example users:*. RedisInsight can show you total memory consumption by key pattern, and also the biggest keys within that key pattern.\nHow are key patterns generated? We assume that you use colon as a separator. If you use a non-standard separator, you have to add key patterns manually.\nConnect to Redis How do I connect to redis-server running on localhost? First, this works only if you have RedisInsight running on your local computer. Depending on your docker version, you can use one of these host names instead of localhost - docker.for.mac.localhost, docker.for.win.localhost or host.docker.internal. If none of those host names work, find the ip address of your computer (usually starts with 192.x.x.x), and use that ip address instead of localhost.\nHow do I connect to AWS Elasticache? You must install RedisInsight inside your VPC, either on an EC2 instance, or using ECS or Fargate. See EC2 Installation Instructions\nWhat are the memory limits on your license terms? Our licensing works on the sum of used memory on the Redis instances you have added to RedisInsight. So if your license allows 15GB, you can add 5 redis instances using 3 GB RAM, or 1 redis server using 15 GB.\nLicense and support Is RedisInsight a free tool? Yes, RedisInsight is available as a non-commercial, free of charge tool. You can review the terms of use in the RedisInsight Licence Terms document.\nWhere can I find the licence terms of RedisInsight? See the official RedisInsight Licence Terms document.\nPrivacy and security Who has access to my Redis servers? We provide RedisInsight as a docker container that you install and run on your hardware or cloud account. We do not have any ability to connect to your installation of RedisInsight or look at data within your redis servers.\nHow can I secure my RedisInsight installation? We recommend:\nInstalling HTTPS Using an allow list to restrict IP access to RedisInsight Installing RedisInsight within your VPN/internal network. Additionally, make sure you use a strong admin password for your database.\nWhat information do you collect about my installation? We use Google Analytics to understand how customers use the software. Per their terms and conditions, we do not track any personally identifiable information.\n","categories":["RI"]},{"uri":"/rc/api/examples/manage-cloud-accounts/","uriRel":"/rc/api/examples/manage-cloud-accounts/","title":"Create and manage cloud accounts","tags":[],"keywords":[],"description":"Cloud accounts specify which account to use when creating and modifying infrastructure resources.","content":"You can use the Redis Enterprise Cloud REST API to create and manage cloud accounts.\nThese examples use the cURL utility; you can use any REST client to work with the Redis Cloud REST API.\nCreate a cloud account To create a cloud account, use the POST /v1/cloud-accounts endpoint.\nThe created cloud account is defined by a JSON document that is sent as the body of the request.\nPOST https://[host]/v1/cloud-accounts { \u0026#34;accessKeyId\u0026#34;: \u0026#34;$ACCESS_KEY_ID\u0026#34;, \u0026#34;accessSecretKey\u0026#34;: \u0026#34;$ACCESS_SECRET_KEY\u0026#34;, \u0026#34;consolePassword\u0026#34;: \u0026#34;$CONSOLE_PASSWORD\u0026#34;, \u0026#34;consoleUsername\u0026#34;: \u0026#34;$CONSOLE_USERNAME\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;My new Cloud Account\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;signInLoginUrl\u0026#34;: \u0026#34;https://$AWS_ACCOUNT_IDENTIFIER.signin.aws.amazon.com/console\u0026#34; } The POST response is a JSON document that contains the taskId. You can use GET /v1/tasks/\u0026lt;taskId\u0026gt; to track the status of the cloud account creation.\n","categories":["RC"]},{"uri":"/kubernetes/memory/","uriRel":"/kubernetes/memory/","title":"Manage memory resources","tags":[],"keywords":[],"description":"Settings and configuration to manage memory resources for your Redis Enterprise cluster.","content":"This section contains settings and configuration to manage memory resources for your Redis Enterprise cluster.\nUse persistent volumes in Redis Enterprise clusters This section covers details about how persistent volumes are sized and specified for Redis Enterprise cluster deployments.\nSize and scale a Redis Enterprise cluster deployment on Kubernetes This section provides information about sizing and scaling Redis Enterprise in a Kubernetes deployment.\nControl node selection This section provides information about how Redis Enterprise cluster pods can be scheduled to only be placed on specific nodes or node pools.\nManage pod stability This section provides information about how you can use quality of service, priority class, eviction thresholds and resource monitoring to maintain cluster node pod stability.\n","categories":["Platforms"]},{"uri":"/kubernetes/memory/node-selection/","uriRel":"/kubernetes/memory/node-selection/","title":"Control node selection","tags":[],"keywords":[],"description":"This section provides information about how Redis Enterprise cluster pods can be scheduled to only be placed on specific nodes or node pools.","content":"Many Kubernetes cluster deployments have different kinds of nodes that have different CPU and memory resources available for scheduling cluster workloads. Redis Enterprise for Kubernetes has various abilities to control the scheduling Redis Enterprise cluster node pods through properties specified in the Redis Enterprise cluster custom resource definition (CRD).\nA Redis Enterprise cluster (REC) is deployed as a StatefulSet which manages the Redis Enterprise cluster node pods. The scheduler chooses a node to deploy a new Redis Enterprise cluster node pod on when:\nThe cluster is created The cluster is resized A pod fails Here are the ways that you can control the pod scheduling:\nUsing node selectors The nodeSelector property of the cluster specification uses the same values and structures as the Kubernetes nodeSelector. In general, node labels are a simple way to make sure that specific nodes are used for Redis Enterprise pods. For example, if nodes \u0026rsquo;n1\u0026rsquo; and \u0026rsquo;n2\u0026rsquo; are labeled as \u0026ldquo;high memory\u0026rdquo;:\nkubectl label nodes n1 memory=high kubectl label nodes n2 memory=high The Redis Enterprise cluster CRD can request to be scheduled on these nodes:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 nodeSelector: memory: high Then, when the operator creates the StatefulSet associated with the pod, the nodeSelector section is part of the pod specification. When the scheduler attempts to create new pods, it needs to satisfy the node selection constraints.\nUsing node pools A node pool is a common part of the underlying infrastructure of the Kubernetes cluster deployment and provider. Often, node pools are similarly-configured classes of nodes such as nodes with the same allocated amount of memory and CPU. Implementors often label these nodes with a consistent set of labels.\nOn Google Kubernetes Engine (GKE), all node pools have the label cloud.google.com/gke-nodepool with a value of the name used during configuration. On Microsoft Azure Kubernetes System (AKS), you can create node pools with a specific set of labels. Other managed cluster services may have similar labeling schemes.\nYou can use the nodeSelector section to request a specific node pool by label values. For example, on GKE:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 nodeSelector: cloud.google.com/gke-nodepool: \u0026#39;high-memory\u0026#39; Using node taints You can use multiple node taints with a set of tolerations to control Redis Enterprise cluster node pod scheduling. The podTolerations property of the cluster specification specifies a list of pod tolerations to use. The value is a list of Kubernetes tolerations.\nFor example, if the cluster has a single node pool, the node taints can control the allowed workloads for a node. You can add taints to the node, for example nodes n1, n2, and n3, reserve a set of nodes for the Redis Enterprise cluster:\nkubectl taint nodes n1 db=rec:NoSchedule kubectl taint nodes n2 db=rec:NoSchedule kubectl taint nodes n3 db=rec:NoSchedule This prevents any pods from being scheduled onto the nodes unless the pods can tolerate the taint db=rec.\nYou can then add the toleration for this taint to the cluster specification:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 podTolerations: - key: db operator: Equal value: rec effect: NoSchedule A set of taints can also handle more complex use cases. For example, a role=test or role=dev taint can be used to designate a node as dedicated for testing or development workloads via pod tolerations.\nUsing pod anti-affinity By default, the Redis Enterprise node pods are not allowed to be placed on the same node for the same cluster:\npodAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: redis-enterprise redis.io/cluster: rec redis.io/role: node topologyKey: kubernetes.io/hostname Each pod has the three labels above where redis.io/cluster is the label for the name of your cluster.\nYou can change this rule to restrict or include nodes that the Redis Enterprise cluster node pods can run on. For example, you can delete the redis.io/cluster label so that even Redis Enterprise node pods from different clusters cannot be scheduled on the same Kubernetes node:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: redis-enterprise redis.io/role: node topologyKey: kubernetes.io/hostname or you can prevent Redis Enterprise nodes from being schedule with other workloads. For example, if all database workloads have the label \u0026rsquo;local/role: database\u0026rsquo;, you can use this label to avoid scheduling two databases on the same node:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 extraLabels: local/role: database podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: local/role: database app: redis-enterprise redis.io/cluster: rec redis.io/role: node topologyKey: kubernetes.io/hostname In this case, any pods that are deployed with the label local/role: database cannot be scheduled on the same node.\nUsing rack awareness You can configure Redis Enterprise with rack-zone awareness to increase availability during partitions or other rack (or region) related failures.\nNote: When creating your rack-zone ID, there are some constraints to consider; see rack-zone awareness for more info. Rack-zone awareness is a single property in the Redis Enterprise cluster CRD named rackAwarenessNodeLabel. This value for this label is commonly topology.kubernetes.io/zone as documented in \u0026lsquo;Running in multiple zones\u0026rsquo;.\nYou can check the value for this label in your nodes with the command:\n$kubectl get nodes -o custom-columns=\u0026#34;name:metadata.name\u0026#34;,\u0026#34;rack\\\\zone:metadata.labels.failure-domain\\.beta\\.kubernetes\\.io/zone\u0026#34; name rack\\zone ip-10-0-x-a.eu-central-1.compute.internal eu-central-1a ip-10-0-x-b.eu-central-1.compute.internal eu-central-1a ip-10-0-x-c.eu-central-1.compute.internal eu-central-1b ip-10-0-x-d.eu-central-1.compute.internal eu-central-1b Enabling the cluster role For the operator to read the cluster node information, you must create a cluster role for the operator and then bind the role to the service account.\nHere\u0026rsquo;s a cluster role:\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: redis-enterprise-operator rules: # needed for rack awareness - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;] And here\u0026rsquo;s how to apply the role:\nkubectl apply -f https://raw.githubusercontent.com/RedisLabs/redis-enterprise-k8s-docs/master/rack_awareness/rack_aware_cluster_role.yaml The binding is typically to the redis-enterprise-operator service account:\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: redis-enterprise-operator subjects: - kind: ServiceAccount namespace: NAMESPACE_OF_SERVICE_ACCOUNT name: redis-enterprise-operator roleRef: kind: ClusterRole name: redis-enterprise-operator apiGroup: rbac.authorization.k8s.io and it can be applied by running:\nkubectl apply -f https://raw.githubusercontent.com/RedisLabs/redis-enterprise-k8s-docs/master/rack_awareness/rack_aware_cluster_role_binding.yaml Once the cluster role and the binding have been applied, you can configure Redis Enterprise clusters to use rack awareness labels.\nConfiguring rack awareness You can configure the node label to read for the rack zone by setting the rackAwarenessNodeLabel property:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: example-redisenterprisecluster spec: nodes: 3 rackAwarenessNodeLabel: topology.kubernetes.io/zone: \u0026#34;your_zone\u0026#34; Note: When you use the rackAwarenessNodeLabel property, the operator will change the topologyKey for the anti-affinity rule to the label name used unless you have specified the podAntiAffinity property as well. If you use rackAwarenessNodeLabel and podAntiAffinity together, you must make sure that the topologyKey in your pod anti-affinity rule is set to the node label name. ","categories":["Platforms"]},{"uri":"/rs/databases/redis-on-flash/rof-quickstart/","uriRel":"/rs/databases/redis-on-flash/rof-quickstart/","title":"Redis on Flash (RoF) quick start","tags":[],"keywords":[],"description":"Get started with Redis on Flash quickly, creating a cluster and database using flash storage.","content":"This page guides you through a quick setup of a Redis on Flash cluster with a single node for testing and demo purposes.\nFor production environments, you can find more detailed installation instructions in the install and setup section.\nThe steps to set up a Redis Enterprise Software cluster using Redis on Flash with a single node are:\nInstall Redis Enterprise Software or run it in a Docker container. Set up a Redis Enterprise Software cluster with Redis on Flash. Create a new Redis on Flash database. Connect to your new database. Install Redis Enterprise Software Bare metal, VM, Cloud instance To install on bare metal, a virtual machine, or an instance:\nDownload the binaries from the Redis Enterprise download center.\nUpload the binaries to a Linux-based operating system.\nExtract the image:\ntar -vxf \u0026lt;downloaded tar file name\u0026gt; After the tar command completes, you can find a new install.sh script in the current directory: sudo ./install.sh -y Docker-based installation For testing purposes, you can run a Redis Enterprise Software Docker container on Windows, MacOS, and Linux.\ndocker run -d --cap-add sys_resource --name rp -p 8443:8443 -p 12000:12000 redislabs/redis:latest Set up a cluster and enable Redis on Flash Direct your browser to https://localhost:8443/ on the host machine to see the Redis Enterprise Software admin console. Select the Setup button to get started. Note: Depending on your browser, you may see a certificate error. Choose \u0026ldquo;continue to the website\u0026rdquo; to get to the setup screen. On the node configuration page, select the Enable flash storage support checkbox and provide a cluster FQDN: mycluster.local. Then select the Next button. If you don\u0026rsquo;t have a license key yet, select the Next button to try the trial version of the product.\nOn the next screen, set up account credentials for a cluster administrator.\nSelect OK to confirm that you are aware of the replacement of the HTTPS SSL/TLS certificate on the node, and proceed through the browser warning.\nCreate a database Select the \u0026ldquo;new redis db flash\u0026rdquo; option.\nOn the create database page:\nEnter myredisflashdb for a database name. Enter 12000 for the endpoint port number. Select show advanced options to see the various alerts available. Select Activate to create your database. You now have a Redis on Flash database!\nConnect to your database You are ready to connect to your database to store data. See the test connectivity page to learn how to connect to your database.\nNext steps If you want to generate load against the database or add a bunch of data for cluster testing, see the memtier_benchmark quick start for help.\nTo see the true performance and scale of Redis on Flash, you must tune your I/O path and set the flash path to the mounted path of SSD or NVMe flash memory as that is what it is designed to run on. For more information, see Redis on Flash.\n","categories":["RS"]},{"uri":"/rs/references/client_references/","uriRel":"/rs/references/client_references/","title":"Develop with Redis clients","tags":[],"keywords":[],"description":"Redis client libraries allow you to connect to Redis instances from within your application. This section provides an overview of several recommended Redis clients for popular programming and scripting languages.","content":"To connect to Redis instances from within your application, use a Redis client library that matches your application\u0026rsquo;s language.\nThe Redis Clients page contains a list of available Redis client libraries for a variety of popular programming and scripting languages.\nThe following links discuss a few recommended clients in more detail, including installation instructions and usage examples.\nLanguage Client description Node.js (node_redis) The node_redis client allows you to use Redis with Node.js. Node.js (ioredis) The ioredis client allows you to use Redis with Node.js. .NET The StackExchange.Redis client allows you to use Redis with .NET. Python The redis-py client allows you to use Redis with Python. Java The clients Lettuce and Jedis allow you to use Redis with Java. PHP (Predis) The Predis client allows you to use Redis with PHP. C The hiredis client lets you use C to connect to Redis databases. Ruby The redis-rb client allows you to use Redis with Ruby. Drupal Configure Drupal to use Redis as a cache. ","categories":["RS"]},{"uri":"/modules/redisai/","uriRel":"/modules/redisai/","title":"RedisAI","tags":[],"keywords":[],"description":"","content":"RedisAI is a Redis module for executing Deep Learning/Machine Learning models and managing their data. Its purpose is being a \u0026ldquo;workhorse\u0026rdquo; for model serving, by providing out-of-the-box support for popular DL/ML frameworks and unparalleled performance. RedisAI both simplifies the deployment and serving of graphs by leveraging on Redis\u0026rsquo; production-proven infrastructure, as well as maximizes computation throughput by adhering to the principle of data locality.\nThis introduction is intended to present the core concepts it uses and the functionality it provides.\nNote: Before diving into RedisAI please make sure that you are familiar with the basic concepts of machine learning and Redis. In broad strokes, RedisAI looks like this:\n+-----------------------------------------------------------------------------+ | SERVER | | +-------------------------------------------------------------------------+ | | | REDIS +----------+ | | | | +----------+ +-----------------+ | Commands | +---------------------+ | | | | | KEYSPACE | | REDISAI +----+-----+ | | | | | | | | ^ | | | | | +----------+ | Data Structures | DL/ML Backends | | | | | | | | +--------+ | +-------------+ | | | | | | mytensor +-----\u0026gt;+ Tensor +\u0026lt;--+ | +--\u0026gt;+ TensorFlow | | | | | | | | | +--------+ | | | +-------------+ | | | | | +----------+ | | | | | | | | | | | | | +--------+ | v | +-------------+ | | | | | | mymodel +-----\u0026gt;+ Model +---+ +----+-----+ +--\u0026gt;+ PyTorch | | | | | | | | | +--------+ | | | | +-------------+ | | | | | +----------+ | +--\u0026gt;+ Engine +\u0026lt;--+ | | | | | | | | +--------+ | | | | +-------------+ | | | | | | myscript +-----\u0026gt;+ Script +---+ +----+-----+ +--\u0026gt;+ ONNXRuntime | | | | | | | | | +--------+ | ^ | +-------------+ | | | | | +----------+ | | | | | | | | | ^ | +--------+ | | | +-------------+ | | | | | | | + DAG +---+ | +--\u0026gt;+ ... | | | | | | | | +--------+ | +-------------+ | | | | | | +------------------------|-----------------------------+ | | | +--------------|--------------------------|-------------------------------+ | | v v | | +--------------+-----------------+ +------------------------------------+ | | | RAM | | DEVICES | | | | | | +-----+ +-----+ +-----+ +-----+ | | | | 00101010 00101010 00101010 ... | | | CPU | | GPU | | TPU | | ... | | | | | | | +-----+ +-----+ +-----+ +-----+ | | | +--------------------------------+ +------------------------------------+ | +-----------------------------------------------------------------------------+ How RedisAI works RedisAI bundles together best-of-breed technologies for delivering stable and performant computation graph serving. Every DL/ML framework ships with a runtime for executing the models developed with it, and the common practice for serving these is building a simple server around them.\nRedisAI aims to be that server, saving you from the need of installing the backend you\u0026rsquo;re using and developing a server for it. By itself that does not justify RedisAI\u0026rsquo;s existence so there\u0026rsquo;s more to it. Because RedisAI is implemented as a Redis module it automatically benefits from the server\u0026rsquo;s capabilities: be it Redis\u0026rsquo; native data types, its robust eco-system of clients, high-availability, persistence, clustering, and Enterprise support.\nBecause Redis is an in-memory data structure server RedisAI uses it for storing all of its data. The main data type supported by RedisAI is the Tensor that is the standard representation of data in the DL/ML domain. Because tensors are stored in the memory space of the Redis server, they are readily accessible to any of RedisAI\u0026rsquo;s backend libraries at minimal latency.\nThe locality of data, which is tensor data in adjacency to DL/ML models backends, allows RedisAI to provide optimal performance when serving models. It also makes it a perfect choice for deploying DL/ML models in production and allowing them to be used by any application.\nFurthermore, RedisAI is also an optimal testbed for models as it allows the parallel execution of multiple computation graphs and, in future versions, assessing their respective performance in real-time.\nData structures RedisAI provides the following data structures:\nTensor: Represents an n-dimensional array of values Model: Represents a computation graph by one of the supported DL/ML framework backends Script: Represents a TorchScript program DL/ML backends RedisAI supports DL/ML identifiers and their respective backend libraries, including:\nTF: The TensorFlow backend TFLITE: The TensorFlow Lite backend TORCH: The PyTorch backend ONNX: ONNXRuntime backend A complete list of supported backends is in the release notes for each version.\nBackend libraries are dynamically loaded as needed, but can also be loaded during booting or at runtime. Refer to these pages for more information on loading backends:\nAI.CONFIG command Backend configuration More info RedisAI commands RedisAI source ","categories":["Modules"]},{"uri":"/rs/references/","uriRel":"/rs/references/","title":"Reference","tags":[],"keywords":[],"description":"","content":"Redis clients Redis client libraries allow you to connect to Redis instances from within your application. This section provides an overview of several recommended Redis clients for popular programming and scripting languages.\nFor a list of available Redis clients sorted by language, see Clients (redis.io).\nCommand-line utilities Reference for Redis Enterprise Software command-line utilities, including rladmin, redis-cli, crdb-cli, and rlcheck.\nREST API Documents the REST API available to Redis Enterprise Software deployments.\nCompatibility with open source Redis Documents compatibility of open source Redis commands and configuration settings with Redis Enterprise Software and Redis Cloud.\nTerminology Explains terms used in Redis Enterprise Software documentation.\n","categories":["RS"]},{"uri":"/modules/redisgears/register-events/","uriRel":"/modules/redisgears/register-events/","title":"Register events","tags":[],"keywords":[],"description":"Register RedisGears functions to run when certain events occur in a Redis database.","content":"You can register RedisGears functions to run when certain events occur in a Redis database.\nRegister on events To register RedisGears functions to run on an event, your code needs to:\nPass KeysReader to a GearsBuilder object.\nCall the GearsBuilder.register() function.\nPass the eventTypes parameter to either:\nThe register function for Python.\nThe KeysReader object for Java.\nFor more information and examples of event registration, see:\nPython references:\nKeysReader\nGearsBuilder.register()\nJava references:\nKeysReader\nGearsBuilder.register()\nEvent types For the list of event types you can register on, see the Redis keyspace notification documentation.\nActive-Active event types In addition to standard Redis events, Redis Enterprise Active-Active databases also support the registration of RedisGears functions for the following event types:\nchange: This event occurs when a key changes on another replica of the Active-Active database. ","categories":["Modules"]},{"uri":"/rs/clusters/remove-node/","uriRel":"/rs/clusters/remove-node/","title":"Remove a cluster node","tags":[],"keywords":[],"description":"Remove a node from your Redis Enterprise cluster.","content":"There are various reasons why you may want to remove a node in Redis Enterprise Software:\nYou no longer need the extra capacity, meaning you want to permanently remove the node. You would like to replace a faulty node with a healthy node. You would like to replace a healthy node with a different node. The following section explains how each of these actions can be achieved, as well as their impact and considerations.\nYou can configure email alerts from the cluster to notify you of cluster changes, including when a node is removed.\nMake sure to read through these explanations thoroughly before taking any action.\nPermanently remove a node Permanently removing a node means you are decreasing cluster capacity. Before trying to remove a node, make sure that the cluster has enough capacity for all resources without that node, otherwise you cannot remove the node.\nIf there is not enough capacity in the cluster to facilitate removing the node, you can either delete databases or add another node instead of the one you would like to remove.\nDuring the removal process, the cluster migrates all resources from the node being removed to other nodes in the cluster. In order to ensure database connectivity, and database high availability (when replication is enabled), the cluster first creates replacement shards or endpoints on one of the other nodes in the cluster, initiates failover as needed, and only then removes the node.\nIf a cluster has only two nodes (which is not recommended for production deployments) and some databases have replication enabled, you cannot remove a node.\nReplace a faulty node If the cluster has a faulty node that you would like to replace, you only need to add a new node to the cluster. The cluster recognizes the existence of a faulty node and automatically replaces the faulty node with the new node.\nFor guidelines, refer to Replacing a faulty node.\nReplace a healthy node If you would like to replace a healthy node with a different node, you must first add the new node to the cluster, migrate all the resources from the node you would like to remove, and only then remove the node.\nFor further guidance, refer to adding a new node to a cluster.\nYou can migrate resources by using the rladmin command-line interface (CLI). For guidelines, refer to rladmin command-line interface (CLI).\nNote: The DNS records must be updated each time a node is added or replaced. Remove a node To remove a node using the admin console:\nClick Remove at the top of the Node page for the node to be removed. Approve the action. RS examines the node and the cluster and takes the actions required to remove the node. At any point, you can click the Abort button to stop the process. When aborted, the current internal action is completed, and then the process stops. Once the process finishes, the node is no longer shown in the UI. To remove a node using the REST API, use POST /v1/nodes/\u0026lt;node_id\u0026gt;/actions/remove with the following JSON data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header.\nFor example:\nPOST https://[host][:port]/v1/nodes/\u0026lt;node_id\u0026gt;/actions/remove \u0026#34;{}\u0026#34; Note: If you need to add a removed node back to the cluster, you must uninstall and reinstall the software on that node. ","categories":["RS"]},{"uri":"/rc/api/","uriRel":"/rc/api/","title":"Redis Enterprise Cloud REST API","tags":[],"keywords":[],"description":"Describes the Redis Cloud REST API and links to additional info","content":"The Redis Enterprise Cloud REST API helps you manage your Redis Cloud subscription programmatically.\nYou can use the API to:\nCreate or manage databases Define or change hosting credentials Audit access via logs Backup or import databases Note: The Redis Cloud REST API is available only with Flexible or Annual subscriptions. It is not supported for Fixed or Free subscriptions. Getting started Enable the API Authenticate and authorize Create API keys Use the API Learn the API lifecycle Create and manage subscriptions Examples Manage subscriptions Database examples Create database Update database Back up and import data Manage cloud accounts Estimate costs View account info More info Use the Redis Cloud API Full API Reference Secure authentication and authorization ","categories":["RC"]},{"uri":"/rs/references/client_references/client_ruby/","uriRel":"/rs/references/client_references/client_ruby/","title":"Redis with Ruby","tags":[],"keywords":[],"description":"The redis-rb client allows you to use Redis with Ruby.","content":"To use Redis with Ruby, you need a Ruby Redis client. The following sections demonstrate the use of redis-rb, a Ruby client library for Redis. Additional Ruby clients for Redis can be found under the Ruby section of the Redis Clients page.\nInstall redis-rb See redis-rb\u0026rsquo;s README file for installation instructions.\nMethod 1 Use gem to install redis-rb:\ngem install redis Method 2 Add the following line to your Gemfile:\ngem \u0026#39;redis\u0026#39; Then run:\nbundle install Connect to Redis The following code creates a connection to Redis using redis-rb:\nrequire \u0026#39;redis\u0026#39; redis = Redis.new ( :host =\u0026gt; \u0026#39;hostname\u0026#39;, :port =\u0026gt; port, :password =\u0026gt; \u0026#39;password\u0026#39;) To adapt this example to your code, replace the following values with your database\u0026rsquo;s values:\nIn line 4, set :host to your database\u0026rsquo;s hostname or IP address In line 5, set :port to your database\u0026rsquo;s port In line 6, set :password to your database\u0026rsquo;s password Example code for Redis commands Once connected to Redis, you can read and write data with Redis command functions.\nThe following code snippet assigns the value bar to the Redis key foo, reads it back, and prints it:\n# open a connection to Redis ... redis.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;); value = redis.get(\u0026#39;foo\u0026#39;); puts value Example output:\n$ ruby example_redis-rb.rb bar SSL The redis-rb client does not support SSL connections natively.\nFor an added security measure, you can secure the connection using stunnel or this redis-rb fork, which includes SSL support.\n","categories":["RS"]},{"uri":"/rc/security/vpc-peering/","uriRel":"/rc/security/vpc-peering/","title":"Enable VPC peering","tags":[],"keywords":[],"description":"VPC peering uses private IP addresses to route traffic between a Redis Cloud VPC and an application VPC.","content":"VPC peering uses private IP addresses to allow network connections between two virtual private clouds (VPCs).\nYou can connect your VPC in the Redis Cloud subscription to the VPC of your application. This lets your application connect securely to your Redis Cloud database using VPC peering to optimize the performance of your application.\nNote: VPC peering is available only with Flexible or Annual subscriptions. It is not supported for Fixed or Free subscriptions. VPC peering configuration requires you to initiate VPC peering on your Redis Cloud subscription and then accept the VPC peering request for the AWS VPC that you want to peer with.\nAWS VPC peering If you want to peer a Redis Cloud VPC with an AWS VPC, you need to:\nConfigure and initiate VPC peering for your Redis Cloud subscription.\nApprove the VPC peering request.\nUpdate the routes tables.\nConfigure VPC peering To set up VPC peering:\nSelect Subscriptions from the admin console menu and then select your subscription from the list.\nSelect the Connectivity tab and then VPC Peering.\nSelect the Add peering button:\nEnter VPC peering details:\nSetting name Description Consumer AWS Account Your AWS account ID (see Finding your AWS account ID) Consumer region AWS VPC region Consumer VPC ID The VPC ID for the application that needs to access your Redis Cloud VPC (see Finding a VPC ID) Consumer VPC CIDRs CIDR-formatted IP addresses for the AWS VPC that needs to access your Redis Cloud VPC; must not overlap with the Redis producer VPC CIDR (see View your VPCs) You can provide up to five VPC CIDRs.\nNote: The Redis Cloud Terraform provider currently supports one VPC CIDR. Additional CIDRs defined by the console will be removed by Terraform. To add multiple VPC CIDRs:\nSelect the Add CIDR button:\nEnter the additional CIDR-formatted IP address in the box:\nSelect the Initiate peering button:\nNote the Peering ID of the VPC peering request:\nApprove VPC peering request After you set up and intitiate VPC peering, you need to approve the VPC peering request:\nFollow the AWS guide to accept the VPC peering connection.\nAfter you accept the peering request, select Modify my route tables now.\nUpdate route tables To finish VPC peering setup, update your route tables for the peering connection with the following details:\nIn the Destination field, enter the Requester VPC CIDRs shown when you accepted the peering request.\nThis is the Redis Cloud VPC CIDR address, to which your application\u0026rsquo;s VPC connects.\nIn the Target field, select Peering Connection and select the relevant Peering ID.\nOnce VPC peering is established, we recommend switching your application connection string to the private endpoint.\nGCP VPC peering If you want to peer a Redis Cloud VPC with a GCP VPC, you need to:\nConfigure and initiate VPC peering for your Redis Cloud subscription.\nApprove the VPC peering request.\nConfigure VPC peering To set up VPC peering:\nSelect Subscriptions from the admin console menu and then select your subscription from the list.\nSelect the Connectivity tab and then select VPC Peering.\nSelect the Add peering button:\nEnter the VPC peering details:\nSetting name Description Project ID GCP project ID (see Identifying projects) Network name GCP VPC network you want to peer with (see View networks) Copy the Google cloud command after you enter the other VPC peering settings. You need this command to accept the peering request later:\nSelect the Initiate peering button:\nNote the Cloud peering ID of the VPC peering request:\nApprove VPC peering request To approve the VPC peering request between Redis Cloud and GCP, use the gcloud CLI to run the Google cloud command that you copied before you initiated VPC peering.\nOnce VPC peering is established, we recommend switching your application connection string to the private endpoint.\n","categories":["RC"]},{"uri":"/rs/release-notes/rs-5-6-0-april-2020/","uriRel":"/rs/release-notes/rs-5-6-0-april-2020/","title":"Redis Enterprise Software Release Notes 5.6.0 (April 2020)","tags":[],"keywords":[],"description":"Install improvements for RHEL 6 and 7.  Active-Active support for HyperLogLog.  Redis on Flash now supports RedisJSON.  Active-Active default changes for high availability and OSS Cluster API support.  Backup support for Google Cloud Storage and Azure Blob storage.","content":"Redis Enterprise Software (RS) 5.6.0 is now available. This major version release includes:\nImproved installation process to be customizable Support for the HyperLogLog data type in Active-Active databases RedisJSON support in Redis on Flash databases Support for Redis OSS Cluster API for Active-Active and Replica Of databases Replica HA enabled by default for Active-Active databases Cloud backup locations support Support for Red Hat Enterprise Linux version 7.7, 7.8 Additional enhancements, and minor bug fixes Information Upgrade Follow these instructions for upgrading to RS 5.6.0 from RS 5.0.2 and above. For Active-Active databases, you must upgrade the feature set version. End of life End of Life (EOL) for Redis Enterprise Software 5.6 and previous RS versions can be found here. EOL for Redis Modules can be found here. Support of Red Hat Enterprise Linux 6 and Oracle Linux 6 operating systems platforms will end on November 30, 2020. Support of OpenStack Object Storage (\u0026ldquo;Swift\u0026rdquo;) for backup, import and export location will end on November 30, 2020. New features Redis Software installer Redis Enterprise Software installer adds the ability to specify custom installation paths and a custom installation user, group, or both on RHEL versions 6 and 7.\nWhen you run the installer you can specify the installation, the configuration and the var as well as the user and the group:\nsudo ./install.sh --install-dir \u0026lt;path\u0026gt; --config-dir \u0026lt;path\u0026gt; --var-dir \u0026lt;path\u0026gt; --os-user \u0026lt;user\u0026gt; --os-group \u0026lt;group\u0026gt; For more information, check out the Redis Enterprise Software installer documentation.\nHyperloglog on Active-Active RS 5.6.0 adds the support of HyperLogLog data structure for Active-Active Redis databases.\nBecause HyperLogLog is a counting data structure by nature, conflicts can occur when deleting entries. For efficiency, performance and memory considerations, conflicts between instances are resolved with DEL (delete) operations winning over ADD operations that took place in concurrent or before the DEL operation.\nFor more information, check out the HyperLogLog on Active-Active documentation.\nRedisJSON on Redis on Flash RS 5.6.0 adds support for Redis on Flash (RoF) databases with Redis Modules. RedisJSON is the first module to be available to run on a Redis on Flash database.\nUsing RedisJSON on Redis on Flash allows you to benefit from high performance redis with high data volume at lower costs.\nOSS Cluster API support For more information, check out the OSS Cluster API documentation.\nSupport for Active-Active and Replica Of databases\nYou can configure Active-Active databases to use the OSS Cluster API using the admin console. The OSS Cluster API improves the performance of user operations against your database.\nYou can also create or modify an Active-Active Redis database in OSS Cluster mode using the crdb-cli tool with the --oss-cluster option to apply the changes to all of the instances.\nCreate and edit database using the admin console\nYou can configure OSS Cluster API for databases using the admin console.\nFor Active-Active databases, you can create the database with OSS Cluster API enabled for all of its instances. When you enable OSS Cluster after the Active-Active database is created, the change applies only to the local instance.\nReplica HA defaults for Active-Active To enhance the availability and the consistency of Active-Active geo-distributed Redis, Replica HA is enabled by default for all existing and new Active-Active Redis Databases.\nTo disable Replica HA for for a local instance of an Active-Active database, run this command on the instance: rladmin tune db \u0026lt;bdb_uid\u0026gt; slave_ha disabled\nCloud backup locations support Redis Enterprise 5.6.0 adds the ability to configure Azure Blob Storage and Google Cloud Storage as backup, import, and export locations.\nRed Hat Enterprise Linux 7.7, 7.8 Redis Enterprise 5.6.0 adds RHEL 7.7, 7.8 to its supported platforms.\nRedis modules The following GA releases of Redis Modules are bundled in RS 5.6.0:\nRedisBloom, version 2.2.1 RedisGraph, version 2.0.10 RedisJson, version 1.0.4 RediSearch, version 1.6.11 (updated) RedisTimeSeries, version 1.2.3 Additional capabilities OSS Redis version 5.0.8 is merged into RS 5.6.0. Starting RS 5.6.0, to upgrade modules during database upgrade you must use the module_args option instead of keep_module_args or or “ ” (for no arguments). The module_args option must follow with one of the following (Always in quotation marks): “keep_args” or “ “ or “”. For more info and examples, check out upgrading Redis Modules documentation . rladmin adds the ability to demote the cluster master node when setting it to maintenance mode by using the demote_node option: rladmin node \u0026lt;node_uid\u0026gt; maintenance_mode on demote_node The SENTINEL MASTER command output format was updated to be aligned with OSS equivalent output. Important fixes RS38315, RS33747 - Added the LUA script name (SHA) and its arguments to the warning message indicating the LUA script has been running for more than 5 seconds. RS38706 - Fixed a bug which caused a stuck state machine in some scenarios of restoring DB from RDB. RS34309 - Improved internal passwords handling. RS38498 - Fixed a bug in the upgrade process of a database to avoid failure when saving large RDB (Redis Database Backup) file. RS38706 - Fixed a bug that caused a stuck state machine after restoring a snapshot of an Active-Active database. RS43996 - Fixed a bug in the upgrade process when using ./install.sh -y and firewalld is not running. RS45777 - Fixed a bug that caused clients on a shared connection to the proxy to get disonnected. Disconnections occured in case a response for a request of an already disconnected client was received. With build 5.6.0-39: RS45627, RS47382 - Fixed bugs causing clients to disconnect when using XREAD and XREADGROUP commands in blocking mode on other clients’ connections. RS44656 - Fixed a bug causing TLS mode for clients connections to toggle between ‘all communication’ to ‘for crdb communication only’ when performing a global configuration change. Known limitations Active-Active databases RS51359 - Active-Active databases, using replication and Append Only File (AOF) for database persistence, are suffering from memory leaks on replica shards, causing them to grow bigger than the master shards. Customers are advised to upgrade to RS 6.0.12. Meanwhile, you can use a snapshot for database persistence or restart the replica shards. Upgrade notes RS 5.4.2 introduced new Active-Active Redis Database capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis Database is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the Active-Active Redis Database protocol. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or lower to version RS 5.4.4 or higher: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Support. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Google Chrome browser on macOS Catalina requires a self-signed certificate generated after June 2019 to include the extendedKeyUsage field in order to connect to the RS admin console. If you use a self-signed certificate that does not include this field, update the self-signed certificate. Modules upgrade We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. Before you upgrade a database with RediSearch Module to Redis 5.0, you must upgrade the RediSearch Module to version 1.4.2 or above. Cluster API The API for removing a node is updated in RS 5.4.2 or higher. The API call must include json data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -i -k -u user@redislabs.com:password https://localhost:9443/v1/nodes/3/actions/remove --data \u0026#34;{}\u0026#34; Discover service For Redis Sentinel (Discovery Service), every database name must be unique across the cluster. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4 and above Starting from RS 5.4.2 and after upgrading the CRDB, TYPE commands for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-14-february-2020/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-14-february-2020/","title":"Redis Enterprise Software Release Notes 5.4.14 (February 2020)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4.14 is now available. This release bundles OSS Redis 5.0.7 and includes new Redis Modules versions, several enhancements, and bug fixes.\nOverview Follow these instructions for upgrading to RS 5.4.14 from RS 5.0.2 and above.\nNew features Version 5.0.7 of OSS Redis is merged into RS 5.4.14. The following GA releases of Redis Modules are bundled in RS 5.4.14: RedisBloom, version 2.2.1 (updated, release notes) RedisGraph, version 2.0.1 (updated, release notes) RedisJSON, version 1.0.4 (update, release notes) RediSearch, version 1.4.25 (updated, release notes) RedisTimeSeries, version 1.2.3 (updated, release notes) Additional capabilities Added the ability to retrieve license details with a REST API command. Now you can get your license details from the admin console (settings \u0026gt; general) or from the REST API command:\nGET https://localhost:9443/v1/license\nThe REST API response includes:\nlicense - The cluster’s name (FQDN) in the key string expired - If the cluster key is expired (True or False) activation_date - The date of the cluster key activation expiration_date - The date of the cluster key expiration shards_limit - The number of shards allowed by the cluster key Added Flush command in the UI console for Active-Active databases. The command flushes the data from all participating clusters.\nUpdated rladmin upgrade module command so module arguments are optional. When you upgrade the module for a database, you can either:\nSpecify the module arguments to replace the existing arguments Specify the keep_module_args flag to use the existing arguments Examples:\nTo upgrade the version of RediSearch to 10017 and replace the module arguments:\nrladmin upgrade module db_name MyAwesomeDB module_name ft version 10017 module_args \u0026quot;PARTITIONS AUTO\u0026quot;\nTo upgrade RedisBloom to version 10100 and remove the current module arguments:\nrladmin upgrade module db_name MyDB module_name bf version 10100 module_args \u0026quot; \u0026quot;\nTo upgrade RedisJSON to 10002 and use the current module arguments:\nrladmin upgrade module db_name MyDB module_name ReJSON version 10002 keep_module_args\nUpgraded js-yaml version to 3.13.1, and lodash version to 4.17.15 (RS31819)\nAdded an alert for actions that run for an extended period of time. For example, a notification is sent when an action like updating database property runs for more than a configurable threshold. Default value is set to 24 hours.\nInformation End of Life (EOL) for Redis Enterprise Software 5.4, as well as for Redis Modules and previous RS versions, can be found here. Google Chrome browser on macOS Catalina requires self-signed certificate generated after June 2019 to include the extendedKeyUsage field in order to connect to the RS admin console. If you use a self-signed certificate that does not include this field, update the self-signed certificate. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or lower to version RS 5.4.4 or higher: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Important fixes RS23396 - Improved the disconnection mechanism in case of inactive UI session RS27924 - Added descriptive error messages in the UI console when validating the license RS29968 - Improved internal mechanism to better support high scale of clients connections RS35675 - Updated the upgrade process of Active-Active Redis databases to save causal consistency and encryption flags RS36922 - Fixed an issue in a specific cluster upgrade scenario from versions earlier than RS 5.4.0 to RS 5.4.0 or higher Known limitations Upgrade When you upgrade a cluster from version 5.0.2-20 to version 5.4.14, you must first upgrade to version 5.2.2 and then to version 5.4.14. RS 5.4.2 introduced new Active-Active Redis (CRDB) capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis (CRDB) is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the CRDB protocol. Before you upgrade a database with RediSearch Module to Redis 5.0, you must upgrade the RediSearch Module to version 1.4.2 or above. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Redis support. We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Cluster API The API for removing a node is updated in RS 5.4.2 or higher. The API call must include json data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; -i -k -u user@redislabs.com:password https://localhost:9443/v1/nodes/3/actions/remove --data \u0026quot;{}\u0026quot;\nDiscovery service For Redis Sentinel (Discovery Service), every database name must be unique across the cluster. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4 and above Starting from RS 5.4.2 and after upgrading the CRDB, TYPE command for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-10-december-2019/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-10-december-2019/","title":"Redis Enterprise Software Release Notes 5.4.10 (December 2019)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4.10 is now available. This release includes an improved synchronization mechanism for Active-Active Redis and Replica-of, several enhancements, and bug fixes.\nOverview Follow these instructions for upgrading to RS 5.4.10 from RS 5.0 and above.\nNew features Synchronization mechanism in Active-Active Redis and Replica-of RS 5.4.10 incorporates the improved Redis synchronization mechanism (PSYNC2) for Active-Active Redis (CRDB) and Replica-of.\nAs a result, failure scenarios in any A-A replica (and the source database of Replica-of), require only partial synchronization between the cross-region replicas instead of full synchronization that can be costly in time and bandwidth.\nRS on RHEL 7 supports OpenSSL 1.0.2 and up To keep RS secure and keep our internal libraries up-to-date, starting from RS 5.4.10 our RHEL 7 installations require a minimum of OpenSSL 1.0.2.\nWhen you install or upgrade RS 5.4.10 on RHEL 7 with older version of OpenSLL, the installation fails with the error:\nError: Package: redislabs-5.4.10-1.rhel7.x86_64 (/redislabs-5.4.10-1.rhel7.x86_64) Requires: libcrypto.so.10(OPENSSL_1.0.2)(64bit) If you see this error, upgrade to OpenSSL 1.0.2 or higher before you install RS.\nAdditional capabilities The following new GA releases of Redis Modules are bundled in RS 5.4.10:\nRedisBloom, version 2.0.3 RedisSearch, version 1.4.18 RedisJson, version 1.0.4 RedisGraph, version 1.2.2 RedisTimeSeries, version 1.0.3 Version 5.0.5 of Redis OSS, along with a fix to a corruption related to the HyperLogLog (that is part of Redis 5.0.6) are merged into RS 5.4.10.\nUsing REST API, you can retrieve various license details such as activation date, expiration date,and the number of licensed shards. To get these details, run:\ncurl -v -u \u0026lt;user\u0026gt;:\u0026lt;password\u0026gt; https://localhost:9443/v1/license\nUpdated PDNS version from 4.1.5 to 4.1.13.\nInformation End of Life (EOL) for Redis Enterprise Software 5.4, as well as for Redis Modules and previous RS versions, can be found here. Google Chrome browser on macOS Catalina requires self-signed certificate generated after June 2019 to include the extendedKeyUsage field in order to connect to the RS admin console. If you use a self-signed certificate that does not include this field, update the self-signed certificate. When you upgrade an Active-Active Redis with active AOF from version RS 5.4.2 or lower to version RS 5.4.4 or higher: If replication is enabled, you must run the BGREWRITEAOF command on all replica shards after the upgrade. If replication is not enabled, you must run the BGREWRITEAOF command on all shards after the upgrade. Important fixes The titles of the ‘rladmin status nodes’ command output were updated from ‘RAM’ to ‘FREE_RAM’ (the amount of RAM in the node that is currently not used) and from ‘AVAILABLE_RAM’ to ‘PROVISIONAL_RAM’ (the amount of RAM in the node that can be provisioned). RS31492 - Upgraded dependent libraries: python-cryptography to version 2.7; NGINX to version 1.16.0; PyYaml to version 5.1.2; python-requests to version 2.22.0; urllib3 to version 1.25.3 RS31187 - Upgraded the internal Python interpreter to version 2.7.16 RS33042 - Fixed Support Package to contain complete SLOWLOG information RS32699 - Avoided unnecessary restart and failover of Redis processes when Active-Active database is upgraded RS32061 - Improved support of the Redis WAIT command RS31759 - Fixed failure during database import RS31747 - Fixed failure in upgrade from version 5.0.0-31 to 5.4.6-11 RS30063 - Fixed the upgrade process when WatchdogAPI fails to bind to its port RS31477 - Fixed wrong calculation of node’s ‘AVAILABLE_RAM’ (‘PROVISIONAL_RAM’) as displayed the output of ‘rladmin status nodes’ command RS30165 - Fixed failover scenario that did not take place during node restart RS29250 - REST API documentation was updated to include the SFTP and Mount Point backup/export options RS27327 - Improved the backup timing when using the database parameter of ‘backup_interval_offset’ through the REST API RS33883 - HCSAN command in Active-Active Redis updated to return Integer instead of a String. Fixed a limitation so Redis 5 and Redis 4 can be selected as the Redis version to use CRDB and RoF Known limitations Upgrade RS 5.4.2 introduced new Active-Active Redis (CRDB) capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis (CRDB) is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the CRDB protocol. Before you upgrade a database with RediSearch Module to Redis 5.0, you must upgrade the RediSearch Module to version 1.4.2 or above. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Redis support. We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Cluster API The API for removing a node is updated in RS 5.4.2 or higher. The API call must include json data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; -i -k -u user@redislabs.com:password https://localhost:9443/v1/nodes/3/actions/remove --data \u0026quot;{}\u0026quot;\nDiscovery service For Redis Sentinel (Discovery Service), every database name must be unique across the cluster. Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4 and above Starting from RS 5.4.2 and after upgrading the CRDB, TYPE command for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-5-4-14-2/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-5-4-14-2/","title":"Redis Enterprise for Kubernetes Release Notes 5.4.14-2 (March 2020)","tags":[],"keywords":[],"description":"Support for Redis Enterprise Software 5.4.14, K8s 1.16, and OpenShift 4.3.","content":"Overview The Redis Enterprise K8s 5.4.14-2 release is a maintenance release providing support for the latest Redis Enterprise Software release 5.4.14 and includes bug fixes as well as the following notable changes:\nSupport for K8s 1.16 This release now correctly handles the API deprecations, and API version changes that K8s release 1.16 introduced.\nSupport for OpenShift 4.3; improved OLM support The release now supports OpenShift 4.3. A new operator version is now available in OpenShift\u0026rsquo;s Operator Hub and now includes a more comprehensive base template as well as references to documentation and support.\nChanges to the upgrade process When the Operator is upgraded to a new release, it now prevents the Redis Enterprise Cluster nodes from being automatically upgraded, unless autoUpgrade is enabled or the RS image version is explicitly updated in the Redis Enterprise Cluster (REC) spec. The change was introduced to avoid situations where an Operator upgrade initiated a rolling update to the cluster nodes\u0026rsquo; StatefulSet.\nDeprecated support for K8s versions 1.9/1.10 This release deprecates support for K8s version 1.9/1.10 and OpenShift 3.9. If you are currently using these releases please contact Redis support for information about migrating to a new K8s release.\nComing soon The Database Custom Resource(CR), which represents Redis Enterprise databases, is in development and planned to be part of an upcoming release. The Database Controller for the Database CR is part of the Redis Enterprise K8s Operator. It is disabled in this release. The next release is planned to support K8s 1.17 and drop support for K8s versions 1.9/1.10. ","categories":["Platforms"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-6-july-2019/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-6-july-2019/","title":"Redis Enterprise Software Release Notes 5.4.6 (July 2019)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4.6 is now available. This release includes the latest version of Redis 5 (5.0.5), bundles the GA release of the new RedisTimeSeries module, and adds other enhancements and bug fixes.\nOverview Follow these instructions for upgrading to RS 5.4.6 from RS 5.0 and above. If you have a version older than 5.0, you must first upgrade to version 5.2 (or at least 5.0).\nNew features Time series New GA release of the RedisTimeSeries (version v1.0.0) module is bundled in RS 5.4.6.\nModules versions Updated versions of GA modules:\nRedisTimeSeries - v1.0.0 (new module) RedisBloom - v2.0.0 (version update) RediSearch - v1.4.11 (version update) RedisGraph - v1.2.2 (no change) RedisJSON - v1.0.3 (no change) Additional capabilities Latest version of Redis 5 (5.0.5) was merged into RS 5.4.6. If a user subscribes to a channel during recovery of a Active-Active Redis (CRDB) from an Append Only File (AOF), the user receives only the new messages. (Fixes known limitation from RS 5.4.4) CROSSSLOT behavior now matches Redis OSS behavior for accessing multiple keys in a single command. Now, when you use CRC16 hash function (default), you can pass a command for different keys/tags that are mapped to the same slot. (RS23189) Cluster upgrade process was improved so that the duration of the cluster upgrade process reduced. Information End of Life (EOL) for Redis Enterprise Software 5.4, as well as for Redis Modules and previous RS versions, can be found here. Important fixes RS28679 - Syncer and peers related statistics for Active-Active Redis (CRDB), which are already available with the REST API, are exposed through Prometheus. RS28946 - Metrics exporter related minor memory leak fix. RS26312 - Ports handling during installation fix. RS31703 - Fixed performance issue with MSET command. Known limitations Upgrade RS 5.4.2 introduced new Active-Active Redis (CRDB) capabilities that improve its compatibility with open source Redis. Now the string data-type in Active-Active Redis (CRDB) is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the CRDB protocol. Before you upgrade a database with RediSearch Module to Redis 5.0, you must upgrade the RediSearch Module to version 1.4.2 or above. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Redis support. We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Cluster API The API for removing a node is updated in RS 5.4.2 or higher. The API call must include json data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -i -k -u user@redislabs.com:passsword https://localhost:9443/v1/nodes/3/actions/remove --data \u0026#34;{}\u0026#34; Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4 and above Starting from RS 5.4.2 and after upgrading the CRDB, TYPE command for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/k8s-5-4-10-8/","uriRel":"/kubernetes/release-notes/previous-releases/k8s-5-4-10-8/","title":"Redis Enterprise for Kubernetes Release Notes 5.4.10-8 (January 2020)","tags":[],"keywords":[],"description":"Support for the Redis Enterprise Software 5.4.10 and multiple enhancements.","content":"Redis Enterprise for Kubernetes 5.4.10-8 is now available. This release includes Redis Enterprise (RS) version 5.4.10-22 and introduces new feature and bug fixes.\nOverview This is a maintenance release providing support for the Redis Enterprise Software release 5.4.10 and includes multiple enhancements.\nFollow these instructions for upgrading to this Kubernetes operator release.\nNew features All-in-one deployment bundle and documentation enhancements - This release is the easiest to deploy yet, with a new quick start guide and an all-in-one file bundle for deploying the Redis Enterprise Operator. GitHub documentation was enhanced to cover advanced deployment scenarios with the complete reference to the custom resource specifications, guides, and examples.\nNote: Pay special attention to the yaml file naming changes and new yaml files that have been created for this release. These are highlighted in the quick start guide. Rack Awareness - Support for the [Redis Software Rack Awareness][/rs/clusters/configure/rack-zone-awareness/] feature was introduced to the Kubernetes deployment. It enables deploying nodes to different zones, in a multi-zone Kubernetes cluster. Databases that are rack-aware will have the cluster populate their master shard and replica shards in different nodes, across different zones or failure domains. This enables maintaining data persistence in case of zone failure.\nOLM Support - Redis Enterprise for Kubernetes is now RedHat OpenShift certified and can be effortlessly configured and deployed in Kubernetes clusters supporting OLM, including OpenShift 4.x clusters, with just a few clicks. OLM based deployments do not require Kubernetes cluster administrator rights to deploy the operator.\nImprove cluster nodes\u0026rsquo; pod scheduling resiliency - Redis Enterprise Cluster pod scheduling is hardened by implementing Kubernetes best practices and providing configuration recommendations to cluster operators. Scheduling resiliency minimizes the chance of cluster node pods eviction or failure to schedule. See the top 4 articles in the new Additonal Topics documentation section.\nUpdate app.redislabs.com API version to stable - We\u0026rsquo;ve updated the Redis Enterprise Cluster custom resource API from alpha to stable to reflect the current state of maturity of our implementation.\nBoth versions of the API are supported by Kubernetes versions that support specifying multiple API versions.\nFor legacy Kubernetes versions, deployment files are available in the documentation repository that utilize the alpha version of the API.\nInformation This new release deprecates the Redis Enterprise Operator support for legacy Kubernetes version 1.8. Known limitations Custom Resource Definition Changes - In order to comply with OLM best practices, a change was required to the Redis Enterprise Cluster Custom Resource Defintion (CRD) to introduce the status sub-resource. As a result of this change, any upgrade from an older release to this release requires a change to the CRD. Once the CRD is updated, all Redis Enterprise Clusters in the same K8s cluster must be updated to the latest release in order to continue operating properly.\nCluster Recovery - ","categories":["Platforms"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-4-june-2019/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-4-june-2019/","title":"Redis Enterprise Software Release Notes 5.4.4 (June 2019)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4.4 is now available. This release enables the functionality of Active-Active Redis (CRDB) combined with RoF (Redis on Flash), supports the creation of Redis databases with multiple modules, and adds other enhancements and bug fixes.\nOverview Follow these instructions for upgrading to RS 5.4.4 from RS 5.0 and above. If you have a version older than 5.0, you must first upgrade to version 5.2 (or at least 5.0).\nNew features Active-Active Redis with RoF RS 5.4.4 lets you create Active-Active Redis databases (CRDBs) with Redis on Flash (RoF) and get the benefits of geo-distributed Redis databases along with the significant cost savings of using Redis on Flash.\nNote: You must select Redis 5 as the Redis version to use CRDB and RoF. Support for multiple modules RS 5.4.4 supports the creation and management of Redis databases with a combination of any of the following GA modules:\nRediSearch (GA) RedisGraph (GA) RedisBloom (GA) RedisJSON (GA) To upgrade databases with multiple modules, you can use the rladmin upgrade command.\nNote: The syntax for the rladmin upgrade command now requires that the module_args parameter be written inside quotation marks. To upgrade only the modules within an existing database, for example with RedisBloom (bf) and RedisJSON:\nrladmin upgrade module db_name db1 module_name bf version 10003 module_args \u0026#34;\u0026#34; module_name ReJSON version 10001 module_args \u0026#34;\u0026#34; To upgrade a database with its modules, for example db1 with RediSearch and RedisGraph:\nrladmin upgrade db db1 and module module_name redisearch version 103 module_args \u0026#34;ON_TIMEOUT FAIL NOGC\u0026#34; and module module_name graph version 10016 module_args \u0026#34;\u0026#34; You can also upgrade the modules with the REST API.\nTo upgrade multiple modules, enter the details of each module in the modules parameter:\ncurl -X POST -u \u0026#34;demo@redislabs.com:password\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;modules\u0026#34;:[{\u0026#34;module_name\u0026#34;: \u0026#34;ReJSON\u0026#34;, \u0026#34;current_module\u0026#34;: \u0026#34;\u0026lt;module_uid\u0026gt;\u0026#34;, \u0026#34;new_module\u0026#34;: \u0026#34;\u0026lt;module_uid\u0026gt;\u0026#34;, \u0026#34;new_module_args\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;current_semantic_version\u0026#34;:\u0026#34;1.0.4\u0026#34;}, {\u0026#34;module_name\u0026#34;:\u0026#34;ft\u0026#34;,\u0026#34;current_module\u0026#34;:\u0026#34;\u0026lt;module_uid\u0026gt;\u0026#34;,\u0026#34;new_module\u0026#34;: \u0026#34;\u0026lt;module_uid\u0026gt;\u0026#34;,\u0026#34;current_semantic_version\u0026#34;:\u0026#34;1.4.3\u0026#34;, \u0026#34;new_module_args\u0026#34;:\u0026#34;PARTITIONS AUTO\u0026#34;}], \u0026#34;force_restart\u0026#34;:true}\u0026#39; https://127.0.0.1:9443/v1/bdbs/2/modules/upgrade To upgrade a single module, you can either:\nEnter the details of the module in the modules parameter Enter the details of the module without the modules parameter, as in previous releases Additional capabilities Support for persistence of AOF every 1 second in Active-Active Redis (CRDB), in addition to AOF every write. We recommend that you use AOF every 1 second for the best performance during the initial CRDB sync of a new replica. NGINX version updated to 1.14.2 Information Stay up to date with the End of Life (EOL) Policy for RS 5.4 and all previous RS versions. Important fixes RS26508 - Fixed redis-cli failure for a specific CRDB data type. RS29191 - Fixed a failure when removing a replica from a CRDB. RS29097 - Fixed a misconfiguration when using SFTP backup and encounter a node failure. RS28286 - Updated SETEX and PSETEX commands output of CRDB to match Redis outputs. RS26984 - Fixed metrics_exporter reports for node and shard level metrics. RS19854 - Fixed uploading a Redis Module so you can upload a Redis Module when the admin console is connected to any node. RS29238 - Improved the compression performance in CRDB. Known limitations Upgrade RS 5.4.2 introduced new Active-Active Redis (CRDB) capabilities that improve its compatibility with open source Redis. Now the string data-type in CRDB is implicitly and dynamically typed, just like open source Redis. To use the new capabilities on nodes that are upgraded from version RS 5.4.2 or lower, you must upgrade the CRDB protocol. Before you upgrade a database with RediSearch Module to Redis 5.0, you must upgrade the RediSearch Module to version 1.4.2 or above. Node upgrade fails if the SSL certificates were configured in version 5.0.2 or above by manually updating the certificates on the disk instead of updating them through the API. For assistance with this issue, contact Redis support. We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. Starting from RS 5.4.2, to preserve the current Redis major.minor version during database upgrade you must use the keep_redis_version option instead of keep_current_version. Subscriptions If a user subscribes to a channel during recovery of a CRDB from AOF, the user receives old messages that are stored in the AOF file. Cluster API The API for removing a node is updated in RS 5.4.2 or higher. The API call must include json data and the \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -i -k -u user@redislabs.com:passsword https://localhost:9443/v1/nodes/3/actions/remove --data \u0026#34;{}\u0026#34; Redis commands The capability of disabling specific Redis commands does not work on commands specific to Redis Modules. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4.x Starting from RS 5.4.2 and after upgrading the CRDB, TYPE command for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-2-april-2019/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-2-april-2019/","title":"Redis Enterprise Software Release Notes 5.4.2 (April 2019)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4.2 is now available. This release improves the compatibility of Active-Active Redis (CRDB) with open source Redis, adds SFTP and Mount Points as backup destinations, email alerting and a number of other enhancements and bug fixes.\nOverview Follow these instructions for upgrading to RS 5.4.2 from RS 5.0 and above. If you have a version older than 5.0, you must first upgrade to version 5.2 (or at least 5.0).\nNew features Active-Active Redis (CRDB) RS 5.4.2 introduces new Active-Active Redis (CRDB) capabilities which improve its compatibility with open source Redis. This simplifies the progress of migration from open source Redis to Active-Active Redis:\nThe string data-type in CRDB is now implicitly and dynamically typed, just like open source Redis. Geospatial data is now supported in CRDB. These changes required upgrading the internal structures of CRDB and the communication protocol between the CRDB replicas. To take advantage of these updates with existing CRDBs you must upgrade the CRDB protocol.\nNew backup destinations - SFTP and mount point The storage options for periodic database backup, data export, and data import are extended to include:\nSFTP - a secured and well accepted protocol A local mount path that can point to network storage These new options are in addition to the existing backup locations of FTP, AWS S3 and OpenStack Swift.\nEmail alerts per database (per team member) Improved distribution of email alerts for cluster or database alerts. The cluster administrator can define, for each team member, the specific databases to receive alerts for, and whether to get the cluster alerts.\nOptional client authentication (TLS) You can now fine tune the TLS configuration, and ease certificates management by excluding client authentication enforcement, so that the database clients, such as applications or other clusters, can connect to your database without authentication.\nNode maintenance mode When you do hardware or operating system maintenance on a server that hosts an Redis Enterprise node, it is important that you move all of the shards on that node to another node to protect the data. Starting with RS 5.4.2 you can use maintenance mode to handle this process simply and efficiently.\nAdditional capabilities Support for open source Redis version 5.0.4\nViewer roles were further limited to improve security compliance. The DB Viewer and Cluster Viewer roles cannot view the Redis password of a database. The DB Viewer role also cannot view the ‘log’ screen.\nNote: These improvements are also included in the REST API and are breaking-changes. If you use the relevant API requests with a DB Viewer or Cluster Viewer role, make sure that you update it accordingly. External user credentials for periodic backup destinations are now hidden. In REST API responses the password is hashed and in the admin console the password is displayed as asterisks.\nInformation For Redis Enterprise Software 5.4 End of Life (EOL), as well as for previous Redis Enterprise Software versions, see product lifecycle. Important fixes RS26985 - Fixed a failure of a DB with all-nodes policy, when a node is rebooted RS26433 - Fixed a scenario in which the CRDB sync stopped on one of the participating clusters RS25968 - Fixed a redis crash on RoF cluster RS25835 - Fixed a failure in periodic backup when AOF rewrite is in progress on the same shard RS25558 - Enabled updating absolute path in periodic backup FTP setup RS25381 - Fixed a race condition while updating a database parameter RS24188 - Fixed a failure during database upgrade RS23508 - Fixed a failure in ‘rladmin status’ command RS22485 - Fixed irregularities in service after increasing the size of a database which uses OSS cluster API Known limitations Upgrade Before you upgrade a database with RediSearch Module to Redis 5.0, the RediSearch Module must be upgraded to version 1.4.2 or above. Node upgrade fails if the SSL certificates were configured in version 5.0.2 and above by updating the certificates on the disk instead of using the new API. For assistance with this issue, see SSL/TLS certificate updates in our documentation or contact Redis support. We recommend that you test module upgrade commands in a test environment before you upgrade modules in a production environment. The module upgrade arguments are not validated during the upgrade process and incorrect arguments can cause unexpected downtime. The rladmin option to use to preserve the current Redis major.minor version during database upgrade is now keep_redis_version, instead of keep_current_version. Cluster API The API for node removal was updated. It must include json data, and \u0026ldquo;Content-Type: application/json\u0026rdquo; header. For example:\ncurl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -i -k -u user@redislabs.com:passsword https://localhost:9443/v1/nodes/3/actions/remove --data \u0026#34;{}\u0026#34; Redis commands The capability of disabling specific Redis commands does not work on Redis Module specific commands. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. CLIENT UNBLOCK command is not supported in RS 5.4 and RS 5.4.2 Starting from RS 5.4.2 and after upgrading the CRDB, TYPE command for string data-type in CRDBs return \u0026ldquo;string\u0026rdquo; (OSS Redis standard). ","categories":["RS"]},{"uri":"/rs/release-notes/rs-5-5-preview-april-2019/","uriRel":"/rs/release-notes/rs-5-5-preview-april-2019/","title":"Redis Enterprise Software Release Notes 5.5 Preview (April 2019)","tags":[],"keywords":[],"description":"Preview release.  Databases support multiple modules.","content":"Redis Enterprise Software (RS) 5.5 Preview Edition is now available.\nRS 5.5 is a preview version that includes all the capabilities of Redis Enterprise 5.4, plus support for creation of Redis databases with multiple modules and support for these modules:\nRediSearch (GA) RedisGraph (GA) RedisBloom (GA) RedisJSON (GA) RedisAI (Preview Version) RedisTimeSeries (Preview Version) RedisGears (Preview Version) New features RS 5.5 lets you create Redis databases with multiple Redis modules.\nPreview considerations This preview version is a standalone version and is not supported for production environments. You cannot upgrade to it from a lower version or upgrade from it to a higher version. Unexpected behaviors/issues found in this release will be addressed in future releases.\nThis preview version is not supported for networks that are isolated from the internet.\nInstallation instructions To set up a cluster with nodes that can host databases with multiple modules, you must follow this procedure on each node in the cluster:\nInstall RS 5.5. To install the modules, run: sudo ./install-modules.sh Either: Set up the node as the first node in the cluster. Join the node to an existing cluster. ","categories":["RS"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-4-december-2018/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-4-december-2018/","title":"Redis Enterprise Software Release Notes 5.4 (December 2018)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.4 is now available. RS 5.4 adds support for Redis 5.0 (GA) with the new Redis Streams data type.\nOverview You can upgrade to RS 5.4 from RS 5.0 and above according to the upgrade instructions. If you have a version older than 5.0, you should first upgrade to version 5.2 (or at least 5.0).\nNew features Redis 5.0 GA - Redis Streams RS 5.4 adds support for Redis 5.0 (GA version- 5.0.2), which introduces the new Redis Streams data type. Redis Streams models a log data structure in-memory and implements additional powerful operations, such as Consumer Groups.\nRedis graph module Starting from RS 5.4, Redis Graph, a new Redis Enterprise Module that introduces the world\u0026rsquo;s fastest graph database, is an integral part of Redis Enterprise package.\nRedisGraph is the first queryable Property Graph database to use sparse matrices to represent the adjacency matrix in graphs and linear algebra to query the graph.\nActive-Active Redis (CRDB) - Creation in non-clustering mode In RS 5.4 you can create Active-Active databases (CRDBs) in a non-clustering mode. As a result, the following creation options are allowed:\nClustering mode - Creates a CRDB that consists of any number of shards in a clustering mode and is subject to multi-key commands limitations. Non-clustering mode - Creates a CRDB that consists of one shard only in a non-clustering mode so that multi-key command limitations do not apply. High availability for replica shards When replica high availability is enabled and a master shard fails, a replica (formerly slave) shard is automatically promoted to a master shard to maintain data availability. This creates a single point of failure until a new replica shard is manually created.\nRS 5.4 expands the high availability capabilities by adding the ability to automatically avoid this single point of failure by configuring the cluster to automatically migrate the replica shard to another available node. In practice, replica migration creates a new replica shard and replicates the data from the master shard to the new replica shard.\n*Note that just as is the case with the Redis open-source project, Redis is in the process of changing the \u0026ldquo;master-replica\u0026rdquo; terminology to \u0026ldquo;master-replica\u0026rdquo; everywhere, including within our documentation.\nAdditional capabilities Support for new operating systems- Ubuntu 18.04 and RHEL 7.6. Product version lifecycle The End of Life (EOL) for Redis Enterprise Software 4.5.X was November 30th, 2018, in accordance with our published policy. We recommend that customers with version 4.5 or below upgrade to the latest version. Important fixes RS23616 - Fixed a failure when updating the memory limit of RoF database. RS22871 - Fixed a certificate verify failure after nodes upgrade. RS2862 - Improved admin console performance in case multiple browsers or windows are directed to the admin console. RS22751 - Fixed an issue in the backup process which caused temporary service outage. RS22636 - Fixed Redis process failure when a ReJSON Module\u0026rsquo;s command is executed. RS22601 - Fixed a failure during shard migration procedure. RS22478 - Fixed a failure in replica-of process between two databases with ReBloom Module. RS21974 - SMTP username and password are not mandatory in the email server settings when there is no need for authentication. RS21801 - Fixed admin console issues when cluster is configured with FIPS compliance. RS21772 - Fixed a failure when trying to update a database\u0026rsquo;s endpoint policy to all-master-shards. RS19842 - Updated permissions of some internal files. RS19433 - Improved RAM eviction process for RoF databases. RS18875 - Added the ability to upgrade database gradually, few shards at a time. RS15207 - Fixed a failure during re-shard operation. Known limitations Installation In default Ubuntu 18.04 installations, port 53 is in use by systemd-resolved (DNS server). In such a case, the system configuration must be changed to make this port available before running RS installation. Upgrade Before you upgrade a database with the RediSearch module to Redis 5.0, you must upgrade the RediSearch module on that DB to 1.4.2 or higher. We recommend that you upgrade the RediSearch module before you upgrade the cluster to RS 5.4. Node upgrade fails if SSL certificates were configured in version 5.0.2 and above by updating the certificates on the disk instead of using the new API. For assistance with this issue, please contact Redis support. Cluster API Removed the deprecated argument backup_path from cluster API. To create or update BDBs, please use backup_location. Redis commands The capability of disabling specific Redis commands does not work on Redis Module specific commands. The CLIENT ID command cannot guarantee incremental IDs between clients that connect to different nodes under multi proxy policies. The length of the socket_path variable, which is defined in a node, cannot exceed 88 characters. CLIENT UNBLOCK command is not supported in RS 5.4. ","categories":["RS"]},{"uri":"/ri/release-notes/v1.11.0/","uriRel":"/ri/release-notes/v1.11.0/","title":"RedisInsight v1.11, Oct 2021","tags":[],"keywords":[],"description":"RedisInsight v1.11.0","content":"1.11.1 (January 2022) This is the maintenance release of RedisInsight 1.11 (v1.11.1)!\nFixes: Core: Fixed a warning about urllib version deprecation. ACL errors are now show in pretty format while in edit database screen. Fixed unnecessary warning about segment when there\u0026rsquo;s no internet connection. RediSearch: Fix index tool support for v2.2.5. Bulk actions: Added support for cross-slot bulk action execution. Fixed a bug where there\u0026rsquo;s a failure when a malformed UTF-8 characters are present in the key. Memory Analysis: Added support for quicklist2 data type. Cluster Management: Generic errors are also displayed in the tool. This is helpful when connected to a vendor provided redis with custom exceptions. 1.11.0 This is the General Availability Release of RedisInsight 1.11 (v1.11.0)!\nHeadlines: Added beta support for RedisAI Fixed the issue with empty fields for Hash objects. Full Details: Core: Fixed a bug where editing cluster returns error. Fixed broken redis links. Browser Check Hash value for emptiness RedisGraph: Added support for point datatype. Fixed a bug where returning relationships without their respective nodes leads to infinite loading RediSearch: Fixed a bug where a malformed unicode string in redisearch doesn\u0026rsquo;t produce results. RedisTimeseries: Added support for TS.REVRANGE and TS.MREVRANGE commands. ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-2-2-august-2018/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-2-2-august-2018/","title":"Redis Enterprise Software 5.2.2 (August 2018)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.2.2 is now available.\nRS 5.2.2 is a minor version that includes important fixes and minor enhancements to Redis Enterprise 5.2.\nOverview If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process. You can upgrade to RS 5.2.2 from RS 4.5 and above. If you have a version older than 4.5, you should first upgrade to version 5.0 (or at least 4.5).\nNew capabilities Support for Redis version 4.0.10 RediSearch Enterprise, which is installed with Redis Enterprise Software by default, has been updated to a newer version (1.4.0)\nImportant fixes RED-21080 – Fixed the memory limit calculation for RoF databases RED-20825 – Updated the ‘RAM limit’ of RoF databases to a range between 10% and 100% RED-20506 – Fixed high CPU and File Descriptors utilization by the node watchdog process (node_wd) RED-20275 – Fixed wrong metrics values while importing a dataset into an RoF database RED-20214 – Fixed obstacles to login to the UI when using an LDAP integration RED-20162 – Fixed known limitation of being able to activate “Require SSL for All Communication” to Redis Enterprise CRDBs via Rest API without providing a certificate RED-19758 – Upgraded NGINX from 1.10.3 to 1.13.12 RED-19415, RED-18945 – Improved support for Lettuce client with OSS Cluster enabled RED-19287 – Fixed a scenario of a stuck shard migration process RED-18459 – Updated the data persistence (AOF / Snapshot) of RoF databases to be handled, by default, by the replica shard/s RED-20541– Improved handling of aof file when its tail is corrupted (using aof-load-corrupt-tail flag) RED-21936 – Improved handling of CRDB configuration update when URL parameter was supplied at creation time RED-19760 – Added the capability to control the minimum TLS version that can be used for encrypting the Discovery Service data Known limitations When updating the general settings of a cluster, the ‘username’ and ‘password’ fields in the email server settings cannot be left empty. In case one wants to update the general settings and prefer to leave the ‘username’ and ‘password’ fields empty, the REST API should be used. An issue prevents the user from defining ‘min_data_TLS_version’ on the source cluster when working with Replica Of or CRDB. ","categories":["RS"]},{"uri":"/kubernetes/logs/collect-logs/","uriRel":"/kubernetes/logs/collect-logs/","title":"Collect logs","tags":[],"keywords":[],"description":"Run the log collector script to package relevant logs into a tar.gz file to send to Redis Support for help troubleshooting your Kubernetes environment.","content":"The Redis Enterprise cluster (REC) log collector script (log_collector.py) creates and fills a directory with the relevant logs for your environment. These logs will help the support team with troubleshooting.\nAs of version 6.2.18-3, the log collector tool has two modes:\nrestricted collects only resources and logs created by the operator and Redis Enterprise deployments This is the default for versions 6.2.18-3 and later all collects everything from your environment This is the default mode for versions 6.2.12-1 and earlier Note: This script requires Python 3.6 or later. Download the latest log_collector.py file.\nRun the script on the system that runs your kubectl or oc commands.\nPass -n parameter to run on a different namespace than the one you are currently on Pass -m parameter to change the log collector mode (all or restricted) Run with -h to see options python log_collector.py Note: If you get an error because the yaml module is not found, install the pyYAML module with pip install pyyaml. Upload the resulting tar.gz file containing all the logs to Redis Support.\n","categories":["Platforms"]},{"uri":"/ri/release-notes/v1.10.0/","uriRel":"/ri/release-notes/v1.10.0/","title":"RedisInsight v1.10, March 2021","tags":[],"keywords":[],"description":"RedisInsight v1.10.0","content":"1.10.1 (April 2021) This is the maintenance release of RedisInsight 1.10 (v1.10.1)!\nFixes: Core: Fixed a bug where launching RedisInsight on macOS mojave (10.14.6) would log out the user. Fixed two major container vulnerability. (CVE-2021-24031, CVE-2021-24032 and CVE-2020-36242) Select existing installation path on upgrades in Windows. CLI: Added support for RAW mode (--raw in redis-cli). Browser: Fixed a bug where a number in redis string datatype is treated as a JSON . RedisJSON - Distinguish between empty and collapsed objects/arrays. Streams: Added ability to configure auto refresh interval. RedisTimeseries: Charts now support milliseconds. Added ability to configure auto refresh interval. RedisGraph: Properly detect module in Redis Enterprise Large queries that are truncated in the query card is provided with a tooltip that displays the query on hover. Properly render boolean data types in the objects. Bulk actions: Fixed a bug where preview returns duplicate dry run commands. 1.10.0 (March 2021) This is the General Availability (GA) Release of RedisInsight 1.10 (v1.10.0)!\nHeadlines: Improvements to the way the Browser tool displays \u0026ldquo;special\u0026rdquo; strings. UX improvements to the RedisGraph tool. Ability to configure the slowlog threshold from within the Slowlog tool. Full Details: Overview: The connection details of the Redis database are now displayed. A message is displayed to indicate a cluster with no replicas instead of an empty table. Fixed a bug where the memory usage chart would display an incorrect graph when the memory usage changes rapidly. Browser: Pretty-print \u0026ldquo;special\u0026rdquo; strings (like JSON, Java serialized object, Python pickle objects, etc.) once the entire value is loaded. Pretty-print \u0026ldquo;special\u0026rdquo; strings (like JSON, Java serialized object, Python pickle objects, etc.) inside container types like Hashes, Sets and Sorted Sets. Allow sorting the members of a sorted set by score. Refresh button for the key list. Delete a key by pressing the \u0026ldquo;Delete\u0026rdquo; key. All keys are now visible by default, i.e, the data type filters are disabled by default. Fixed bug where switching logical databases did not work correctly sometimes. Fixed bug where adding a field to a hash with an empty value crashes the UI. Fixed bug where setting TTL to -1 does not effectively delete the key. Added tooltip explaining how to use the logical database selector along with a submit button. Streams: Refresh button for the list of streams. RedisGraph: Node size is now dependent on the number of direct relationships. Added support for pasting the full GRAPH.QUERY command into the query textbox. RediSearch: Fixed bug where the application fails to execute queries on indices starting with/enclosed within single-quotes. Bulk Actions: Improved support for operations on a large number of keys. Slowlog: Allow configuring the slowlog threshold from within the tool for non-cluster databases. ","categories":[]},{"uri":"/ri/","uriRel":"/ri/","title":"RedisInsight","tags":[],"keywords":[],"description":"","content":"RedisInsight is a free GUI for Redis that is available on all platforms (Windows, Mac, Linux, and Docker) and works with all variants of Redis. RedisInsight allows you to:\nView performance metrics for your Redis instance with the Overview tool View data structures visually with the Browser tool Manage basic properties of your Redis cluster such as cluster node timeout, IP, or port with the Cluster Management tool Run commands with a REPL (read-eval-print-loop) with the the CLI tool Analyze memory usage with the Memory Analysis tool Identify and troubleshoot bottlenecks with the Slowlog tool Edit the configuration of your Redis instance with the Configuration tool RedisInsight also has support for several Redis modules, including RedisGraph, RedisTimeSeries, and RediSearch.\nStart using RedisInsight Install RedisInsight Add a Redis database ","categories":["RI"]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-3-beta-july-2018/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-3-beta-july-2018/","title":"Redis Enterprise Software Release Notes 5.3 BETA (July 2018)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.3 is now available.\nRS 5.3 is a preview version that includes all the capabilities of Redis Enterprise 5.2, plus support for Redis 5.0 with the new data type, which is called Streams.\nNew features RS 5.3 adds support for Redis 5.0 and is based on its latest Release Candidate (RC3). Redis 5.0 exposes the new Redis Streams data type, which provides a super fast in-memory abstraction of an append-only log. For more information and usage examples, check out the Streams documentation here.\nPreview considerations As a preview version, we do not support upgrading to RS 5.3 from any previous version or upgrading from RS 5.3 to any future version. In addition, we do not commit to fixing bugs in RS 5.3.\nKnown limitations The RS 5.3 preview version does not support Enterprise Modules (- RediSearch, ReJSON and ReBloom), Redis on Flash (RoF) or active-active Redis (CRDB).\n","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/","uriRel":"/rs/databases/active-active/develop/data-types/","title":"Data types for Active-Active databases","tags":[],"keywords":[],"description":"Introduction to differences in data types between standalone and Active-Active Redis databases.","content":"Active-Active databases use conflict-free replicated data types (CRDTs). From a developer perspective, most supported data types work the same for Active-Active and standard Redis databases. However, a few methods also come with specific requirements in Active-Active databases.\nEven though they look identical to standard Redis data types, there are specific rules that govern the handling of conflicting concurrent writes for each data type.\nAs conflict handling rules differ between data types, some commands have slightly different requirements in Active-Active databases versus standard Redis databases.\nSee the following articles for more information\n","categories":["RS"]},{"uri":"/kubernetes/re-clusters/delete_custom_resources/","uriRel":"/kubernetes/re-clusters/delete_custom_resources/","title":"Delete custom resources","tags":[],"keywords":[],"description":"This article explains how to delete Redis Enterprise clusters and Redis Enterprise databases from your Kubernetes environment.","content":"Delete a database (REDB) To delete a database managed by the Redis Enterprise Kubernetes operator, run kubectl delete redb \u0026lt;your-db-name\u0026gt; from your K8s cluster.\nWhen you delete a database, your data and the REDB custom resource are also deleted.\nDelete a Redis Enterprise cluster (REC) To delete a Redis Enterprise cluster managed by the operator:\nDelete all the databases in your cluster.\nRun kubectl delete rec \u0026lt;your-rec-name\u0026gt; from your K8s cluster.\nWhen you delete your cluster, your databases and the REC custom resource are also deleted. However, persistent volume claims (PVCs) for your cluster are not deleted in the process. If you want to delete your PVCs, you\u0026rsquo;ll have to delete them manually.\nDelete the operator To delete the operator from your K8s cluster and namespace, you can delete the operator bundle with:\nkubectl delete -f bundle.yaml for non-OpenShift K8s deployments kubectl delete -f openshift.bundle.yaml for OpenShift deployments This will remove the operator and its custom resource definitions (CRDs) from your K8s cluster.\nWarning - The Redis Enterprise CRDs are non-namespaced resources, meaning they are shared across your entire K8s cluster. Deleting CRDs in one namespace will delete custom resources in every other namespace across the K8s cluster. If you have Redis Enterprise clusters running in different namespaces on the same K8s cluster, deleting the entire operator bundle might cause data loss.\nTo safely delete the operator from one namespace without affecting the others, delete the operator files individually, excluding the CRD files:\nkubectl delete -f role.yaml kubectl delete -f role_binding.yaml kubectl delete -f service_account.yaml kubectl delete -f admission-service.yaml kubectl delete -f operator.yaml You will also need to remove the namespaceSelector section from the validating webhook.\nTroubleshoot REDB deletion The operator attaches a finalizer to the Redis Enterprise database (REDB) object. This makes sure the database is deleted before the REDB custom resource is removed from the K8s cluster.\nIf the operator isn\u0026rsquo;t running, or some other fatal error occurs, the finalizer isn\u0026rsquo;t removed automatically by the operator. In this case, you won\u0026rsquo;t be able to delete your REDB resource.\nIf this happens, you can remove the finalizer manually.\nWarning - If you remove the finalizer manually, there is no guarantee that the underlying REC has been deleted. This may cause resource issues and require manual intervention. kubectl patch redb \u0026lt;your-db-name\u0026gt; --type=json -p \\ \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;finalizer.redisenterprisedatabases.app.redislabs.com\u0026#34;}]\u0026#39; Troubleshoot REC deletion The operator attaches a finalizer to the Redis Enterprise cluster (REC) object. This makes sure the Redis cluster is deleted before the REC custom resource is removed from the K8s cluster.\nIf the operator isn\u0026rsquo;t running, or some other fatal error occurs, the finalizer isn\u0026rsquo;t removed automatically by the operator. In this case, you won\u0026rsquo;t be able to delete your REC resource.\nIf this happens, you can remove the finalizer manually.\nWarning - If you remove the finalizer manually, there is no guarantee that the underlying REC has been deleted. This may cause resource issues and require manual intervention. kubectl patch rec \u0026lt;your-rec-name\u0026gt; --type=json -p \\ \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;redbfinalizer.redisenterpriseclusters.app.redislabs.com\u0026#34;}]\u0026#39; ","categories":["Platforms"]},{"uri":"/kubernetes/deployment/using-kustomize/","uriRel":"/kubernetes/deployment/using-kustomize/","title":"Deploy with kustomize","tags":[],"keywords":[],"description":"How to use the kustomize tool with Redis Enterprise for Kubernetes","content":"Kustomize is a template-free, Kubernetes native way to customize application configuration. Kustomize is available in kubectl by running kubectl apply -k. Kustomize provides a declarative approach to configuration management that you can use with a variety of deployment tools for Kubernetes.\nMotivation for use with Redis Enterprise Our operator is deployed onto your target Kubernetes cluster via a bundle or automation tools like OpenShift’s OLM. The standard practice is to deploy this bundle without any configuration changes as the specific settings are part of how our product has been designed. You are unlikely to need any configuration changes for the operator itself.\nThe need for configuration management arises in the use of the operator via the Custom Resources defined for Redis Enterprise clusters and databases (see the documentation for the cluster CR and database CR). You will likely want to set different parameters for different applications while having consistency for company policies. At the same time, you\u0026rsquo;ll want to manage the differences in deployment topologies for development, QA, and production clusters and databases.\nFor example, production clusters will likely require more resources and greater limits than development or testing systems. In addition, your secrets, passwords, and other security settings are likely to differ across deployment environments. Kustomize makes it easier to handle these differences between environments.\nA simple example Kustomize works on the building variants from a hierarchy of configurations. In the simplest setup, there is one “base” directory containing all of the common resources. For each variant, there are different directories, each with instructions on how to produce the variant.\nFor example, say we want to control the limits and requests for a node so that they are different values for development and production. To start, we must create a “base” directory that contains a default configuration for a simple cluster:\nmkdir -p base cat \u0026lt;\u0026lt; EOF \u0026gt; base/cluster.yaml apiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: test spec: nodes: 3 username: \u0026#34;app@example.org\u0026#34; EOF We then need to configure the “base” directory to be recognized by kustomize. The simplest configuration lists only the resources:\ncat \u0026lt;\u0026lt; EOF \u0026gt; base/kustomization.yaml resources: - cluster.yaml EOF Now we can create a variant for development that tunes down the CPU requirements and also creates a single-node cluster while changing the namespace:\nmkdir -p dev cat \u0026lt;\u0026lt; EOF \u0026gt; dev/cluster.yaml apiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: test spec: nodes: 1 redisEnterpriseNodeResources: limits: cpu: 1 memory: 4Gi requests: cpu: 1 memory: 4Gi EOF In this example, the cluster resource must repeat the prolog metadata so that kustomize can match our resources. At minimum, we need apiVersion, kind, and metadata.name.\nAs before, we need to tell kustomize about our resources using a kustomization.yaml file. In this case, we need to tell kustomize what to do with our variant of cluster.yaml. Specifically, we want to patch the base cluster.yaml by strategically merging properties:\ncat \u0026lt;\u0026lt; EOF \u0026gt; dev/kustomization.yaml bases: - ../base namespace: dev patchesStrategicMerge: - cluster.yaml EOF For a QA deployment, we might need more resources:\nmkdir -p qa cat \u0026lt;\u0026lt; EOF \u0026gt; qa/cluster.yaml apiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: test spec: nodes: 5 redisEnterpriseNodeResources: limits: cpu: 2 memory: 25Gi requests: cpu: 2 memory: 25Gi EOF cat \u0026lt;\u0026lt; EOF \u0026gt; qa/kustomization.yaml bases: - ../base namespace: qa patchesStrategicMerge: - cluster.yaml EOF We can now test our “dev” customization without applying it to the cluster by running the following:\nkubectl kustomize dev and the output should look something like this:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: test namespace: dev spec: nodes: 1 redisEnterpriseNodeResources: limits: cpu: 1 memory: 4Gi requests: cpu: 1 memory: 4Gi username: app@example.org Here you’ll see that the various properties under “spec” from the base version of cluster.yaml have been preserved in our new variant. Meanwhile, we’ve added a namespace and the additional properties for the resource limits and requests.\nThis strategy preserves the common spec settings in the base so that they don’t appear in more than one place. We can also do things list adjust names with namePrefix or create complex hierarchy of changes, patches via strategic merges, and addition of resources like deployment-specific secrets.\nHow to use kustomize Kustomize is a tool that fits in with the idea of “configuration as code”. There is a wide range of ways that kustomize can be used successfully, and it depends on where your project or organization is in terms of automation. These configurations are compatible with many CI/CD tools for Kubernetes such as Flux CD.\nThe simplest way is to run kustomize is with kubectl apply -k. In the example, we could create our development cluster by:\nkubectl apply -k dev If we need to feed the output to another tool, the output of kubectl kustomize has the appropriate separators to be used as a bundle. We can easily redirect the output for use in another tool:\nkubectl kustomize dev \u0026gt; bundle.yaml Finally, you can run kustomize as its own binary outside of kubectl. The “kustomize build” command has the same outcome as “kubectl kustomize”:\nkustomize build dev Note: you can install kustomize directly in a variety of ways (e.g. via homebrew on the Mac).\n","categories":["Platforms"]},{"uri":"/rs/references/client_references/client_drupal/","uriRel":"/rs/references/client_references/client_drupal/","title":"Redis with Drupal 7","tags":[],"keywords":[],"description":"Configure Drupal to use Redis as a cache.","content":"Install Redis for Drupal Follow these steps to install Redis as a cache for Drupal:\nInstall Predis under sites/all/libraries/predis. Download and install the Drupal Redis module. Configure Drupal To configure Drupal to use Redis as a cache, add the following lines to your settings.php file with the specified changes:\n$conf[\u0026#39;redis_client_interface\u0026#39;] = \u0026#39;Predis\u0026#39;; $conf[\u0026#39;redis_client_host\u0026#39;] = \u0026#39;hostname\u0026#39;; $conf[\u0026#39;redis_client_port\u0026#39;] = port; $conf[\u0026#39;redis_client_password\u0026#39;] = \u0026#39;password\u0026#39;; $conf[\u0026#39;lock_inc\u0026#39;] = \u0026#39;sites/all/modules/contrib/redis/redis.lock.inc\u0026#39;; $conf[\u0026#39;cache_backends\u0026#39;][] = \u0026#39;sites/all/modules/contrib/redis/redis.autoload.inc\u0026#39;; $conf[\u0026#39;cache_default_class\u0026#39;] = \u0026#39;Redis_Cache\u0026#39;; In line 2, replace hostname with your database\u0026rsquo;s hostname or IP address. In line 3, replace port with your database\u0026rsquo;s port. In line 4, replace password with your database\u0026rsquo;s password. ","categories":["RS"]},{"uri":"/ri/memory-optimizations/","uriRel":"/ri/memory-optimizations/","title":"Memory Optimization for Redis","tags":[],"keywords":[],"description":"","content":"RedisInsight can monitor the health of your databases. Here are some tips to increase memory efficiency in Redis.\nTo get the best performance from your databases, ensure you are using the latest version of Redis.\nDeveloper best practices Avoid dynamic Lua script Refrain from generating dynamic scripts, which can cause your Lua cache to grow and get out of control. Memory is consumed as scripts are loaded. Memory is consumed:\nBy the server.lua_scripts dictionary holding original text. Internally by Lua to keep the compiled byte-code. So, If you have to use dynamic scripting, then just use the EVAL command, because it doesn\u0026rsquo;t make sense to load them first.\nThings to keep in mind if using dynamic Lua scripts Remember to track your Lua memory consumption and flush the cache periodically with a SCRIPT FLUSH. Do not hardcode and/or programmatically generate key names in your Lua scripts because it makes them useless in a clustered Redis setup. Switch to 32-bits Redis gives you these statistics for a 64-bit machine.\nAn empty instance uses ~ 3MB of memory. 1 million small keys - String Value pairs use ~ 85MB of memory. 1 million keys - Hash value, representing an object with 5 fields, use ~ 160 MB of memory. 64-bit has more memory available as compared to a 32-bit machine. But if you are sure that your data size does not exceed 3 GB then storing in 32 bits is a good option.\n64-bit systems use considerably more memory than 32-bit systems to store the same keys, especially if the keys and values are small. This is because small keys are allocated full 64 bits resulting in the wastage of the unused bits.\nAdvantages Switching to 32-bit from 64-bit machine can substantially reduce the cost of the machine used and can optimize the usage of memory.\nTrade-offs For the 32-bit Redis variant, any key name larger than 32 bits requires the key to span to multiple bytes, thereby increasing the memory usage.\nWhen to avoid switching to 32 bit If your data size is expected to increase more than 3 GB then you should avoid switching.\nReclaim expired keys memory faster When you set an expiry on a key, redis does not expire it at that instant. Instead, it uses a randomized algorithm to find out keys that should be expired. Since this algorithm is random, there are chances that the keys are not expired. This means that redis consumes memory to hold keys that have already expired. The moment the key is accessed, it is deleted.\nIf there are only a few keys that have expired and redis hasn\u0026rsquo;t deleted them - it is fine. It\u0026rsquo;s only a problem when a large number of keys haven\u0026rsquo;t expired.\nCheck if memory is not reclaimed after expiry Run the INFO command and find the total_memory_used and sum of all the keys for all the databases. Then take a Redis Dump(RDB) and find out the total memory and total number of keys. Looking at the difference you can clearly point out that lot of memory is still not reclaimed for the keys that have expired.\nHow to reclaim expired keys memory faster You can follow one of these three steps to reclaim the memory:\nRestart your redis-server To reclaim the expired keys faster, increase memory-samples in redis.conf (default is 5). You can set up a cron job that runs the scan command after an interval which helps in reclaiming the memory of the expired keys. Trade-offs If you increase the maxmemory-samples configuration parameter, it expires the keys faster, but it costs more CPU cycles, which increases latency of commands. Secondly, increasing the expiry of keys helps but that requires significant changes to application logic.\nUse better serializer Redis does not have any specific data type to store the serialized objects, they are stored as byte array in Redis. If you are using regular means of serializing our Java, Python, and PHP objects, they can be of larger size which impacts the memory consumption and latency.\nChoosing a serializer Instead of using the default serializer of your programming language (Java serialized objects, Python pickle, PHP serialize, and so on), switch to a better library like Protocol Buffers, MessagePack, and so on.\nMessagePack MessagePack is an efficient binary serialization format. It lets you exchange data among multiple languages like JSON. But it\u0026rsquo;s faster and smaller. Small integers are encoded into a single byte, and typical short strings require only one extra byte in addition to the strings themselves.\nAs stated by Salvatore Sanfilippo, creator of Redis, \u0026ldquo;Redis scripting has support for MessagePack because it is a fast and compact serialization format with a simple to implement specification. I liked it so much that I implemented a MessagePack C extension for Lua just to include it into Redis.\u0026rdquo;\nProtocol buffers Protocol buffers, usually referred as Protobuf, is a protocol developed by Google to allow serialization and deserialization of structured data. Google developed it with the goal to provide a better way, compared to XML, to make systems communicate. So they focused on making it simpler, smaller, faster, and more maintainable then XML.\nData modeling recommendations Combine smaller strings to hashes Strings data type has an overhead of about 90 bytes on a 64 bit machine. In other words, calling set foo bar uses about 96 bytes, of which 90 bytes is overhead. You should use the String data type only if:\nThe value is at least greater than 100 bytes. You are storing encoded data in the string - JSON encoded or Protocol buffer. You are using the string data type as an array or a bitset. If you are not doing any of the above, then use Hashes.\nHow to convert strings to hashes Suppose you have to store the number of comments on the posts of a user. You can have a key names like user:{userId}:post:{postId}:comments.\nThis way, you have a key per post for each user. So now if you need to find the total number of comments for whole application you can do:\nRedis::mget(\u0026#34;user:{$userId}:post:1\u0026#34;, \u0026#34;user:{$userId}:post:2\u0026#34;, ...); For converting this to Hash you can do something like this.\nRedis::hmset(\u0026#34;user:{$userId}:comments\u0026#34;, \u0026#34;post:1\u0026#34;, 20, \u0026#34;post:2\u0026#34;, 50); This builds a Redis hash with two fields post:1 and post:2 holding the values 20 and 50.\nAdvantages Combining small strings to hashes reduces the memory used and in return save a cost.\nHashes can be encoded to use memory efficiently, so Redis makers recommend that you use hashes whenever possible because a few keys use a lot more memory than a single key containing a hash with a few fields. A key represents a Redis object and holds a lot more information than just its value. On the other hand, a hash field only holds the value assigned, which is much more efficient.\nTrade-offs Performance comes with a cost. By converting the strings to hash, you conserve memory because it saves only the string value and no extra information like: idle time, expiration, object reference count, and encoding related to it. But, if you want the key with the expiration value, you can\u0026rsquo;t associate it with a hash structure because expiration is not available.\nWhen to avoid combining strings to hashes The decision depends on the number of strings. If it less than 1 million and the memory consumption is not high, the conversion is not affected much and there is no point in increasing the complexity of code.\nBut, if the strings are more than 1 million and the memory consumption is high, then this approach should definitely be followed.\nConvert a hash table to ziplist for hashes Hashes have two types of encoding: a hash table and ziplist. The decision of storing in which of the data structures in done based on the two configurations Redis provides - hash-max-ziplist-entries and hash-max-ziplist-values.\nBy default the redis configuration parameters have these settings as:\nhash-max-ziplist-entries = 512 hash-max-ziplist-values = 64 So if any value for a key exceeds the two configurations it is stored automatically as a hash table instead of a ziplist. A hash table consumes almost double the memory compared to a ziplist. To save your memory, you can increase the two configurations and convert your hash tables to ziplists.\nWhy ziplists use less memory The ziplist implementation in Redis achieves its small memory size by storing only three pieces of data per entry: the length of the previous entry, the length of the current entry, and the stored data. Therefore, ziplists consume less memory.\nTrade-offs This brevity comes at a cost because more time is required to change the size and retrieve the entry. Hence, there is an increase in latency and possibly increase in CPU utilization on your Redis server.\nNote: Similarly, sorted sets can also be converted to ziplists, but the only difference is that zset-max-ziplist-entries is 128, which is less than for hashes. Switch from set to IntSet Sets that contain only integers are extremely efficient memory wise. If your set contains strings, try to use integers by mapping string identifiers to integers.\nYou can either use enums in your programming language or you can use a Redis hash data structure to map values to integers. Once you switch to integers, Redis uses the IntSet encoding internally.\nThis encoding is extremely memory efficient. By default, the value of set-max-intset-entries is 512, but you can set this value in redis.conf.\nTrade-offs By increasing the value of set-max-intset-entries, latency increases in set operations, and CPU utilization is also increased on your Redis server. Check this by running this command before and after making this change.\nRun `info commandstats` Use smaller keys Redis keys can increase the memory consumption for your Redis instances. In general, you should always prefer descriptive keys but if you have a large dataset having millions of keys then these large keys can be expensive.\nHow to convert to smaller keys In a well-written application, switching to shorter keys usually involves updating a few constant strings in the application code.\nYou have to identify all the big keys in your Redis instance and shorten it by removing extra characters from it. You can achieve this in two ways:\nYou can identify the big keys in your Redis instance by using RedisInsight. This gives you details about all the keys and a way to sort your data based on the length of keys. Alternatively, you can run the command redis-cli --bigkeys. An advantage of using RedisInsight is that it gives you the big keys from the whole dataset, whereas big-key commands run over a certain set of records and return the big keys from that set, hence it is difficult to identify the big keys from the whole dataset using big keys.\nAdvantages Suppose you have 100,000,000 keys name, as follows:\nmy-descriptive-large-keyname (28 characters) Now, if you shorten the key name like so:\nmy-des-lg-kn (12 characters) You save 16 characters by shortening your key, i.e. 16 bytes, which lets you save 1,000,000,000*16 = 1.6GB of RAM Memory !.\nTrade-offs Large keys are more descriptive then shortened keys. Hence, when reading through your database, you may find the keys less relatable but the memory and cost savings much more efficient.\nConvert to a list instead of hash A Redis hash stores field names and values. If you have thousands of small hash objects with similar field names, the memory used by field names adds up. To prevent this, consider using a list instead of a hash. The field names become indexes into the list.\nWhile this may save memory, you should only use this approach if you have thousands of hashes and if each of those hashes have similar fields. Compressed Field Names are another way to reduce memory used by field names.\nLet\u0026rsquo;s take an example. Suppose you want to set user details in Redis. You do something like this:\nhmset user:123 id 123 firstname Bob lastname Lee location CA twitter bob_lee Now, Redis stores this internally as a ziplist; you can confirm by running debug object user:123 and look at the encoding field. In this encoding, key value pairs are stored sequentially, so the user object you created above would roughly look like this [\u0026quot;firstname\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;lastname\u0026quot;, \u0026quot;Lee\u0026quot;, \u0026quot;location\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;twitter\u0026quot;, \u0026quot;bob_lee\u0026quot;]\nNow, if you create a second user, the keys are duplicated. If you have a million users, well, it\u0026rsquo;s a big waste repeating the keys again. To get around this, you can borrow a concept from Python, named tuples.\nHow does a NamedTuple work A NamedTuple is simply a read-only list, but with some magic to make that list look like a dictionary. Your application needs to maintain a mapping from field names to indexes, like \u0026quot;firstname\u0026quot; =\u0026gt; 0, \u0026quot;lastname\u0026quot; =\u0026gt; 1 and so on.\nThen, you simply create a list instead of a hash, like- lpush user:123 Bob Lee CA bob_lee. With the right abstractions in your application, you can save significant memory.\nTrade-offs The only trade-offs are related to code complexity. Redis internally uses the same encoding (ziplist) for small hashes and small lists, so there is no performance impact when you switch to a list. However, if you have more than 512 fields in your hash, this approach is not recommended.\nWhen to avoid converting hash to list Avoid converting hashes to lists when:\nYour hash contains fewer than 50,000 field-value pairs. The size of your hash values are not consistent (for instance, when some hashes contain only a few field-value pairs while others contain many). Shard big hashes to small hashes If you have a hash with a large number of key-value pairs. If each key-value pair is small enough, break it into smaller hashes to save memory. To shard a hash table, you need to choose a method of partitioning your data.\nHashes themselves have keys that can be used for partitioning the keys into different shards. The number of shards are determined by the total number of keys you want to store and the shard size. Using this and the hash value you can determine the shard ID in which the key resides.\nHow sharding happens Numeric keys - For numeric keys, keys are assigned to a shard ID based on their numeric key value (keeping numerically similar keys in the same shard).\nNon-numeric Keys - For non-numeric keys, CRC32 checksum is used. CRC32 is used in this case because it returns a simple integer without additional work and is fast to calculate (much faster than MD5 or SHA1 hashes).\nThings to keep in mind Be consistent about the total number of expected elements and the shard size while sharding because these two pieces of information are required to keep the number of shards down. Ideally, you should not change the values because this changes the number of shards.\nIf you want to change any one of the values, you should have a process for moving your data from the old datashards to the new data shards (this is generally known as resharding).\nTrade-offs Converting big hashes to small hashes increases the complexity of your code.\nSwitch from a set to Bloom filter, cuckoo filter, or HyperLogLog Suppose you are using large sets to solve one of the following problems:\nCount the number of unique observations in a stream. Check if an observation already appeared in the stream. Find the fraction or the number of observations in the stream that are smaller or larger than a given value. If you are ready to trade accuracy with speed and memory usage, consider using one of the following probabilistic data structures. You can use:\nHyperLogLog to estimate the number of unique observations in a set. Bloom filter or cuckoo filter to check if an observation has already appeared in the stream. False positive matches are possible, but false negatives are not. t-digest to estimate the fraction or the number of observations in the stream that are smaller or larger than a given value. Bloom filter and cuckoo filter require RedisBloom.\nTrade-offs Following are the trade-offs of using HyperLogLog:\nThe results achieved from HyperLogLog have a standard error of 0.81%. Hyperloglogcan can be used only for estimating the count of unique observations, and not for checking if a specific value was observed. For example, if you want to maintain how many unique IP addresses made an API call today, HyperLogLog tells you 46966 unique IPs for today.\nBut if you want to retrieve all the unique IP Addresses, you would need to maintain a set containing all these addresses.\nSwitch from a sorted set to count-min sketch or top-k Suppose you are using large sorted sets to solve one of the following problems:\nDetermine the count of a given observation in a stream. Maintain a list of the most frequent observations in a stream. If you want to trade accuracy for speed and memory usage, consider using one of the following probabilistic data structures. You can use:\nCount-min sketch to estimate the count of a given observation in a stream. Top-K to maintain a list of the k most frequent observations in a stream. Count-min sketch and top-K require RedisBloom.\nData compression methods Compress field names A Redis hash consists of fields and their values. Like values, a field name also consumes memory, so it is required to keep in mind while assigning field names. If you have a large number of hashes with similar field names, the memory adds up significantly. To reduce memory usage, you can use smaller field names.\nWhat does it mean to compress field names Referring to the previous example of converting hashes to list, you had a hash with user details.\nhmset user:123 id 123 firstname Bob lastname Lee location CA twitter bob_lee In this case, firstname, lastname, location, and twitter are field names that can been shortened to fn, ln, loc, and so on. By doing so, you can save some memory that is used by the field names.\nCompress values Redis and clients are typically IO bound and the IO costs are typically at least 2 orders of magnitude in respect to the rest of the request/reply sequence. Redis by default does not compress any value that is stored in it. Therefore, you should compress your data before storing it in Redis. This helps to reduce the payload, which in return gives you higher throughput, lower latency, and higher cost savings.\nHow to compress strings There are several compression algorithms to choose from, each with its own trade-offs.\nSnappy aims for high speed and reasonable compression. LZO compresses fast and decompresses faster. Others, such as Gzip, are more widely available. The Gzip compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. Gzip is often a good choice for cold data, which is accessed infrequently. Snappy or LZO are a better choice for hot data, which is accessed frequently.\nCompressing strings requires code changes. Some libraries can transparently compress objects, and you would only need to configure your library. In other cases, you might have to compress the data manually.\nAdvantages Compressing strings can save you anywhere between 30 and 50 percent of the memory. By compressing strings, you also reduce the network bandwidth between your application and Redis databases.\nTrade-offs Compression/decompression requires that your application do the extra work. This trade-off is usually worth it. If you are concerned about additional CPU load, switch to a faster algorithm like Snappy or LZO.\nWhen to avoid compression Compression should not be followed blindly. Sometimes compression does not help you reduce your memory but rather increases your CPU utilization. Avoid compression:\nFor shorter strings: it\u0026rsquo;s likely a waste of time. Short strings generally don\u0026rsquo;t compress much, so the gain would be too small. When the data isn\u0026rsquo;t well structured. JSON and XML are good at compression because they have repeated characters and tags. Enable compression for lists A list is just a link list of arrays, where none of the arrays are compressed. By default, Redis does not compress elements inside a list. However, if you use long lists, and mostly access elements from the head and tail only, then you can enable compression.\nYou have two configurations:\nList-max-ziplist-size: 8kb (default) List-compression-depth: 0 (default) A configuration change in redis.conf \u0026gt; list-compression-depth=1 helps you achieve compression.\nAbout compression depth Compression depth is the number of list nodes from each end of the list to leave untouched before you start compressing inner nodes.\nExample:\nList-compression-depth=1 compresses every list node except the head and tail of the list. List-compression-depth=2 never compresses the head or head-\u0026gt;next or the tail or tail-\u0026gt;prev. List-compression-depth=3 starts compression after the head-\u0026gt;next-\u0026gt;next and before the tail-\u0026gt;prev-\u0026gt;prev, etc. Trade-offs For small values (for example 40 bytes per list entry here), compression has minimal performance impact. When using 40 byte values with a max ziplist size of 8k, that\u0026rsquo;s around 200 individual elements per ziplist. You only pay the extra compression overhead cost when a new ziplist gets created (in this case, once every 200 inserts).\nFor larger values (for example 1024 bytes per list entry here), compression does have a noticeable performance impact, but Redis is still operating at over 150,000 operations per second for all good values of ziplist size (-2). When using 1024 byte values with a max ziplist size of 8k, that works out to 7 elements per ziplist. In this case, you pay the extra compression overhead once every seven inserts. That\u0026rsquo;s why the performance is slightly less in the 1024 byte element case.\n","categories":["RI"]},{"uri":"/rs/clusters/monitoring/nagios-plugin/","uriRel":"/rs/clusters/monitoring/nagios-plugin/","title":"Nagios integration with Redis Enterprise Software","tags":[],"keywords":[],"description":"The Redis Enterprise Software (RS) Nagios plugin enables you to monitor the status of RS related objects and alerts. The RS alerts can be related to the cluster, nodes, or databases.","content":"The Redis Enterprise Software (RS) Nagios plugin enables you to monitor the status of RS related objects and alerts. The RS alerts can be related to the cluster, nodes, or databases.\nThe alerts that can be monitored via Nagios are the same alerts that can be configured in the RS UI in the Settings ­\u0026gt; Alerts page, or the specific Database ­\u0026gt; Configuration page.\nAll alert configurations (active / not active, setting thresholds, etc\u0026rsquo;) can only be done through the RS UI, they cannot be configured in Nagios. Through Nagios you can only view the status and information of the alerts.\nThe full list of alerts can be found in the plugin package itself (in \u0026ldquo;/rlec_obj/rlec_services.cfg\u0026rdquo; file, more details below).\nRS Nagios plugin support API password retrieval from Gnome keyring, KWallet, Windows credential vault, Mac OS X Keychain, if present, or otherwise Linux Secret Service compatible password store. With no keyring service available, the password is saved with base64 encoding, under the user home directory.\nConfiguring the Nagios plugin In order to configure the Nagios plugin you need to copy the files that come with the package into your Nagios environment and place them in a Nagios configuration directory. Or, alternatively you can copy parts of the package configuration into your existing Nagios configuration.\nIf Keyring capabilities are needed to store the password, python keyring package should be installed and used by following the below steps from the operating system CLI on Nagios machine:\npip install keyring ­to install the package (See https://pip.pypa.io/en/stable/installing/ on how to install python pip if needed). keyring set RS-Nagios ­\u0026lt;RS user email\u0026gt; to set the password. User email should be identical to the email used in Nagios configuration and the password should be set using the same user that run the Nagios server. Then, you need to update the local parameters, such as hostnames, addresses, and object IDs, to the values relevant for your RS deployment.\nFinally, you need to set the configuration for each node and database you would like to monitor. More details below.\nThe RS Nagios package includes two components:\nThe plugin itself ­- with suffix \u0026ldquo;rlec_nagios_plugin\u0026rdquo; Configuration files - with suffix \u0026ldquo;rlec_nagios_conf\u0026rdquo; Below is the list of files included in these packages and instructions regarding what updates need to be made to these flies.\nNote : The instructions below assume you are running on Ubuntu, have a clean Nagios installation, and the base Nagios directory is \u0026ldquo;/usr/local/nagios/\u0026rdquo;\nStep 1 Copy the folder named \u0026ldquo;libexec\u0026rdquo; from the plugin folder and its contents to \u0026ldquo;/usr/local/nagios/\u0026rdquo;\nThese files included in it are:\ncheck_rlec_alert check_rlec_node check_rlec_bdb email_stub rlecdigest.py Note : The check_rlec_alert, check_rlec_node, check_rlec_bdb files are the actual plugin implementation. You can run each of them with a \u0026ldquo;­h\u0026rdquo; switch in order to retrieve their documentation and their expected parameters.\nStep 2 Add the following lines to your \u0026ldquo;nagios.cfg\u0026rdquo;:\ncfg_dir=/usr/local/nagios/etc/rlec_obj cfg_dir=/usr/local/nagios/etc/rlec_local resource_file=/usr/local/nagios/etc/rlec_resource.cfg Step 3 Copy the configuration files along with their folders to \u0026ldquo;/usr/local/nagios/etc\u0026rdquo; and make the required updates, as detailed below.\nUnder the \u0026ldquo;/etc\u0026rdquo; folder: \u0026ldquo;rlec_resource.cfg \u0026quot; ­ holds global variables definitions for the user and password to use to connect to RS. You should update the variables to the relevant user and password for your deployment. \u0026ldquo;rlec_local \u0026quot; folder \u0026ldquo;rlec_obj\u0026rdquo; folder Under the \u0026ldquo;/rlec_local\u0026rdquo; folder: \u0026ldquo;cluster.cfg \u0026quot; ­ holds configuration details at the cluster level. If you would like to monitor more than one cluster then you need to duplicate the two existing entries in the file for each cluster. The first \u0026ldquo;define host\u0026rdquo; section defines a variable for the IP address of the cluster that is used in other configuration files. Update the \u0026ldquo;address\u0026rdquo; to the Cluster Name (FQDN) as defined in DNS, or the IP address of one of the nodes in the cluster. If you are configuring more than one RS then when duplicating this section you should make sure: The \u0026ldquo;name\u0026rdquo; is unique. In the second \u0026ldquo;define host\u0026rdquo; section: The \u0026ldquo;host_name \u0026quot; in each entry must be unique. The \u0026ldquo;display_name\u0026rdquo; in each entry can be updated to a user-friendly name that are shown in Nagios UI. \u0026ldquo;contacts.cfg \u0026quot; ­ holds configuration details who to send emails to. It should be updated to values relevant for your deployment. If this file already exists in your existing Nagios environment then you should update it accordingly. \u0026ldquo;databases.cfg\u0026rdquo; ­ holds configuration details of the databases to monitor. The \u0026ldquo;define host\u0026rdquo; section should be duplicated for every database to monitor. \u0026ldquo;host_name\u0026rdquo; should be a unique value. \u0026ldquo;display_name \u0026quot; should be updated to a user-friendly name to show in the UI. \u0026ldquo;_RLECID \u0026quot; should be the database\u0026rsquo;s internal ID that can be retrieved from rladmin status command output. \u0026ldquo;nodes.cfg \u0026quot; ­ holds configuration details of the nodes in the cluster. The \u0026ldquo;define host\u0026rdquo; section should be duplicated for every node in the cluster. \u0026ldquo;host_name\u0026rdquo; should be a unique value. \u0026ldquo;display_name \u0026quot; should be updated to a user-friendly name to show in the UI. \u0026ldquo;address\u0026rdquo; should be updated to the DNS name mapped to the IP address of the node, or to the IP address itself. \u0026ldquo;_RLECID \u0026quot; should be the node\u0026rsquo;s internal ID that can be retrieved from rladmin status command output. Under the \u0026ldquo;/rlec_obj\u0026rdquo; folder: \u0026ldquo;rlec_cmd.cfg\u0026rdquo; ­ holds configuration details of how to activate the plugin. No need to make any updates to it. \u0026ldquo;rlec_groups.cfg\u0026rdquo; holds definitions of host groups. No need to make any updates to it. \u0026ldquo;rlec_services.cfg\u0026rdquo; holds definitions of all alerts that are monitored. No need to make any updates to it. \u0026ldquo;rlec_templates.cfg\u0026rdquo; holds general RS Nagios definitions. No need to make any updates to it. ","categories":["RS"]},{"uri":"/kubernetes/memory/pod-stability/","uriRel":"/kubernetes/memory/pod-stability/","title":"Manage pod stability","tags":[],"keywords":[],"description":"This section provides information about how you can use quality of service, priority class, eviction thresholds and resource monitoring to maintain cluster node pod stability.","content":"Kubernetes clusters manage the allocation of system resources and can evict pods to release system resources. Here are some ways that you can configure the Redis Enterprise node pods to maintain pod stability:\nGuaranteed quality of service A running pod has a quality of service measure assigned to it that is one of three quality of service classes: Guaranteed, Burstable, and Best Effort. You can assure the Guaranteed class is assigned to the Redis Enterprise node pods by following the right guidelines.\nTo get a Guaranteed quality of service class assigned:\nEvery container in the pod must have a memory limit and a memory request, and they must be the same. Every container in the pod must have a CPU limit and a CPU request, and they must be the same. If resources limits and requests are not specified in the Redis Enterprise CRD, these requirements are met in the default version created by the operator. of your Redis Enterprise cluster CRD, Otherwise, you must set the limits and requests to the same value for memory and CPU in the redisEnterpriseNodeResources section of the CRD.\nSidecar containers also impact the quality of service class assignment for the pod.\nTo check the quality of service class of any running Redis Enterprise node pod, run:\nkubectl get pod rec-0 --o jsonpath=\u0026#34;{.status.qosClass}\u0026#34; where rec-0 is the name of one of the pods associated with the Redis Enterprise cluster.\nUsing priority to protect from preemption When a Redis Enterprise node pod is scheduled, it can be assigned a priority class with the priorityClassName property. This property value is the name of a priority class that must already exist within the cluster.\nA sufficiently high priority will prevent other workloads with a lower priority from preempting the scheduling of Redis Enterprise Nodes. Similarly, a high value may also prevent eviction when lower priority workloads are deployed on the same cluster.\nThe successful use of this strategy involves first creating a priority class with a very large priority value:\napiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: redis-enterprise-priority value: 1000000000 globalDefault: false description: \u0026#34;This priority class should be used for Redis Enterprise pods only.\u0026#34; Then, you refer to the priority class by name in your Redis Enterprise cluster CRD:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: example-redisenterprisecluster spec: size: 3 priorityClassName: \u0026#34;redis-enterprise-priority\u0026#34; Alternatively, you can also disable preemption entirely.\nManaging eviction thresholds Eviction thresholds are typically managed by kubelet arguments. You can set the thresholds:\nOn OpenShift - In the config file. On GKE - In the managed settings. We recommend that you:\nSet the soft eviction threshold to be higher than the hard eviction threshold. The high soft threshold makes the node condition change earlier, and alerts the administrator. Set eviction-max-pod-grace-period high enough to allow the RS pods to migrate the Redis databases before the pods are force killed. Set the eviction-soft-grace-period high enough that the administrator (or a k8s auto-scaling mechanism) scales k8s up or out. Monitoring for memory and disk usage We recommend that you monitor the node for MemoryPressure and DiskPressure. When both of these conditions are true, then an eviction threshold is met and the pod is evicted.\nTo retrieve the flags, run the command:\n$kubectl get nodes -o jsonpath=\u0026#39;{range .items[*]}name:{.metadata.name}{\u0026#34;\\t\u0026#34;}MemoryPressure:{.status.conditions[?(@.type == \u0026#34;MemoryPressure\u0026#34;)].status}{\u0026#34;\\t\u0026#34;}DiskPressure:{.status.conditions[?(@.type == \u0026#34;DiskPressure\u0026#34;)].status}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; name:gke-55d1ac88-213c\tMemoryPressure:False\tDiskPressure:False name:gke-55d1ac88-vrpp\tMemoryPressure:False\tDiskPressure:False name:gke-7253cc19-42g0\tMemoryPressure:False\tDiskPressure:False ","categories":["Platforms"]},{"uri":"/kubernetes/reference/","uriRel":"/kubernetes/reference/","title":"Reference","tags":[],"keywords":[],"description":"Reference material for the operator, cluster, and database deployment options.","content":"This section contains the references to the operator, cluster, database deployment options for Kubernetes.\nSupported distributions Support matrix for the current Redis Enterprise K8s operator\nREC custom resource options A primer for the configuration options for Redis Enterprise cluster Custom Resource Definitions.\nREDB custom resource options A primer for the configuration options for Redis Enterprise database custom resource definitions.\n","categories":["Platforms"]},{"uri":"/kubernetes/release-notes/","uriRel":"/kubernetes/release-notes/","title":"Redis Enterprise Software for Kubernetes release notes","tags":[],"keywords":[],"description":"Redis Enterprise Software for Kubernetes operator release notes.","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major changes 6.2.18-41 (Jan 2023) This is a maintenance release for 6.2.18 and includes support for Redis Enterprise 6.2.18-72. 6.2.18-41 (Dec 2022) This is a maintenance release for 6.2.18 and includes bug fixes. 6.2.18-3 (Nov 2022) Support added for additional distributions, as well as feature improvements and bug fixes. 6.2.12-1 (Sept 2022) Support added for additional distributions, as well as feature improvements and bug fixes. 6.2.10-45 (July 2022) Support added for additional distributions as well as some feature improvements. 6.2.10-34 (May 2022) Support for REBD specific upgrade policies, memcached type REDBs, and RHEL8 for OpenShift, as well as feature improvements and bug fixes. 6.2.10-4 (March 2022) Added support for RS 6.2.10 as well as feature improvements and bug fixes. 6.2.8-15 (January 2022) Maintenance release with bug fixes 6.2.8-11 (January 2022) Support for Istio as ingress controller, K8s 1.22 (AKS, kOps, GKE), OpenShift 4.9 6.2.8-2 (November 2021) Support for RS 6.2.8, certificate management, and Redis upgrade policy. 6.2.4-1 (September 2021) Support for RS 6.2.4 and OpenShift 4.8. Support for K8s 1.19 (EKS and AKS), K8s 1.20 (Rancher, EKS, AKS), K8s 1.21 (GKE and kOps). Previous versions Release notes for versions of Redis Enterprise for Kubernetes released more than 18 months ago. ","categories":["Platforms"]},{"uri":"/rs/release-notes/","uriRel":"/rs/release-notes/","title":"Release notes","tags":[],"keywords":[],"description":"","content":"Here\u0026rsquo;s what changed recently in Redis Enterprise Software:\nVersion\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major changes OSS\u0026nbsp;Redis compatibility 6.4.2 (February 2023) Pub/sub ACLs \u0026amp; default permissions. Validate client certificates by subject attributes. Redis 6.2.6 6.2.18 (September 2022) Database auditing. Private key encryption. Active-Active database support for MEMORY USAGE command. Improvements to crdb-cli Redis 6.2.6 6.2.12 (August 2022) OCSP Support. Password \u0026amp; session configuration changes. RHEL 8.6 support. Redis 6.2.6 6.2.10 (February 2022) Python 3 support. RHEL 8.5 support. Redis 6.2.5 6.2.8 (October 2021) RHEL 8 support. Set backup start time. Redis 6.2.3 6.2.4 (August 2021) Internode encryption. Nginx replaced by envoy. New upgrade policies/behavior. Redis 6.2 6.0.20 (April 2021) Role-based LDAP integration. Enhanced client mutual authentication. Active-Active improvements for eviction policies, migration, and the BITFIELD data type. Redis 6.0.9 6.0.12 (January 2021) Distribute synchronization across nodes for Active-Active and Active-Passive databases. Disable internal services to free memory. User accounts support password rotation. Module depdencies automatically installed. Syncer process recovery. Redis 6.0.6 6.0.8 (September 2020) RediSearch 2.0 support. Improved rladmin support for module upgrades. Redis 6.0.5 6.0 (May 2020) ACL and RBAC improvements for database access. Active-Active databases support Redis Streams. Redis 6 5.6.0 (April 2020) Install improvements for RHEL 6 and 7. Active-Active support for HyperLogLog. Redis on Flash now supports RedisJSON. Active-Active default changes for high availability and OSS Cluster API support. Backup support for Google Cloud Storage and Azure Blob storage. Redis 5.0.8 5.5 Preview (April 2019) Preview release. Databases support multiple modules. Previous releases Release notes for Redis Enterprise Software 5.4.14 (February 2020) and earlier versions. ","categories":["RS"]},{"uri":"/rs/clusters/replace-node/","uriRel":"/rs/clusters/replace-node/","title":"Replace a faulty cluster node","tags":[],"keywords":[],"description":"Replace a node in your cluster that is down.","content":"If a node in your Redis Enterprise Software cluster is faulty, its status appears as Down in the Status column of the Nodes page, and in the Cluster \u0026gt; Configuration page.\nTo replace a faulty node:\nAcquire a new node that is identical to the old node, install and configure Redis Enterprise Software on it per the install instructions.\nNote: If you are using Redis on Flash, you must make sure the required flash storage is set up on this new node. Add a new node, as described in adding a new node to a cluster.\nMake sure the new node has as much available memory as the faulty node.\nA message appears, informing you that the cluster has a faulty node and that the new node replaces the faulty node.\nIf the new node has insufficient memory, you are prompted to add a different node - one with sufficient memory.\nNote: If there is a faulty node in the cluster to which you are adding a node, RS enforces using the new node to replace the faulty one. If you are using the DNS NS record based connection approach, the DNS records must be updated each time a node is added or replaced. ","categories":["RS"]},{"uri":"/rs/databases/active-active/syncer/","uriRel":"/rs/databases/active-active/syncer/","title":"Syncer process","tags":[],"keywords":[],"description":"Detailed information about the syncer process and its role in distributed databases.","content":"Syncer process Each node in a cluster containing an instance of an Active-Active database hosts a process called the syncer. The syncer process:\nConnects to the proxy on another participating cluster Reads data from that database instance Writes the data to the local cluster\u0026rsquo;s primary(master) shard Some replication capabilities are also included in open source redis.\nThe primary (also known as master) shard at the top of the primary-replica tree creates a replication ID. This replication ID is identical for all replicas in that tree. When a new primary is appointed, the replication ID changes, but a partial sync from the previous ID is still possible.\nIn a partial sync, the backlog of operations since the offset are transferred as raw operations. In a full sync, the data from the primary is transferred to the replica as an RDB file which is followed by a partial sync.\nPartial synchronization requires a backlog large enough to store the data operations until connection is restored. See replication backlog for more info on changing the replication backlog size.\nSyncer in Active-Active replication In the case of an Active-Active database:\nMultiple past replication IDs and offsets are stored to allow for multiple syncs The Active-Active replication backlog is also sent to the replica during a full sync. Warning - Full sync triggers heavy data transfers between geo-replicated instances of an Active-Active database. An Active-Active database uses partial synchronization in the following situations:\nFailover of primary shard to replica shard Restart or crash of replica shard that requires sync from primary Migrate replica shard to another node Migrate primary shard to another node as a replica using failover and replica migration Migrate primary shard and preserve roles using failover, replica migration, and second failover to return shard to primary Note: Synchronization of data from the primary shard to the replica shard is always a full synchronization. ","categories":["RS"]},{"uri":"/ri/release-notes/v1.9.0/","uriRel":"/ri/release-notes/v1.9.0/","title":"RedisInsight v1.9, January 2021","tags":[],"keywords":[],"description":"RedisInsight v1.9.0","content":"1.9.0 (January 2021) This is the General Availability Release of RedisInsight 1.9 (v1.9.0)!\nHeadlines: RedisGraph tools is getting improved with better UX and new interactions capabilities Adding the ability to configure a database, by just using its direct URL to auto fill all required fields CLI is now providing ability to configure your favorite key-bindings between Emacs or Vim Full Details: Core: Support for configuration of Redis where the number of databases goes over the default 16. Ability to add a Redis database using a shareable URL. RedisGraph UX improvements for large queries: fixed long pause while results are being rendered. Made various improvements to interactions with the graph visualization: Selected node\u0026rsquo;s size increased to make it easier to distinguish. Zoom via mouse wheel. Double click to zoom in. Double right-click to zoom out. Keyboard shortcuts to zoom. Center on node on fetching direct neighbours. Halo masking indirect edges on selected node. Button to reset view: center entire graph. Button to center on the selected node. New zoom buttons CLI Basic navigation key-bindings for Emacs and Vim. UX improvements: the inputs and other controls are now disabled and a message is shown while the command is executing. ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rs-5-2-june-2018/","uriRel":"/rs/release-notes/legacy-release-notes/rs-5-2-june-2018/","title":"Redis Enterprise Software Release Notes 5.2 (June 2018)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software (RS) 5.2 is now available. Key features include new data types and casual consistency for active-active (also known as the Redis CRDT or CRDB Conflict-free Replicated Database), as well as enhanced security features.\nOverview If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process. You can upgrade to RS 5.2 from RS 4.5 and above. If you have a version older than 4.5, you should first upgrade to version 5.0 (or at least 4.5).\nNew features Active-Active Redis (CRDB) supports sorted sets and lists RS 5.2 adds active-active Redis (CRDB) support for Sorted Sets and Lists. Now all major Redis data types are supported with CRDT, so you can use Redis Enterprise in an active-active manner for all your Redis use cases, with seamless conflict resolution. Click here for more information about how to develop applications with geo-replicated CRDBs.\nCausal consistency in Active-Active Redis (CRDB) casual consistency in active-active Redis (CRDB) guarantees that the order of operations on a specific key will be maintained across all CRDB instances. For instance, if operations A and B were applied on the same key, and B was performed after the effect of A was observed by the CRDB instance that initiated B, then all CRDB instances would observe the effect of A before observing the effect of B.\nThis way, any causal relationship between operations on the same key is also observed and maintained by every replica. Such capability is important for various applications, e.g. social network status updates or chat applications (assuring that the chronology of messages doesn\u0026rsquo;t get mixed up). For more information, click here.\nEnhanced security features Admin action audit trail Management actions performed with Redis Software are audited in order to fulfill two major objectives: (1) make sure that system management tasks are appropriately performed and/or monitored by the Administrator(s), and (2) facilitate compliance with regulatory standards such as HIPAA, SOC2, PCI, etc.\nIn order to fulfill both objectives, audit records contain the following information: (1) who performed the action, (2) what exactly was the performed action, (3) when the action was performed, and (4) whether the action succeeded.\nTo get the list of audit records/events, one can use the REST API or the Log page in the UI; the Log page displays the system and user events regarding alerts, notifications and configurations.\nAbility to disable TLS versions Version 5.2 introduces the option to set the minimum TLS version that can be used for encrypting the various flows. You can do so using the REST API or the following rladmin commands:\nFor the management UI and the REST API: rladmin\u0026gt; cluster config min_control_TLS_version [version, e.g. 1.2] For the data path encryption: rladmin\u0026gt; cluster config min_data_TLS_version [version, e.g. 1.2] Note that communications that use older TLS versions will not be allowed.\nHTTPS enforcement With this feature, you can restrict REST API access to only those using HTTPS. In order to do so, use the REST API or the following rladmin command:\nrladmin\u0026gt; cluster config http_support [disabled | enabled] Additional capabilities Support for Redis version 4.0.9 Fixes for Redis important security issues related to the Lua scripting engine Information The end-of-service-life (EOSL) for Redis Enterprise Software 4.4.X is June 30th, 2018, in accordance with our published policy. We recommend that customers running version 4.4 or below upgrade to the latest version. Important fixes RS19755 - Added the ability to enforce HTTPS communication when accessing REST API\nRS19571 - Fixed RHEL installation when FIPS mode is enabled\nRS19490 - Fixed a failure while upgrading from 4.5 to 5.0.2\nRS19475 - Added the ability to disable TLS versions for the control and data paths\nRS18712 - Fixed a failure in endpoint failover\nRS17208 - Added an option to disable Redis commands via Rest API\nRS14973 - Updated NGINX.conf template to turn off exposing the server version\nRS11181 - Set file permissions of log files, redis-conf files, rdb files and their rotations to \u0026ldquo;640\u0026rdquo;\nKnown limitations After creating Redis Enterprise CRDBs which \u0026ldquo;Require SSL for CRDB communication only,\u0026rdquo; one can set definitions via Rest API to activate \u0026ldquo;Require SSL for All Communication\u0026rdquo; without providing a certificate. In such case, connections to the cluster will be blocked. ","categories":["RS"]},{"uri":"/kubernetes/release-notes/previous-releases/","uriRel":"/kubernetes/release-notes/previous-releases/","title":"Previous version Redis Enterprise for Kubernetes release notes","tags":[],"keywords":[],"description":"Release notes for versions of Redis Enterprise for Kubernetes released more than 18 months ago.","content":"Below are archived release notes for Redis Enterprise for Kubernetes versions released more than 18 months ago.\nVersion\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major changes 6.0.20-12 (July 2021) Support for RS 6.0.20-97, EKS, Hashicorp Vault, and added feature support for OpenShift OLM. 6.0.20-4 (May 2021) Support for 6.0.20-69, OpenShift 4.7, and K8s 1.20. Hashicorp Vault integration with REC and REDB secrets. 6.0.12-5 (February 2021) Support for RS 6.0.12-57, Amazon Kubernetes Service (AKS), and role permissions on custom resources. 6.0.8-20 (December 2020) Support for RS 6.0.8, consumer namespaces, Gesher admission controller proxy, and custom resource validation via schema. 6.0.8-1 (October 2020) Support for RS 6.0.8-28, OpenShift 4.5, K8s 1.18, and Gesher admission controller proxy. 6.0.6-24 (August 2020) Various bug fixes. 6.0.6-23 (August 2020) Support for Redis Enterprise Software 6.0.6-39, Rancher support, new database backup and alert options. 6.0.6-11 (July 2020) Maintenance release; support RS 6.0.6-39 and various bug fixes. 6.0.6-6 (June 2020) Support for RS 6.0.6, new database and admission controllers, various improvements and bug fixes. 5.4.14-2 (March 2020) Support for Redis Enterprise Software 5.4.14, K8s 1.16, and OpenShift 4.3. 5.4.10-8 (January 2020) Support for the Redis Enterprise Software 5.4.10 and multiple enhancements. ","categories":["Platforms"]},{"uri":"/ri/release-notes/v1.8.0/","uriRel":"/ri/release-notes/v1.8.0/","title":"RedisInsight v1.8, November 2020","tags":[],"keywords":[],"description":"RedisInsight v1.8.0","content":"1.8.3 (January 2021) This is a maintenance release of RedisInsight 1.8 (v1.8.3)!\nThis fixes the crash on MacOS Big Sur (11.1) for the MacOS package. RedisInsight is supported on Mac hardware with Intel chips, but not for Mac hardware with the Apple M1 (ARM) chip.\n1.8.2 (24 December 2020) This is the maintenance release of RedisInsight 1.8 (v1.8.2)!\nFixes: Browser: Improved handling of large strings, lists, hashes, sets and sorted sets. Better error message when loading a key\u0026rsquo;s value fails. More robust handling of Java serialized objects - malformed Java serialized objects are now displayed as binary strings. Increased the default width of the key selector. Memory Analysis: Fixed crash on databases with modules that store auxiliary module data in the RDB (RediSearch 2, RedisGears, etc.). Fix integer overflow which results in the size of large keys not being reported properly. Add Streams statistics to the charts shown in the Memory Analysis Overview tool. Better error message when online analysis fails due to both the SYNC and DUMP commands being unavailable. 1.8.1 (17 December 2020) Maintenance release for RedisInsight 1.8 including bug fixes and enhancements.\nImportant Note: We\u0026rsquo;d love to learn more how you are using RedisInsight. Now we have a user survey in the application, but you can also get to it here.\nFixes: Core: Fixed placeholder page for modules which was not appearing on desktop packagings of RedisInsight. Fixed error in handling disconnection to databases on desktop packaging of RedisInsight. Fixed auto-fill-on-URL-paste when adding Redis databases. Fixed add database form: scroll to error message if adding a database fails. Fixed typo in Settings page. CLI: Fixed on the repeat command option which was not properly cleared when the command was changed. 1.8.0 (November 2020) This is the General Availability Release of RedisInsight 1.8 (v1.8.0)!\nImportant Note: We\u0026rsquo;d love to learn more how you are using RedisInsight. We introduce a user survey, you\u0026rsquo;ll see it in the application, but it\u0026rsquo;s happening here.\nHeadlines: Guided experience to get started with RedisInsight, add a database and experiment with modules CLI now supports \u0026ldquo;help\u0026rdquo; commands and lets you repeat commands Ability to provide your CA Certificate and skip-verify option for TLS authentications Ability to display RediSearch indices summary Adding a database is getting simpler by auto-filling all database information from the Redis connection URL New environment variables for configuring HOST, PORT and application LOG LEVEL Support for RedisJSON on Redis Cluster Full Details: Core: New welcome page when no databases are configured New information pages for each modules, when they are not configured New environment variables for configuring HOST, PORT and application LOG LEVEL Ability to add a CA Certificate and skip-verify for TLS authentications Added auto-filling database details from Redis Connection URL Browser: Added error message when trying to visualize large keys Fixed issue when copying key with \u0026quot; character RedisGraph, RediSearch, RedisTimeseries: Added ability to copy commands with a button click RedisSearch: Display the selected Index\u0026rsquo;s Summary RedisGraph: Added ability to persist nodes display choices between queries ReJSON: Support for RedisJSON on Redis Cluster ClientList: Sort clients by type CLI: Added support for the help command in CLI Added ability to repeat commands multiple times with increments Added the ability to close the Hint window Added stream command assists Profiler: Fixed issue with TLS databases ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/release-notes-redis-enterprise-software-v5-0-2/","uriRel":"/rs/release-notes/legacy-release-notes/release-notes-redis-enterprise-software-v5-0-2/","title":"Redis Enterprise Software 5.0.2 (2018 March)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software 5.0.2 is now available. Key features include functional and performance updates for CRDB, changes to module deployment, and general fixes.\nOverview If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process.\nYou can upgrade to RS 5.0.2 from RS 4.4.2 and above. If you have a version older than 4.4.2, you must first upgrade to at least 5.0.\nNote: Starting from RS 5.0.2, ports 3338 and 3339 should also be opened on each node for the purpose of internal cluster communication. For more information, check the \u0026rsquo;network port configurations\u0026rsquo; page New features CRDBs The ability to add and remove participating clusters from a CRDB Communications between participating clusters can be encrypted using SSL/TLS Imports can be done to an existing database without flushing the existing data beforehand. Modules Redis Enterprise Modules are installed with Redis Enterprise Software by default RedisBloom and RediSearch Enterprise have been updated to newer versions Other Discovery Service supports encryption using SSL/TLS Starting from version 5.0.2 build #30, Redis Enterprise Software is supported on RHEL 7.5 Important fixes RS16153 \u0026ndash; Supervisord version update RS16667 - Fixed issue with \u0026lsquo;rladmin status\u0026rsquo; timeout RS17746 - Fixed upgrade issue when -s option used RS17997, RS18088 - Upgrade issues when using non-default socket file path RS8584 \u0026ndash; Endpoint migration provided misleading plan message RS17696 - Fixed issues with Multi-proxy and intermittent network issues RS17362 - Upgrade fails under some circumstances RS18351 - Listener active after node has been declared dead RS18874 - Fixed OOM issue due to redis_mgr high memory consumption RS19002 - Fixed wrong message when an upgrade of a quorum node with all-nodes policy takes place Important Fixes in Build #30:\nRS19701 - Fixed high CPU usage on large scale clusters RS19869- Added support for Redis version 4.0.9 RS20153- Fixed Redis important security issues related to the Lua scripting engine RS19852- Fixed proxy crash which might happen for SSL-enabled DBs Known limitations Since Redis Enterprise CRDBs have counters, unlike traditional Redis databases, they must be handled differently when importing. There is a special type of import because of importing counter data types. When performing the import through the admin console, you will be prompted to confirm you want to add the data to the CRDB or stop and go flush the database. This version of RS comes with a pre-bundled python which might over-ride your default installed python version, this can be solved by changing your PATH environment variable. Uploading a Redis Module through the admin console, can be performed only when the admin console is connected to the master node. Write operations are not allowed for database which was created with password of exactly 50-characters. ","categories":["RS"]},{"uri":"/kubernetes/re-clusters/connect-prometheus-operator/","uriRel":"/kubernetes/re-clusters/connect-prometheus-operator/","title":"Connect the Prometheus operator to Redis Enterprise for Kubernetes","tags":[],"keywords":[],"description":"This article describes how to configure a Prometheus operator custom resource to allow it to export metrics from Redis Enterprise for Kubernetes.","content":"To collect metrics data from your databases and Redis Enterprise cluster (REC), you can connect your Prometheus server to an endpoint exposed on your REC. Redis Enterprise for Kubernetes creates a dedicated service to expose the prometheus port (8070) for data collection. A custom resource called ServiceMonitor allows the Prometheus operator to connect to this port and collect data from Redis Enterprise.\nPrerequisites Before connecting Redis Enterprise to Prometheus on your Kubernetes cluster, make sure you\u0026rsquo;ve done the following:\nDeploy Redis Enterprise for Kubernetes (version 6.2.10-4 or newer) Deploy the Prometheus operator (version 0.19.0 or newer) Create a Redis Enterprise cluster Create a ServiceMonitor custom resource Below is an example ServiceMonitor custom resource file. By specifying the service label (app: redis.io/service=prom-metrics) in the selector.matchLabels section, you can point the Prometheus operator to the correct Redis Enterprise service (\u0026lt;rec_name\u0026gt;-prom).\nYou\u0026rsquo;ll need to configure the following fields to connect Prometheus to Redis Enterprise:\nSection Field Value spec.endpoints port Name of exposed port (prometheus) spec.namespaceSelector matchNames Namespace for your REC spec.selector matchLabels REC service label (app: redis.io/service=prom-metrics) Apply the file in the same namespace as your Redis Enterprise cluster (REC). Note: If Redis Enterprise and Prometheus are deployed in different namespaces, you\u0026rsquo;ll also need to add the serviceMonitorNamespaceSelector field to your Prometheus resource. See the Prometheus operator documentation for more details on cross-namespace ServiceMonitor configuration. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: redis-enterprise spec: endpoints: - interval: 15s port: prometheus scheme: https tlsConfig: insecureSkipVerify: true namespaceSelector: matchNames: - \u0026lt;your_REC_namespace\u0026gt; selector: matchLabels: redis.io/service: prom-metrics For more info about configuring the ServiceMonitor resource, see the ServiceMonitorSpec API documentation.\nMore info github.com/prometheus-operator Getting started Running exporters Related resources Troubleshooting ServiceMonitor changes docs.redis.com Metrics in Prometheus Monitoring and metrics ","categories":["Platforms"]},{"uri":"/kubernetes/deployment/container-images/","uriRel":"/kubernetes/deployment/container-images/","title":"Use a private registry for container images","tags":[],"keywords":[],"description":"This section details how the Redis Enterprise Software and Kubernetes operator images can be configured to be pulled from a variety of sources. This page describes how to configure alternate private repositories for images, plus some techniques for handling public repositories with rate limiting.","content":"Redis Enterprise Software, its Kubernetes operator, and the Service Rigger are all distributed as separate container images. Your Kubernetes deployment will pull these images as needed. You can control where these images are pulled from within the operator deployment and also via the Redis Enterprise custom resources.\nIn general, images for deployments that do not have a registry domain name (e.g., gcr.io or localhost:5000) are pulled from the default registry associated with the Kubernetes cluster. A plain reference to redislabs/redis will likely pull from DockerHub (except on OpenShift where it pulls from Red Hat).\nFor security reasons (e.g., in air-gapped environments), you may want to pull the images from a public registry once and then push them to a private registry under your control.\nWarning - It is very important that the images you are pushing to the private registry have the same exact version tag as the original images. Furthermore, because Docker rate limits public pulls, you may want to consider pulling images from a private registry to avoid deployment failures when you hit your DockerHub rate limit.\nThe information below will help you track and configure where your deployments pull container images.\nNote: IMPORTANT\nEach version of the Redis Enterprise operator is mapped to a specific version of Redis Enterprise Software. The semantic versions always match (e.g., 6.0.8), although the specific release numbers may be different (e.g., 6.0.8-1 is the operator version for RS 6.0.8-28). A specific operator version only supports a specific Redis Enterprise version. Other combinations of operator and Redis Enterprise versions are not supported. Find container sources Every pod in your deployed application has a source registry. Any image not prefixed by a registry domain name (e.g., \u0026ldquo;gcr.io\u0026rdquo;) will pull from the default registry for the Kubernetes cluster (i.e., DockerHub). You can use the commands below to discover the pull sources for the images on your cluster.\nTo list all the images used by your cluster:\nkubectl get pods --all-namespaces -o jsonpath=\u0026#34;{..image}\u0026#34; |tr -s \u0026#39;[[:space:]]\u0026#39; \u0026#39;\\n\u0026#39; | uniq -c To specifically determine the pull source for the Redis Enterprise operator itself, run the following command:\nkubectl get pods --all-namespaces -o jsonpath=\u0026#34;{..image}\u0026#34; |tr -s \u0026#39;[[:space:]]\u0026#39; \u0026#39;\\n\u0026#39; | uniq -c | grep redislabs You can limit this command to specific namespaces by replacing the --all-namespaces parameter with a set of -n {namespace} parameters, where each {namespace} is a specific namespace of interest on your cluster.\nCreate a private container registry You can set up a private container registry in a couple of ways:\nOn-premise via Docker registry, Red Hat Quay, or other providers Cloud provider based registries (e.g., Azure Container Registry, Google Container Registry, etc.). Once you have set up a private container registry, you will identify the container registry using:\nA domain name A port (optional) A repository path (optional) Push images to a private container registry Important images for a Redis Enterprise Software deployment include:\nRedis Enterprise Software Bootstrapping a Redis Enterprise cluster node (in the operator image) The Service Rigger The Redis Enterprise Software operator You will need to push all these images to your private container registry. In general, to push the images you must:\nPull the various images locally for the Redis Enterprise Software, the Service Rigger, and the operator. Tag the local images with the private container registry, repository, and version tag. Push the newly tagged images. The example below shows the commands for pushing the images for Redis Enterprise Software and its operator to a private container registry:\nPRIVATE_REPO=...your repo... RS_VERSION=6.0.8-28 OPERATOR_VERSION=6.0.8-1 docker pull redislabs/redis:${RS_VERSION} docker pull redislabs/operator:${OPERATOR_VERSION} docker pull redislabs/k8s-controller:${OPERATOR_VERSION} docker tag redislabs/redis:${RS_VERSION} ${PRIVATE_REPO}/redislabs/redis:${RS_VERSION} docker tag redislabs/operator:${OPERATOR_VERSION} ${PRIVATE_REPO}/redislabs/operator:${OPERATOR_VERSION} docker tag redislabs/k8s-controller:${OPERATOR_VERSION} ${PRIVATE_REPO}/redislabs/k8s-controller:${OPERATOR_VERSION} docker push ${PRIVATE_REPO}/redislabs/redis:${RS_VERSION} docker push ${PRIVATE_REPO}/redislabs/operator:${OPERATOR_VERSION} docker push ${PRIVATE_REPO}/redislabs/k8s-controller:${OPERATOR_VERSION} Configure deployments to use a private container registry Once you push your images to your private container registry, you need to configure your deployments to use that registry for Redis Enterprise Software and operator deployments. The operator container image is configured directly by the operator deployment bundle. The Redis Enterprise cluster pod (RS and bootstrapper) and Service Rigger images are configured in the Redis Enterprise custom resource.\nDepending on your Kubernetes platform, your private container registry may require authentication. If you do need authentication, add a pull secret to your namespace. Then you\u0026rsquo;ll need to configure Kubernetes and the operator to use the pull secret. The two following sections have examples of adding the imagePullSecrets to the operator deployment and pullSecrets to the cluster custom resource.\nSpecify the operator image source The operator bundle contains the operator deployment and the reference to the operator image (redislabs/operator). To use a private container registry, you must change this image reference in your operator deployment file before you deploy the operator. If you apply this change to modify an existing operator deployment, the operator\u0026rsquo;s pod will restart.\nIn the operator deployment file, \u0026lsquo;containers:image\u0026rsquo; should point to the same repository and tag you used when pushing to the private container registry:\n${PRIVATE_REPO}/redislabs/operator:${OPERATOR_VERSION} The example below specifies a 6.0.8-1 operator image in a Google Container Registry:\napiVersion: apps/v1 kind: Deployment metadata: name: redis-enterprise-operator spec: replicas: 1 selector: matchLabels: name: redis-enterprise-operator template: metadata: labels: name: redis-enterprise-operator spec: serviceAccountName: redis-enterprise-operator containers: - name: redis-enterprise-operator image: gcr.io/yourproject/redislabs/operator:6.0.8-1 ... If your container registry requires a pull secret, configure imagePullSecrets on the operator deployment:\nspec: template: spec: imagePullSecrets: - name: regcred Specify the Redis Enterprise cluster images source A Redis Enterprise cluster managed by the operator consists of three container images:\nredislabs/redis: the Redis Enterprise Software container image redislabs/operator: the bootstrapper is packaged within the operator container image redislabs/k8s-controller: the Service Rigger container image By default, a new Redis Enterprise cluster is created using the container images listed above. These container images are pulled from the K8s cluster\u0026rsquo;s default container registry.\nTo pull the Redis Enterprise container images from a private container registry, you must specify them in the Redis Enterprise cluster custom resource.\nAdd the following sections to the spec section of your RedisEnterpriseCluster resource file:\nredisEnterpriseImageSpec: controls the Redis Enterprise Software container image. The version should match the RS version associated with the operator version. bootstrapperImageSpec\u0026quot;: controls the bootstrapper container image. The version must match the operator version. redisEnterpriseServicesRiggerImageSpec: controls the Service Rigger container image. The version must match the operator version. The REC custom resource example below pulls all three container images from a GCR private registry:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 redisEnterpriseImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/redis versionTag: 6.0.8-28 bootstrapperImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/operator versionTag: 6.0.8-1 redisEnterpriseServicesRiggerImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/k8s-controller versionTag: 6.0.8-1 If your private container registry requires pull secrets, you must add pullSecrets to the spec section:\napiVersion: app.redislabs.com/v1 kind: RedisEnterpriseCluster metadata: name: rec spec: nodes: 3 pullSecrets: -name: regcred redisEnterpriseImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/redis versionTag: 6.0.8-28 bootstrapperImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/operator versionTag: 6.0.8-1 redisEnterpriseServicesRiggerImageSpec: imagePullPolicy: IfNotPresent repository: gcr.io/yourproject/redislabs/k8s-controller versionTag: 6.0.8-1 Rate limiting with DockerHub Docker has rate limits for image pulls. Anonymous users are allowed a certain number of pulls every 6 hours. For authenticated users, the limit is larger. These rate limits may affect your Kubernetes cluster in a number of ways:\nThe cluster nodes will likely be treated as a single anonymous user. The number of pulls during a deployment might exceed the rate limit for other deployment dependencies, including the operator, Redis Enterprise Software, or other non-Redis pods. Pull failures may prevent your deployment from downloading the required images in a timely manner. Delays here can affect the stability of deployments used by the Redis Enterprise operator. For these reasons, you should seriously consider where your images are pulled from to avoid failures caused by rate limiting. The easiest solution is to push the required images to a private container registry under your control.\n","categories":["Platforms"]},{"uri":"/modules/redisearch/release-notes/redisearch-2.6-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-2.6-release-notes/","title":"RediSearch 2.6 release notes","tags":[],"keywords":[],"description":"Search using wildcard queries for TEXT and TAG fields, multi-value indexing and querying of attributes for any attribute type, and indexing double-precision floating-point vectors and range queries from a given vector.","content":"Requirements RediSearch v2.6.4 requires:\nMinimum Redis compatibility version (database): 6.0.16 Minimum Redis Enterprise Software version (cluster): 6.2.8 v2.6.4 (December 2022) This is a maintenance release for RediSearch 2.6.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#3289 Potential crash when querying multiple fields (MOD-4639) #3279 Potential crash when querying using wildcard * on TAG field (MOD-4653) Improvements:\n#3256 Support IPv6 on cluster set command #3194 Add the query dialects that are in use to FT.INFO and INFO MODULE commands (MOD-4232) #3258 Add the module version and Redis version to INFO MODULE v2.6 GA (v2.6.3) (November 2022) This is the General Availability release of RediSearch 2.6.\nHighlights This new major version introduces the ability to search using wildcard queries for TEXT and TAG fields. This enables the frequently requested feature suffix search (*vatore and ant?rez are now supported). In addition, the 2.6 release is all about multi-value indexing and querying of attributes for any attribute type ( Text, Tag, Numeric, Geo and Vector) defined by a JSONPath leading to an array or to multiple scalar values. Lastly, this version adds support for indexing double-precision floating-point vectors and range queries from a given vector.\nWhat\u0026rsquo;s new in 2.6 Details Improvements:\n#2886 Support for wildcard queries for TEXT and TAG fields, where ? matches any single character * matches zero or more characters use ' and \\ for escaping, other special characters are ignored #2932 Optimized wildcard query support (i.e., suffix trie) Multi-value indexing and querying #2819, #2947 Multi-value text search - perform full-text search on an array of strings or on a JSONPath leading to multiple strings #3131 Geo #3118 Vector #2985 Numeric #3180 Tag #3060 Return JSON rather than scalars from multi-value attributes. This is enabled via Dialect 3 in order not to break existing applications. Support indexing and querying of multi-value JSONPath attributes and/or arrays (requires JSON \u0026gt;2.4.1) #3182 Support for SORTABLE fields on JSON in an implicit un-normalized form (UNF) #3156 Vector similarity 0.5.1: Better space optimization selection (#175) Aligning index capacity with block size (#177) #3129 Support FLOAT64 as vector data type #3176 Range query support #3105 Support query attributes for vector queries Bugs (since 2.6-RC1 / v2.6.1):\n#3197 Failure to create temporary indices #3098 Wrong return value in Geo query #3230 Use the correct total number of matching records Note: With this release, we stop supporting direct upgrades from RediSearch v1.4 and v1.6 that are End-of-Life. Such RDB files can still be upgraded to RediSearch 2.0 first. Note: If indexing and querying RedisJSON data structures, this version is best combined with RedisJSON 2.4 GA (v2.4.1 onwards). ","categories":["Modules"]},{"uri":"/ri/release-notes/v1.7.0/","uriRel":"/ri/release-notes/v1.7.0/","title":"RedisInsight v1.7, September 2020","tags":[],"keywords":[],"description":"RediSearch 2.0 support and stability improvements","content":"1.7.1 (October 2020) Maintenance release for RedisInsight 1.7 including bug fixes and enhancements.\nHeadlines: Core: New public health-check API to make monitoring deployments easier. Display progress information during memory analysis. Full details: Enhancements and bug fixes\nCore: Fixed support for TLS in Redis Cluster databases. Application name is properly capitalized on MacOSX. Fixed update notifications on Docker - Now links to Docker Hub page and provides instructions for updating. Memory Analysis: Information about the current stage of analysis is now displayed while the analysis runs. Fixed issue with running Memory Analysis on MacOSX (related to system OpenSSL libraries). Browser: Visual improvements to key details view to improve the experience working with long key names. CLI: Improvements for Redis Cluster databases - Controls to target specific nodes, all nodes, only masters/replicas, etc. Streams: Fixed consumer groups functionality on Redis Cluster databases. Telemetry: Report specific modules even when the MODULE LIST command is not available. 1.7.0 (September 2020) Headlines: Support for RediSearch 2.0 Full Details: Core: Added explanation of the supported subscription types for Redis Enterprise Cloud database auto-discovery. Fixed a bug where upgrading from some previous versions would give an error on startup. Use a non-root group by default for the RedisInsight Docker container. Memory Analysis: Improved UI for offline analysis via RDB file stored in S3. Fixed bug where using RDB stored in S3 sub-folder would fail. Browser: Improved support for searching members of large collections (hashes, sets and sorted sets). Streams: Improved UX for the handle to resize key selector. RediSearch: Fixed support for Redis Enterprise Cloud Essentials databases. RedisGraph: Fixed an issue where localstorage is filled with unnecessary data. Analytics: Reporting the subscription type for auto-discovered Redis Enterprise Cloud databases. ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/redis-enterprise-5/","uriRel":"/rs/release-notes/legacy-release-notes/redis-enterprise-5/","title":"Redis Enterprise Pack 5.0 Release Notes (November 2017)","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software 5.0 is now available. Key features include Geo-Distributed Active-Active Conflict-free Replicated Databases (CRDB), LDAP integration, Redis Module integration, and support for the Redis Cluster API.\nOverview If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process.\nYou can upgrade to RS 5.0 from RS 4.4.2 and above. If you have a version older than this, you must first upgrade to at least 4.4.2.\nNew features Support for Redis Cluster API The Redis Cluster API support in Redis Enterprise Software (RS) provides a simple mechanism for Cluster enabled Redis clients to learn and know the cluster topology. This enables clients to connect directly to an RS proxy on the node hosting the master shard for the data being operated on. The result is that for all but the initial call to get the cluster topology or reacquire the location of the master shard, the client will connect to the RS endpoint proxy where the master shard is located. Learn more about the Cluster API implementation.\nGeo-distributed Active-Active conflict-free replicated databases (CRDB) Developing globally distributed applications can be challenging, as developers have to think about race conditions and complex combinations of events under geo-failovers and cross-region write conflicts. CRDBs simplify developing such applications by directly using built-in smarts for handling conflicting writes based on the data type in use. Instead of depending on just simplistic \u0026ldquo;last-writer-wins\u0026rdquo; type conflict resolution, geo-distributed CRDBs combines techniques defined in CRDT (conflict-free replicated data types) research with Redis types to provide smart and automatic conflict resolution based on the data type\u0026rsquo;s intent.\nFor more information, go here. For information, go to Developing with CRDBs.\nRedis modules Redis Modules enable you to extend the functionality of Redis Enterprise Pack. One can add new data types, capabilities, etc. to tailor the cluster to a specific use case or need. Once installed, modules benefit from the high performance, scalability, and high availability that Redis Enterprise is known for.\nRedis modules Redis has developed and certified the following modules for use with Redis Enterprise Pack:\nRediSearch This module turns RS into a supercharged distributed in-memory full-text indexing and search beast. ReJSON Now you have the convenience JSON as a built-in data type and easily able to address nested data via a path. RedisBloom Enables RS to have a scalable bloom filter as a data type. Bloom filters are probabilistic data structures that quickly determine if values are in a set. Custom modules In addition, Redis Enterprise Pack provides the ability to load and use custom Redis modules or of your own creation.\nSupport for Docker Deploying and running your Redis Enterprise Software cluster on Docker containers is supported in development systems and available to pull from Docker hub. With the official image, you can easily and quickly test several containers to build the scalable and highly available cluster Redis Enterprise Software is famous for.\nFor more information, go to quick start with Redis Enterprise on Docker.\nLDAP Integration As part of our continued emphasis on security, administrative user accounts in Redis Enterprise Pack can now use either built-in authentication or authenticate externally via LDAP with saslauthd. The accounts can be used for administering resources on the cluster via command line, Rest API, or admin console.\nFor more information see LDAP Integration.\nAdditional capabilities Support for additional Redis commands and features:\nSupport for Redis version 4.0.2 Support added for RHEL/CentOS 6.9 and 7.4 Information In the node bootstrap API, the structure of the JSON has changed for adding an external IP during the bootstrap process. End-of-Life for RHEL/CentOS 6.5 and 6.6 have been reached, so those versions are no longer supported. Modules are not supported in Redis Enterprise Pack 5.0 for RHEL/CentOS 6.x Important fixes 5.0.0-31 RP9299 - Issue with reliability of metric ingress RP9680 - Redis Enterprise Pack starting before /etc/rc.local script executed RP12363 - In some cases, flash drives do not mount following a stop/start of the node RP12493 - Allow change to debug package creation location RP13079 - DNS doesn\u0026rsquo;t change after having removed the external IP address in some cases RP13933 - rladmin balance sometimes shows incorrect information RP14060 - pubsub stats aren\u0026rsquo;t reflected correctly by the stats archiver RP14939 - Add license checks to all needed entry points in the cluster RP15090 - Problem in log rotate script causes other logs to not rotate RP15104 - rlutil check fix doesn\u0026rsquo;t work sometimes RP15130 - Fixed permission on two logs with incorrect ownership RP15160 - Allow option to change ports for API and CM in NGINX RP15164 - Allow unix socket folder to be configurable at build time RP15499 - rladmin command not responding on a cluster with large number of shards RP15853 - When trying to add a new db the \u0026lsquo;activate\u0026rsquo; button was changed to \u0026lsquo;update\u0026rsquo; and was grayed out. RP15861 - Allow unix socket folder to be configurable at install time RP16115 - TTL bug with Replica Of and import RP16447 - DMC client connection reports incorrect number of connections to monitoring applications. RP16481 - In some cases, resource_mgr uses incorrect directories to compute persistent and ephemeral storage ","categories":["RS"]},{"uri":"/kubernetes/security/manage-rec-credentials/","uriRel":"/kubernetes/security/manage-rec-credentials/","title":"Manage Redis Enterprise cluster (REC) credentials","tags":[],"keywords":[],"description":"","content":"Redis Enterprise for Kubernetes uses a custom resource called RedisEnterpriseCluster to create a Redis Enterprise cluster (REC). During creation it generates random credentials for the operator to use. The credentials are saved in a Kubernetes (K8s) secret. The secret name defaults to the name of the cluster.\nNote: This procedure is only supported for operator versions 6.0.20-12 and above. Retrieve the current username and password The credentials can be used to access the Redis Enterprise admin console or the API. Connectivity must be configured to the REC pods using an appropriate service (or port forwarding).\nInspect the random username and password created by the operator during creation with the kubectl get secret command.\nkubectl get secret rec -o jsonpath=\u0026#39;{.data}\u0026#39; The command outputs the encoded password and username, similar to the example below.\nmap[password:MVUyTjd1Mm0= username:ZGVtb0ByZWRpc2xhYnMuY29t] Decode the password and username with the echo command and the password from the previous step.\necho MVUyTjd1Mm0= | base64 --decodexc This outputs the password and username in plain text. In this example, the plain text password is 12345678 and the username is demo@redis.com.\nChange the Redis Enterprise cluster (REC) credentials Change the REC password for the current username Access a pod running a Redis Enterprise cluster.\nkubectl exec -it \u0026lt;rec-resource-name\u0026gt;-0 bash Add a new password for the existing user.\nREC_USER=\u0026#34;`cat /opt/redislabs/credentials/username`\u0026#34; \\ REC_PASSWORD=\u0026#34;`cat /opt/redislabs/credentials/password`\u0026#34; \\ curl -k --request POST \\ --url https://localhost:9443/v1/users/password \\ -u \u0026#34;$REC_USER:$REC_PASSWORD\u0026#34; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#34;{\\\u0026#34;username\\\u0026#34;:\\\u0026#34;$REC_USER\\\u0026#34;, \\ \\\u0026#34;old_password\\\u0026#34;:\\\u0026#34;$REC_PASSWORD\\\u0026#34;, \\ \\\u0026#34;new_password\\\u0026#34;:\\\u0026#34;\u0026lt;NEW PASSWORD\u0026gt;\\\u0026#34;}\u0026#34; From outside the pod, update the REC credential secret.\nSave the existing username to a text file .\necho -n \u0026#34;\u0026lt;current_username\u0026gt;\u0026#34; \u0026gt; username Save the new password to a text file.\necho -n \u0026#34;\u0026lt;new_password\u0026gt;\u0026#34; \u0026gt; password Update the REC credential secret.\nkubectl create secret generic \u0026lt;cluster_secret_name\u0026gt; \\ --from-file=./username \\ --from-file=./password --dry-run \\ -o yaml kubectl apply -f Wait five minutes for all the components to read the new password from the updated secret. If you proceed to the next step too soon, the account could get locked.\nAccess a pod running a Redis Enterprise cluster again.\nkubectl exec -it \u0026lt;rec-resource-name\u0026gt;-0 bash Remove the previous password to ensure only the new one applies.\nREC_USER=\u0026#34;`cat /opt/redislabs/credentials/username`\u0026#34;; \\ REC_PASSWORD=\u0026#34;`cat /opt/redislabs/credentials/password`\u0026#34;; \\ curl -k --request DELETE \\ --url https://localhost:9443/v1/users/password \\ -u \u0026#34;$REC_USER:$REC_PASSWORD\u0026#34; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#34;{\\\u0026#34;username\\\u0026#34;:\\\u0026#34;$REC_USER\\\u0026#34;, \\ \\\u0026#34;old_password\\\u0026#34;:\\\u0026#34;\u0026lt;OLD PASSWORD\\\u0026#34;}\u0026#34; Note: The username for the K8s secret is the email displayed on the Redis Enterprise admin console. Change both the REC username and password Connect to the admin console\nAdd another admin user and choose a new password.\nSpecify the new username in the username field of your REC custom resource spec.\nUpdate the REC credential secret:\nSave the existing username to a text file.\necho -n \u0026#34;\u0026lt;current_username\u0026gt;\u0026#34; \u0026gt; username Save the new password to a text file.\necho -n \u0026#34;\u0026lt;new_password\u0026gt;\u0026#34; \u0026gt; password Update the REC credential secret.\nkubectl create secret generic \u0026lt;cluster_secret_name\u0026gt; \\ --from-file=./username \\ --from-file=./password --dry-run \\ -o yaml kubectl apply -f Wait five minutes for all the components to read the new password from the updated secret. If you proceed to the next step too soon, the account could get locked.\nDelete the previous admin user from the cluster.\nNote: The operator may log errors in the time between updating the username in the REC spec and the secret update. Update the credentials secret in Vault If you store your secrets with Hashicorp Vault, update the secret for the REC credentials with the following key-value pairs:\nusername:\u0026lt;desired_username\u0026gt;, password:\u0026lt;desired_password\u0026gt; For more information about Vault integration with the Redis Enterprise Cluster see Integrating Redis Enterprise for Kubernetes with Hashicorp Vault.\n","categories":["Platforms"]},{"uri":"/modules/redisearch/release-notes/redisearch-2.4-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-2.4-release-notes/","title":"RediSearch 2.4 release notes","tags":[],"keywords":[],"description":"Vector Similarity Search (VSS). New query syntax Dialect version 2. Choose between Dialect 1 and Dialect 2 for query parser behavior. Hybrid queries.","content":"Requirements RediSearch v2.4.16 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.0 v2.4.16 (November 2022) This is a maintenance release for RediSearch 2.4\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2962 Crash upon AOF preload - Use local GC context in periodic callback (MOD-3951) #2863 High CPU Utilization - Change MAX_LEV_DISTANCE to 4 (MOD-3563) #3041, #3063, #3051, #3143 Several memory leaks (MOD-4121, MOD-4252) #3119 Crash on intersect iterator GetCriteriaTester (MOD-4200) #3128 Filter rules must be reevaluated per index per document (MOD-4207) #3127 Fix assertion failure on wrong result counting, which leads to a crash (MOD-4214) #3171 Missing implementation of NumericRangeIterator_OnReopen which lead to crash (MOD-4255) #3191 Wrong query iterator casting which resulted in 100% CPU utilization (MOD-4290) #3197 Release failed to create temporary indices on the main thread (MOD-4388) #2981 Double freeing in iterators of hybrid queries resulting in crash (MOD-4411) #3161 Latency used to increase over time when combining INKEYS and wildcard query (MOD-4343) v2.4.15 (October 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#3095 Replace order of parsing the parameters and query in the coordinator (MOD-4205) #3012 Improved efficiency of LLAPI findInfo, which could reduce stability during upgrade in Redis Enterprise (MOD-4197, MOD-4052) #3040, #3049 Fix for SORTBY numeric field for non-SORTABLE fields on the coordinator (MOD-4115) #3050 Results from fields from a missing value should come last when combined with SORTBY (MOD-4120) Improvements:\n#3047 Add strlen string function to FT.AGGREGATE (MOD-4141) #3038 Add number_of_uses to FT.INFO for the number of times the index was queried v2.4.14 (August 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nImprovements:\nVector similarity v0.3.2 #2955 Add timeout during prefix query (MOD-3949) #2957 Efficient removal from prefix list for cases with many indices Bug fixes:\n#2937 Returning NULL response after encountering an expired document (MOD-3515) #2962 Crash upon AOF preload (MOD-3951) #2986 Memory leak related to schema prefixes v2.4.11 (July 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2892 Combining SORTBY with MAX on FT.SEARCH (which is not supported) caused an inconsistent response and out-of-memory error (MOD-3540, MOD-3644) VecSim v0.3.1 HNSW indices: reclaim memory upon deletion - HNSW index\u0026rsquo;s data structures now reclaim memory and shrink upon deletion Improvements:\nVecSim v0.3.1 HNSW indices: delete procedure is up to 40% faster More accurate memory consumption reporting for HNSW indices v2.4.10 (July 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2863 Crash due to too high (Levenshtein) DISTANCE in FT.SPELLCHECK. This fix limits the DISTANCE to 4. (MOD-3563) #2875 Not all documents with vector fields were indexed with Redis on Flash (MOD-3584) #2846 Enforce Redis Enterprise memory limit for vector indices v2.4.9 (June 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2837, #2836 Crash on FT.AGGREGATE \u0026ldquo;\u0026hellip; APPLY \u0026lsquo;-INF % -1\u0026rsquo;\u0026hellip;\u0026rdquo; #2814 FT.EXPLAIN without parameters causes a crash #2790 Incorrect num_terms value in FT.INFO after a term is deleted from all the docs (garbage collection) #2804 Freeze when OFFSET+LIMIT was greater than maxSearchResults (config) #2791 Add BlockedClientMeasureTime to coordinator for more accurate performance stats #2802 Tagged parts of keys (curly brackets {}) are now returned by FT.SEARCH Improvements:\n#2806 Do not load the JSON API when RediSearch is initialized as a library Minor breaking change:\nAs pointed out above, #2802 is a bug fix. However, if your application relies on RediSearch incorrectly trimming the tagged part of a key (using {}), this could break your application. This only applies to users who are using RediSearch in clustered databases. v2.4.8 (May 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent. However, if you\u0026rsquo;re using Vector Similarity (introduced in RediSearch 2.4), there are some critical bugs that may affect a subset of users. In this case, you should upgrade.\nDetails:\nBug fixes:\n#2739 Memory leak in coordinator related to Vector Similarity (MOD-3023) #2736, #2782 Memory allocation restrictions for Vector Similarity indices (causing OOM) (MOD-3195) #2755 Compare the entire vector field name instead of a prefix when creating a new vector index #2780 Initialize all variables in EvalContext (which might have led to crashes in clustered databases) Improvements:\n#2740 Performance optimization for hybrid vector queries v2.4.6 (May 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2716 Removed assert statement that could cause crashes with replica of (MOD-3008, MOD-3012) #2734 ON_TIMEOUT RETURN policy fix: return results obtained until timeout rather than discarding them #2714 Memory leak on non-TLS setup in coordinator v2.4.5 (April 2022) This is a maintenance release for RediSearch 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2702 INKEYS combined with Vector Similarity caused server unresponsiveness (MOD-2952) #2705 Incorrect results when deleting a document that was skipped at index time #2698 Synonyms in Chinese Improvements:\n#2694 Performance: In a TEXT field, skip term iterator if term does not appear in requested field v2.4.3 (March 2022) This is the General Availability release of RediSearch 2.4.\nHeadlines RediSearch 2.4 introduces a new capability, Vector Similarity Search (VSS), which allows indexing and querying vector data stored (as BLOBs) in Redis hashes.\nIt also introduces a new query syntax to address query parser inconsistencies found in previous versions of RediSearch. Users can now choose between Dialect version 1 (to keep existing query parser behavior) or Dialect version 2 (to switch to the updated behavior).\nAll VSS queries or any query using the PARAMS option must use Dialect version 2.\nWhat\u0026rsquo;s new in 2.4 FT.CREATE is extended to support the creation of 2 popular types of vector indexes:\nFLAT Index\nThis type of index is used when the recall is more important than the speed of query execution. A query vector will be compared to all vectors in a flat index. The search results will return the exact top k nearest neighbors to the query vector.\nHierarchical Navigable Small World (HNSW) Index\nThis index is a modified implementation of the library written by the author of this influential academic paper. An HNSW vector index is used when the speed of query execution is preferred over recall. The results returned are approximate nearest neighbors (ANNs).\nYou can try out different HNSW index parameters (M, EFCONSTRUCTION, EFRUNTIME) to improve the “recall versus speed” balance.\nUse FT.SEARCH to retrieve the top K hashes with the most similar vectors to a given query vector.\nHybrid queries in FT.SEARCH:\nUse hybrid queries to retrieve Redis hashes that match a combination of vector and non-vector search criteria. Non-vector search criteria can include expressions combining NUMERIC, TEXT, TAG, and GEO fields.\nHybrid queries are often used in modern ecommerce search applications featuring “visual” similarity plus metadata similarity. For example, you can use a single hybrid query to find products that are visually similar to a given image within a price range and/or geo-location.\nUse FT.CONFIG to set DEFAULT_DIALECT at the module level. By default, DEFAULT_DIALECT is set to 1.\nOverride DIALECT:\nIt is possible to override the module-level dialect for a specific command at runtime. You can specify the dialect when executing any of the following commands:\nFT.SEARCH\nFT.AGGREGATE\nFT.EXPLAIN\nFT.EXPLAINCLI\nFT.SPELLCHECK\nIf you do not specify dialect when running any of these commands, they will use the default module-level dialect value.\nDetails Features:\n#2671 Add Dialect support Performance enhancements (since 2.4-RC1):\n#2647 Normalize vector once for ad-hoc flat search #2638 Optimized hybrid query when no scores are required #2653 Updating specific field load optimization rule #2670 Use REDISMODULE_EVENT_SHUTDOWN to clear resources Security and privacy (since 2.4-RC1):\n#2584 Fix MOD-2086, added support for TLS passphrase Bug fixes (since 2.4-RC1):\n#2651 Fix client freeze on docs expire during query #2641 Memory leak in the coordinator #2645 Ignore NULL value on ingest #2654 VecSim hybrid query - return empty iterator for invalid intersection child iterator Introducing DIALECT RediSearch 2.4.3 introduces a new query syntax to address query parser inconsistencies found in previous versions of RediSearch. Users can now choose between:\nDialect version 1 (to keep the query dialect as in RediSearch 2.2)\nDialect version 2 (to use the updated dialect)\nExisting RediSearch 2.2 users will not have to modify their queries since the default dialect is 1. However, all RediSearch users should gradually update their queries to use dialect version 2.\nBackground Under certain conditions, some query parsing rules did not behave as originally intended. Queries containing the following operators could return unexpected results:\nAND quotes, ~, -, and % (exact, optional, negation, fuzzy) OR To minimize the impact on existing, unaffected RediSearch users, a DIALECT setting was introduced to allow:\nExisting queries to run without any modification (DIALECT 1)\nNew queries to benefit from the updated query-parsing behavior (DIALECT 2)\nExamples of impacted queries Your existing queries may behave differently under different DIALECT versions, if they fall into any of the following categories:\nYour query has a field modifier followed by multiple words.\nConsider this simple query @name:James Brown.\nThe field modifier @name is followed by 2 words: James and Brown.\nWith DIALECT 1, the parser interprets this query as \u0026ldquo;find James Brown in the @name field.\u0026rdquo;\nWith DIALECT 2, the parser interprets it as \u0026ldquo;find James in the @name field AND Brown in ANY text field.\u0026rdquo; In other words, the query parser interprets it as (@name:James) Brown.\nWith DIALECT 2, you could achieve the default behavior from DIALECT 1 by updating your query to @name:(James Brown).\nYour query uses quotes, ~, -, % (exact, optional, negation, fuzzy).\nConsider a simple query with negation -hello world.\nWith DIALECT 1, the parser interprets this query as \u0026ldquo;find values in any field that does not contain hello AND does not contain world.\u0026rdquo; This is the equivalent of -(hello world) or -hello -world.\nWith DIALECT 2, the parser interprets it as -hello AND world, so only hello is negated.\nWith DIALECT 2, you could achieve the default behavior from dialect 1 by updating your query to -(hello world).\nAnother example that illustrates the differences in parser behavior is hello world | \u0026quot;goodbye\u0026quot; moon:\nWith DIALECT 1, the parser interprets this query as searching for (hello world | \u0026quot;goodbye\u0026quot;) moon\nWith DIALECT 2, the parser interprets it as searching for either hello world OR \u0026quot;goodbye\u0026quot; moon.\nNote: This is the first GA version of 2.4. The version inside Redis will be 2.4.3 in semantic versioning. Since the version of a module in Redis is numeric, we could not add a GA flag. ","categories":["Modules"]},{"uri":"/ri/release-notes/v1.6.0/","uriRel":"/ri/release-notes/v1.6.0/","title":"RedisInsight v1.6, June 2020","tags":[],"keywords":[],"description":"Rootless Docker container, Copy keys in Browser and Stream UX improvements","content":"1.6.3 (July 2020) Maintenance release for RedisInsight 1.6 including bug fixes and enhancements.\nHeadlines: Mac application is now getting notarized which simplifies the installation process on OS X Fixed the resize of the keys explorer allowing to see long keys Fixed filtering of keys in the browser with the filter capability in the browser Full details: Enhancements and bug fixes\nCore: Mac application is properly signed and notarized on Apple services. Browser: Fixed resizing the key explorer allows to see more characters of long keys. Asynchronous loading of keys in large databases (discovered keys are actionable while search continues). Improved performance for exact key searches (when search pattern is not using *). Improved UI showing the progress of scanning the keys in the database. Fixed filtering the keys by data structures could be to wrong. Fixed behavior of EXISTS and TYPE command when those have ACL restrictions. Fixed behavior when keys matching filters have ACL restrictions. Improved “Stop Scan” button behavior to respond immediately. Added visual indicator to show that by default, the browser is filtering out inner keys from modules. 1.6.2 (30 June 2020) Maintenance release for RedisInsight 1.6 including bug fixes and enhancements.\nHeadlines: Performance improvements to Profiler tool for TLS-enabled databases. Bugfix: Feedback button was not visible. Full details: Enhancements and bug fixes Core: Bugfix: Feedback button was not visible. Profiler: The native code implementation of the profiling logic was updated to add full support for TLS connections to Redis. Graph: Updated to use a newer version of the Ogma graph visualization library. Analytics: Bugfix: Report the OS/platform correctly. 1.6.1 (24 June 2020) Maintenance release for RedisInsight 1.6 including bug fixes and enhancements.\nHeadlines: Improved support for Redis 6 ACLs with Cluster and Sentinel databases Added support for Redis Cluster in RedisGraph tool UX improvements for RedisGraph tool Enriched captured usage events Full details: Enhancements and bug fixes Core: Improved support for Redis 6 ACLs with Cluster and Sentinel databases Added events capturing usage of RedisInsight Graph: Add support for Redis Cluster Added option to configure labels to be displayed in graph\u0026rsquo;s nodes (right-click on the node) Added ability to submit query with \u0026lsquo;ctrl + enter\u0026rsquo; in single line mode TimeSeries: Added ability to submit query with \u0026lsquo;ctrl + enter\u0026rsquo; in single line mode RediSearch: Added ability to submit query with \u0026lsquo;ctrl + enter\u0026rsquo; in single line mode Better handling of long index names in index selector dropdown Fixed bug with pagination on queries with whitespace in the query string Gears: Added button to remove an execution from executed functions list Added option to get rid of the warning message when executing a function Fixed error message when it\u0026rsquo;s impossible to visualize the graph Streams: Added persistence of user\u0026rsquo;s selected stream columns when switching pages 1.6.0 (11 June 2020) This is the General Availability Release of RedisInsight 1.6!\nHeadlines: RedisInsight docker container is now rootless being compliant with best practices for containers The Browser gets improved to allow quick copy of keys and resizing of the key explorer Streams is now allowing to sort entries by timestamp, active/unactive live streaming of entries and keep persisted user\u0026rsquo;s selection of fields to save context when switching between streams or other tools of RedisInsight. New telemetry system allowing to capture tools usage and updated privacy settings Full details: Features\nCore: Improved docker container by making it rootless Added visual indicator to show configured user when connecting to Redis 6 using ACLs Improved navigation to application\u0026rsquo;s settings Browser: Added ability to resize the Key explorer panel Added options to easily copy keys Added ability to filter out the inner keys Streams: Added persisting selected fields to be displayed to save context when switching to another stream or tool of RedisInsight Added ability to sort entries \u0026ldquo;ascending\u0026rdquo; or \u0026ldquo;descending\u0026rdquo; based on the timestamp Added ability to active/unactive the live streaming of events Updated timestamp font family for consistency CLI: Added ACL commands hints and summary info in CLI Bug Fixes:\nCore: Fixed issue fetching data from Redis Enterprise Cloud and replica enabled Browser: Fixed issue not shown field named \u0026ldquo;key\u0026rdquo; in hash keys Fixed wrong number of database\u0026rsquo;s keys being displayed Fixed error when trying to view a Java serialized object Stream: Fixed issue with live streaming of entries Fixed UI when no entries are present in a stream Bulk Actions: Fixed responsiveness of the UI RedisGears: Fixed focus on editor and display of requirements ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/redis-pack-4-5-0-may-2017/","uriRel":"/rs/release-notes/legacy-release-notes/redis-pack-4-5-0-may-2017/","title":"Redis Enterprise Pack 4.5 Release Notes (May 2017)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process.\nYou can upgrade to this version from any 4.4 version. If you have a version older than 4.4 you must first upgrade to 4.4 or higher, and only then upgrade to this version.\nNew features The new discovery service with supportfor Redis Sentinel API The Discovery Service provides an IP-based connection management service used when connecting to Redis Enterprise Pack databases. When used in conjunction with Redis Enterprise Pack\u0026rsquo;s other high availability features, the Discovery Service assists an application cope with topology changes such as adding, removing of nodes, node failovers and so on. It does this by providing your application with the ability to easily discover which node hosts the database endpoint. The API used for discovery service is compliant with the Redis Sentinel API.\nDiscovery Service is an alternative for applications that do not want to depend on DNS name resolution for their connectivity. Discovery Service and DNS based connectivity are not mutually exclusive. They can be used side by side in a given cluster where some clients can use Discovery Service based connection while others can use DNS name resolution when connecting to databases.\nBuilding large databases with RAM and Flash memory in Redis on Flash v2.0 With Redis Enterprise Pack 4.5, Redis on Flash v2 is production ready. RFv2 brings performance, reliability, and stability enhancements as well as many features customers have been waiting for.\nRedis on Flash (RoF) offers users of Redis Enterprise Pack and Redis Enterprise Cloud Private the unique ability to operate a Redis database that spans both RAM and flash memory (SSD), but remains separate from Redis Enterprise Pack\u0026rsquo;s persistence mechanisms. Whilst keys are always stored in RAM, RoF intelligently manages the location of their values (RAM vs Flash) in the database via a LRU-based (least-recently-used) mechanism. Hot values will be in RAM and infrequently used, while warm values will be ejected to flash memory. This enables you to have much larger datasets with RAM-like latency and performance, but at dramatically lower cost than an all-RAM database.\nAdditional capabilities Support for additional Redis commands and features:\nThe Redis TOUCH command is now supported Redis version upgraded to 3.2.8 Support for OBJECT in Lua scripts Support has been added RHEL 7.3 with this version.\nImportant fixes RP10106 - SSL Certificate should not need to be signed with a stronger hashing algorithm to be accepted RP10465 - failover times can be higher under certain scenarios in local watchdog profile RP10633 - Improve install.sh and answers file RP11880 - Improved replica sync and add node robustness RP8689 - Minimized impact when changing RAM-Flash limit on Redis Enterprise Flash RP12063 - Improved Redis Flash data import/population performance RP11608 - Improvements to databases.txt creation in support package RP11941 - Eliminated warning and errors during upgrade on RHEL6 with leash and python2.6 is installed RP11994 - Databases under certain cases may not display in UI even though they are in the cluster metadata and safely operating. RP12438 - Email alerts with Amazon SES may fail under certain conditions. RP12538 - Redis failover was initiated by node_wd during sync to new replica RP10264 - Improved debuginfo package for better supportability Important fixes in RP 4.5.0-22\nRP12667 - NGINX security improvements, improved TLS and SSL related warnings on security scans. RP12690 - Added simpler ability to recover cluster from AWS S3 RP13359 - Advanced memory allocation and booking enhancements Important fixes in RP 4.5.0-31\nRP13060 - Client may experience reconnect issues after failover of the endpoint. RP13711 - Disabling IPv6 may cause startup of node services to fail (NGINX) on RHEL 7 RP12747 - Calculation can be incorrect for memory quota for databases Important fixes in RP 4.5.0-35\nRP12844 - cnm_exec crashes on \u0026ldquo;not enough arguments for format string\u0026rdquo; Important fixes in RP 4.5.0-43\nRP9846 - In UI, Used Memory may show incorrect values RP12211 - UI fails import from a Redis OSS DB with password RP13356 - Enable failover when license expires RP14692 - rladmin status command may crash during backup RP14541 - In rare cases, DMC log grew quickly and caused stability issues RP15107 - When using Redis on Flash, it may cause DMC proxy crashes Important fixes in RP 4.5.0-47\nMultiple important Redis on Flash updates. Important fixes in RP 4.5.0-51\n15161 - Make Unix socket folder configurable at install time 15164 - Make Unix socket folder configurable at build time 16082 - Make Unix socket folder configurable at runtime Fixed an issue where on import of data, TTL information was set incorrectly ","categories":["RS"]},{"uri":"/kubernetes/security/manage-rec-certificates/","uriRel":"/kubernetes/security/manage-rec-certificates/","title":"Manage Redis Enterprise cluster (REC) certificates","tags":[],"keywords":[],"description":"Install your own certificates to be used by the Redis Enterprise cluster&#39;s operator.","content":"By default, Redis Enterprise Software for Kubernetes generates TLS certificates for the cluster during creation. These self-signed certificates are generated on the first node of each Redis Enterprise cluster (REC) and are copied to all other nodes added to the cluster. For the list of of certificates used by Redis Enterprise Software and the traffic they encrypt, see the certificates table.\nTo install and use your own certificates with Kubernetes on your Redis Enterprise cluster, they need to be stored in secrets. The REC custom resource also needs to be configured with those secret names to read and use the certificates.\nCreate a secret to hold the new certificate Create the secret with the required fields shown below.\n```sh kubectl create secret generic \u0026lt;secret-name\u0026gt; \\ --from-file=certificate=\u0026lt;/PATH/TO/certificate.pem\u0026gt; \\ --from-file=key=\u0026lt;/PATH/TO/key.pem\u0026gt; \\ --from-literal=name=\u0026lt;proxy | api | cm | syncer | metrics_exporter\u0026gt; ``` Update certificates in the REC custom resource Edit the Redis Enterprise cluster (REC) custom resource to add a certificates subsection under the spec section. You are only required to add the fields for the certificates you are installing.\nspec: certificates: apiCertificateSecretName: \u0026lt;apicert-secret-name\u0026gt; cmCertificateSecretName: \u0026lt;cmcert-secret-name\u0026gt; syncerCertificateSecretName: \u0026lt;syncercert-secret-name\u0026gt; metricsExporterCertificateSecretName: \u0026lt;metricscert-secret-name\u0026gt; proxyCertificateSecretName: \u0026lt;proxycert-secret-name\u0026gt; Update certificates through the API Alternatively, you can also update the REC certificates via the API:\nPUT /v1/cluster/update_cert { \u0026#34;certificate\u0026#34;: \u0026lt;certificate\u0026gt;, \u0026#34;key\u0026#34;: \u0026lt;cert-key\u0026gt;, \u0026#34;name\u0026#34;: \u0026lt;cert-name\u0026gt; } Verify the certificate was updated Check the operator logs and use the API to verify the certificate has been updated.\nGET /v1/cluster/certificates More info Update certificates Install your own certificates Glossary/Transport Layer Security (TLS) ","categories":["Platforms"]},{"uri":"/modules/redisearch/release-notes/redisearch-2.2-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-2.2-release-notes/","title":"RediSearch 2.2 release notes","tags":[],"keywords":[],"description":"Search and index JSON documents. Profiling queries. Field aliasing.","content":"Requirements RediSearch v2.2.10 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.0 v2.2.10 (March 2022) This is a maintenance release for RediSearch 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nImprovements:\nMac M1 support #2645 Ignore NULL values in JSON documents on indexing (prior behaviour would ignore the entire document) #2623 Improved Multi Sortby error message for FT.SEARCH Bug fixes:\n#2641 Memory leak in Coordinator #2651 Client freeze on docs expire during query #2670 Memory leak in RediSearch found in Active-Active (MOD-2518) v2.2.9 (March 2022) This is a maintenance release for RediSearch 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nImprovements:\n#2605 Added support for tls-key-file-pass capability (MOD-2086) #2583 Release index-specific information off the main thread (performance enhancement) Bug fixes:\n#2436 When indexing JSON documents, filters cause no documents to be indexed (MOD-2214) #2507 QUANTILE aggregation function outputting wrong values (MOD-2432) #2521 contains() with an empty string argument leaves Redis hanging at CPU 100% indefinitely (MOD-2428) #2560 Free prefix and cursor efficiently for cases with many indices (MOD-2080) #2541 Numeric types for FT.INFO on coordinator #2553 Fix union high iterator #2404 Update coordination strategy of FlatSearchCommandHandler v2.2.7 (February 2022) This is a maintenance release for RediSearch 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2466 FT.PROFILE for FT.AGGREGATE on clustered databases #2473 FT.PROFILE with no result processor present #2490 Case sensitivity issue in searches of TAG field on JSON Improvements:\n#2469 Add API for TAG children query nodes v2.2.6 (January 2022) This is a maintenance release for RediSearch 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2362 Crash on empty field name #2407 Inconsistency with FT.CREATE and ON: return error for spec without fields parameters #2392 Remove limit on Geo field precision #2440 Fix in NOT(-) iterator #2414 Prevent server freeze when FT.SEARCH timeout while sorting #2386 Memory leak in numeric field index Improvements:\n#2424, #2425 Performance improvements for numeric indices #2408 API: Added RediSearch_IndexInfo #2448 API: Added setLang and setScore v2.2.5 (November 2021) This is the General Availability release of RediSearch 2.2.\nHeadlines Searching and indexing JSON documents This release introduces the ability to index, query, and full-text search JSON documents using JSONPath queries.\nOn the schema creation FT.CREATE, it is now possible to map a JSONPath query with a field. When a JSON document is indexed, the value extracted by the JSONPath query is indexed in the given field.\nThis features require the module RedisJSON 2.0 to be installed.\nProfiling queries With the new FT.PROFILE command, it is now possible to profile in detail the execution time of several internal steps involved in the execution of FT.SEARCH and FT.AGGREGATE. That way, it is possible to understand which part of the query is taking most of the resources.\nField aliasing With the support of JSON document indexing, it is now possible to map a JSONPath query to an alias. Therefore, it is possible to index the same value in different indexing attributes with different indexing strategies.\nNote: As of RediSearch v2.2, index fields are now known as attributes in order to avoid confusion with hash fields. This change impacts the FT.INFO command\u0026rsquo;s response, which might break clients. It is advised to upgrade your application to the latest supported Redis clients prior to upgrading to RediSearch 2.2. Details Enhancements:\n#2337 Add support for Redis COPY command #2243 Add LOAD * for FT.AGGREGATE #2207 Add multi value recursive decent tag #2188 Add UNF flag for SORTABLE fields #2184 LLAPI getter functions for score, language, and stopwords list #2133 JSON array can be stored in a TAG field #2153 Improve FT.INFO complexity to O(1) #2138 Add CASESENSITIVE to TAG fields #2137 FT.INFO has identifier and attribute for fields Bug fixes:\n#2341 Fix score field for JSON #2325 Fix escaping for tags #2269 Remove empty tag values #2223 Replace NULL with empty iterator for child of negative iterator #2215 Update field limit on tags #2143 Partial JSON documents are not indexed #2109 Field loaded with \u0026lsquo;AS\u0026rsquo; can\u0026rsquo;t be used by functions Notes: This is the first GA version of 2.2. The version inside Redis will be 2.2.5 in semantic versioning. Since the version of a module in Redis is numeric, we could not add a GA flag.\n","categories":["Modules"]},{"uri":"/ri/release-notes/v1.5.0/","uriRel":"/ri/release-notes/v1.5.0/","title":"RedisInsight v1.5, May 2020","tags":[],"keywords":[],"description":"New tool for RedisGears, Multi-line query builder and improved suppport of Redis 6 ACLs","content":"This is the General Availability Release of RedisInsight 1.5 (v1.5.0)!\nHeadlines Added beta support for RedisGears module Added multi-line query editing for RediSearch, RedisGraph and Timeseries Improved support of Redis 6 ACLs Full details: Features\nCore: Improved support for Redis 6 managing ACL permissions for each different capabilities Gears: Beta support for Redis Gears module Explore the latest executed functions and analyze the results or errors Manage registered functions and get execution summary Code, build and execute functions RediSearch: Multi-line for building queries RedisGraph: Multi-line for building queries Timeseries: Multi-line for building queries Bug Fixes:\nConfiguration: Fixed issue not showing the list of modules Search: Fixed issue preventing users to see all documents matching a search query Fixed issue with retrieving the search indexes in case of large database ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-4-4-dec-2016/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-4-4-dec-2016/","title":"RLEC 4.4 Release Notes (December 2016)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the upgrade instructions before beginning the upgrade process.\nYou can upgrade to this version from any 4.3 version. If you have a version older than 4.3 you must first upgrade to 4.3 and only then upgrade to this version.\nNew features Databases can now be configured to have multiple proxies for improved performance. Note that when you upgrade the cluster to this version and then upgrade existing databases, the databases will be updated to use the Single proxy policy and Dense shard placement policy. For additional details, refer to Multiple active proxies. Support for Redis version 3.2 added. When you install or upgrade the cluster the new default version for Redis databases will be 3.2 and when you upgrade the databases they will be updated to this version. If you would like to change the default version to Redis 3.0, refer to the instruction in the Upgrading databases If you would like to upgrade existing databases to the latest 3.0 minor version, refer to the Known Issues section below. The cluster can now be configured to support both private and public IPs to connect to database endpoints through both public and private networks. For additional details, refer to Private and Public Endpoints. rladmin status command output has been enhanced to include an indication on which node rladmin is running by adding the \u0026lsquo;*\u0026rsquo; sign next to the node entry, and to show the host name of the machine the node is running on. Users can now be assigned security roles to control what level of the databases or cluster the users can view and/or edit. Changes As result of adding the support for multiple proxies for a database, the following changes have been made: When you upgrade the cluster to this version and then upgrade existing databases, the databases will be updated to use the Single proxy policy and Dense shard placement policy. rladmin status command output has been updated. failover [db \u0026lt;db:id | name\u0026gt;] endpoint \u0026lt;id1 .. idN\u0026gt; and migrate [db \u0026lt;db:id | name\u0026gt; | node \u0026lt;origin node:id\u0026gt;] endpoint \u0026lt;id\u0026gt; target_node \u0026lt;id\u0026gt; commands are no longer relevant for databases using the single | all-master-shards | all-nodes proxy policy. Instead proxies can be bound or unbounded to databases as needed. New rladmin commands were added, such as bind and placement. RLEC has been updated to remove the need to use sudo in runtime. You still need to be root or use sudo when initially installing RLEC. You no longer need to be root or use sudo to run the rladmin command, now it is preferred to be a non-privileged user that is member of the redislabs group to run the command. All cluster services are now run using the supervisor mechanism. As a result starting, stopping and restarting RLEC services should be done using supervisorctl command from the OS CLI. Linux OS vm.swappiness is now advised to be set to zero, for more information see Disabling Swap in Linux. Important fixed issues since 4.3.0 RLEC-7542 - Add ability to create and manage role based user security RLEC-8283 - The cluster recovery process does not work properly when the cluster that needs to be recovered does not have a node with ID 1. RLEC-8284 - Add functionality to rladmin to mark a node as a quorum only node RLEC-8498 - Backup fails under rare conditions RLEC-8579 - rladmin supports uppercase for external_addr value RLEC-8656 - Fixed conflict with SELinux RLEC-8687 - Fixed issue where strong password requirements were not honored correctly. RLEC-8694 - DMC failed while creating DB with 75 (150 replicated) shards RLEC-8700 - Fixed issue with network split scenario RLEC-8833 - Fixed issue where in some cases endpoint were not getting new IPs after node replacement. RLEC-9069 - Fixed issue related to RHEL 7 and IPv6. RLEC-9156 - Fixed issue causing a full resync of data when a source or destination failure occurred. RLEC-9173 - Issue with writing data after master and replica failed RLEC-9235 - Issue with SSL connection error and self signed certificates RLEC-9491 - Fixed alerting issue due to incorrect measurement RLEC-9534 - Fixed issue with node remove command after RLEC uninstalled RLEC-9658 - Failed to import backup file from FTP server. RLEC-9737 - Fixed issue with backup process to use ephemeral storage when needed RLEC-9761 - UI had incorrect value increments RLEC-9827 - Server with a high number of cores and running RHEL can have issues running systune.sh RLEC-9853 - Fixed issues with logrotate on RHEL 7.1 so it runs as non-privileged user RLEC-9858 - If proxy crashed, in some cases this would prevent completion of redis failover process RLEC-9893 - DB recovery process doesn\u0026rsquo;t recognize original rack name when in uppercase RLEC-9905 - x.509 certificate signed by custom CA cannot be loaded in UI RLEC-9925 - master endpoint and shards goes down if co-hosted with master of the cluster and the node goes down (single proxy policy) RLEC-9926 - Master shard could remain down if on the same node as the master of the cluster and the entire node goes down RLEC-10340 - Fixed a typo that crashed rladmin status output in some cases Changes in 4.4.2-42:\nRLEC-11941 - Upgrade to 4.4.2-35 on RHEL6 - leash failed when python2.6 is installed RLEC-11994 - RLEC 4.4.2-35: the UI doesn\u0026rsquo;t display the DBs with replication Changes in 4.4.2 - 49\nRLEC-11209 - Unable to run upgrade due to running_actions check RLEC-12647 - Backup to S3 with periods in bucket name are failing in some cases Known issues Issue: When upgrading to this version from a previous RLEC version, rladmin status output will show the database status as having an old version. When you upgrade the Redis database (using rladmin upgrade db command) the Redis version will be updated to 3.2 even if you updated the cluster\u0026rsquo;s Redis default version to 3.0. Workaround: If you would like to cancel the old version indication in rladmin status without upgrading the Redis version to 3.2 you should run the rladmin upgrade db command with the keep_current_version flag which will ensure the database is upgraded to the latest 3.0 version supported by RLEC. Issue: RLEC-9200 - in a database configured with multiple proxies, if a client sends the MONITOR, CLIENT LIST or CLIENT KILL commands, only commands from clients connected from the same proxy are returned instead of all commands from all connections. Workaround: If you would like to get a result across all clients, you need to send the monitor command to all proxies and aggregate them. Issue: RLEC-9296 - Different actions in the cluster, like node failure or taking a node offline, might cause the Proxy policy to change Manual. Workaround: You can use the rladmin bind [db \u0026lt;db:id | name\u0026gt;] endpoint \u0026lt;id\u0026gt; policy \u0026lt;single | all-master-shards | all-nodes\u0026gt; command to set the policy back to the required policy, which will ensure all needed proxies are bounded. Note that existing client connections might disconnected as result of this process. Issue: RLEC-8787 - In some cases when using the replica-of feature, if the source database(s) are larget than the target database, the memory limit on the target database is not enforced and that used memory of the target database can go over the memory limit set. Workaround: You should make sure that the total memory limit of all source databases is not bigger than the memory limit of the target database. Issue: RLEC-8487 - Some Redis processes stay running after purging RLEC from the machine and causes an attempt to reinstall RLEC to fail. Workaround: Run the purge process for a second time and ensure that the Redis processes were removed. Issue: RLEC-8747 - When upgrading to this version, if the UI is open in the browser the UI might not work properly after the upgrade. Workaround: Refresh the browser and the UI will return to work properly. Issue: In the Replica Of process, if the target database does not have replication enabled and it is restarted or fails for any reason, the data on the target database might not be in sync with the source database, although the status of the Replica Of process indicates that it is. Workaround: You must manually stop and restart the synchronization process in order to ensure the databases are in sync. Issue: In the Replica Of process, if the source database is resharded while the Replica Of process is active, the synchronization process will fail. Workaround: You must manually stop and restart the synchronization process after the resharding of the source database is done. Issue: In the Replica Of process, if there is very high traffic on the database the Replica Of process might be restarted frequently due to the \u0026ldquo;replica buffer\u0026rdquo; being exceeded. In this case, you will often see the status of the Replica Of process display as \u0026ldquo;Syncing\u0026rdquo;. Workaround: You must manually increase the \u0026ldquo;replica buffer\u0026rdquo; size through rladmin. To find the appropriate buffer size please contact support at: support@redislabs.com. Issue: In a cluster that is configured to support rack-zone awareness, if the user forces migration of a master or replica shard through rladmin to a node on the same rack-zone as its corresponding master or replica shard, and later runs the rebalance process, the rebalance process will not migrate the shards to ensure rack-zone awareness compliance. Workaround: In the scenario described above, you must use rladmin to manually migrate the shard to a node on a valid rack-zone in order to ensure rack-zone awareness compliance. Issue: DNS doesn\u0026rsquo;t change after having removed the external IP address. Workaround: Unbind IP from affected node and then bind it back. Issue: CCS gets an error and won\u0026rsquo;t start if /var/opt/redislabs/persist/ does not exist. Workaround: Make sure this directory is not deleted and continues to exist. ","categories":["RS"]},{"uri":"/kubernetes/security/add-client-certificates/","uriRel":"/kubernetes/security/add-client-certificates/","title":"Add client certificates","tags":[],"keywords":[],"description":"Add client certificates to your REDB custom resource.","content":"For each client certificate you want to use with your database, you need to create a Kubernetes secret to hold it. You can then reference that secret in your Redis Enterprise database (REDB) custom resource spec.\nCreate a secret to hold the new certificate Create the secret config file with the required fields shown below.\napiVersion: v1 kind: Secret type: Opaque metadata: name: \u0026lt;client-cert-secret\u0026gt; namespace: \u0026lt;your-rec-namespace\u0026gt; data: cert: \u0026lt;client-certificate\u0026gt; Apply the file to create the secret resource.\nkubectl apply -f \u0026lt;client-cert-secret\u0026gt;.yaml Edit the REDB resource Add the secret name to the REDB custom resource (redb.yaml) with the clientAuthenticationCertificates property in the spec section. spec: clientAuthenticationCertificates: - \u0026lt;client-cert-secret\u0026gt; ","categories":["Platforms"]},{"uri":"/rs/clusters/logging/","uriRel":"/rs/clusters/logging/","title":"Logging and audit events","tags":[],"keywords":[],"description":"Management actions performed with Redis Enterprise are audited in order to fulfill two major objectives.","content":"Management actions performed with Redis Enterprise are audited in order to fulfill two major objectives:\nTo make sure that system management tasks are appropriately performed and/or monitored by the Administrators To facilitate compliance with regulatory standards In order to fulfill both objectives, the audit records contain the following information:\nWho performed the action? What exactly was the performed action? When was the action performed? Did the action succeed or not? To get the list of audit records/events, you can use the REST API or the Log page in the UI. The Log page displays the system and user events regarding alerts, notifications and configurations.\nIf you need to look at the audit log of what a user on the cluster has done, e.g. edited a DB configuration, this is where you could look.\nRedis slow log rsyslog logging Viewing logs in the admin console Redis Enterprise provides log files for auditing and troubleshooting. You can see these logs in the admin console and on the host operating system.\nTo view the audit logs:\nLog in to the Redis Enterprise Software admin console. Go to the Log tab. Review logs directly in the UI, or export them to CSV using the export button. Viewing logs on the server Server logs can be found by default in the directory /var/opt/redislabs/log/.\nThese log files are used by the Redis support team to troubleshoot issues. The logs you will most frequently interact with is \u0026rsquo;event_log.log\u0026rsquo;. This log file is where logs of configuration actions within Redis are stored and is useful to determine events that occur within Redis Enterprise.\nSetting log timestamps Redis Enterprise allows you to configure log timestamps. To configure log timestamps:\nIn Settings \u0026gt; General navigate to the timezone section. Select the timezone for the logs based on your location. ","categories":["RS"]},{"uri":"/modules/redisearch/release-notes/redisearch-2.0-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-2.0-release-notes/","title":"RediSearch 2.0 release notes","tags":[],"keywords":[],"description":"Automatically indexes data based on a key pattern. Scale a single index over multiple Redis shards. Improved query performance.","content":"Requirements RediSearch v2.0.13 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.0 v2.0.15 (December 2021) This is a maintenance release for RediSearch 2.0.\nUpdate urgency: MODERATE - Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes: #2388 Garbage Collection (GC) for empty ranges in numeric index #2409 Introduction of FORK_GC_CLEAN_NUMERIC_EMPTY_NODES true module argument to enable #2388 (off by default) #325 Used Redis allocator in hiredis (RSCoordinator) #2362 Crash on empty field name v2.0.13 (November 2021) This is a maintenance release for version 2.0.\nDetails:\nEnhancements:\n#2243 Add LOAD * for FT.AGGREGATE which will load all fields Bug fixes:\n#2269 #2291 Remove TAG values from trie with no entries on Garbage Collection. #2287 Uninitialized read on FT.ADD #2342 Check for NULL result on intersect iterator #2350 Crash on FT.AGGREGATE with LIMIT 0 0 v2.0.12 (September 2021) This is a maintenance release for version 2.0.\nDetails:\nEnhancements:\n#2184 API: getter functions for score, language and stop words list #2188 Introduced the UNF parameter to SORTABLE to disable normalisation on TAG/TEXT fields #2218 API: added RediSearch_CreateDocument2 Bug fix:\n#2153 Restore FT.INFO complexity to O(1) #2203 FT.AGGREGATE returns inaccurate results when TAG field is not set in hash v2.0.11 (August 2021) This is a maintenance release for version 2.0.\nDetails:\nEnhancements:\n#2156 TAG fields can now be case sensitive using the CASESENSITIVE parameter #2113 An already existing document that can\u0026rsquo;t be updated is removed from the index (JIRA MOD-1266) #267 #287 Updated Hiredis to support Intershard TLS Bug Fixes:\n#2117 #2115 Fix crash on coordinator on first value reducer v2.0.10 (July 2021) This is a maintenance release for version 2.0.\nDetails:\nEnhancements:\n#2025 Improve performances on numeric range search #1958 #2033 Support of sortable on the GEO type #2079 Update to Snowball 2.1.0, adds Armenian, Serbian and Yiddish stemming support #2002 Add stopwords list support in the API Bug fix:\n#2045 Possible crash when loading an RDB file (silently ignore double load of alias) #2099 #2101 Fixes possible crash with CRDT on FT.DROPINDEX #1994 Skip intersect iterator qsort if INORDER flag is used #257 Switch coordinator to send _FT.CURSOR instead FT.CURSOR to prevent data access without holding the lock v2.0.9 (May 2021) This is a maintenance release for version 2.0.\nDetails:\nBug fix in RSCoordinator: #259: Fix deadlock on cursor read by performing cursor command on background thread v2.0.8 (May 2021) This is a maintenance release for version 2.0.\nThis release fixes an important regression introduced by the 2.0 release. The payload is supposed to be returned only when the WITHPAYLOADS parameter is set.\nDetails:\nBug fixes: #1959 Renamed parse_time() to parsetime() #1932 Fixed crash resulted from LIMIT arguments #1919 Prevent GC fork from crashing Minor enhancements: #1880 Optimisation of intersect iterator #1914 Do not return payload as a field v2.0.7 (May 2021) This is a maintenance release for version 2.0.\nDetails:\nMajor enhancements: #1864 Improve query time by predetermining reply array length. #1879 Improve loading time by calling RM_ScanKey instead of RM_Call Major bug fix: #1866 Fix a linking issue causing incompatibility with Redis 6.2.0. #1842 #1852 Fix macOS build. Minor bug fixes: #1850 Fix a race condition on drop temporary index. #1855 Fix a binary payload corruption. #1876 Fix crash if the depth of the reply array is larger than 7. #1843 #1860 Fix low-level API issues. v2.0.6 (February 2021) This is a maintenance release for version 2.0.\nDetails:\nMinor additions: #1696 The maximum number of results produced by FT.AGGREGATE is now configurable: MAXAGGREGATERESULTS. #1708 Stemming updated with support of new languages: Basque, Catalan, Greek, Indonesian, Irish, Lithuanian, Nepali. Minor bugfixes: #1668 Fixes support of stop words in tag fields. Solves also the following related issues: #166, #984, #1237, #1294. #1689 Consistency fix and performance improvement when using FT.SUGGET with RSCoordinator. #1774 MINPREFIX and MAXFILTEREXPANSION configuration options can be changed at runtime. #1745 Enforce 0 value for REDUCER COUNT. #1757 Return an error when reaching the maximum number of sortable fields, instead of crashing. #1762 Align the maximum number of sortable fields with the maximum number of fields (1024) v2.0.5 (December 2020) This is a maintenance release for version 2.0.\nDetails:\nMinor features: #1696 Add MAXAGGREGATERESULTS module configuration for FT.AGGREGATE. Similar to MAXSEARCHRESULTS for FT.SEARCH, it limits the maximum number of results returned. v2.0.4 (December 2020) This is a maintenance release for version 2.0.\nDetails:\nBugfixes in RediSearch: #1668 Stopwords are not filtered out on tag fields. Bugfixes in RSCoordinator: #206 FT.AGGREGATE with LIMIT and offset greater than 0 returned fewer results than requested. v2.0.3 (November 2020) This is a maintenance release for version 2.0.\nMinor bugfixes:\nAdded OSS_GLOBAL_PASSWORD config argument to allow specify shards password on OSS cluster. Update min_redis_pack_version to 6.0.8 v2.0.2 (November 2020) This is a maintenance release for version 2.0.\nMinor enhancements:\n#1625 MAXPREFIXEXPANSIONS configuration which replaces the now deprecated config MAXEXPANSIONS. Same behaviour, more declarative naming. #1614 Prevent multiple sortby steps on FT.AGGREGATE Minor bug fixes:\n#1605 Rare bug where identical results would get a lower score v2.0.1 (October 2020) This is a maintenance release for version 2.0.\nMinor additions:\n#1432 FT.AGGREGATE allows filtering by key and returning the key by using LOAD \u0026ldquo;@__key\u0026rdquo; . Minor bug fixes:\n#1571 Using FILTER in FT.CREATE might cause index to not be in sync with data. #1572 Crash when using WITHSORTKEYS without SORTBY. #1540 SORTBY should raise an error if the field is not defined as SORTABLE. v2.0.0 (September 2020) RediSearch 2.0 is a public preview release meeting GA standards. This release includes several improvements in performance and usability over RediSearch 1.0. These improvements necessitate a few backward-breaking changes to the API.\nHighlights For this release, we changed the way in which the search indexes are kept in sync with your data. In RediSearch 1.x, you had to manually add data to your indexes using the FT.ADD command. In RediSearch 2.x, your data is indexed automatically based on a key pattern.\nThese changes are designed to enhance developer productivity, and to ensure that your search indexes are always kept in sync with your data. To support this, we\u0026rsquo;ve made a few changes to the API.\nIn addition to simplifying indexing, RediSearch 2.0 allows you to scale a single index over multiple Redis shards using the Redis cluster API.\nFinally, RediSearch 2.x keeps its indexes outside of the main Redis key space. Improvements to the indexing code have increased query performance 2.4x.\nYou can read more details in the RediSearch 2.0 announcement blog post, and you can get started by checking out this quick start blog post.\nDetails When you create an index, you must specify a prefix condition and/or a filter. This determines which hashes RediSearch will index. Several RediSearch commands now map to their Redis equivalents: FT.ADD -\u0026gt; HSET, FT.DEL -\u0026gt; DEL (equivalent to FT.DEL with the DD flag in RediSearch 1.x), FT.GET -\u0026gt; HGETALL, FT.MGET -\u0026gt; HGETALL. RediSearch indexes no longer reside within the key space, and the indexes are no longer saved to the RDB. You can upgrade from RediSearch 1.x to RediSearch 2.x. Noteworthy changes #1246: geodistance function for FT.AGGREGATE APPLY operation. #1394: Expired documents (TTL) will be removed from the index. #1394: Optimization to avoid reindexing documents when non-indexed fields are updated. After index creation, an initial scan starts for existing documents. You can check the status of this scan by calling FT.INFO and looking at the indexing and percent_indexed values. While indexing is true, queries return partial results. #1435: NOINITIALINDEX flag on FT.CREATE to skip the initial scan of documents on index creation. #1401: Support upgrade from v1.x and for reading RDB\u0026rsquo;s created by RediSearch 1.x (more information). #1445: Support for load event. This event indexes documents when they are loaded from RDB, ensuring that indexes are fully available when RDB loading is complete (available from Redis 6.0.7 and above). #1384: FT.DROPINDEX, which by default does not delete documents underlying the index (see deprecated FT.DROP). #1385: Add index definition to FT.INFO response. #1097: Add Hindi snowball stemmer. The FT._LIST command returns a list of all available indices. Note that this is a temporary command, as indicated by the _ in the name, so it\u0026rsquo;s not documented. We\u0026rsquo;re working on a SCAN-like command for databases with many indexes. The RediSearch version will appear in Redis as 20000, which is equivalent to 2.0.0 in semantic versioning. Since the version of a module in Redis is numeric, we cannot explicitly add an GA flag. RediSearch 2.x requires Redis 6.0 or later. Behavior changes Make sure you review these changes before upgrading to RediSearch 2.0:\n#1381: FT.SYNADD is removed; use FT.SYNUPDATE instead. FT.SYNUPDATE requires both and index name and a synonym group ID. This ID can be any ASCII string. #1437: Documents that expire during query execution time will not appear in the results (but might have been counted in the number of produced documents). #1221: Synonyms support for lower case. This can result in a different result set on FT.SEARCH when using synonyms. RediSearch will not index hashes whose fields do not match an existing index schema. You can see the number of hashes not indexed using FT.INFO - hash_indexing_failures . The requirement for adding support for partially indexing and blocking is captured here: #1455. Removed support for NOSAVE (for details see v1.6 docs). RDB loading will take longer due to the index not being persisted. Field names in the query syntax are now case-sensitive. Deprecated commands: FT.DROP (replaced by FT.DROPINDEX, which by default keeps the documents) FT.ADD (mapped to HSET for backward compatibility) FT.DEL (mapped to DEL for backward compatibility) FT.GET (mapped to HGETALL for backward compatibility) FT.MGET (mapped to HGETALL for backward compatibility) Removed commands: FT.ADDHASH (no longer makes sense) FT.SYNADD (see #1381) FT.OPTIMIZE (see v1.6 docs) ","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-2.8-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-2.8-release-notes/","title":"RedisGraph 2.8 release notes","tags":[],"keywords":[],"description":"Introduces multi-labeled nodes, indexes over relationship properties, and additional expressivity (Cypher construct, functions, and operators). Major performance enhancements. Many bug fixes.","content":"Requirements RedisGraph v2.8.24 requires:\nMinimum Redis compatibility version (database): 6.2.0 Minimum Redis Enterprise Software version (cluster): 6.2.8 v2.8.24 (February 2023) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2777, #2841 Potential crash when sending queries from multiple connections and timeout is not 0 #2844 Potential partial results when same parametrized query is running from multiple connections #2739, #2774 Paths with exact variable length \u0026gt;1 are not matched Improvements:\n#2758 Improved edge deletion performance v2.8.21 (January 2023) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2754 Partial sync may hang (MOD-4594) #2695 Potential crash on certain write queries (MOD-4286, MOD-4545) #2637, #2460, #2680 Crash on invalid queries #2484 Indexes can be created on invalid property names #2672 Wrong matching result on multiple labels #2643 Duplicate reports when matching relationship type :R|R #2687, #2414 Error when UNWINDing relationships #2635 Cannot UNWIND an expression that is not a list #2636 MERGE \u0026hellip; ON \u0026hellip; - cannot remove a property by setting it to null Improvements:\n#2790 Improved performance by disabling SuiteSparse:GraphBLAS\u0026rsquo; global free pool #2757 Improved performance of indegree and outdegree #2740 Don’t show partial results for timed out GRAPH.PROFILE v2.8.20 (September 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2591 Potential crash trying to utilize a nonexistent index #2558 Multi-relationship properties created before index creation are not indexed #2571 min and max return wrong results when the argument is an array #2587 Some queries generate a \u0026ldquo;forced unlocking commit flow\u0026rdquo; warning Improvements:\n#2533 Graph slow log can be reset with GRAPH.SLOWLOG g RESET v2.8.19 (August 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#2517 Crash on invalid REDUCE queries #2525 toJSON - node labels are not separated with a comma (,) #2467 Possibly wrong results when using a variable named anon_N #2522 Deleting an edge index leaves traces #2477 Cannot extract the latitude or the longitude of a point Improvements:\n#2519 When a query is cached and then a relevant index is created, recalculate the execution plan v2.8.17 (July 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2499 Potential crash with concurrent connections due to missing lock - additional fixes #2424 Potential crash when using ORDER BY #2491 Whitespaces between MATCH terms can render the query invalid v2.8.16 (July 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2478 Potential crash with concurrent connections due to missing lock #2370 Potential crash / wrong results / warning messages when using edge indexes #2473 Crash on invalid distance() query with index v2.8.15 (June 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2241 Possible crash on queries with MERGE operation in a Cartesian product (MOD-3500) #2394 Possible crash when freeing an index immediately after its creation v2.8.14 (June 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nFeatures:\n#2403 Introduce toFloat function #2365 Commands that don\u0026rsquo;t execute on the main thread now also appear in SLOWLOG Bug fixes:\n#2381 Rare data corruption on data encoding #2393, #2395 Crash when searching an index for a runtime-determined value #2377 Crash on INT_MIN % -1 #2390 Crash on distance filter #2407 Crash on double to string #2422 toJSON returned floating points cropped to 6 decimals Note: New RDB version (v12). RDB files created with v2.8.14 or later are not backward compatible. v2.8.13 (May 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2351 Potential memory leak on query timeout #2348 Crash when converting certain Cypher queries to RediSearch queries #2331 Two memory leaks (one on failed RDB loading, one on certain invalid queries) #2328, #2306, #2307, #2326 Disallow redeclaration of variables; fixed false redeclaration errors #2363 Nodes were sometimes created with more labels than those specified v2.8.12 (May 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2278, #2301 Potential crash on bulk update #2262, #2271, #2272, #2273, #2282, #2288, #2299, #2312, #2321, #2325, #2323 Potential crash on certain queries #1441 Query returns wrong result: projection before Cartesian product causes only one result to be returned #2298 Query returns wrong result: wrong value when fetching a string property from a map #2318 Memory leak Note: Duplicate column names in a query are no longer valid. For example, the query GRAPH.QUERY g \u0026quot;MATCH (e) RETURN e, e\u0026quot; is not valid. If you need to return the same column twice, you can rewrite the query as GRAPH.QUERY g \u0026quot;MATCH (e) RETURN e, e as e2\u0026quot;. v2.8.11 (March 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#2259, #2258 Fix memory leak and potential crash on RDB saving v2.8.10 (March 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nFeatures:\n#2245 Support graphs eviction Bug fixes:\n#1493, #2240 Fixed crash on certain queries #2229, #2222 Fixed crash on certain queries #2209, #2228 Fixed crash on certain invalid DELETE queries #2237, #2242 Fixed crash on certain PROFILE queries #2230, #2232 Fixed wrong number of reported deleted relationships on certain queries #2233 Certain valid queries were reported invalid #2246 Fixed memory leaks Improvements:\n#2235 Improved RDB loading performance v2.8.9 (March 2022) This is a maintenance release for RedisGraph 2.8.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nFeatures:\n#2181, #2182 Full support for ARM builds Bug fixes:\n#2167 Fixed a potential crash: filter placement in OPTIONAL subtrees #2176 Fixed a potential crash: invalid memory access in nested DISTINCT functions #2217 Fixed a potential crash: memory access after free on FLUSHALL #2207 Fixed memory leak when MAX_QUEUED_QUERIES is used #2220 WITH * WHERE - the WHERE filters were ignored #2151 Return correct results for aggregations with no inputs #2163 Emit error correctly on multi-query inputs Improvements:\n#2173 Improve performance of breadth-first search v2.8.8 (February 2022) This is the General Availability release of RedisGraph 2.8.\nHighlights RedisGraph 2.8 introduces multi-labeled nodes, indexes over relationship properties, additional expressivity (construct, functions, and operators), major performance enhancements, and many bug fixes.\nWhat\u0026rsquo;s new in 2.8 Multi-labeled nodes\nNote: The labels function\u0026rsquo;s signature has changed. The function now returns a list of labels instead of a single label. If you are using this function and upgrading to RedisGraph 2.8, a simple fix is to replace any call to labels(x) with labels(x)[0]. This returns the first label associated with node x. For uni-labeled nodes, the result in RedisGraph 2.8 is similar to the results of labels(x) in RedisGraph 2.4. Indexes over relationship properties\nEnhanced full-text search\nDelta matrices: node and relationships additions and deletions are much faster, as they are first updated in small delta matrices. The main matrices are then bulk-updated.\nAdditional Cypher construct, functions, and operators\nRediSearch 2.2.7\nSuiteSparse (GraphBLAS) 6.0.0\nDetails Features (since 2.8-M02):\n#2109 Introduce allShortestPaths BFS function #2099 Introduce keys function #2047 Introduce reduce function #2076 Introduce XOR operation in filter trees #2088 Introduce pattern comprehensions #2051 Allow copying of entity attribute sets in SET clauses #2067 Allow modification of virtual key entity count (VKEY_MAX_ENTITY_COUNT) at runtime #2102 New load time configuration option NODE_CREATION_BUFFER - see documentation (MOD-2348) #2049 RediSearch supports field definitions Performance improvements (since 2.8-M02):\n#2097 Locks favor writers to prevent write exhaustion #1945 Track node count per label in graph statistics #1872 Delta matrices are always hypersparse #1871 Matrix sync policies reduce the number of syncs required #1869 Transposed matrices are always boolean #2101 Entity annotation has been replaced with an AST toString function #1878 Slowlog queries no longer create graphs #2067 Index graph entities incrementally on restore Faster AOF recovery (PM-1252) Bug fixes (since 2.8-M02):\n#2016 Implement new BFS algorithm #2105 Creating a node with multiple properties using the same key only accepts the last value #2055 Avoid arithmetic overflow in avg function #2048 Modulo by zero emits division by zero error #2020 Fix evaluation of variable-length edges in expression ordering #2028 Fix utilization of record offset in procedure calls, refactor outputs #2014 Update label for every node in the AST #2002 Fix crash in index utilization using wrong query_graph #1976 Use operand matrix when available #1973 Emit error on a query that only contains parameters #1950 Print ExecutionPlan in GRAPH.EXPLAIN only if no errors are encountered in the construction #1933 Free thread-local data on graph deletion #1942 Fix dimensions of transposed delta matrices #1940 Do not use block client if deny blocking is specified #1898 Error when setting a property to an array containing an invalid type #1931 Sync matrices on parent process before serialization fork #1897 Aliases in WITH \u0026hellip; ORDER BY must be valid references #1913 Update thread-local AST for every cloned operation #1915 Validate function references in parameters #1911 Refactor cron task for managing query timeouts #1902 Fix incorrect behavior on NULL values in CASE\u0026hellip;WHEN expressions #1904 Allow reconfiguring query timeout to 0 #1888 Synchronize matrices on creation in RDB load #1892 Validate values specified in SET clauses #1889 Tuple iterator now updates properly when changing matrix to serialize #1870 Fix crash in range function #2125 Fix crash on UNION \u0026hellip; RETURN * queries (MOD-2524) #2043 Avoid serialization of duplicate graph keys #2067 Numeric indices no longer lose precision on very large values #2072, #2081 CRLF sequences embedded in strings no longer trigger a protocol error when being emitted #2139 Fix crash when trying to retrieve an out-of-bounds item #2149 Fix crash when matching a node engages an index to search for a value that is a RediSearch stop word Note: This is the first GA version of 2.8. The version inside Redis will be 2.8.8 in semantic versioning. Since the version of a module in Redis is numeric, we could not add a GA flag.\nMinimum Redis version: 6.2\n","categories":["Modules"]},{"uri":"/ri/release-notes/v1.4.0/","uriRel":"/ri/release-notes/v1.4.0/","title":"RedisInsight v1.4, April 2020","tags":[],"keywords":[],"description":"Redis 6 ACLs support, improved CLI and full screen support in Graph, TimeSeries and RedisSearch","content":"This is the General Availability Release of RedisInsight 1.4 (v1.4.0)!\nHeadlines Support for Redis 6, Redis Enterprise 6 and ACLs Improve CLI capabilities with removed command restrictions Full screen support in Graph, TimeSeries and RediSearch Full details: Features\nCore: Added support for Redis 6 + RE6 and authentication using ACL Added Full screen support for Graph, TimeSeries and RediSearch. Improved UI consistency (colors and styles) in Graph and Timeseries CLI: Removed the command restrictions, unless a command is specifically blacklisted. Command responses are displayed in exactly the same way as in redis-cli RedisGraph: Optimized performances when getting nodes relationships (edges) from user\u0026rsquo;s queries Stream: Improved UX when defining the timing range of events to be filtered Bug Fixes:\nCore: Fixed issue when connecting to Redis Enterprise with a password using a special character Stream: Fixed ability to properly visualize all events Known issues Core: Authentication to Redis 6 OSS in cluster mode is not supported yet CLI: Blocking commands are not supported Commands which return non-standard streaming responses are not supported: MONITOR, SUBSCRIBE, PSUBSCRIBE, SYNC, PSYNC, SCRIPT DEBUG ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-4-3-aug-2016/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-4-3-aug-2016/","title":"RLEC 4.3.0-230 Release Notes (August 2, 2016)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the [upgrade instructions](/rs/installing-upgrading/upgrading.md\u0026quot; \u0026gt;}}) before running through the upgrade process.\nYou can upgrade to this version from any 4.2 version. If you have a version older than 4.2 you should first upgrade to 4.2 and only then upgrade to this version.\nNew features Various improvements to internal performance and stability were implemented. RLEC Flash functionality added. For additional details, refer to Redis on Flash and contact support@redislabs.com if you are interested in this functionality. Support for Redis version 3.0 added. When you install or upgrade the cluster the new default version for Redis databases will be 3.0 and when you upgrade the databases they will be updated to this version. If you would like to change the default version to Redis 2.8 refer to the instruction in the Upgrading databases section. If you would like to upgrade existing databases to the latest 2.8 minor version, refer to the Known Issues section below. Complete cluster failure recovery instructions added. For additional details, refer to Cluster Recovery. Major improvements made to database replication performance process by using diskless replication between master and replica shards. The data between the master and replica shards is streamed directly, instead of using the default file-on-disk mechanism. This behavior can be changed for the entire cluster or per database through rladmin. Major enhancements made to rladmin command line interface to add new administration functionalities. rlcheck installation verification utility added to facilitate checking node health. For additional details, refer to rlcheck installation verification utility. Added the ability to allow the user to configure how machine IP addresses are used in Node Configuration setup in the management UI. For additional details, refer to Initial setup - creating a new cluster. Connection to database endpoint can now be encrypted with SSL. For additional details, refer to Securing client connection with SSL. Added support for running the cluster on the following operating systems and versions: RHEL/CentOS 6.6, 7.1, 7.2, RHEL 6.7, Oracle Linux 6.5. Changes Environment configuration profile with name \u0026ldquo;default\u0026rdquo; has been changed to \u0026ldquo;cloud\u0026rdquo; and the default value has been changed to \u0026ldquo;local-network\u0026rdquo;. For additional details, refer to Performance optimization section. In the REST API, when creating a database and not setting the database replication parameter to \u0026ldquo;true\u0026rdquo;, the default value assigned by the cluster has changed from \u0026ldquo;true\u0026rdquo; to \u0026ldquo;false\u0026rdquo;. rladmin syntax updates can affect commands written for prior versions of RLEC. In this version commands that are run directly from the operating system CLI prompt (not through the rladmin prompt) no longer require quotation marks for text with special characters. Option added to the Replica-of process that allows gradual \u0026ldquo;shard-by- shard\u0026rdquo; replication of a sharded database, reducing the load on internal buffers. This optimization setting can be configured on the target database using the gradual_sync_mode parameter in rladmin. The functionality for taking a node offline was removed from the UI. Fixed issues RLEC-7110 - node does not recover properly after restart in case ephemeral storage is not available yet RLEC-7502 - log rotate job not working properly on RHEL operating system RLEC-7599 - issues running on a server with no IPv6 kernel support RLEC-7561, RLEC-7597 - issues connecting to database endpoint as result of cluster name containing capital letters RLEC-7245 - on machines with multiple IPs sometimes the wrong IP address is chosen for internal traffic RLEC-6815 - wrong log entry is added when enabling cluster alert regarding database version compatibility RLEC-7652 - database is down in certain failover scenarios only when the database is completely empty RLEC-7737 - issue where in a specific scenario after node restarts, a database with replication both master and replica shards are reported as down RLEC-7712 - in some cases, the Replica Of process may fail when Redis password is set RLEC-7726 - node object \u0026ldquo;avg_latency\u0026rdquo; statistic is not returned in the REST API RLEC-7358 - install script issue when running on LVM disks RLEC-8086 - port 9443 missing from redislabs-clients.xml RLEC-7281 - rotation of internal log files not working properly RLEC-8279 - updates to a user definition might cause password reset to be required RLEC-8512 - when upgrading an existing cluster that has uppercase letters in the cluster name (FQDN) the cluster might not function properly after the upgrade and attempts to connect to a database might fail RLEC-8371 - email alerts do not work when using Amazon SES service In certain scenarios the node upgrade process may fail if the node is in the offline state Known issues Issue: When upgrading to this version from a previous RLEC version, rladmin status output will show the database status as having an old version. When you upgrade the Redis database (using rladmin upgrade db command) the Redis version will be updated to 3.0 even if you updated the cluster\u0026rsquo;s Redis default version to 2.8.\nWorkaround: If you would like to cancel the old version indication in rladmin status without upgrading the Redis version to 3.0 you should first change the cluster default version to 2.8 (using rladmin tune cluster command), and then trigger the Redis process to be restarted by migrating the database shards (using rladmin migrate db command).\nIssue: RLEC-8486 - On Ubuntu, when uninstalling RLEC using the apt-get purge command, some of the Redis processes on the machine might continue running.\nWorkaround: If you encounter this issue you must manually kill the Redis processes.\nIssue: RLEC-8283 - The cluster recovery process does not work properly when the cluster that needs to be recovered does not have a node with ID 1.\nWorkaround: If you encounter this issue please contact Redis support\nIssue: In the Replica Of process, if the target database does not have replication enabled and it is restarted or fails for any reason, the data on the target database might not be in sync with the source database, although the status of the Replica Of process indicates that it is.\nWorkaround: You must manually stop and restart the synchronization process in order to ensure the databases are in sync.\nIssue: In the Replica Of process, if the source database is resharded while the Replica Of process is active, the synchronization process will fail.\nWorkaround: You must manually stop and restart the synchronization process after the resharding of the source database is done.\nIssue: In the Replica Of process, high database traffic might restart the Replica Of process due to the \u0026ldquo;replica buffer\u0026rdquo; being exceeded. In this case you will often see the status of the Replica Of process display as \u0026ldquo;Syncing\u0026rdquo;.\nWorkaround: You must manually increase the \u0026ldquo;replica buffer\u0026rdquo; size through rladmin. In order to find the appropriate buffer size please contact Redis support\nIssue: In a cluster that is configured to support rack-zone awareness, if the user forces migration of a master or replica shard through rladmin to a node on the same rack-zone as its corresponding master or replica shard, and later runs the rebalance process, the rebalance process will not migrate the shards to ensure rack-zone awareness compliance.\nWorkaround: In the scenario described above, you must use rladmin to manually migrate the shard, to a node on a valid rack-zone in order to ensure rack-zone awareness compliance.\n","categories":["RS"]},{"uri":"/rs/clusters/monitoring/","uriRel":"/rs/clusters/monitoring/","title":"Monitoring with metrics and alerts","tags":[],"keywords":[],"description":"You can use the metrics that measure the performance of your Redis Enterprise Software (RS) clusters, nodes, databases and shards to keep an eye on the performance of your databases.","content":"You can use the metrics that measure the performance of your Redis Enterprise Software (RS) clusters, nodes, databases and shards to keep an eye on the performance of your databases. In the RS admin console, you can see the real-time metrics and you can configure alerts that send notifications based on alert parameters.\nYou can also access the metrics and configure alerts through the REST API so that you can integrate the RS metrics into your monitoring environment, for example, using Prometheus with Grafana or Uptrace.\nMake sure you read the definition of each metric so that you understand exactly what it represents.\nReal-time metrics You can see the metrics of the cluster in:\nCluster \u0026gt; Metrics, including individual nodes Node \u0026gt; Metrics Database \u0026gt; Metrics, including individual shards Shards \u0026gt; Metrics The scale selector at the top of the page allows you to set the X-axis (time) scale of the graph.\nTo choose which metrics to display in the two large graphs at the top of the page:\nHover over the graph you want to show in a large graph. Click on the right or left arrow to choose which side to show the graph. We recommend that you show two similar metrics in the top graphs so you can compare them side-by-side.\nCluster alerts In settings \u0026gt; alerts, you can enable alerts for node or cluster events, such as high memory usage or throughput.\nConfigured alerts are shown:\nAs a warning icon () for the node and cluster In the log In email notifications, if you configure email alerts Note: If you enable alerts for \u0026ldquo;Node joined\u0026rdquo; or \u0026ldquo;Node removed\u0026rdquo; actions, you must also enable \u0026ldquo;Receive email alerts\u0026rdquo; so that the notifications are sent. To enable alerts for a cluster:\nIn settings \u0026gt; alerts, select the alerts that you want to show for the cluster and click Save. Database alerts For each database, you can enable alerts for database events, such as high memory usage or throughput.\nConfigured alerts are shown:\nAs a warning icon () for the database In the log In emails, if you configure email alerts To enable alerts for a database:\nIn configuration for each database, click show advanced options to see the database alerts and select the alerts that you want to get for the database. Click Update. Sending alerts by email To send cluster or database alerts by email:\nIn settings \u0026gt; alerts, select Receive email alerts at the bottom of the page. Configure the email server settings. In access control, select the database and cluster alerts that you want each user to receive. ","categories":["RS"]},{"uri":"/modules/redisearch/release-notes/redisearch-1.6-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-1.6-release-notes/","title":"RediSearch 1.6 release notes","tags":[],"keywords":[],"description":"Improved performance of full-text search and aggregation queries. Support for aliasing of indices. Added a C API to embed RediSearch in other modules. Forked process garbage collection.","content":"Requirements RediSearch v1.6.16 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v1.6.16 (June 2021) This is a maintenance release for version 1.6.\nUpdate urgency: MODERATE - Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fix:\n#2018: FT.ADD REPLACE leaves fields from the previous document that are not included in the new document #647 #1193 v1.6.15 (February 2021) This is a maintenance release for version 1.6.\nUpdate urgency: Low\nDetails:\nMinor enhancements:\n#1225 Allow scientific representation of numbers for numeric fields. #1574 Allow SORTBY for non-sortable fields. Minor bugfixes:\n#1683 Add a module parameter _NUMERIC_COMPRESS which prevent double -\u0026gt; float compression. It prevents an issue where an exact match on some floating-point numbers is not found. #1757 Remove assertion on the limitation of the number of sortable fields, instead return an error. #1668 Query words from stopword list on tag fields. #1745 Enforce 0 value for count reducer. #1774 MINPREFIX \u0026amp; MAXEXPANSION can be changed in runtime. #1861 Fix issue for FT.SCOREEXPLAIN where reply array depth can exceed 7. #1689 FT.SUGGET results from RSCoordinator are more consistent. Various small tweaks under the hood. v1.6.14 (September 2020) This is a maintenance release for version 1.6. Details:\nMinor features:\n#1420 The hard limit of the number of results produced by FT.SEARCH is now configurable with MAXSEARCHRESULTS. Bug fixes:\n#1313 Wrong error of unsupported phonetic field. #1286 Possible crash on optional search. #1449 Rare file descriptor leak on FORK GC. #1469 Endless loop when reaching internal docid above uint32_max. v1.6.13 (May 2020) This is a maintenance release for version 1.6.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1186 #1188 incorrect values for inverted_sz_mb and num_records in FT.INFO command. v1.6.12 (April 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nFeatures: #1172 Added exists function that can be used on conditional updates (REPLACE PARTIAL) to check if a field exists in the document. Minor Enhancements: #1172 Lazy evaluation of the right side of \u0026lsquo;or\u0026rsquo;/\u0026lsquo;and\u0026rsquo; clauses in IF condition. #1134 Remove hard limit on LIMIT when using FT.SEARCH. Bugfixes: #1124 NOINDEX tag fields could not be updated on UPDATE PARTIAL with no indexed fields. #1120 Release loop in II_GetCriteriaTester which released the same criteria tester multiple times. #1161 Case where setting MAXDOCTABLESIZE had no effect. #1169 FIRST_VALUE reducer crashed when value did not exist. #1159 Infinite loop on NOT criteria tester.% v1.6.11 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1126 Memory leak introduced by queries for tag fields that have no results. v1.6.10 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. This release fixes certain backwards compatibility issues compared to 1.4. Although they are rare cases, it is recommended when upgrading to 1.6 to use this version or newer. Details:\nMinor Enhancements: #1062 Added Custom stopwords list in FT.INFO Fixed backwards incompatible issues: #1075 Fields should always be returned to the user as a string. #1074 Don\u0026rsquo;t truncate possible integral values when printing. #1065 Revert \u0026ldquo;Change how generated reducer aliases are made\u0026rdquo;. Bugfixes: #1085 Min and max value on non leaf nodes in the numeric tree should not be updated. #1106 Pipe leak on FORK GC caused by closing the fork without holding the lock. #1114 PR #986 reverted the work from #985, #989. This PR reintroduces these features. v1.6.9 (February 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1052 Remove wrong optimization on Quantile. #1057 Memory pool did not release memory when certain limit was reached. v1.6.8 (February 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1052 Remove wrong optimization on Quantile. #1057 Memory pool did not release memory when certain limit was reached. v1.6 GA (January 2020) This is the General Availability Release of RediSearch 1.6 (v1.6.7).\nHeadlines:\nSeveral performance improvements increasing full-text search queries up to 60% and aggregation queries up to 73%. Support for aliasing of indices. Low-level API in C (and Rust bindings) to make RediSearch embeddable in other Redis modules. RedisGraph is the first GA consumer. Forked process Garbage Collection (FORK GC) allows for stable read latencies. Full details:\nAdded functionality\n#658 FT.ADD … REPLACE … NOCREATE will not add the document if the document does not exist. #575 Add index aliasing. This allows users to provide (or remove) ‘links’ to indexes. The commands are FT.ALIASADD, FT.ALIASDEL, and FT.ALIASUPDATE. See documentation for full details New C API to make RediSearch embeddable in other Redis modules. This API allows other Redis modules to use functionality of RedisSearch without actually having the “module” functionality active. Note that this must still be used on Redis proper. Modules that are already incorporating this API RedisGraph GA RedisTimeSeries (WIP) RedisJSON (WIP) Performance improvements\nImprove performance when using many union (|) iterators Improve performance when using many intersect iterators Improve overall index reading performance #598 Do not return foo: NULL if foo is not present in the document. This conserves network bandwidth Bugfixes - Semantics\n#688 #623 Fix various issues with optional (~) search operator: Fixes omitted results when using union operators in addition to optional iterators. Allow optional iterator to be used in isolation in promotion-only mode (without a filter query) Fix issue where weight attribute was being ignored #653 FT.GET will no longer return a document as existing if it was not added by FT.ADD, even if the document exists in the server as a plain redis hash FT.AGGREGATE is now more stringent with its semantics, avoiding nonsensical queries or referencing fields which do not exist in the schema or LOADed. #779 Added to_number() and to_str() functions for ambiguity reasons #906 A description of how scores were calculated can be added by adding \u0026lsquo;EXPLAINSCORE\u0026rsquo; #897 FORK GC has now lowest priority over indexing and read queries Added automated tests to ensure macOs build works Bugfixes - Crash/Stability\nImproved overall architectural stability #666 Fix crash when conflict between internal key name and user key name is encountered; e.g. creating a new document with ft:two/two #697 #588 Fix memory leaks #691 Fix crash on FT.EXPLAIN Proper module-level and index-level cleanup functionality Simplified concurrency model #898 Fix rare issue where FORK GC doesn\u0026rsquo;t exists on termination of Redis #865 When using FT.SEARCH with SORTBY, it will only be possible to sort by one field #917 Fix wrong results introduced in a skip optimisation #888 NULL terms cause FORK GC to crash #887 Chinese searches not being converted to simplified Chinese. Fix FORK GC issue where read from pipe did not returned all the data ","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/redistimeseries-1.8-release-notes/","uriRel":"/modules/redistimeseries/release-notes/redistimeseries-1.8-release-notes/","title":"RedisTimeSeries 1.8 release notes","tags":[],"keywords":[],"description":"Added a time-weighted average aggregator, gap filling, ability to control bucket timestamps, ability to control alignment for compaction rules, new reducer types, and ability to include the latest (possibly partial) raw bucket samples when retrieving compactions","content":"Requirements RedisTimeSeries v1.8.5 requires:\nMinimum Redis compatibility version (database): 6.0.16 Minimum Redis Enterprise Software version (cluster): 6.2.8 v1.8.5 (January 2023) This is a maintenance release for RedisTimeSeries 1.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#1388 Potential crash when upgrading from v1.6 to 1.8 if there are compactions with min or max aggregation (MOD-4695) v1.8.4 (December 2022) This is a maintenance release for RedisTimeSeries 1.8.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#1360 Potential crash when upgrading from v1.6 to 1.8 if there are compactions with min or max aggregation (MOD-4559) #1370 Potential crash when using TS.REVRANGE or TS.MREVRANGE with aggregation #1347 When adding samples with TS.ADD or TS.MADD using * as timestamp, the timestamp could differ between master and replica shards Improvements:\n#1215 OSS cluster: Support TLS and IPv6; introduce new configuration parameter: OSS_GLOBAL_PASSWORD v1.8 GA (v1.8.3) (November 2022) This is the General Availability release of RedisTimeSeries 1.8.\nHighlights RedisTimeSeries 1.8 introduces seven highly requested features, performance improvements, and bug fixes.\nWhat\u0026rsquo;s new in 1.8 Optionally retrieving aggregation results for the latest (still open) bucket for compactions\nTill version 1.8, when a time series is a compaction, TS.GET, TS.MGET, TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE did not report the compacted value of the latest bucket. The reason is that the data in the latest bucket of a compaction is still partial. A bucket is ‘closed’ and compacted only upon the arrival of data that ‘opens’ a ‘new latest’ bucket.\nThere are use cases, however, where the compaction of the latest bucket should be retrieved as well. For example, a user may want to receive the count of events since the start of the decade, and the retention period for raw data is only one month. Till version 1.8, the user would have to run two queries - one on a compaction and one on the latest raw data, and then sum the results. Since version 1.8, by specifying LATEST, it is possible to retrieve the latest (possibly partial) bucket as well.\nTo report aggregations for the latest bucket, use the new optional LATEST flag to TS.GET, TS.MGET, TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE.\nOptionally retrieving aggregation results for empty buckets\nThe commands TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE have an optional [AGGREGATION aggregator bucketDuration] parameter. When this parameter is specified, raw reports are aggregated per bucket.\nTill version 1.8, results were not reported for empty buckets. With EMPTY, it is now possible to report aggregations for empty buckets as well.\nThe two primary reasons for wanting to retrieve values for empty buckets:\nIt is easier to align results from similar queries over multiple time series\nIt is easier to use the retrieved results with some external tools (e.g., charting tools)\nFor the sum and count aggregators, the value 0 is reported for empty buckets.\nFor the min, max, range, avg, first, std.p, and std.s aggregators, the value NaN (not a number) is reported.\nFor the last aggregator and the new twa aggregator, the EMPTY flag is used for gap filling (see below).\nTo report aggregations for empty buckets, use the new optional EMPTY flag in TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE.\nRegardless of the values of fromTimestamp and toTimestamp, no data is reported for empty buckets that end before the earliest sample or begin after the latest sample in the time series.\nA new aggregator: time-weighted average\nWhen a time series holds discrete samples of a continuous signal (e.g., temperature), using avg to estimate the average value over a given timeframe would produce a good estimate only when the signal is sampled at constant intervals. If, however, samples are available at non-constant intervals (e.g., when some samples are missing), the twa aggregator produces a more accurate estimate by averaging samples over time instead of simply averaging the samples.\nThis is an extreme example: the signal in the diagram above has 4 samples in its ‘high’ value and 13 samples in its ‘low’ value. However, the period in each of those states is about the same. It is easy to see that the simple average (avg) of all the 17 samples does not represent the signal’s average over time.\nTime-weighted average (twa) adds weight to each sample. The weight is proportional to the time interval that the sample represents. In the diagram, the time-weighted average over the whole period assigns appropriate weight to each sample, so the result represents the signal’s average value over the whole period much more accurately.\nGap filling: optionally interpolate or repeat last value for empty buckets\nGap filling is performed when using EMPTY together with either last or twa aggregator.\nUsing EMPTY with the twa aggregator allows us to estimate the average of a continuous signal even for buckets where no samples were collected (gap-filling).\nConsider we want to use TS.RANGE to calculate the average value of each bucket (p1, p2, p3 in the diagram above). Using avg, the value reported for bucket p2 would be NaN, as this bucket contains no samples. If we use EMPTY with twa, on the other hand, the average value for bucket p2 would be calculated based on the linear interpolation of the value left of p2 and the value right of p2.\nWhen sampling a continuous signal, we can use this ‘gap-filling’ capability to calculate the average value of the signal over equal-width buckets without concern about bucket alignment or missing samples.\nUsing EMPTY with the last aggregator allows filling empty buckets by repeating the value of the previous sample. This is useful, for example, when values in the time series represent stock prices and the price has not been changed during a bucket’s timeframe.\nAbility to control how bucket timestamps are reported\nTill version 1.8, TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE returned the start time of each reported bucket as its timestamp.\nChanging this behavior and reporting each bucket’s start time, end time, or mid-time is now possible. This is required in many use cases. For example, when drawing bars in trading applications, annotating each bar with the end timestamp of the bucket it represents is very common.\nThe way bucket timestamps are reported can be specified with the new optional BUCKETTIMESTAMP parameter of TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE:\n- or low: the bucket\u0026rsquo;s start time (default)\n+ or high: the bucket\u0026rsquo;s end time\n~ or mid: the bucket\u0026rsquo;s mid-time (rounded down if not an integer)\nAbility to control alignment for compaction rules\nTill version 1.8, compaction rules could not be aligned. One could specify a compaction rule with 24-hour buckets, and as a result, each bucket would aggregate events from midnight till the next midnight. The first bucket always started at the epoch and all other buckets were aligned accordingly.\nBut what if we want to aggregate daily events from 06:00 to 06:00 the next day? We can now specify alignment for compaction rules.\nAlignment can be specified with the new optional alignTimestamp parameter of TS.CREATERULE and the COMPACTION_POLICY configuration parameter. Specifying alignTimestamp ensures that there is a bucket that starts exactly at alignTimestamp and all other buckets are aligned accordingly. alignTimestamp is expressed in milliseconds. The default value is 0 (aligned with the epoch).\nNew reducers\nTill version 1.8, only the sum, min, and max could be used as reducer types.\nIt is now possible, for example, to calculate the maximal temperature per timeframe for each sensor and then report the average temperature (avg reducer) over groups of sensors (grouped by a given label\u0026rsquo;s value).\nThis can be specified with the new reducer types (TS.MRANGE and TS.MREVRANGE): avg, range, count, std.p, std.s, var.p, and var.s.\nNote: New RDB version (v7). RDB files created with v1.8.3 are not backward compatible. ","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-2.4-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-2.4-release-notes/","title":"RedisGraph 2.4 release notes","tags":[],"keywords":[],"description":"Added Map and Geospatial Point data types.","content":"Requirements RedisGraph v2.4.14 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.8 v2.4.14 (May 2022) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: LOW: No need to upgrade unless there are new features you want to use.\nDetails:\nBug fixes:\n#2072, #2081 CRLF sequences embedded in strings no longer trigger a protocol error when being emitted Improvements:\n#2102 New load-time configuration option NODE_CREATION_BUFFER - see documentation (MOD-2348) Note: For Redis Enterprise users who want to upgrade to this patch, this version requires being on version 6.2.8 or later. v2.4.13 (December 2021) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nPerformance improvements\n#2040 Use optimal order of traversals Bug fixes:\n#2046 Avoid serialization of duplicate graph keys (high urgency) #2033 Update RediSearch to 2.0.13 v2.4.12 (October 2021) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes: #1981 Crash in index scan v2.4.11 (October 2021) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: MODERATE : Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes: #1931 Fix race condition on calling BGSAVE while flushing matrices #1898 Error when setting a property to an array containing an invalid type #1897 Aliases in WITH...ORDER BY must be valid references% v2.4.10 (September 2021) This is a maintenance release for RedisGraph 2.4\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nCritical bug fixes:\n#1911 When a query timeout is set and the graph contained indexes, a crash could occur while releasing indexes Bug fixes:\n#1913 Update thread-local AST for every cloned operation #1915 Validate function references in parameters% v2.4.7 (July 2021) This is a maintenance release for RedisGraph 2.4\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes: #1746 Avoid invalid memory access after reallocating labels array #1748 Only allocate space for deleted nodes and edges #1749, #1730 Support updating properties with map referenced by parameter or variable #1754 Map should return volatile data #1773 Matrix resize doesn\u0026rsquo;t set both rows and columns atomically #1799 Always instantiate new persistent matrices as sparse% v2.4.6 (June 2021) This is a maintenance release for RedisGraph 2.4\nUpdate urgency: Medium\nDetails:\nPerformance improvements:\n#1702 Optimize matrix synchronization in GRAPH.BULK commands #1716 Reduce lock calls in retrieving matrices for reading #1741 Improve edge extraction #1388 Node creation buffer #1724 Update RediSearch 2.0 #1731 Release lock after MergeCreate builds entities #1734 Added multi edge flag Bug fixes:\n#1726 JsonEncoder: invalid cast #1709 #1712 Fix potential deadlock in BGSAVE #1703 Uniquing logic in UNION subqueries #1720 Propagate configuration updates to replicas #1714 Don\u0026rsquo;t build scan operations for bound variables #1708 Regression in GRAPH.CONFIG GET * #1715 Validate function private data before freeing it Miscellaneous:\n#1691 Reword error message on setting property value to invalid type v2.4.4 (15 May 2021) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: Medium\nDetails:\nMinor Enhancements:\n#1666 Add configuration parameter to limit the memory usage of queries #1639 Add functionality to SET all properties on an entity to a map Bug fixes:\n#1678 Crash on multiple procedure calls in a single query v2.4.3 (8 May 2021) This is a maintenance release for RedisGraph 2.4.\nUpdate urgency: Medium\nDetails:\nFeatures:\n#1657 Support filtering on variable-length edges in traversals #1514 Add support for shortestPath in MATCH clauses #1664 Add support for limiting max queued queries #1653, #1655 Add support for square root (sqrt) function Performance improvements:\n#1641 Use indexes when filters can only be resolved at runtime #1668 Apply path filters after all other filters Bug fixes:\n#1650 Fix memory leak on allocated keys in OpAggregate #1671 Disallow nested aggregates in map values, circumventing a bug that introduced invalid group keys #1644 Fix bug in which values for unknown schema types are cached #1656 Fix potential crash when releasing thread pools on server shutdown v2.4 GA (March 2021) Headlines:\nThis release introduces the Map and Geospatial Point data types. Details:\nFeatures:\n#1514 Add support for Map data type. #1516 Add support for Geospatial Point data type. #1562 Add toJSON function. #1607 Querying full-text indexes can yield the score of each result. #1610 Expose runtime configuration for read query timeouts. Performance improvements:\n#1596 Redisgraph-bulk-loader no longer blocks the server. Minor Enhancements\n#1590 Improve handling of runtime errors. #1580 Enable assertions in debug mode. Bugfixes (compared to RC1)\n#1618 Fix relationship types being omitted in traversals of 3 or more types. Note: This is the first GA version of 2.4. The version inside Redis will be 2.4.2 in semantic versioning. Since the version of a module in Redis is numeric, we could not add an GA flag. ","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/redisbloom-2.4-release-notes/","uriRel":"/modules/redisbloom/release-notes/redisbloom-2.4-release-notes/","title":"RedisBloom 2.4 release notes","tags":[],"keywords":[],"description":"Added t-digest - a probabilistic data structure for estimating quantiles based on a data stream or a large dataset of floating-point values.","content":"Requirements RedisBloom v2.4.3 requires:\nMinimum Redis compatibility version (database): 6.0.16 Minimum Redis Enterprise Software version (cluster): 6.2.8 v2.4 GA (v2.4.3) (November 2022) This is the General Availability release of RedisBloom 2.4.\nHighlights RedisBloom 2.4 introduces a new sketch data structure: t-digest.\nWhat\u0026rsquo;s new in 2.4 t-digest is a probabilistic data structure for estimating quantiles based on a data stream or a large dataset of floating-point values. It can be used to answer the following questions:\nWhat fraction of the values in the data stream are smaller than a given value? How many values in the data stream are smaller than a given value? Which value is smaller than p percent of the values in the data stream? (what is the p-percentile value)? What is the mean value between the p1-percentile value and the p2-percentile value? What is the value of the n-th smallest / largest value in the data stream? (what is the value with [reverse] rank n)? As for any other probabilistic data structures, t-digest requires sublinear space and has controllable space-accuracy tradeoffs.\nUsing t-digest is simple and straightforward:\nCreating a sketch and adding observations\nTDIGEST.CREATE key [COMPRESSION compression] initializes a new t-digest sketch (and errors if the key already exists). The COMPRESSION argument is used to specify the tradeoff between accuracy and memory consumption. The default is 100. Higher values mean more accuracy.\nTDIGEST.ADD key value... adds new observations (floating-point values) to the sketch. You can repeat calling TDIGEST.ADD whenever new observations are available.\nEstimating fractions or ranks by values\nUse TDIGEST.CDF key value... to retrieve, for each input value, an estimation of the fraction of (observations smaller than the given value + half the observations equal to the given value).\nTDIGEST.RANK key value... is similar to TDIGEST.CDF, but used for estimating the number of observations instead of the fraction of observations. More accurately it returns, for each input value, an estimation of the number of (observations smaller than a given value + half the observations equal to the given value).\nAnd lastly, TDIGEST.REVRANK key value... is similar to TDIGEST.RANK, but returns, for each input value, an estimation of the number of (observations larger than a given value + half the observations equal to the given value).\nEstimating values by fractions or ranks\nTDIGEST.QUANTILE key fraction... returns, for each input fraction, an estimation of the value (floating point) that is smaller than the given fraction of observations.\nTDIGEST.BYRANK key rank... returns, for each input rank, an estimation of the value (floating point) with that rank.\nTDIGEST.BYREVRANK key rank... returns, for each input reverse rank, an estimation of the value (floating point) with that reverse rank.\nEstimating trimmed mean\nUse TDIGEST.TRIMMED_MEAN key lowFraction highFraction to retrieve an estimation of the mean value between the specified fractions.\nThis is especially useful for calculating the average value ignoring outliers. For example, calculating the average value between the 20th percentile and the 80th percentile.\nMerging sketches\nSometimes it is useful to merge sketches. For example, suppose we measure latencies for 3 servers, and we want to calculate the 90%, 95%, and 99% latencies for all the servers combined.\nTDIGEST.MERGE destKey numKeys sourceKey... [COMPRESSION compression] [OVERRIDE] merges multiple sketches into a single sketch.\nIf destKey does not exist, a new sketch is created.\nIf destKey is an existing sketch, its values are merged with the values of the source keys. To override the destination key contents, use OVERRIDE.\nRetrieving sketch information\nUse TDIGEST.MIN key and TDIGEST.MAX key to retrieve the minimal and maximal values in the sketch, respectively.\nBoth return NaN (Not a Number) when the sketch is empty.\nBoth commands return accurate results and are equivalent to TDIGEST.BYRANK key 0 and TDIGEST.BYREVRANK key 0 respectively.\nUse TDIGEST.INFO key to retrieve some additional information about the sketch.\nResetting a sketch\nTDIGEST.RESET key empties the sketch and reinitializes it.\n","categories":["Modules"]},{"uri":"/ri/release-notes/v1.3.0/","uriRel":"/ri/release-notes/v1.3.0/","title":"RedisInsight v1.3, March 2020","tags":[],"keywords":[],"description":"Auto-discovery of Redis Cloud databases, visualising paths in RedisGraph","content":"RedisInsight v1.3.1 release notes (April 2020) This is the maintenance release of RedisInsight 1.3 (v1.3.1).\nUpdate urgency: Medium\nHeadlines Fixed support for connecting to Redis database on TLS-enabled hosts with SNI enforcement. Details Bug Fixes: Core: Fixed support for connecting to Redis database on TLS-enabled hosts with SNI enforcement. Memory Analysis Fixed wrong display of table columns in Memory Analyzer view. Browser Fixed bug where the TTL on string and RedisJSON keys was being reset on edit. Configuration: Fixed freezing/flashing on refreshing configuration. CLI Fixed minor visual bug in inline command documentation. Security Updated frontend dependencies that had developed security vulnerabilities. RedisInsight v1.3.0 release notes (March 2020) This is the General Availability release of RedisInsight 1.3 (v1.3.0)!\nHeadlines The Windows installer is now signed with a Microsoft Authenticode certificate. Auto-Discovery of databases for Redis Cloud Pro. Visualising paths in RedisGraph Details Features:\nSecurity: The Windows installer now signed with a Microsoft Authenticode certificate Core: Auto-Discovery for Redis Cloud Pro: Redis Cloud Pro subscribers can automatically add their cloud databases with just a few clicks Support for editing the connection details of an added database Better support for Sentinel-monitored databases with different passwords for the sentinel instance(s) and database UI improvements to the add database form Added a button in the top-right menu to reach the online documentation with one click RedisGraph: Added support for visualising queries that use path functions Memory Analysis: Added support for virtual hosted-style S3 paths for Offline Analysis Browser: Added tooltip to make it easier to view the name of long keys Bug Fixes:\nCore: Fixed fonts that were being loaded from the Internet, causing jarring visual changes on slow connections RedisGraph: Improved rendering of Array records Removed GRAPH.EXPLAIN calls for now until we have execution plan visualisation ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-4-2-october-2015/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-4-2-october-2015/","title":"RLEC 4.2.1-30 Release Notes (October 18, 2015)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the upgrade instructions before running through the upgrade process.\nNew features rsyslog logging support - RLEC now writes by default to syslog, which enables monitoring through rsyslog. Profile support for tuning cloud and non-cloud environments \u0026ndash; enables the administrator to configure RLEC to run with different performance profiles that are optimized for either cloud or non-cloud environments. For additional details, refer to the Performance optimization section. SLA for AOF rewrite - enables the administrator to configure database parameters, by running the rladmin tune command, related to when AOF rewrite is triggered based on the time it would take to load the database from the AOF file, and on the maximum AOF rewrite file size. In addition, updates to the AOF rewrite mechanism minimize chances of the disk getting full. New warning and alerts related to AOF rewrite mechanism - a warning is shown during the setup process in case the disk size is lower than twice the size of the RAM. New cluster level alerts added to alert when node available disk space is lower than the needed disk space for AOF rewrite purposes, and when node performance is degraded due to reaching disk I/O limits. Replica Of support for multiple sources - the Replica Of feature is enhanced to support creating a database that is a replica of multiple source databases. For additional details, refer to the Replica Of section. Cross cluster Replica Of - the Replica Of feature now supports defining a database that is a replica of databases that belong to a different RLEC cluster. For additional details, refer to the Replica Of section. Multi-IP support - on a node that has multiple IPs, enables the administrator to specify which IP address is used for internal traffic and which IP addresses are used for external traffic. For additional details, refer to Multi-IP \u0026amp; IPv6 support. IPv6 support for external traffic - on a node that has multiple IPs, external IP addresses can be of IPv6 type. For additional details, refer to Multi-IP \u0026amp; IPv6 support section. Support for OpenStack Object Store (\u0026ldquo;Swift\u0026rdquo;) location for import / export / backup. For additional details, refer to Database backup and Importing data to a database sections. Import of a sharded database - support for importing data of a sharded database by indicating multiple files paths. For additional details, refer to the Importing data to a database section. Enable running the install script in silent mode using \u0026ldquo;-y\u0026rdquo; parameter for default answers (\u0026ldquo;Y\u0026rdquo;) or \u0026ldquo;-c\u0026rdquo; for file path parameters for custom answers. For additional details, refer to Accessing and installing the setup package section. New rladmin command-line-interface \u0026ldquo;info\u0026rdquo; command allows for fetching current value of tunable parameters. Changes rladmin command-line-interface can only be run under user root or redislabs. For additional details, refer to the rladmin command-line interface (CLI) section. Import / export / backup to/from Amazon S3 requires supplying the credentials per usage instance; it does not use central cloud credentials that used to be supplied in the Settings -\u0026gt; General page. Fields related to storing Amazon S3 Cloud credentials have been removed from Settings -\u0026gt; General page. Performance optimization in the database resharding mechanism. Persistent and ephemeral storage cluster level alerts are enabled by default and set to 70%. Various enhancements to rladmin command-line-interface (CLI) to support new commands. Redis version updated to 2.8.21 that addresses CVE-2015-4335/DSA-3279 - Redis Lua Sandbox Escape. Port 3336 added to the list of ports being used by RLEC. Node \u0026ldquo;Network utilization\u0026rdquo; alert measured in percentages (%) has been updated to \u0026ldquo;Network throughput\u0026rdquo; alert measured in MBps. If the alert was defined then it will be disabled when upgrading to this version and the user needs to reconfigure it with a new value. Performance improvements to the database Import to make the process much faster. Enhancements to the support package to include additional details. Removed support for SSL 2.0/3.0 protocols due to security vulnerabilities. Disabled \u0026ldquo;dofile\u0026rdquo; functionality in Redis to solve a possible security vulnerability. rladmin CLI \u0026ldquo;tune watchdog profile\u0026rdquo; command syntax updated to \u0026ldquo;tune cluster watchdog_profile\u0026rdquo;. Fixed issues Support for relative path for backup and import functionalities. Fix issue with TLS configuration of email server settings. Email alerts not sent for database export and import processes. Fix erroneous entries in the Log page. Sometimes a wrong value is reported in the UI for node used ephemeral storage space. Sometime the wrong Replica Of Lag value is reported in the UI. RLEC-6875 - email server settings not working when using port 587. RLEC-5498 - Improve rladmin response time when a node is down. Validation of Replica Of source definition did not fail in the UI and would only fail in runtime if it was using the Cluster Name (FQDN) and the FQDN was not properly configured in the DNS. Various improvements to error messages reported by rladmin. Known issues Issue: Connecting from a client to a database endpoint with mixed upper case and lower case letters can result in a slow response from the database.\nWorkaround: The cluster name (FQDN) should consist of only lower-case letters. When connecting from a client to a database endpoint, only lower case letters should be used in the endpoint.\nIssue: When upgrading a node to a new RLEC version (refer to Upgrading nodes while the node is in the offline state (refer to Taking a node offline, the upgrade process succeeds but might result in an unstable cluster.\nWorkaround: Do not try to upgrade a node while it is in the offline state.\nIssue: In Red Hat Enterprise Linux, and CentOS operating systems, the process used for cleaning up internal log files does not work, thereby allowing the log files to grow and possibly result in disk space issues.\nWorkaround: On each machine that functions as a node in the cluster, create a file named \u0026ldquo;redislabs\u0026rdquo;, and save it in the following location: \u0026ldquo;/etc/logrotate.d/\u0026rdquo; (e.g. \u0026ldquo;/etc/logrotate.d/redislabs\u0026rdquo;). The file should contain the following text:\n/var/opt/redislabs/log/\\*.log { daily missingok copytruncate rotate 7 compress notifempty } The file\u0026rsquo;s permissions should be root:root, 644. Afterwards, from the operating system command line interface (CLI) run the following commands:\nyum install policycoreutils-python semanage fcontext -a -t var_log_t \u0026#39;/var/opt/redislabs/log(/.\\*)?\u0026#39; restsorecon -R /var/opt/redislabs/log Issue: In the Replica Of process, if the target database does not have replication enabled and it is restarted or fails for any reason, the data on target database might not be in sync with the source database, although the status of the Replica Of process indicates it is.\nWorkaround: You need to manually stop and restart the synchronization process in order to ensure the databases are in sync.\nIssue: In the Replica Of process, if the source database is resharded while the replica of process is active, the synchronization process will fail.\nWorkaround: You need to manually stop and restart the synchronization process after resharding of the source database is done.\nIssue: In the replica of process, high database traffic might restart the Replica of process as result of the \u0026ldquo;replica buffer\u0026rdquo; being exceeded. In this case you see the status of the replica of process as \u0026ldquo;Syncing\u0026rdquo; frequently.\nWorkaround: You need to manually reconfigure the \u0026ldquo;replica buffer\u0026rdquo; through rladmin and set the buffer size to a new size. In order to find the appropriate buffer size please contact support at: support@redislabs.com.\nIssue: In a cluster that is configured to support rack-zone awareness, if the user forces migration of a master or replica shard, through rladmin, to a node on the same rack-zone as its corresponding master or replica shard, and later runs the rebalance process, the rebalance process will not migrate the shards to ensure rack-zone awareness compliance.\nWorkaround: In the scenario described above, you need to manually migrate the shard, through rladmin, to a node on a valid rack-zone in order to ensure rack-zone awareness compliance.\n","categories":["RS"]},{"uri":"/modules/redisearch/release-notes/redisearch-1.4-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-1.4-release-notes/","title":"RediSearch 1.4 release notes","tags":[],"keywords":[],"description":"Conditional updates. Schema modification. Query spelling correction. Phonetic matching. More fuzziness in search. Retrieve and change runtime configuration. Unlimited autocomplete results.","content":"Requirements RediSearch v1.4.28 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v1.4.28 (May 2020) This is a maintenance release for version 1.4.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1218 Potential crash when running without concurrency and using the cursor API. v1.4.27 (April 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nFeatures: #1172 Added exists function that can be used on conditional updates (REPLACE PARTIAL) to check if a field exists in the document. Minor Enhancements: #1172 Lazy evaluation of the right side of \u0026lsquo;or\u0026rsquo;/\u0026lsquo;and\u0026rsquo; clauses in IF condition. Bugfixes: #1110 Rare GC failure when accessing uninitialized variable. #1131 Crash on highlighting a search query where the document no longer exists. v1.4.26 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nFix rare FORK GC crash which caused by accessing uninitialized variable. v1.4.25 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nFeatures: #1051 Added support for updating tag fields on document updates with NOINDEX fields. Bugfixes: #1051 FORK GC was not updating the unique sum of the numeric index. v1.4.24 (January 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1038 Memory leak on cursor. #1049 Crash on conversion error when freeing other indexed fields. v1.4.23 Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: Memory leak when cursor timed out and cursor wasn\u0026rsquo;t consumed. v1.4.22 Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: FILTER option was not working correctly with coordinator. Memory leak when cursor was combined with sorted fields. v1.4.21 Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1031 Highlighting crashed when used with NOINDEX fields. v1.4.20 (January 2020) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nImprovements #1020 Performance improvement for reading fields that are not sortable. Bugfixes #1022 Illegal memory access by queries during GC run. #1022 Recreating the index with the same name (delete+create) removed the index from cursor list. #1022 Memory leak when performing FT.AGGREGATE on non-existing index. #1022 Potential data corruption during GC run. #1025 Aliasing not working properly with FT.AGGREGATE. v1.4.19 (December 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBugfixes #1000 - FT.DEL was not replicated to replica correctly #1004 - Memory leak on TAG array on certain situations #1006 - Unexposed error on conditional update IF that caused the error message to leak v1.4.18 (November 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBug fixes #947 Fix short read on FORK GC pipe that could result in a crash and potential data corruption v1.4.17 (October 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nFeatures: #779 Added to_number() and to_str() functions for ambiguity reasons Improvements #891 All memory allocations will now use the redis memory allocator. This means that all the memory will be exposed correctly in the redis INFO MEMORY command. v1.4.16 (September 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nMain features:\n#883 Forkgc optimizations - introduce new config parameter FORK_GC_CLEAN_THRESHOLD. RediSearch will only start to clean when the number of not cleaned documents is exceeding this threshold. Main Fixes:\n#848 RediSearch will not crash when sorting on fields that don\u0026rsquo;t exists in all documents. #884 Fix wrong results on intersect iterator. v1.4.15 (28 August 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nMain Fixes:\n#866 - Fix RDB corruption caused by deleting none-existing terms from the suggestion trie. When Redis exits, forked processes by FORK Garbage Collection will now be closed accordingly. For indices that are not temporary and interleaved: When an index is dropped, the indexer thread is now closed. v1.4.14 (20 August 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nMain Fixes:\n#851 In interleaved mode (non safemode), documents deleted by concurrent updates, will be ignored. v1.4.13 (8 August 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\n#798 fix issue where phonetic queries return wrong results #820 fix crash on getting a none existing doc from a doc table Fix issue with invalid memory read when using tags with \u0026rsquo; \u0026rsquo; (space) separator v1.4.12 (5 August 2019) Update urgency: Medium This is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain features:\n#741 Allow Chinese tokenizing to recognise -escape for punctuations Main Fixes:\n#739 Fix crash on search error #346 Fix issue where fuzzy can not be used with numbers #769 Fix rare crash on rdb loading #749 On prefix searches, do not expand prefixes to terms which have no documents v1.4.11 (June 2019) Update urgency: Medium\nThis release only add support for aliasing to the previews 1.4.10 release.\nAdded functionality #731 Add index aliasing. This allows users to provide (or remove) ‘links’ to indexes. The commands are FT.ALIASADD, FT.ALIASDEL, and FT.ALIASUPDATE. See ftaliasadd for details. v1.4.10 (28 May 2019) Update urgency: Medium\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFix memory leak when combining SORT with APPLY on FT.AGGREGATE v1.4.9 (18 May 2019) Update urgency: Medium\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFork GC will now squash the empty blocks of the inverted indexes. Fix invalid memory access when using aggregate with GROUPBY. Fix issue where using limit with SORTBY might return duplicate results. Known Issue:\nMemory leak when combining SORT and APPLY in FT.AGGREGATE. This issue is fixed on 1.4.10. It\u0026rsquo;s recommended to skip directly to 1.4.10. v1.4.8 (29 April 2019) Update urgency: Low\nTechnical release, no changes nor fixes.\nv1.4.7 (29 April 2019) Update urgency: Medium\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFix issue where Dropping and recreating the same index while querying might cause crashes. v1.4.6 (8 April 2019) Update urgency: Medium\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nLoading a potentially corrupted RDB files generated by versions lower than 1.4 Fix issue where REPLACE PARTIAL might not work properly (#621) v1.4.5 (March 2019) Update urgency: Low\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFixed issue where FORK GC causing redis rdb fork to fail v1.4.4 (21 February 2019) Update urgency: Low\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFixed memory leak on Fork GC Fixed key close after releasing GIL on Fork GC (might cause crashed on rare situations) v1.4.3 (4 February 2019) Update urgency: Low\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes for issues found after the previous release .\nMain Fixes:\nFixed memory leak on mempool destroy Fixed process crash when running with fork GC (preview) Fixed fork GC (preview) deadlock when using Tags Fixed memory leaks on index creation and deletion v1.4.2 (27 November 2018) Update urgency: Low\nThis is a maintenance release for version 1.4.\nThis release improves overall stability and provides fixes to issues found.\nv1.4.1 (12 November 2018) Update urgency: Medium\nThis is a maintenance release for version 1.4, with the next version planned for release being 2.0.\nThis release improves overall stability and focuses on performance improvements of the garbage collector. It also includes:\nNew: Runtime Configuration Change: Unlimited Autocomplete Results Garbage collector RediSearch employs a garbage collector that removes deleted documents from the internal data structures.\nIn this release the garbage collection mechanism was improved in terms of efficiency, i.e. the amount of memory it reclaims, as well as in terms of performance (i.e. it is faster). The improved mechanism uses forked threads. Additional details can be found in this post: How We Increased Garbage Collection Performance with RediSearch 1.4.1.\nThe improved collection mechanism is currently *experimental- and is not enabled by default. Enabling the new garbage collection mechanism requires setting the GC_POLICY configuration option to FORK at load time, for example:\nredis-server --loadmodule ./redisearch.so GC_POLICY FORK Runtime configuration RediSearch\u0026rsquo;s configuration is applied via arguments passed to the module at load time. This release introduces the new FT.CONFIG command that allows to retrieve the current configuration as well as change it during runtime.\nUnlimited autocomplete results This version removes the limit of 10 results from FT.SUGGET - you can set the MAX num as high as needed.\nv1.4.0 (August 2018) **Update urgency:*- Medium - mainly due to numerous fixes\nThis version improves overall stability and performance of RediSearch. It also delivers better support for use cases in which documents are continuously updated. New features:\nConditional updates Schema modification Query spelling correction Phonetic matching Enhancement: More fuzziness in search Continuous updates Like most search engines, RediSearch was designed for maintaining append-mostly indices. To update an existing document, the document is actually replaced - that is deleted and added to the index.\nBecause RediSearch provides realtime indexing and searching, it is sometimes used to search near-realtime data (or the results of its processing). In such cases, the number of indexed documents stays mostly static, but their contents are continuously updated.\nTo support this use case, RediSearch has been reworked internally to use 64-bit internal document IDs so that an index can sustain high update throughputs without overflowing. Furthermore and as a result, significant effort has been put into improving memory management and garbage collection.\nConditional updates The IF subcommand has been add to FT.ADD. When used with the existing REPLACE [PARTIAL] subcommand, the document will be updated only if the condition provided evaluates to a truth value, otherwise a special NOADD reply is returned.\nSchema modification The FT.ALTER command has been introduced, and provides the ability to add new fields to the definition of an existing index. The contents of such newly-added fields are indexed only for new or updated documents.\nQuery spelling correction Query spelling correction, a.k.a \u0026ldquo;did you mean\u0026rdquo;, is now provided via the FT.SPELLCHECK command. It enables generating suggestions for search terms that could be misspelled. For more details see Query Spelling Correction.\nPhonetic matching Phonetic matching, a.k.a \u0026ldquo;Jon or John?\u0026rdquo;, is now supported via the PHONETIC text field attribute. The terms in such fields are also indexed by their phonetic equivalents, and search results include these by default. For more details see Phonetic Matching.\nMore fuzziness in search The fuzzy match operator, \u0026lsquo;%\u0026rsquo;, can now be repeated up to three times to specify the Levenshtein distance. That means that the queries %hello%, %%hello%% and %%%hello%%% will perform fuzzy matching on \u0026lsquo;hello\u0026rsquo; for all terms with LD of 1, 2 and 3, respectively.\n","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/redistimeseries-1.6-release-notes/","uriRel":"/modules/redistimeseries/release-notes/redistimeseries-1.6-release-notes/","title":"RedisTimeSeries 1.6 release notes","tags":[],"keywords":[],"description":"Added support for aggregating across multiple time series (multi-key). Can compute queries such as “the maximum observed value of a set of time series” server-side instead of client-side.","content":"Requirements RedisTimeSeries v1.6.17 requires:\nMinimum Redis compatibility version (database): 6.0.16 Minimum Redis Enterprise Software version (cluster): 6.2.8 v1.6.17 (July 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#1240 Compaction rules are not saved to RoF (Redis Enterprise) v1.6.16 (June 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nFeatures:\n#1193 Commands that don’t execute on the main thread now appear in SLOWLOG Bug fixes:\n#1203 Compaction rules are not replicated (Replica Of) on Redis Enterprise #1204 When the last sample is deleted with TS.DEL, it may still be accessible with TS.GET #1226 TS.MRANGE, TS.MREVRANGE: on a multi-shard environment, some chunks may be skipped Note: New RDB version (v5). RDB files created with 1.6.16 are not backward compatible. v1.6.13 (June 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#1176, #1187 When executing DEL, chunk index could be set to a wrong value and cause some data to be inaccessible #1180 When executing MADD, make sure that only successful insertions are replicated v1.6.11 (May 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#1166 Stop forwarding multi-shard commands during cluster resharding and upgrade (MOD-3154) #1165 Stop forwarding multi-shard commands during transactions (MULTI EXEC) (MOD-3182) LibMR: Fixed crash on multi-shard commands in some rare scenarios (MOD-3182) v1.6.10 (May 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#1074 RANGE, REVRANGE, MRANGE, and MREVRANGE: Possibly incorrect result when using ALIGN and aggregating a bucket with a timestamp close to 0 #1094 LibMR: Potential memory leak; memory release delay #1127 Memory leak on RANGE and REVRANGE when argument parsing fails #1096 RANGE, REVRANGE, MRANGE, and MREVRANGE: Using FILTER_BY_TS without specifying timestamps now returns an error as expected v1.6.9 (February 2022) This is a maintenance release for RedisTimeSeries 1.6.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nSecurity and privacy:\n#1061 Internode communications encryption: support passphrases for PEM files Bug fixes:\n#1056 Return an error when a shard is down (in v1.6.8, returned an empty result) v1.6 GA (v1.6.8) (January 2022) This is the General Availability release of RedisTimeSeries 1.6.\nHighlights RedisTimeSeries 1.6 adds support for aggregating across multiple time series (multi-key). Before this version, queries such as “the maximum observed value of a set of time series” needed to be calculated client-side. Such queries can now be computed server-side, leveraging the heart of RedisGears (LibMR) for clustered databases.\nWhat\u0026rsquo;s new in 1.6 Introduction of GROUPBY and REDUCE in TS.MRANGE and TS.MREVRANGE to add support for \u0026ldquo;multi-key aggregation\u0026rdquo; and support for such aggregations spanning multiple shards, leveraging LibMR. Currently, we support min, max, and sum as reducers and grouping by a label.\n#722, #275 Filter results using FILTER_BY_TS by providing a list of timestamps and FILTER_BY_VALUE by providing a min and a max value (TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE).\n#603, #611, #841 Introduction of TS.DEL which allows deleting samples in a time series within two timestamps (inclusive).\n#762 Limit the number of returned labels in the response of read commands (TS.MRANGE, TS.MREVRANGE, and TS.MGET) using SELECTED_LABELS. This can be a significant performance improvement when returning a large number of series.\n#655, #801 Ability to align the aggregation buckets with the requested start, end, or specific timestamp on aggregation queries using ALIGN (TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE).\n#675 Add keyspace notifications for all CRUD commands. Check out this test for the details.\n#882 Redis on Flash (RoF) support.\n","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/redisbloom-2.2-release-notes/","uriRel":"/modules/redisbloom/release-notes/redisbloom-2.2-release-notes/","title":"RedisBloom 2.2 release notes","tags":[],"keywords":[],"description":"BF.INFO returns bloom filter details. CF.INFO returns cuckoo filter details. Scalable bloom and cuckoo filters. Configurable bucket size for cuckoo filters. CMS.INCRBY returns count.","content":"Requirements RedisBloom v2.2.18 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v2.2.18 (July 2022) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#481 CF crashes on expansion 0 #478 BF.INFO reports an inaccurate result about the memory footprint v2.2.17 (June 2022) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nDetails:\nBug fixes:\n#451 TOPK.INCRBY: fix calculation when old fingerprint count is reduced to 0 #462 CF.RESERVE: fix potential crash on incorrect number of parameters #434 CF.INSERT: fix crash when capacity is negative #450 TOPK.INCRBY: limit increment to 100,000 to prevent potential long freezes v2.2.15 (May 2022) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: None.\nDetails:\nEnhancements:\nAdded support for RedisBloom for AArch64 Linux v2.2.14 (March 2022) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#419 CMS.INCRBY now returns an error on overflow #412 Fixed macOS build v2.2.12 (February 2022) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#392 Fixed a potential crash on Bloom filter expansion when loading from AOF #404 Fixed a potential crash on Cuckoo filter when calling CF.LOADCHUNK on a filter with EXPANSION greater than 1 v2.2.9 (November 2021) This is a maintenance release for RedisBloom 2.2.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#281 Memory leak when loading TOPK keys from RDB.\n#303 CMS.INCRBY for negative or non-number\n#354 TOPK List Duplicate entries\n#369 Free only latest filter on CF.COMPACT\n#371 CF.SCANDUMP for cuckoo filter\n#374 ReplicateVerbatim for CF.LOADCHUNK (similar to #309)\nImprovements:\n#331 Add WITHCOUNT flag to TOPK.LIST v2.2.6 (August 2021) This is a maintenance release for version 1.0.\nUpdate urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nEnhancements: #333 Support inter shards TLS capability v2.2.4 (July 2020) Headlines:\nThis release improves overall stability and provides fixes for found issues. Details:\nBug fixes: #215 Count-Min-Sketch CMS.INCRBY command to reply with correct min result. #219 Cuckoo Filter CF.DEBUG correct response formatting. v2.2.3 (July 2020) Headlines:\nThis release improves overall stability and provides fixes for found issues. Details:\nBug fixes: #217 Client hung on BF.INSERT with multiple new items when a non-scaling filter is full. v2.2.2 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor enhancements: Bloom #180 Removed the upper limit on Bloom Filter capacity. v2.2.1 (January 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor Enhancements: Bloom #179 Allow storing filters with size over 4294967295 (UINT32_MAX). Now 64 bits. #177 Prevent passing both EXPANSION and NONSCALING parameters to BF.RESERVE. v2.2.0 (December 2019) Added functionality\nBloom #149 BF.INFO returns details about a specific bloom filter Scalable #153 Ability to change the EXPANSION rate. This means each subsequent sub-filter will be expansion times larger as the previous one. #160 Optimise the scaling up of filter according to the Scalable Bloom Filter paper #161 Optional NONSCALING argument to disable scaling. (This saves space since less hash functions are used) #155 Disabling rounding up functionality Cuckoo #149 CF.INFO returns details about a specific cuckoo filter Scalable #138 Configurable EXPANSION. When an additional filter is created, its size will be the size of the current filter multiplied by the expansion. Higher expansion rates will result in lower error rates. #142 The maximum number of expansions limited to 32. #131 Configurable MAXITERATIONS. Number of attempts to swap buckets before declaring filter as full and creating an additional filter. #135 Configurable BUCKETSIZE. Number of items in each bucket. Higher bucket size value improves the fill rate but result in a higher error rate and slightly slower operation speed. #142 use of 64bit hash function #136 expose compaction of filters in the API CMS #97 CMS.INCRBY returns count instead of ‘OK’ Minor bug fixes\nBloom #154 Check error rate is 1\u0026lt; (cannot be equal) Cuckoo #134 Added CuckooInsert_MemAllocFailed exception #130 Number of deletes wasn’t saved to RDB General #117 Using RMUtil_RegisterWriteDenyOOMCmd #121 Moved ReplicaVerbatim to end of functions ","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-2.2-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-2.2-release-notes/","title":"RedisGraph 2.2 release notes","tags":[],"keywords":[],"description":"Support for scaling reads, OPTIONAL MATCH, query caching, and GRAPH.SLOWLOG.","content":"Requirements RedisGraph v2.2.16 requires:\nMinimum Redis compatibility version (database): 5.0.7 Minimum Redis Enterprise Software version (cluster): 6.0.8 v2.2.16 (April 2021) This is a maintenance release for version 2.2.\nUpdate urgency: Medium\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nEnhancements:\n#1610 Add module-level configuration for query timeouts #1572 Remove query size limitation from parser #1590 Improve reporting of runtime errors #1596 Run bulk loader commands on a non-blocking thread Performance Improvements:\n#1569 Avoid network overhead of emitting arrays with postponed length values Bugfixes:\n#1573 Lock graph on building/deleting full-text indexes, fixing a bug in full-text index replication #1618 Fix error in traversals of 3 or more possible relationship types Infrastructure:\n#1557 Add automated performance benchmarks and regression testing #1587, #1598 Create build artifacts for multiple environments v2.2.14 (16 February 2021) This is a maintenance release for version 2.2.\nUpdate urgency: Medium\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nPerformance improvements:\n#1565 Reserve a single thread for performing all write queries. #1548, #1553, #1554 only perform SET property updates when property value is changed. #1543 Enable O3 compiler optimizations on libcypher-parser. Bugfixes:\n#1556 Avoid premature freeing of Records in create and update contexts. #1541 Fix excessive memory consumption in OpApply. v2.2.13 (16 February 2021) This is a maintenance release for version 2.2.\nUpdate urgency: Medium\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor enhancements:\n#1490 Add support for startNode() and endNode() functions. #1519 Add support for db.indexes() procedure. Performance improvements:\n#1393 Use one unified cache for all RedisGraph worker threads. #1498 Consider both filters and labels in selecting a traversal\u0026rsquo;s starting point. #1468 Unify logic and error handling for aggregate and scalar functions. Bugfixes:\n#1534 Outdated label information used when performing index scans. #1532 Invalid optimization of IN [array] lookups when values are indexed. #1524 Memory error in accessing complex values after CREATE operations. #1523 Erroneous results when traversing over some transposed matrix sequences. #1499 Erroneous results on variable-length traversals on repeatedly-transposed arrays. #1504 Crash on certain constructions of 0-hop traversals. #1495 Disallow access of properties that have not yet been created. #1503 Only allow literal values as parameters. #1496 Disallow assignment of complex data types to property values in MERGE contexts. #1494 Disallow assignment of values to properties on deleted entities. Build:\n#1524 Update test suite to run under Python 3. v2.2.11 (20 December 2020) This is a maintenance release for version 2.2.\nHeadlines:\nThis release introduces support for runtime configuration of the maximum number of results to be returned.\nDetails:\nFeatures #1480 Introduce GRAPH.CONFIG SET/GET to allow result-set size to be configured at runtime. v2.2.10 (20 December 2020) This is a maintenance release for version 2.2.\nHeadlines:\nThis release improves overall stability and fixes an issue with backwards compatibility logic for indexing.\nDetails:\nBugfixes: #1475 Fix erroneous property update when using SET with an unindexed property followed by an indexed one. #1473 Fix backwards compatibility issue in rebuilding and dropping exact-match and full-text indexes. v2.2.9 (4 December 2020) This is a maintenance release for version 2.2.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nDetails:\nPerformance improvements: #1426 Improve load times of adjacency matrices that always have different source and destination nodes. Minor enhancements: #1463 Remove all assertions from production builds. #1442 NULL property values should be ignored in CREATE clauses and trigger errors in MERGE clauses. #1429 Improve error handling of a number of invalid query constructions. #1431 Traversal patterns are only allowed in MATCH, MERGE, CREATE, and WHERE contexts. Bugfixes: #1429 Fix potential misapplications of WHERE filters. #1460 RETURN clauses can only be followed by UNION clauses. #1467 Disallow assignment of complex data types to property values. #1437 Failed property updates on MERGE should be handled gracefully. #1446 Fix errors in Cartesian Product filter placement. v2.2 GA (November 2020) This is the General Availability Release of RedisGraph 2.2 (v2.2.8)!\nHeadlines:\nEnhanced support for scaling reads OPTIONAL MATCH (Left outer join) Query cache: Improve performance by caching the query execution plan Tooling to increase developer experience Details:\nSupport for scaling reads\n#1054 Drastic reduction of memory consumption during replication (and Active-Passive). Break down a large graph into several portions, each accommodating a virtual key and distribute those for reconstruction at the replica end, by doing so we reduce memory consumption on the replica\u0026rsquo;s end. OPTIONAL MATCH support Unlike MATCH, which requires for a pattern to exist, OPTIONAL MATCH continues processing when the optional pattern doesn\u0026rsquo;t exist. We can easily compare OPTIONAL MATCH in Cypher to an outer join in SQL. It works just like a regular MATCH with the difference that if no matches are found, OPTIONAL MATCH will use a null for missing parts of the pattern.\nQuery cache: Improve performance by caching the query execution plan\nWhen executing parameterised queries, RedisGraph will cache the execution plan for increased performance. By caching query\u0026rsquo;s execution-plan, RedisGraph skips the parsing and execution-plan construction phase. Cache size can be configured at module load time. The default value is 25.\nTooling to increase developer experience\nIntroduction of GRAPH.SLOWLOG command which returns the longest running queries. In addition #1274 introduces query timeouts with an optional query flag.\nSmaller features and enhancements:\n#1225 Only update index on change of relevant Node properties. #1229 RediSearch 1.8.2. (Dependency update) #1266 , #1277 Add support for any() and all() functions in list comprehension. #877 Transpose matrices: maintain transpose relationship matrices such that we won\u0026rsquo;t have to compute the transpose of a matrix at run-time. This feature is on by default but can configured to reduce memory consumption. Small updates compared to RC7\nMinor enhancements: #1365 Support array properties in bulk loader. #1377 Adding a new procedure dbms.procedures that returns all the procedures in the system. #1389 Allow property accesses on non-identifier entity references. Bug fixes: #1406 Query validation should not check if procedure outputs have been defined. #1382 Add graph version to graph context object, response with an error when client graph version mismatch. #1361 Don\u0026rsquo;t migrate WITH filters into Merge and Apply operation scope. Notes: This is the GA version of 2.2. The version inside Redis will be 20208 or 2.2.8 in semantic versioning. Since the version of a module in Redis is numeric, we could not add a GA flag.\n","categories":["Modules"]},{"uri":"/modules/redisjson/release-notes/redisjson-2.4-release-notes/","uriRel":"/modules/redisjson/release-notes/redisjson-2.4-release-notes/","title":"RedisJSON 2.4 release notes","tags":[],"keywords":[],"description":"Low-level API changes in order to support the multi-value indexing and querying support that comes with RediSearch 2.6. RediSearch 2.6 comes with multi-value indexing and querying of attributes for any attribute type (Text, Tag, Numeric, Geo and Vector) defined by a JSONPath leading to an array or to multiple scalar values.","content":"Requirements RedisJSON v2.4.3 requires:\nMinimum Redis compatibility version (database): 6.0.16 Minimum Redis Enterprise Software version (cluster): 6.2.18 v2.4.3 (December 2022) This is a maintenance release for RedisJSON 2.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#890 JSONPath ignores any filter condition beyond the second (MOD-4602) Improvements:\n#892 Allow JSON.ARRINDEX with nonscalar values v2.4 GA (v2.4.2) (November 2022) This is the General Availability release of RedisJSON 2.4.\nHighlights RedisJSON 2.4 contains several low-level API changes in order to support the multi-value indexing and querying support that comes with RediSearch 2.6. RediSearch 2.6 comes with multi-value indexing and querying of attributes for any attribute type (Text, Tag, Numeric, Geo and Vector) defined by a JSONPath leading to an array or to multiple scalar values.\nWhat\u0026rsquo;s new in 2.4 Features:\n#848 Add JSONPath filter the regexp match operator (MOD-4432) #861 Support legacy JSONPath with the dollar sign $ (MOD-4436) Performance enhancements:\n#699 A new JSONPath library which enhances the performance of parsing any JSONPath expression in RedisJSON. Changing behaviour:\n#699 Floating point numbers which become round numbers due to some operation, such as JSON.NUMINCRBY, will now return as a floating point with a trailing .0, e.g., instead of just 42, now 42.0 will be returned. Bugs fixes (since 2.4-RC1/ v2.4.0):\n#850 Allow repetition of filter relation instead of optional (MOD-4431) ","categories":["Modules"]},{"uri":"/ri/release-notes/v1.2.0/","uriRel":"/ri/release-notes/v1.2.0/","title":"RedisInsight v1.2, January 2020","tags":[],"keywords":[],"description":"TLS Client side authentication support and stability improvements","content":"RedisInsight v1.2.2 release notes Update urgency: Medium\nThis is a maintenance release for version 1.2.\nDetails Bug Fixes: Core: This releases fixes the possible false positive malware issues flagged by certain antivirus vendors introduced by pyinstaller which was reported on reddit. RedisInsight v1.2.1 release notes Update urgency: Medium\nThis is a maintenance release for version 1.2.\nDetails Enhancements: Core: Upgrade notifications: When you open RedisInsight, a notification is shown if a new version is available. RediSearch: Support for RediSearch 1.6. Minor Bug Fixes: RedisTimeSeries: Time was interpreted as seconds instead of milliseconds as (Issue 332) RedisInsight v1.2.0 release notes Headlines This release improves overall stability and provides fixes for issues found after the previous release. Added support for Client side TLS authentication. Resolved bug which caused blank pages at startup. Details New features: Core: Added support for Redis databases that require TLS client authentication (as in Redis Enterprise) RedisTimeseries: Initial auto-updating functionality when the query\u0026rsquo;s end timestamp is + Minor Enhancements: Core: Check whether the port is available before starting. Made localhost the default host instead of 0.0.0.0. Improved logging during startup. RedisGraph: Fixed height of query cards. RediSearch: Add support for zero-length and whitespace-only index names. Bug Fixes: Core: Moved server to another thread instead of a separate process. In certain situations, the server process was being orphaned after the main process died. This resulted in a several issues, of which the \u0026ldquo;blank page issue\u0026rdquo; was the most common. Now that the server process is in a thread instead of a process, the server is not left running when the process exits. RediSearch: Fixed several bugs in the display of summarized results in the table view. Browser: Better handling of unsupported values - link to other tools that support it or show better error message. Fixed UI issues when the screen size is varied to provide better responsiveness. ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-4-0-june-2015/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-4-0-june-2015/","title":"RLEC 4.0.0-49 Release Notes (June 18, 2015)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the upgrade instructions before running through the upgrade process.\nIn addition, when running the install.sh script to upgrade the software, you might be prompted to approve changes to a configuration file named ccs-redis.conf. It is crucial that you choose Yes when asked whether to update this file.\nNew features Support for Red Hat Enterprise Linux (RHEL) and CentOS 6.5 and 7 operating systems. Support for additional AWS AMIs for Ubuntu and Amazon Linux, on multiple AWS regions. Support for additional browsers and operating systems for the management UI. Replica Of feature which enables creating a Redis database that keeps synchronizing data from another Redis database. Rack-zone awareness feature which enables mapping nodes to racks/zones to ensure a more sophisticated high-availability mechanism. Database-related alerts and email alerts. Auto-configuration of synchronization of cluster server clocks with NTP as part of installation script. Database Export functionality. Email alerts on database Export and Import. Changes Database Backup Now functionality replaced with Export functionality. Database performance improvements to increase throughput and reduce latency. Improvement to AOF rewrite mechanism to deal with extreme-write scenarios and limited disk space configurations. Enhancements to rladmin CLI to support additional commands. Fixed issues Cluster stability improvements related to removing nodes and taking nodes offline. rladmin CLI bug fixes. Known issues Issue: RLEC-6819 - Uninstall on Red Hat Enterprise Linux does not stop all services and if you try to install the software again on the same machine the new installation might use prior installation data.\nWorkaround: Before installing the software again restart the machine or verify that all services are down.\nIssue: In the replica of process, if the source database is resharded while the replica of process is active, the synchronization process will fail.\nWorkaround: You need to manually stop and restart the synchronization process after resharding of the source database is done.\nIssue: In the replica of process, high database traffic might cause the replica of process to restart frequently as result of the \u0026ldquo;replica buffer\u0026rdquo; being exceeded. In this case you see the status of the replica of process as \u0026ldquo;Syncing\u0026rdquo; frequently.\nWorkaround: You need to manually reconfigure the \u0026ldquo;replica buffer\u0026rdquo; through rladmin and set the buffer size to a new size. In order to find the appropriate buffer size please contact support at: support@redislabs.com.\nIssue: In a cluster that is configured to support rack-zone awareness, if the user forces migration of a master or replica shard, through rladmin, to a node on the same rack-zone as its corresponding master or replica shard, and later runs the rebalance process, the rebalance process will not migrate the shards to ensure rack-zone awareness compliance.\nWorkaround: In the scenario described above, you need to manually migrate the shard, through rladmin, to a node on a valid rack-zone in order to ensure rack-zone awareness compliance.\nIssue: In case you deploy a cluster and use the DNS option for the cluster name (see details in How to set the Cluster Name (FQDN), do not configure the DNS entries for the cluster nodes, and try to configure a database that is a replica of another database within the cluster, then the UI allows you to configure the source database but the replica of process fails in runtime.\nWorkaround: The configuration indicated in this issue is not a valid cluster configuration. If you choose to use the DNS option for the cluster name then you must configure DNS entries for the nodes, otherwise the cluster does not operate correctly. You have to either update the DNS accordingly, or recreate the cluster and use the mDNS option for the cluster name as described in How to set the Cluster Name (FQDN).\nIssue: When taking a node offline or removing a node, if the node being taken offline or removed is currently serving as the web server for the web browser being used to view the management UI, then the management UI appears down while the node is down.\nWorkaround: If you are using the cluster name in order to connect to the management UI in the browser, and the cluster name is registered in your external DNS or you are using the mDNS option, then the DNS entries will be updated to point to another node in the cluster after a few seconds and the UI will open properly. If you are not using the cluster name but rather the node IP in order to connect to the management UI in the web browser, you have to use the IP of another node in the cluster to access the management UI.\n","categories":["RS"]},{"uri":"/modules/redisearch/release-notes/redisearch-1.2-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-1.2-release-notes/","title":"RediSearch 1.2 release notes","tags":[],"keywords":[],"description":"Aggregation filters. Query attributes. Fuzzy matching. Conditional updates. Backslash escaping. Synonyms support.","content":"Requirements RediSearch v1.2.0 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 RediSearch 1.2.0 (June 2018) This version is the last version that I (@dvirsky) will be releasing, and it includes a lot of new cool features, and actually NO bug-fixes over 1.1.0! So long and thanks for all the fish!\nNew features Aggregation filters As an addition to the aggregation API in FT.AGGREGATE, it is possible to do post-index filtering of the pipeline, using the FILTER keyword. e.g.:\nFT.AGGREGATE idx \u0026#34;*\u0026#34; GROUPBY 1 @foo REDUCE count 0 AS num FILTER \u0026#34;@num \u0026lt; 100\u0026#34; See http://redisearch.io/Aggregations/ for more details.\nQuery attributes It is now possible to apply specific query modifying attributes to specific clauses of the query (see #212).\nThe syntax is (foo bar) =\u0026gt; { $attribute: value; $attribute:value; ...}, e.g:\n(foo bar) =\u0026gt; { $weight: 2.0; $slop: 1 } ~(bar baz) =\u0026gt; { $weight: 0.5; } The supported attributes are:\n$weight: determines the weight of the sub-query or token in the overall ranking on the result. $slop: determines the maximum allowed \u0026ldquo;slop\u0026rdquo; (space between terms) in the query clause. $inorder: whether or not the terms in a query clause must appear in the same order as in the query. Fuzzy matching Wrapping a search term with % will cause the index to expand the query to terms that are within an Edit Distance of 1 from the original term. For example, querying for %redis% will expand it to query for redis, jedis, credis, predis, etc (provided the terms appear in documents in the index).\nNotice that each query term needs to be wrapped independently, and that we limit the maximum amount of expansions to 200 per term, as this hurts performance significantly.\nConditional updates It is now possible to update documents (FT.ADD ... REPLACE [PARTIAL]) only if a certain condition is met regarding the document\u0026rsquo;s state before the updates. So for example, if our document has a timestamp field, and we would like to update its title only if the timestamp is below a certain value, we can do the following:\nFT.ADD myIndex myDoc 1.0 REPLACE PARTIAL IF \u0026#34;@timestamp \u0026lt; 12313134523\u0026#34; FIELDS title \u0026#34;new title\u0026#34; Backslash escaping Following several user requests, it is now possible to escape separator characters in documents (it is already done in the query itself), and avoid tokenization when needed.\nFor example, indexing the text hello\\-world hello world will create the tokenization [\u0026quot;hello-world\u0026quot;, \u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;]. Notice that in most languages and in redis-cli, you will need to escape the backslash itself, so instead of hello\\-world you will need to send the string hello\\\\-world.\nThe same goes to the query string: If the document contains the token hello-world, it can be found by running in redis-cli: `FT.SEARCH idx \u0026ldquo;hello\\-world\u0026rdquo;, which will cause the query not to be separated.\nSynonyms support It is now possible to provide the index with synonym groups (.e.g boy, child, kid), and have it automatically index synonymous terms so that searching for one will return documents containing another.\nSee http://redisearch.io/Synonyms/ for more details and examples.\n","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/redistimeseries-1.4-release-notes/","uriRel":"/modules/redistimeseries/release-notes/redistimeseries-1.4-release-notes/","title":"RedisTimeSeries 1.4 release notes","tags":[],"keywords":[],"description":"Added ability to backfill time series.","content":"Requirements RedisTimeSeries v1.4.14 requires:\nMinimum Redis compatibility version (database): 5.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.12 v1.4.14 (February 2022) This is a maintenance release for RedisTimeSeries 1.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nBug fixes:\n#891, #892 Fixed memory leak in parseCreateArgs when parsing error occurs (MOD-1958) v1.4.13 (November 2021) This is a maintenance release for RedisTimeSeries 1.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nBug fixes:\n#881 Replicate only successful insertion of TS.MADD v1.4.11 (November 2021) This is a maintenance release for RedisTimeSeries 1.4.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nBug fixes:\n#862 Index dictionary should be freed on removing the last element v1.4.10 (July 2021) This is a maintenance release for RedisTimeSeries 1.4.\nUpdate urgency: HIGH: There is a critical bug that may affect a subset of users. Upgrade!\nBug fixes:\n#760 Avoid closing the same key twice, causing server crash on RENAME of other keys v1.4.9 (May 2021) This is a maintenance release for version 1.4.\nUpdate urgency: Medium\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Bug fixes:\n##712 Missing keytype check on TS.INCRBY/DECRBY causes shards to crash ##719 Support for renaming time series keys v1.4.8 (March 2021) This is a maintenance release for version 1.4.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Bug fixes:\n#612 Crash on MGET/MRANGE #606 Memory leak when key loaded from RDB #624 Uninitialised memory access on log v1.4.7 (December 2020) This is a maintenance release for version 1.4.\nHeadlines:\nThis release improves overall stability, and provides fixes for issues found after the previous release. Bug fixes:\n#581 Misaligned allocators usage might crash Redis. #588 ON_DUPLICATE min/max rules not working for negative value. v1.4.6 (November 2020) This is a maintenance release for version 1.4.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nMinor enhancements:\n#565 duplicate policy: add SUM option: If a previous sample exists, add the new sample to it so that the updated value is equal to (previous + new). If no previous sample exists, set the updated value equal to the new value. (PR #565 #559 Compressed chunk will be be the default global option #559 Added chunkType to TS.INFO Bug fixes:\n#528 Out of order insert might crash Redis if there\u0026rsquo;s an update to an empty downsampled key #561 TS.MRANGE command might crash if there\u0026rsquo;s an expired key that was deleted in the result set v1.4 GA (September 2020) This is the General Availability release for RedisTimeSeries 1.4.\nHighlights:\nAbility to backfill time series! You can now add samples to a time series where the time of the sample is older than the newest sample in the series. This enables:\nAdding out of order of samples to time series. Batch loading of historical samples into an existing series. Updating existing samples (for example for compliance reasons). This has been the most requested feature for RedisTimeSeries. We look forward to your feedback so we can move to a general availability release soon.\nDetails:\nAdded functionality: #254 TS.REVRANGE and TS.MREVRANGE [commands] allow for querying in descending order of Timestamps. (https://oss.redislabs.com/redistimeseries/1.4/commands/#tsmrangetsmrevrange) #503 - RDB saves the whole chunk instead of individual samples giving a speed and space improvement when saving or loading an RDB file #502 - The ability to set, at creation time, the data section size of each chunk using flag CHUNK_SIZE. TS.INFO uses chunkSize instead of maxSamplesPerChunk. #437 Allow backfilling of samples and updating of existing samples Works with compressed and uncompressed series. This comes with a performance hit when a sample is written out-of-order. We will publish numbers once we are generally available, but are still considering optimisations. #521 DUPLICATE_POLICY allows to configure on module, series and sample level how to handle duplicate samples. A duplicate sample is a sample for which the series holds already a sample on the same timestamp. Note that the default behaviour is equal to v1.2: BLOCK Notes: The version inside Redis will be 10405 or 1.4.5 in semantic versioning. Since the version of a module in Redis is numeric, we could not add an GA flag.\n","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-2.0-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-2.0-release-notes/","title":"RedisGraph 2.0 release notes","tags":[],"keywords":[],"description":"Enabled graph-aided search and graph visualisation. Cypher coverage. Performance improvements.","content":"Requirements RedisGraph v2.0.21 requires:\nMinimum Redis compatibility version (database): 5.0.7 Minimum Redis Enterprise Software version (cluster): 5.4.11 v2.0.21 (October 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor enhancements:\n#1326, #1330 Enable comments in Cypher queries. Bug fixes:\n#1338 Update parser to support negative values.\n#1319 Replace assertion with runtime error - execution plan building process is unable locate an operation suitable for a filter.\n#1184 EXPLAIN and PROFILE are invalid clauses. They have counterpart commands GRAPH.EXPLAIN and GRAPH.PROFILE\n#1212 Emit error on the creation of undirected edges.\nv2.0.20 (September 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor updates:\n#1315 RediSearch 1.8.3 #1276 Graph info is added to Redis crash report. #1265 Debug assertion functionality. v2.0.19 (August 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor updates: #1229 Upgrade RediSearch dependency to 1.8.2 Performance enhancements: #1218 Enable parameters support for ID-based entity retrieval #1242 Improve update logic, update RediSearch indices only once per update #1226 Let RediSearch perform document deletion when replacing a document Bug Fixes: #1223 Resolve race condition in accessing/updating attribute maps. #1196 Enable RediSearch Garbage Collection v2.0.15 (25 June 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBug Fixes: #1159 IN operator didn\u0026rsquo;t result in an index array lookup with parameterised array (IN $param). v2.0.14 (22 June 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBug Fixes: #1157 Index search not using query parameters. v2.0.13 (15 June 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nEnhancements: #1118 Added module configuration parameters for the number of threads in OpenMP and concurrent queries. #1121 RediSearch v1.8.1 v2.0.12 (May 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nEnhancements: #1103 RediSearch 1.8. This upgrade will reduce the resources required for Garbage Collection of indices inside RedisGraph. v2.0.11 (April 2020) This is a maintenance release for version 2.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor Enhancements: #1056 Added DISTINCT support for paths and arrays. Minor Bugfixes: #1056 Streamlined logic for uniquing entities. #1056 Enabled comparison routine for paths. v2.0.10 (29 March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor Enhancements: #1035 - RediSearch version 1.6.11. Bugfixes: #1017, #1019 - Algebraic expressions correctness. #1020 - Support parameterised SKIP and LIMIT. v2.0.9 (19 March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1028 Ensure proper placement of Index Scans when partially replacing Filter ops. v2.0.8 (18 March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #1023 Fix regression in checking argument counts to GRAPH endpoints. v2.0.6 (15 March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\n#897 GRAPH.SLOWLOG #1004 Re-enable GRAPH.PROFILE #917, #991, #940, #984 Memory leak fixes #925 Bug fix within RediSearch #1001 Bug fix label scan invalid range v2.0.5 (23 February 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nEnhancement: #955 Switch OR,AND boolean semiring to ANY,PAIR. v2.0.4 (23 February 2020) Details:\nEnhancement: Reduce GraphBLAS size v2.0.2 (23 February 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nImproved error reporting: #925 RediSearch query error reporting #919 Added non existing entity runtime error Enhancements: #942 Update GraphBLAS version (3.2.0) #922 Filter tree compaction #906 Optimize cartesian product #898 Granular matrix locking Bug fixes: #917 #940 Resolve memory leaks #938 Label matrix should be fetch right before eval v2.0 GA (2.0.1 - January 2020) This is the General Availability (GA) release of RedisGraph 2.0 (2.0.1)!\nHeadlines:\nFull Text Search (FTS) enabling Graph-aided Search. Full graph response enabling visualisation. Substantial amount of Cypher coverage. Performance improvements of up to 4x compared to RedisGraph 1.2. (We will blog about this release soon including performance improvements results and the link here)\nFull details:\nMajor features #339 Full Graph Response. RedisGraph now allows to return Graph entities such as Nodes and Relationships. This feature also enables graph visualisation. #558 Indexing functionality replaced by RediSearch. This results in support for compound indices full text search graph-aided search #691 RediSearch index is used with IN operator on indexed properties. #488 Replace flex/lemon parser with libcypher-parser. #574 Introduction of the array data type. This introduces significant support of Cypher as well as properties can be arrays on both nodes and relationships. Cypher #714 MERGE can be combined with any other clause. MERGE will take into account bounded entities. #786 Support for passing all supported graph data types (Node, Relationship, Array,\u0026hellip;) as arguments to procedure calls, rather than requiring all arguments to be Strings. #708 Named path support. #717 count(*) #730 UNION #757 coalesce #658 Support for COUNT DISTINCT #574 with the array data support comes IN, collect, head,range, reverse, size, tail #624 randomUUID() #632 IS NULL and IS NOT NULL. #594 Support dynamic inline properties for CREATE and MERGE patterns. #583 NOT Operator. #569 timestamp() function. #586 CASE WHEN (simple form). #582 contains function. #596 indegree and outdegree functions for nodes. #587 improved boolean logic. #539 Reusable entities in pattern matching. #668 PageRank support. #713 Parameterised queries support. Most client drivers are updated. This enables future performance enhancements for query caching. #662 Support for AOF. Enhancements #752 Use GraphBLAS 3.1.1. #674 GRAPH.QUERY will not fail even if graph doesn\u0026rsquo;t exist. #613 Runtime arithmetic error handling. Performance Improvements #773 Bulk deletion of relationships. Deletion of relationships within the same query will be efficiently handled by batch operations in GraphBLAS. #783 Better replication support. Only write queries that mutate the graph or create indices will be replicated to AOF and slaves. #783 Better commit flow. The Redis Global Lock and RedisGraphs\u0026rsquo;s R/W lock are released once writes are done. Fixed duplicate replications in queries with multiple write segments. #640 GraphBlas to support OpenMP. #532 #535 Counting edges of given type by reducing matrices. #534 #571 Query parsing and GRAPH.EXPLAIN in dedicated thread. #550 #555 Optimize cartesian product by reducing to join. #641 Cartesian products with multiple incoming streams \u0026gt;2 can now be optimized with \u0026ldquo;HashJoin\u0026rdquo;. Bugfixes #783 Master-replica replication - slaves were dropping index mutations caused by procedure calls. #785 Solved several major memory leaks. #795 The underlying graph object\u0026rsquo;s attributes were not updated on RENAME. #720 Validation of function calls in WITH and CREATE clauses e.g. CREATE (a {v: fake()}). #732, #736, #747 Detect cycles in graph and generate algebraic expression. #734 Fix access of uninitialised GrB_Info. #735 Index scans with IN filters did not compare with strings properly, #758 Validate that during UNION all return clauses are annotated. #412 Better handling for GRAPH.DELETE. #537 Wrong row index within expand-into, enabled few TCK tests. #591 Emit validation error when an alias refers to both nodes and edges. #606 Fix Record ID of edges held in ExpandInto op. #607 All ops NULL-set variables in their free routines (memory). #893 Preserve the children array order when replacing operations. Note: The version inside Redis will be 20001 or 2.0.1 in semantic versioning. ","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/redisbloom-2.0-release-notes/","uriRel":"/modules/redisbloom/release-notes/redisbloom-2.0-release-notes/","title":"RedisBloom 2.0 release notes","tags":[],"keywords":[],"description":"Added more probabilistic data structures, including top-K and count-min sketch.","content":"Requirements RedisBloom v2.0.3 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v2.0.3 (July 2019) Performance improvements: #95 Top-K - Reduce checks on heap, now only checks if item count is larger than minimum in heap. #95 Top-K - The power of decay was calculated every time. Changed to use a lookup table. Major bug fix: #88 Replication available for newly added Top-K and Count-min sketch Minor bug fixes: #89 Module update broke rdb files #98 Compilation for macOS v2.0.0 (June 2019) We are proud to announce that we doubled the number of probabilistic data structures that are generally available in RedisBloom. Full documentation is available on redisbloom.io\n#70 Top-K\nCommands Algorithm #65 Count-min Sketch\nCommands Algorithm ","categories":["Modules"]},{"uri":"/modules/redisjson/release-notes/redisjson-2.2-release-notes/","uriRel":"/modules/redisjson/release-notes/redisjson-2.2-release-notes/","title":"RedisJSON 2.2 release notes","tags":[],"keywords":[],"description":"Preview of Active-Active support for JSON.","content":"Requirements RedisJSON v2.2.0 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.2.18 v2.2.0 (July 2022) A preview of RedisJSON 2.2 is available for Free and Fixed subscription plans in Redis Cloud.\nHeadlines This release adds support for the JSON data structure as a CRDT (Conflict-free Replicated Data Type) when used with Redis Enterprise Active-Active databases.\nActive-Active JSON requires Redis Enterprise Software v6.2.12. Contact your account manager or support to access the preview of RedisJSON 2.2.\nDetails Enhancements:\n#758 Add support for COPY ","categories":["Modules"]},{"uri":"/ri/release-notes/v1.1.0/","uriRel":"/ri/release-notes/v1.1.0/","title":"RedisInsight v1.1, December 2019","tags":[],"keywords":[],"description":"Stability improvements and other fixes","content":"RedisInsight v1.1.0 release notes (December 2019) Headlines This release improves overall stability and provides fixes for issues found after the previous release. Details Minor Enhancements: Core: Enable mouse wheel support inside the querycard. Browser: Enable enter key press for adding keys in browser RediSearch: Disable HIGHLIGHT markup in JSON view. Browser: Improve error message when database is unreachable Add a reload/refresh button to refresh the value of a key Enable enter key press for adding keys in browser Improve error message for unsupported value types Bug Fixes: RedisGraph: Fix initial node placement in the view. Fix initial zoom with respect to the number of nodes in the result. Other minor fixes. ","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-0-99-february-2015/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-0-99-february-2015/","title":"RLEC 0.99.5-24 Release Notes (February 15, 2015)","tags":[],"keywords":[],"description":"","content":"If you are upgrading from a previous version, make sure to review the upgrade instructions before running through the upgrade process.\nIf you are upgrading from version 0.99.5-11:\nYou must restart the services after the upgrade by running the following command with user root (sudo su). From the operating system\u0026rsquo;s CLI, run the following command: cnm_ctl restart After the upgrade, rladmin status command will report that the databases are from an old version. It is recommended that you upgrade the databases as soon as possible, as described in the upgrade instructions. New features None.\nChanges Enhancements to memtier_benchmark tool that is included in the installation package. You can find more details in the GitHub project. Fixed issues Improvements and fixes related to node failover, remove node and take node offline functionality. Known issues Issue: When taking a node offline or removing a node, if the node being taken offline or removed is currently serving as the web server for the web browser being used to view the management UI, the management UI appears down while the node is down. Workaround: If you are using the cluster name in order to connect to the management UI in the browser, and the cluster name is registered in your external DNS or you are using the mDNS option, then the DNS entries will be updated to point to another node in the cluster after a few seconds and the UI will open properly. If you are not using the cluster name but rather the node IP in order to connect to the management UI in the web browser, you have to use the IP of another node in the cluster to access the management UI. ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/app-failover-active-active/","uriRel":"/rs/databases/active-active/develop/app-failover-active-active/","title":"Application failover with Active-Active databases","tags":[],"keywords":[],"description":"How to failover your application to connect to a remote replica.","content":"Active-Active Redis deployments don\u0026rsquo;t have a built-in failover or failback mechanism for application connections. An application deployed with an Active-Active database connects to a replica of the database that is geographically nearby. If that replica is not available, the application can failover to a remote replica, and failback again if necessary. In this article we explain how this process works.\nActive-Active connection failover can improve data availability, but can negatively impact data consistency. Active-Active replication, like Redis replication, is asynchronous. An application that fails over to another replica can miss write operations. If the failed replica saved the write operations in persistent storage, then the write operations are processed when the failed replica recovers.\nDetecting Failure Your application can detect two types of failure:\nLocal failures - The local replica is down or otherwise unavailable Replication failures - The local replica is available but fails to replicate to or from remote replicas Local Failures Local failure is detected when the application is unable to connect to the database endpoint for any reason. Reasons for a local failure can include: multiple node failures, configuration errors, connection refused, connection timed out, unexpected protocol level errors.\nReplication Failures Replication failures are more difficult to detect reliably without causing false positives. Replication failures can include: network split, replication configuration issues, remote replica failures.\nThe most reliable method for health-checking replication is by using the Redis publish/subscribe (pub/sub) mechanism.\nNote: Note that this document does not suggest that Redis pub/sub is reliable in the common sense. Messages can get lost in certain conditions, but that is acceptable in this case because typically the application determines that replication is down only after not being able to deliver a number of messages over a period of time. When you use the pub/sub data type to detect failures, the application:\nConnects to all replicas and subscribes to a dedicated channel for each replica. Connects to all replicas and periodically publishes a uniquely identifiable message. Monitors received messages and ensures that it is able to receive its own messages within a predetermined window of time. You can also use known dataset changes to monitor the reliability of the replication stream, but pub/sub is preferred method because:\nIt does not involve dataset changes. It does not make any assumptions about the dataset. Pub/sub messages are delivered as replicated effects and are a more reliable indicator of a live replication link. In certain cases, dataset keys may appear to be modified even if the replication link fails. This happens because keys may receive updates through full-state replication (re-sync) or through online replication of effects. Impact of sharding on failure detection If your sharding configuration is symmetric, make sure to use at least one key (PUB/SUB channels or real dataset key) per shard. Shards are replicated individually and are vulnerable to failure. Symmetric sharding configurations have the same number of shards and hash slots for all replicas. We do not recommend an asymmetric sharding configuration, which requires at least one key per hash slot that intersects with a pair of shards.\nTo make sure that there is at least one key per shard, the application should:\nUse the Cluster API to retrieve the database sharding configuration. Compute a number of key names, such that there is one key per shard. Use those key names as channel names for the pub/sub mechanism. Failing over When the application needs to failover to another replica, it should simply re-establish its connections with the endpoint on the remote replica. Because Active/Active and Redis replication are asynchronous, the remote endpoint may not have all of the locally performed and acknowledged writes.\nIt\u0026rsquo;s best if your application doesn\u0026rsquo;t read its own recent writes. Those writes can be either:\nLost forever, if the local replica has an event such as a double failure or loss of persistent files. Temporarily unavailable, but will be available at a later time if the local replica\u0026rsquo;s failure is temporary. Failback decision Your application can use the same checks described above to continue monitoring the state of the failed replica after failover.\nTo monitor the state of a replica during the failback process, you must make sure the replica is available, re-synced with the remote replicas, and not in stale mode. The PUB/SUB mechanism is an effective way to monitor this.\nDataset-based mechanisms are potentially less reliable for several reasons:\nIn order to determine that a local replica is not stale, it is not enough to simply read keys from it. You must also attempt to write to it. As stated above, remote writes for some keys appear in the local replica before the replication link is back up and while the replica is still in stale mode. A replica that was never written to never becomes stale, so on startup it is immediately ready but serves stale data for a longer period of time. Replica Configuration Changes All failover and failback operations should be done strictly on the application side, and should not involve changes to the Active-Active configuration. The only valid case for re-configuring the Active-Active deployment and removing a replica is when memory consumption becomes too high because garbage collection cannot be performed. Once a replica is removed, it can only be re-joined as a new replica and it loses any writes that were not converged.\n","categories":["RS"]},{"uri":"/rs/security/access-control/ldap/cluster-based-ldap-authentication/","uriRel":"/rs/security/access-control/ldap/cluster-based-ldap-authentication/","title":"Cluster-based LDAP authentication","tags":[],"keywords":[],"description":"(Deprecatd) Describes cluder-based LDAP integration, an earlier mechanism to enable LDAP support for Redis Software.  See role-based LDAP for current approach.","content":" Warning - As of v6.2.12, the features described in this article are obsolete and have been removed from Redis Enterprise Software.\nVersion 6.0.20 introduced a role-based LDAP integration that replaced the cluster-based integration described here.\nAt the time, the earlier integration remained in order to provide a transition period for migration. That period has now passed and the cluster-based integration has been removed.\nThis article has been archived and will no longer be maintained.\nRedis Enterprise Software supports Lightweight Directory Access Protocol (LDAP).\nNote: Known Limitations:\nLDAP access for database access is available only when using the role-based LDAP authentication. This process does not apply when running Redis Enterprise for Kubernetes. Support for this integration was removed from Redis Enterprise Software v6.2.12. Enable LDAP To enable LDAP:\nImport the saslauthd configuration. Restart saslauthd service. Configure LDAP users. Configure LDAP To provide the LDAP configuration information:\nEdit the configuration file located at /etc/opt/redislabs/saslauthd.conf or the installation directory of your choice during initial configuration.\nProvide the following information associated with each variable:\nldap_servers: the ldap servers that you authenticate against and the port to use Provide the following information associated with each variable ldap_servers: the ldap servers that you authenticate against and the port to use Port 389 is standardly used for unencrypted LDAP connections Port 636 is standardly used for encrypted LDAP connections and is strongly recommended. Ldap_tls_cacert_file (optional): The path to your CA Certificates. This is required for encrypted LDAP connections only. ldap_filter: the filter used to search for users. ldap_bind_dn: The distinguished name for the user that will be used to authenticate to the LDAP server. ldap_password: The password used for the user specified in ldap_bind_dn Import the saslauthd configuration into Redis Enterprise using the below command. This will distribute the configuration to all nodes in the cluster.\nrladmin cluster config saslauthd_ldap_conf \u0026lt;path_to_saslauthd.conf\u0026gt; Note: If this is a new server installation, for this command to work, a cluster must be set up already. Restart saslauthd:\nsudo supervisorctl restart saslauthd An example configuration for your reference may be found below:\nldap_servers: ldaps://ldap1.mydomain.com:636 ldap://ldap2.mydomain.com:636 ldap_tls_cacert_file: /path/to/your/CARootCert.crt ldap_search_base: ou=coolUsers,dc=company,dc=com ldap_search_base: ou=coolUsers,dc=company,dc=com ldap_filter: (sAMAccountName=%u) ldap_bind_dn: cn=admin,dc=company,dc=com ldap_password: secretSquirrel Set up LDAP users in Redis Enterprise To set up an LDAP user, simply select an external account type when configuring the user following the procedure to configure users.\n","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/","uriRel":"/rs/databases/active-active/develop/","title":"Active-Active Redis applications","tags":[],"keywords":[],"description":"General information to keep in mind while developing applications for an Active-Active database.","content":"Developing globally distributed applications can be challenging, as developers have to think about race conditions and complex combinations of events under geo-failovers and cross-region write conflicts. In Redis Enterprise Software (RS), Active-Active databases simplify developing such applications by directly using built-in smarts for handling conflicting writes based on the data type in use. Instead of depending on just simplistic \u0026ldquo;last-writer-wins\u0026rdquo; type conflict resolution, geo-distributed Active-Active databases (formerly known as CRDBs) combines techniques defined in CRDT (conflict-free replicated data types) research with Redis types to provide smart and automatic conflict resolution based on the data types intent.\nAn Active-Active database is a globally distributed database that spans multiple Redis Enterprise Software clusters. Each Active-Active database can have many Active-Active database instances that come with added smarts for handling globally distributed writes using the proven CRDT approach. CRDT research describes a set of techniques for creating systems that can handle conflicting writes. CRDBs are powered by Multi-Master Replication (MMR) provides a straightforward and effective way to replicate your data between regions and simplify development of complex applications that can maintain correctness under geo-failovers and concurrent cross-region writes to the same data.\nActive-Active databases replicate data between multiple Redis Enterprise Software clusters. Common uses for Active-Active databases include disaster recovery, geographically redundant applications, and keeping data closer to your user\u0026rsquo;s locations. MMR is always multi-directional amongst the clusters configured in the Active-Active database. For unidirectional replication, please see the Replica Of capabilities in Redis Enterprise Software.\nExample of synchronization In the example below, database writes are concurrent at the point in times t1 and t2 and happen before a sync can communicate the changes. However, writes at times t4 and t6 are not concurrent as a sync happened in between.\nTime CRDB Instance1 CRDB Instance2 t1 SET key1 “a” t2 SET key1 “b” t3 — Sync — — Sync — t4 SET key1 “c” t5 — Sync — — Sync — t6 SET key1 “d” Learn more about synchronization for each supported data type and how to develop with them on Redis Enterprise Software.\n","categories":["RS"]},{"uri":"/kubernetes/security/internode-encryption/","uriRel":"/kubernetes/security/internode-encryption/","title":"Enable internode encryption","tags":[],"keywords":[],"description":"Enable encryption for communication between REC nodes in your K8s cluster.","content":"Internode encryption provides added security by encrypting communication between nodes in your Redis Enterprise cluster (REC).\nEnable internode encryption in the spec section of your REC custom resource file.\nspec: dataInternodeEncryption: true This change will apply to all databases created in the REC. You can override the cluster-wide setting for individual databases.\nEdit your Redis Enterprise database (REDB) custom resource file to disable internode encryption for only that database.\nspec: dataInternodeEncryption: false To learn more about internode encryption, see Internode encryption for Redis Enterprise Software.\n","categories":["Platforms"]},{"uri":"/modules/redisai/release-notes/","uriRel":"/modules/redisai/release-notes/","title":"RedisAI release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v1.2 (November 2021) Strings tensor support. Backend updates - TF 2.6, PyTorch 1.9, ONNXRuntime 1.9. Redis now manages ONNXRuntime memory. 6.0.0 6.2.2 v1.0 (May 2020) Auto-batching support. Added AI.DAGRUN and AI.DAGRUN_RO commands. AI.MODELSET allows you to provide a model in chunks. Standardized GET methods (TENSORGET,MODELGET,SCRIPTGET) replies (breaking change for clients). Cache model blobs. 5.0.7 5.4.11 ","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/","uriRel":"/modules/redisbloom/release-notes/","title":"RedisBloom release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v2.4 (November 2022) Added t-digest - a probabilistic data structure for estimating quantiles based on a data stream or a large dataset of floating-point values. 6.0.16 6.2.8 v2.2 (December 2019) BF.INFO returns bloom filter details. CF.INFO returns cuckoo filter details. Scalable bloom and cuckoo filters. Configurable bucket size for cuckoo filters. CMS.INCRBY returns count. 4.0.0 5.0.0 v2.0 (June 2019) Added more probabilistic data structures, including top-K and count-min sketch. 4.0.0 5.0.0 v1.1 (February 2019) Updated version. 4.0.0 5.0.0 v1.0 (September 2017) First GA release of RedisBloom. 4.0.0 5.0.0 ","categories":["Modules"]},{"uri":"/modules/redisearch/release-notes/","uriRel":"/modules/redisearch/release-notes/","title":"RediSearch release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v2.6 (November 2022) Search using wildcard queries for TEXT and TAG fields, multi-value indexing and querying of attributes for any attribute type, and indexing double-precision floating-point vectors and range queries from a given vector. 6.0.16 6.2.8 v2.4 (March 2022) Vector Similarity Search (VSS). New query syntax Dialect version 2. Choose between Dialect 1 and Dialect 2 for query parser behavior. Hybrid queries. 6.0.0 6.0.0 v2.2 (November 2021) Search and index JSON documents. Profiling queries. Field aliasing. 6.0.0 6.0.0 v2.0 (September 2020) Automatically indexes data based on a key pattern. Scale a single index over multiple Redis shards. Improved query performance. 6.0.0 6.0.0 v1.6 (January 2020) Improved performance of full-text search and aggregation queries. Support for aliasing of indices. Added a C API to embed RediSearch in other modules. Forked process garbage collection. 4.0.0 5.0.0 v1.4 (August 2018) Conditional updates. Schema modification. Query spelling correction. Phonetic matching. More fuzziness in search. Retrieve and change runtime configuration. Unlimited autocomplete results. 4.0.0 5.0.0 v1.2 (June 2018) Aggregation filters. Query attributes. Fuzzy matching. Conditional updates. Backslash escaping. Synonyms support. 4.0.0 5.0.0 v1.1 (April 2018) Aggregations engine. 4.0.0 5.0.0 v1.0 (April 2018) Hamming distance scoring. Wildcard queries. Optional deletion of documents in FT.DEL. Optionally keep document hashes in FT.DROP. Delete geo-sets when dropping an index. Tag prefix completion support. 4.0.0 5.0.0 ","categories":["Modules"]},{"uri":"/modules/redisgears/release-notes/","uriRel":"/modules/redisgears/release-notes/","title":"RedisGears release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v1.2 (February 2022) Plugins and JVM support. Python async await. Override commands API. Register functions on key miss events. Tracks new statistics. Python profiler support. Extended RedisAI integration. 6.0.0 6.0.12 v1.0 (May 2020) First GA release of RedisGears. Built-in C-API and Python interpreter. Run a serverless engine in memory next to your Redis data. 6.0.0 6.0.0 ","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/","uriRel":"/modules/redisgraph/release-notes/","title":"RedisGraph release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v2.8 (February 2022) Introduces multi-labeled nodes, indexes over relationship properties, and additional expressivity (Cypher construct, functions, and operators). Major performance enhancements. Many bug fixes. 6.2.0 6.2.8 v2.4 (March 2021) Added Map and Geospatial Point data types. 6.0.0 6.0.8 v2.2 (November 2020) Support for scaling reads, OPTIONAL MATCH, query caching, and GRAPH.SLOWLOG. 5.0.7 6.0.8 v2.0 (January 2020) Enabled graph-aided search and graph visualisation. Cypher coverage. Performance improvements. 5.0.7 5.4.11 v1.2 (April 2019) Support multiple relationships of the same type R connecting two nodes. Lexer elides escape characters in string creation. Performance improvements. 4.0.0 5.0.0 v1.0 (November 2018) Fixed memory leaks. Support ‘*’ within RETURN clause. Added TYPE function. Initial support for UNWIND clause. 4.0.0 5.0.0 ","categories":["Modules"]},{"uri":"/modules/redisjson/release-notes/","uriRel":"/modules/redisjson/release-notes/","title":"RedisJSON release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v2.4 (November 2022) Low-level API changes in order to support the multi-value indexing and querying support that comes with RediSearch 2.6. RediSearch 2.6 comes with multi-value indexing and querying of attributes for any attribute type (Text, Tag, Numeric, Geo and Vector) defined by a JSONPath leading to an array or to multiple scalar values. 6.0.16 6.2.18 v2.2 (July 2022) Preview of Active-Active support for JSON. 6.0.0 6.2.12 v2.0 (November 2021) Index JSON documents. JSONPath support. Commands operate on multiple paths. 6.0.0 6.0.0 v1.0 (December 2017) Serialization cache for JSON.GET. 4.0.0 5.0.0 ","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/","uriRel":"/modules/redistimeseries/release-notes/","title":"RedisTimeSeries release notes","tags":[],"keywords":[],"description":"","content":" Version\u0026nbsp;(Release\u0026nbsp;date)\u0026nbsp; Major\u0026nbsp;changes Min\u0026nbsp;Redisversion Min\u0026nbsp;clusterversion v1.8 (November 2022) Added a time-weighted average aggregator, gap filling, ability to control bucket timestamps, ability to control alignment for compaction rules, new reducer types, and ability to include the latest (possibly partial) raw bucket samples when retrieving compactions 6.0.16 6.2.8 v1.6 (January 2022) Added support for aggregating across multiple time series (multi-key). Can compute queries such as “the maximum observed value of a set of time series” server-side instead of client-side. 6.0.16 6.2.8 v1.4 (September 2020) Added ability to backfill time series. 5.0.0 6.0.12 v1.2 (January 2020) Added compression. Stable ingestion time independent of the number of the data points on a time series. API performance improvements. Extended client support. 5.0.0 6.0.12 v1.0 (June 2019) Downsampling/compaction. Secondary indexing. Aggregation at read time. Integration with Prometheus, Grafana, and Telegraph. 5.0.0 5.4.0 ","categories":["Modules"]},{"uri":"/modules/redisearch/release-notes/redisearch-1.1-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-1.1-release-notes/","title":"RediSearch 1.1 release notes","tags":[],"keywords":[],"description":"Aggregations engine.","content":"Requirements RediSearch v1.1.0 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 RediSearch 1.1.0 (April 2018) This is a major version (we almost named it 2.0), which includes months of work, mostly on the brand new aggregations engine.\nAggregations Aggregations are a way to process the results of a search query, group, sort and transform them - and extract analytic insights from them. Much like aggregation queries in other databases and search engines, they can be used to create analytics report, or to perform Faceted Search style queries.\nExample aggregation request For example, indexing a web-server\u0026rsquo;s logs, we can create report for unique users by hour. Suppose our schema includes the SORTABLE fields timestamp (Unix-timestamp) and userId:\nFT.AGGREGATE idx \u0026#34;*\u0026#34; APPLY hour(@timestamp) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @userId AS unique_users SORTBY 2 @hour ASC See the full documentation on aggregations for more details\nBug fixes over 1.0.10 Fixed #313 - removed -mpopcnt compile flag.\nFixed #312 - crash on highlighting\n","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/redisbloom-1.1-release-notes/","uriRel":"/modules/redisbloom/release-notes/redisbloom-1.1-release-notes/","title":"RedisBloom 1.1 release notes","tags":[],"keywords":[],"description":"Updated version.","content":"Requirements RedisBloom v1.1.1 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 RedisBloom 1.1.1 (21 February 2019) Update version to 1.1.1\nRedisBloom 1.1.0 (12 February 2019) Release of 1.1.0\n","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-1.2-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-1.2-release-notes/","title":"RedisGraph 1.2 release notes","tags":[],"keywords":[],"description":"Support multiple relationships of the same type R connecting two nodes. Lexer elides escape characters in string creation. Performance improvements.","content":"Requirements RedisGraph v1.2.2 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v1.2.2 (May 2019) Update urgency: Medium This is a maintenance release for version 1.2.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nv1.2.0 (April 2019) This is a major release for RedisGraph. Compared to previous release 1.0.15:\nAdded functionality #452 Support multiple relationships of the same type R connecting two nodes #392 Lexer elides escape characters in string creation: support for property values with ' or \u0026quot; Performance improvements #442 Seek graph entity by id: MATCH (n) WHERE ID(n) = 5 RETURN n #422 MATCH (n) RETURN COUNT(n) is now O(1) #421 and #459 Lazy loading of matrices on subsequent writes of relationships and nodes #399 Single transpose of matrices Minor Bugfixes: #456 Memory leaks #439 When label doesn\u0026rsquo;t exists, the node count should return 0 #439 #435 Referencing non existing properties #435 Compared to 1.0:\nAdded functionality #390 GraphBLAS 2.3.0 release notes #452 Support multiple relationships of the same type R connecting two nodes #392 Lexer elides escape characters in string creation: support for property values with \u0026rsquo; or \u0026quot; Enhanced cypher Initial support for WITH clause: Allows query parts to be chained together, piping the results from one to be used as starting points or criteria of the next. ref p.78 #247 Initial support for UNWIND #244 RETURN * (give me everything bro) #236 TYPE function (returns the type of a relationship) #288 Querying patterns where a relationship between two nodes can be of different types. ()-[:A|:B]-\u0026gt;() #252 Multiple MATCH clauses #327 Multiple CREATE clauses #305 MERGE + SET #348 Smaller memory footprint - encode properties Faster and better bulk loading Performance improvements #442 Seek graph entity by id: MATCH (n) WHERE ID(n) = 5 RETURN n #422 MATCH (n) RETURN COUNT(n) is now O(1) #421 and #459 Lazy loading of matrices on subsequent writes of relationships and nodes #399 Single transpose of matrices #393 Discard distinct when performing aggregation #289 Index utilization when performing Cartesian product #308 Granular writer locking Most noticeable bugfixes #456 Memory leaks #439 When label doesn\u0026rsquo;t exists, the node count should return 0 #435 Referencing non existing properties #359 Adding ORDER BY changes the number of returned hits when used in combination with LIMIT #363 Remove graph entity property when it is set to null #386 Return updated values on queries that modify data #249 reset operation within execution plan should propagate upwards #259 Replace operations appropriately when rewriting execution plan #262 Entity returned from datablock should have its internals cleared #264 Loaded triemap strings are not guaranteed space for a null terminator ","categories":["Modules"]},{"uri":"/modules/redisgears/release-notes/redisgears-1.2-release-notes/","uriRel":"/modules/redisgears/release-notes/redisgears-1.2-release-notes/","title":"RedisGears 1.2 release notes","tags":[],"keywords":[],"description":"Plugins and JVM support. Python async await. Override commands API. Register functions on key miss events. Tracks new statistics. Python profiler support. Extended RedisAI integration.","content":"Requirements RedisGears v1.2.5 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.12 v1.2.5 (July 2022) This is a maintenance release for RedisGears 1.2.\nUpdate urgency: MODERATE : Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nBug fixes:\n#792, #798 Execution was triggered infinitely when trimming is turned off. #791, #796 Stop triggering executions during pause even on failure. #794, #797 Use PythonInstallationDir configuration to find the virtual environment location on Redis Enterprise. (MOD-1734) v1.2.4 (May 2022) This is a maintenance release for RedisGears 1.2.\nUpdate urgency: LOW : No need to upgrade unless there are new features you want to use.\nDetails:\nImprovements:\n#772 Added the ability to upgrade a dependency at runtime with FORCE_REQUIREMENTS_REINSTALLATION on RG.PYEXECUTE. #765 Allow deactivating override Python allocators for performance improvements. Bug fixes:\n#761, #760, #778 StreamReader fixes to pause and unregister stream processing. v1.2.3 (April 2022) This is a maintenance release for RedisGears 1.2.\nUpdate urgency: LOW : No need to upgrade unless there are new features you want to use.\nDetails:\nImprovements:\n#739 Added TLS support #734 Pause/Unpause registrations #741 Added build for Python-only artifacts Bug fixes:\n#740 Fix -nan value on registration stats v1.2.2 (February 2022) This is the General Availability release of RedisGears 1.2.\nHeadlines Plugins and JVM support RedisGears 1.2 comes with a new plugin mechanism that allows you to decide which languages you want to load into RedisGears. Currently, we support two languages: Python and Java (JVM languages). You can decide which language you want to use using the new Plugin configuration.\nFull documentation for JVM support can be found on the Redis documentation website.\nPython async await support RedisGears provides support for Python coroutines. Each step of your gears function can now be a Python coroutine that will take the execution to the background or will wait for some event to happen. Refer to the following links for more information:\nAsync Await Support Async Await Advanced Topics Override commands API You can override Redis vanilla commands with a function. For more information, refer to the RedisGears command hooks documentation.\nKey miss event for read-through pattern Requested by many users, RedisGears 1.2 allows you to register functions on key miss event. One use case for this is to implement a read-through caching pattern. For more information about this topic, refer to the following links:\nKey Miss Event in the RedisGears documentation. rghibernate recipe that leverages the key miss event to implement read-through from external databases. Better visibility and analyzing tools We improved the experience during the development phase by enabling better debugging and troubleshooting. There is still room for improvement but RedisGears 1.2 makes the first steps toward a simpler API that is easier to use. This new version allows you to name your code and upgrade it with a single Redis command. For more information, refer to the upgrade section of the RedisGears introduction documentation.\nRedisGears now tracks the following new statistics to better analyze your registrations:\nlastRunDurationMS - duration in milliseconds of the last execution totalRunDurationMS - total runtime of all executions in milliseconds avgRunDurationMS - average execution runtime in milliseconds For streams, RedisGears also tracks the following data:\nlastEstimatedLagMS - gives the last batch lag (the time difference between the first batch entry in the stream and the time the entire batch finished processing) avgEstimatedLagMS - average of the lastEstimatedLagMS field. The RG.DUMPREGISTRATIONS command exposes these new statistics.\nRedisGears 1.2 also adds support for a Python profiler, specifically cProfile. For more information, refer to the documentation for the following commands:\nRG.PYPROFILE STATS RG.PYPROFILE RESET RedisAI integration Although RedisAI integration was already supported in v1.0, RedisGears 1.2 adds official support for all capabilities in RedisAI v1.2. The API was extended to support RedisAI DAG and was combined with the new async await API to achieve the best performance possible.\nDetails Bug fixes (since 1.0.9):\n#557, #554 RG.CONFIGGET returns user-defined configuration #572 Lock Redis GIL when creating RedisAI DAG #661, #536 Added RG.TRIGGERONKEY #650 Do not propagate MULTI EXEC on Redis 7 #671, #558 Wait for cluster to be initialized when reading stream data #656 Stream reader creates more than one execution on a stream #676 Globals dictionary not set correctly after deserialization #665, #679 Allow case-insensitive event type on command reader #697 hashtag() function for Redis Enterprise #688, #545 Check REDISMODULE_CTX_FLAGS_DENY_BLOCKING flag before blocking the client Note: This is the first GA version of 1.2. The version inside Redis is 1.2.2 in semantic versioning. Since the version of a module in Redis is numeric, we could not add a GA flag.\nMinimum Redis version: 6.0.0\n","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/redistimeseries-1.2-release-notes/","uriRel":"/modules/redistimeseries/release-notes/redistimeseries-1.2-release-notes/","title":"RedisTimeSeries 1.2 release rotes","tags":[],"keywords":[],"description":"Added compression. Stable ingestion time independent of the number of the data points on a time series. API performance improvements. Extended client support.","content":"Requirements RedisTimeSeries v1.2.7 requires:\nMinimum Redis compatibility version (database): 5.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.12 v1.2.7 (June 2020) This is a maintenance release for version 1.2.\nDetails:\nBugfixes:\n#414 Crash when a query had an empty label (foo,) v1.2.6 (May 2020) This is a maintenance release for version 1.2.\nDetails:\nMinor enhancements:\n#403 Support for multi-value filtering in TS.MGET and TS.MRANGE. Bugfixes:\n#378 Using snprintf to ensure the same precision of floating-point value replies. #374 TS.RANGE crashed when COUNT argument was missing. #395 Check minimum compatible Redis version at module load time. v1.2.5 (March 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #358 Wrong behaviour in TS.RANGE due to shifting left. #353 Crash where the name of a time-series was already taken due to auto-compaction. v1.2.3 (February 2020) Headlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBugfixes: #338 reverting #319. Aggregation should happen on deterministic time buckets. v1.2 GA (1.2.2 - January 2020) This is the general availability (GA) release of RedisTimeSeries 1.2 (1.2.2).\nHeadlines:\nCompression added which can reduce memory up to 98% and improve read performance up to 50%. Stable ingestion time independent of the number of the data points on a time-series. Reviewed API with performance improvements and removed ambiguity. Extended client support (we will blog about this release soon including performance improvements results and the link here)\nFull details:\nAdded functionality\n#261 Samples are compressed using Double Delta compression which results in cost savings and faster query times. Based on the Gorilla paper. In theory, this can save space up to 98%. (2 bits per sample in stead of 128). In practice, a memory reduction of 5-8x is common but depends on the use case. Initial benchmarks show 94% memory savings and performance improvements in reads up to XX%. UNCOMPRESSED option in TS.CREATE. API changes / Enhancements\n#241 Overwriting the last sample with the same timestamp is not allowed. #242 revised TS.INCRBY/DECRBY Returns a timestamp. The behaviour is now aligned with TS.ADD. The RESET functionality was removed. RESET contradicted the rewriting of the last sample (#241). Alternatively, you can reconstruct similar behaviour by TS.ADD ts * 1 + sum aggregation TS.INCRBY ts 1 + range aggregation #317 Aligning response on empty series of TS.GET with TS.RANGE. #285 #318 Changed default behaviour of TS.MRANGE and TS.MGET to no longer returns the labels of each time-series in order reduce network traffic. Optional WITHLABELS argument added. #319 TS.RANGE and TS.MRANGE aggregation starting from requested timestamp. Performance improvements\n#237 Downsampling after time window is closed vs. downsampling with each sample. #285 #318 Optional WITHLABELS argument added. This feature improves read performance drastically. Minor Enhancements\n#230 TS.INFO now includes total samples, memory usage,first time stamp, \u0026hellip; #230 MEMORY calculates series memory footprint. Bugfixes since 1.0.3\n#204 Module initialization params changed to 64 bits. #266 Memory leak in the aggregator context. #260 Better error messages. #259 #257 #219 Miscellaneous. #320 Delete the existing key prior to restoring it. #323 Empty first sample on aggregation. Note: The version inside Redis will be 10202 or 1.2.2 in semantic versioning. ","categories":["Modules"]},{"uri":"/modules/redisai/release-notes/redisai-1.2-release-notes/","uriRel":"/modules/redisai/release-notes/redisai-1.2-release-notes/","title":"RedisAI 1.2 release notes","tags":[],"keywords":[],"description":"Strings tensor support. Backend updates - TF 2.6, PyTorch 1.9, ONNXRuntime 1.9. Redis now manages ONNXRuntime memory.","content":"Requirements RedisAI v1.2.7 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.2.2 1.2.7 (June 2022) This is a maintenance release for RedisAI 1.2.\nUpdate urgency: MODERATE.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nMinor enhancements:\n#918, #924 Add AI.CONFIG GET sub-command #914, #915, #917 Backends update - TF 2.8, PyTorch 1.11, ONNXRuntime 1.11 #904 Enable saving model/script run statistics when run by a low-level API (by using RedisGears integration in particular) and retrieving the statistics with the AI.INFO command #897 Restore support for MacOS build scripts #893 Removed support for Linux Xenial Bug fixes:\n#923 Fix a synchronization issue, regarding updates to the number of background threads, that rarely caused a crash upon executing models in ONNX 1.2.5 (November 2021) Headlines:\n#832 Strings tensor support (TF and ONNXRUNTIME only). #847, #806 Backends update - TF 2.6, PyTorch 1.9, ONNXRuntime 1.9 #827 ONNXRuntime memory is now managed by Redis (Cloud readiness) Bug fixes:\n#829 Remove deprecation warnings from deprecated commands on Redis logs. #852 Fixed invalid deletion of outputs after execution error in TF ","categories":["Modules"]},{"uri":"/modules/redisjson/release-notes/redisjson-2.0-release-notes/","uriRel":"/modules/redisjson/release-notes/redisjson-2.0-release-notes/","title":"RedisJSON 2.0 release notes","tags":[],"keywords":[],"description":"Index JSON documents. JSONPath support. Commands operate on multiple paths.","content":"Requirements RedisJSON v2.0.11 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.0 v2.0.11 (July 2022) This is a maintenance release for RedisJSON 2.0. Update urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nImprovements:\nMinor documentation changes and internal build improvements v2.0.9 (June 2022) This is a maintenance release for RedisJSON 2.0.\nUpdate urgency: LOW: No need to upgrade unless there are new features you want to use.\nDetails:\nBug fixes:\n#721 Skip String and Boolean scalars in JSON.CLEAR (MOD-3136) Improvements:\n#709 Allow internal JSON API\u0026rsquo;s getdouble to succeed with integer values v2.0.8 (April 2022) This is a maintenance release for RedisJSON 2.0.\nUpdate urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nBug fixes:\n#691, #667 Duplicate results in JSONPath query v2.0.7 (March 2022) This is a maintenance release for RedisJSON 2.0.\nUpdate urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nImprovements:\n#632, #605 Support JSON.CLEAR for string, bool, and numeric scalars (MOD-2394) #637 Add intershard_tls_pass support (MOD-2522) #594 Support for MEMORY USAGE and memory info in JSON.DEBUG (MOD- 2079) Bug fixes:\n#646, #644 Do not fail JSON.MGET on wrong/unregistered key type (MOD-2511) #643 Null-terminate JSON string in rdb_save #591 Avoid crash on overflow in JSON.NUMINCRBY or JSON.NUMMULTBY (MOD-2513) #593 Return no updates when performing JSON.SET with NX to an existing array element (MOD-2512) v2.0.6 (December 2021) This is a maintenance release for RedisJSON 2.0.\nUpdate urgency: MODERATE: Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nThis patch neutralizes the increased memory consumption from v1 to v2.\nDetails:\nImprovements:\n#563 Introduction of ijson. v2.0.5 (December 2021) This is a maintenance release for RedisJSON 2.0.\nDetails:\nBug fixes: #553 Return an empty array on a nonexistent path #548 Align error handling behavior #546 #545 Fix key location in JSON.DEBUG MEMORY v2.0.4 (November 2021) This is the General Availability release of RedisJSON 2.0.\nHeadlines RedisJSON is a high-performance JSON document store that allows developers to build modern applications. It stores and processes JSON in-memory, supporting millions of operations per second with sub-millisecond response times. The combination of RediSearch, native indexing, querying, and full-text search of JSON documents allows developers to create secondary indexes and query data at lightning speed.\nIndexing JSON documents Using RediSearch, it is now possible to index, query, and search JSON documents, gaining full-text search capabilities and document retrieving based on their content.\nTo do so, you must install both modules, RedisJSON and RediSearch, on the same database.\nSupport of JSON Path The commands support JSONPath as specified in the original specifications.\nThe legacy path syntax is still supported.\nCommands operate on multiple paths A JSONPath query may resolve to several paths. Every command supports multiple paths and applies the operation to all the encountered paths.\nNotice that the output of the commands evolved to provide multiple results according to the number of paths impacted.\nDetails Enhancements\n#477 Support of Multipath #336 Added generic JSON path implementation #525 Error messages prefixed with ERR or WRONGTYPE #490 Performance: Discard to_value method #426 Move from next_string to next_str #464 Initial RedisJSON commands.json file #488 Docker with RediSearch revisited Bug fixes\n#515 JSON.DEL count deleted null value #499 Avoid crash in ARRTRIM #458 Docker/Debian: moved from Buster to Bullseye #397 Support RDB Short Read in RedisJSON (a.k.a diskless-load) #398 Avoid path clone when not needed #416 Add testGetWithBracketNotation ","categories":["Modules"]},{"uri":"/ri/release-notes/v1.0.0/","uriRel":"/ri/release-notes/v1.0.0/","title":"RedisInsight v1.0, November 2019","tags":[],"keywords":[],"description":"The initial release after Redis acquired RDBTools","content":"RedisInsight v1.0.0 release notes (November 2019) This is the initial release after Redis acquired RDBTools.\n","categories":[]},{"uri":"/kubernetes/faqs/","uriRel":"/kubernetes/faqs/","title":"Redis Enterprise for Kubernetes FAQs","tags":[],"keywords":[],"description":"","content":"Here are some frequently asked questions about Redis Enterprise on integration platforms.\nWhat is an Operator? An operator is a Kubernetes custom controller which extends the native K8s API. Refer to the article Redis Enterprise K8s Operator-based deployments – Overview.\nDoes Redis Enterprise operator support multiple RECs per namespace? Redis Enterprise for Kubernetes may only deploy a single Redis Enterprise cluster (REC) per namespace. Each REC can run multiple databases while maintaining high capacity and performance.\nDo I need to deploy a Redis Enterprise operator per namespace? Yes, one operator per namespace, each managing a single Redis Enterprise cluster. Each REC can run multiple databases while maintaining high capacity and performance.\nHow can I see the custom resource definitions (CRDs) created for my Redis Enterprise cluster? Run the following:\nkubectl get rec kubectl describe rec \u0026lt;my-cluster-name\u0026gt; How can I change the Redis Enterprise cluster admin user password? The cluster admin user password is created by the operator during the deployment of the Redis Enterprise cluster (REC) and is stored in a Kubernetes secret.\nSee Manage REC credentials for instructions on changing the admin password.\nHow is using Redis Enterprise operator superior to using Helm charts? While Helm charts help automate multi-resource deployments, they do not provide the lifecycle management and lack many of the benefits provided by the operator:\nOperators are a K8s standard, while Helm is a proprietary tool Using operators means better packaging for different Kubernetes deployments and distributions, as Helm is not supported in a straightforward way everywhere Operators allow full control over the Redis Enterprise cluster lifecycle We’ve experienced difficulties managing the state and lifecycle of the application through Helm, as it essentially only allows to determine the resources being deployed, which is a problem when upgrading and evolve the Redis Enterprise Cluster settings Operators support advanced flows which would otherwise require using an additional third party product How to connect to the Redis Enterprise cluster user interface Create a port forwarding rule to expose the cluster user interface (UI) port. For example, when the default port 8443 is used, run:\nkubectl port-forward –namespace \u0026lt;namespace\u0026gt; service/\u0026lt;name\u0026gt;-cluster-ui 8443:8443 Connect to the UI by pointing your browser to https://localhost:8443\nHow should I size Redis Enterprise cluster nodes? For nodes hosting the Redis Enterprise cluster statefulSet pods, follow the guidelines provided for Redis Enterprise in the hardware requirements.\nFor additional information please also refer to Kubernetes operator deployment – persistent volumes.\nHow to retrieve the username/password for a Redis Enterprise Cluster? The Redis Enterprise cluster stores the username/password of the UI in a K8s secret.\nFind the secret by retrieving secrets and locating one of type Opaque with a name identical or containing your Redis Enterprise cluster name.\nFor example, run:\nkubectl get secrets A possible response may look like this:\nNAME TYPE DATA AGE redis-enterprise-cluster Opaque 2 5d To retrieve the secret run:\nkubectl get secret redis-enterprise-cluster -o yaml A possible response may look like this:\napiVersion: v1 data: password: Q2h5N1BBY28= username: cmVkaXNsYWJzLnNi kind: Secret metadata: creationTimestamp: 2018-09-03T14:06:39Z labels: app: redis-enterprise redis.io/cluster: test name: redis-enterprise-cluster namespace: redis ownerReferences: – apiVersion: app.redislabs.com/v1alpha1 blockOwnerDeletion: true controller: true kind: RedisEnterpriseCluster name: test uid: 8b247469-c715-11e8-a5d5-0a778671fc2e resourceVersion: “911969” selfLink: /api/v1/namespaces/redis/secrets/redis-enterprise-cluster uid: 8c4ff52e-c715-11e8-80f5-02cc4fca9682 type: Opaque Next, decode, for example, the password field. Run:\necho \u0026#34;Q2h5N1BBY28=\u0026#34; | base64 –-decode How to retrieve the username/password for a Redis Enterprise Cluster through the OpenShift Console? To retrieve your password, navigate to the OpenShift management console, select your project name, go to Resources-\u0026gt;Secrets-\u0026gt;your_cluster_name\nRetrieve your password by selecting “Reveal Secret.” What capabilities, privileges and permissions are defined by the Security Context Constraint (SCC) yaml and the Pod Security Policy (PSP) yaml? The scc.yaml file is defined like this:\nkind: SecurityContextConstraints apiVersion: security.openshift.io/v1 metadata: name: redis-enterprise-scc allowPrivilegedContainer: false allowedCapabilities: - SYS_RESOURCE runAsUser: type: MustRunAs uid: 1001 FSGroup: type: MustRunAs ranges: 1001,1001 seLinuxContext: type: RunAsAny (latest version on GitHub)\nThe psp.yaml file is defined like this:\napiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: redis-enterprise-psp spec: privileged: false allowPrivilegeEscalation: false allowedCapabilities: - SYS_RESOURCE runAsUser: rule: MustRunAsNonRoot fsGroup: rule: MustRunAs ranges: - min: 1001 max: 1001 seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - \u0026#39;*\u0026#39; (latest version on GitHub)\nThe SYS_RESOURCE capability is required by the Redis Enterprise cluster (REC) container so that REC can set correct out of memory (OOM) scores to its processes inside the container. Also, some of the REC services must be able to increase default resource limits, especially the number of open file descriptors.\nWhile the REC container runs as user 1001, there are no limits currently set on users and user groups in the default scc.yaml file. The psp.yaml example defines the specific uid.\nThe REC SCC definitions are only applied to the project namespace when you apply them to the namespace specific Service Account as described in the OpenShift CLI deployment article.\nREC PSP definitions are controlled with role-based access control (RBAC). A cluster role allowing the REC PSP is granted to the redis-enterprise-operator service account and allows that account to create pods with the PSP shown above.\nNote: Removing NET_RAW blocks \u0026lsquo;ping\u0026rsquo; from being used on the solution containers. These changes were made as of release 5.4.6-1183 to better align the deployment with container and Kubernetes security best practices: The NET_RAW capability requirement in PSP was removed. The allowPrivilegeEscalation is set to \u0026lsquo;false\u0026rsquo; by default. ","categories":["Platforms"]},{"uri":"/glossary/","uriRel":"/glossary/","title":"Glossary","tags":[],"keywords":[],"description":"","content":" A, B access control list (ACL) Allows you to manage permissions based on key patterns.\nMore info: redis.io/topics/acl; ACL wikipedia; Database access control; Update database ACLs; Passwords, users, and roles\nActive-Active database (CRDB) Geo-distributed databases that span multiple Redis Enterprise Software clusters. Active-Active databases, also known as conflict-free replicated databases (CRDB), depend on multi-master replication (MMR) and conflict-free replicated data types (CRDTs) to power a simple development experience for geo-distributed applications.\nMore info: Active-Active geo-distributed Redis, Geo-distributed Active-Active Redis applications, Developing applications for Active-Active databases\nActive-Active database instance A “member database” of a global Active-Active database which is made up of its own master and replica shards spanning a single cluster.\nactive-passive database replication Provides applications read-only access to replicas of the data set from different geographical locations. The Redis Enterprise implementation of active-passive replication is called Replica Of.\nadmin console Each node runs a web server that is used to provide the user with access to the Redis Enterprise Software admin console (previously known as management UI). The admin console allows viewing and managing the entire cluster, so it does not matter which node is used to access it.\nadmission controller A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.\nMore info: Using Admission Controllers\nappend-only file (AoF) Log files that keep a record of data changes by writing to the end of a file. This happens with every write, or every second to allow data recovering the entire dataset by replaying the append-only log from the beginning to the end.\nMore info: Data Persistence, Data Persistence with Redis Enterprise Software\nC causal consistency A distributed database is causally consistent if it maintains the same order of operations on a piece of data across all database copies.\nMore info: Causal consistency wikipedia, Causal consistency in an Active-Active database\nCIDR allowlist Classless Inter-Domain Routing (CIDR) is a method to allocate and route IP addresses. A CIDR allowlist defines a range of IP addresses and permits connections to them.\nMore info: CIDR wikipedia, Configure CIDR allowlist\nconcurrent writes Concurrency or updates and writes refer to more than events that happen at the same wall clock time across member Active-Active databases. Concurrent updates refer to the fact that updates happen in between sync events that catch up member Active-Active databases with updates that happened on other member Active-Active databases.\nconsistency Consistency models describe the way a distributed system keeps replicated data consistent between copies.\nMore info: Consistency models\ncluster A Redis Enterprise cluster is composed of identical nodes that are deployed within a data center or stretched across local availability zones.\nMore info: Database clustering\nCluster Configuration Store (CSS) An internally managed Redis database that acts a single repository for all cluster meta-data.\nCluster Node Manager (CNM) A collection of Redis Enterprise services responsible for provisioning, migration, monitoring, re-sharding, re-balancing, de-provisioning, auto-scaling\nconflict-free replicated databases (CRDB) Conflict-free replicated databases (CRDB) are an alternate name for Active-Active databases.\nconflict-free replicated data types (CRDT) Techniques used by Redis data types in Active-Active databases that handle conflicting concurrent writes across member Active-Active databases. The Redis Enterprise implementation of CRDT is called an Active-Active database (formerly known as CRDB).\nMore info: CRDT info, Active-Active geo-distributed Redis, CRDT wikipedia\nCustomResourceDefinition (CRD) Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.\nMore info: CustomResourceDefinition, Custom Resources\nD - F data eviction policy Defines how excess data is handled when the database exceeds the memory limit.\nMore info: Data Eviction Policy\ndeprecated Features are marked as deprecated when they\u0026rsquo;re scheduled to be removed from our products, generally because they\u0026rsquo;ve been replaced by new features.\nFor details, see obsolete.\nDomain Name Service (DNS) Naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities.\nMore info: DNS wikipedia\neventual consistency After updating data on one instance of a distributed database, the other database copies may have stale data for a short time while they sync. Eventual consistency means that the updated data will eventually be the same across all database copies.\nMore info: Eventual consistency wikipedia\nFully qualified domain name (FQDN) A domain name that includes a list of domain labels to specify the exact location in the DNS.\nMore info: FQDN wikipedia\n`fysnc` Linux command to synchronize a file\u0026rsquo;s in-core state with a storage device\nMore info: fsync man page\nG - J hash slot The result of a hash calculation.\nMore info: Database clustering\nhash tag A part of the key that is used in the hash calculation.\nMore info: Database clustering\nhigh availability High availability (HA) is a characteristic of distributed systems that keeps systems available for users for longer than normal periods of time. This is done by reducing single points of failure, increasing redundancy, and making recovering from failures easier.\nMore info: Redis Enterprise durability and high availability, High availability wikipedia\ningress An API object that manages external access to the services in a Kubernetes cluster, typically HTTP.\nMore info: Ingress, Ingress routing for Redis Enterprise for Kubernetes\nK, L kubectl A command-line tool for communicating with a Kubernetes API server.\nMore info: Overview of kubectl\nLightweight Directory Access Protocol (LDAP) A protocol for accessing and maintaining distributed directory services over an IP network, often used to authenticate users.\nMore info: LDAP wikipedia, LDAP authentication\nM - O master node Node that operates as the leader of a cluster. Also known as the primary node.\nmigration Deciding when and where shards will be moved if more network throughput, memory, or CPU resources are needed\nmulticast DNS (mDNS) Protocol that resolves hostnames to the IP addresses that do not include a local name server.\nMore info: multicast DNS wikipedia\nmulti-factor authentication (MFA) Method of authenticating users with pieces of evidence of the user\u0026rsquo;s identity. When MFA is enabled on Redis Enterprise Cloud, users must enter their username, password, and an authentication code when logging in.\nMore info: Multi-factor authentication\nmulti-primary replication Also known as multi-master replication, Active-Active databases have multiple primary nodes (one on each participating cluster) to enable concurrent writes operations.\nMore info: Multi-primary replication\nnamespace An abstraction used by Kubernetes to support multiple virtual clusters on the same physical cluster.\nMore info: Namespaces\nobsolete When features are removed from our products, they\u0026rsquo;re generally replaced by new features that provide a better experience, more functionality, improved security, and other benefits.\nTo provide a transition period, we mark older features as deprecated when introducing replacement features. This gives you time to adjust your deployments, apps, and processes to support the new features. During this transition, the older features continue to work as a courtesy.\nEventually, older features are removed from the product. When this happens, they\u0026rsquo;re considered obsolete, partly because they can no longer be used.\nFor best results, we advise against relying on deprecated features for any length of time.\noperator Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components.\nMore info: operator pattern\nOut-of-Memory (OOM) If a member Active-Active database is in an out of memory situation, that member is marked “inconsistent” by Redis Enterprise Software, the member stops responding to user traffic, and the syncer initiates full reconciliation with other peers in the Active-Active database.\nP - Q participating clusters Clusters participating in the multi-primary replication of an Active-Active database.\nprovisioning Deciding where shards will be created and placed.\nproxy policy Determines the number and location of active proxies tied to a single endpoint that receive incoming traffic for a database.\nFor more info, see Proxy policy.\nquorum node Node provisioned only for cluster operations that can be elected as a master node. The quorum node participates in the cluster quorum and must be explicitly assigned this role via the rladmin command.\nR rack-zone awareness Redis Enterprise feature that helps to ensure high availability in the event of a rack or zone failure. In the event of a rack or zone failure, the replicas and endpoints in the remaining racks/zones will be promoted.\nMore info: Rack-zone awareness in Redis Enterprise Software\nreplication backlog Databases using replication or Active-Active maintain a backlog to synchronize the primary and replica shards.\nre-sharding Distributing keys and their values among new shards.\nrebalancing Moving shards to nodes where more resources are available.\nRedis Enterprise Cloud The cloud version of Redis Enterprise.\nRedis Enterprise cluster Collection of Redis Enterprise nodes. A cluster pools system resources across nodes in the cluster and supports multi-tenant database instances.\nRedis Enterprise database Logical entity that manages your entire dataset across multiple Redis instances. It segments the data into shards and distributes them among nodes.\nRedis Enterprise nodes Physical or virtual machines or containers that runs a collection of Redis Enterprise services\nRedis Enterprise Software The on-premises version of Redis Enterprise.\nRedis instance Single-threaded Redis OSS database.\nRedis on Flash (RoF) Enables your Redis databases to span both RAM and dedicated flash memory (SSD). Redis on Flash manages the location of key values (RAM vs Flash) in the database via a LRU-based (least-recently-used) mechanism.\nMore info: Redis on Flash, Redis on Flash (RoF) quick start\nreplica high availability (replicaHA) High availability feature of Redis Enterprise Software. After a node failure, the cluster automatically migrates remaining replica shards to available nodes. Previously known as \u0026ldquo;Slave HA\u0026rdquo; or slave_ha.\nMore info: High availability for replica shards\nReplica Of The Redis Enterprise implementation of active-passive database replication.\nMore info: Replica Of ReplicaSet A ReplicaSet is a type of Kubernetes resource that (aims to) maintain a set of replica pods running at any given time.\nMore info: ReplicaSet\nreplication Database replication provides a mechanism to ensure high availability. When replication is enabled, your dataset is replicated to a replica shard, which is constantly synchronized with the primary shard. If the primary shard fails, an automatic failover happens and the replica shard is promoted.\nMore info: Database replication\nrole-based access control (RBAC) A security approach that restricts system access to authorized users.\nMore info: RBAC wikipedia; Database access control; Passwords, users, and roles\nS secret Kubernetes term for object that stores sensitive information, such as passwords, OAuth tokens, and ssh keys.\nshard Redis process that is part of the Redis clustered database.\nMore info: Database clustering, terminology\nsharding Technique that has been used to scale larger data storage and processing loads. Sharding take your data, partitions it into smaller pieces and then send the data to different locations depending on which partition the data has been assigned to.\nSimple Authentication and Security Layer (SASL) Framework for adding authentication support and data security to connection-based protocols via replaceable mechanisms.\nMore info: SASL wikipedia\nsnapshot (RDB) Data persistence file that performs a data dump every one, six, or twelve hours.\nsyncer Process on each node hosting an Active-Active database instance that synchronizes a backlog of operations between participating clusters.\nMore info: Syncer process\nT - Z tombstone A key that is logically deleted but stays in memory until it is collected by the garbage collector.\nTransport Layer Security (TLS) Protocols that provide communications security over a computer network.\nMore info: TLS wikipedia, Cloud database TLS, Redis Enterprise TLS\nVPC peering Networking connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network.\nMore info: VPC wikipedia, Enable VPC peering\n","categories":["Glossary"]},{"uri":"/rs/release-notes/legacy-release-notes/","uriRel":"/rs/release-notes/legacy-release-notes/","title":"Previous releases","tags":[],"keywords":[],"description":"Release notes for Redis Enterprise Software 5.4.14 (February 2020) and earlier versions.","content":" Redis Enterprise Software Release Notes 5.4.14 (February 2020) Redis Enterprise Software Release Notes 5.4.10 (December 2019) Redis Enterprise Software Release Notes 5.4.6 (July 2019) Redis Enterprise Software Release Notes 5.4.4 (June 2019) Redis Enterprise Software Release Notes 5.4.2 (April 2019) Redis Enterprise Software Release Notes 5.4 (December 2018) Redis Enterprise Software 5.2.2 (August 2018) Redis Enterprise Software Release Notes 5.3 BETA (July 2018) Redis Enterprise Software Release Notes 5.2 (June 2018) Redis Enterprise Software 5.0.2 (2018 March) Redis Enterprise Pack 5.0 Release Notes (November 2017) Redis Enterprise Pack 4.5 Release Notes (May 2017) RLEC 4.4 Release Notes (December 2016) RLEC 4.3.0-230 Release Notes (August 2, 2016) RLEC 4.2.1-30 Release Notes (October 18, 2015) RLEC 4.0.0-49 Release Notes (June 18, 2015) RLEC 0.99.5-24 Release Notes (February 15, 2015) RLEC 0.99.5-11 Release Notes (January 5, 2015) ","categories":["RS"]},{"uri":"/rs/installing-upgrading/product-lifecycle/","uriRel":"/rs/installing-upgrading/product-lifecycle/","title":"Redis Enterprise Software product lifecycle","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software product lifecycle fully reflects our subscription agreement. However, for any discrepancy between the two policies, the subscription agreement prevails.\nRedis Enterprise modules follow the modules lifecycle.\nRelease numbers Redis uses a four-place numbering scheme to designate released versions of its products. The format is “Major1.Major2.Minor-Build”.\nMajor sections of the version number represents fundamental changes and additions in capabilities to Redis Enterprise Software. The Major1 and Major2 part of the version number are incremented based on the size and scale of the changes in each release. The Minor section of the version number represents quality improvements and fixes to existing capabilities. We increment the minor number when many quality improvements are added to the release. Build number is incremented with any changes to the product. Build number is incremented with each build when any change is made to the binaries. Redis Enterprise Software typically gets two major releases every year but the product shipping cycles may vary.\nEnd-of-life schedule End-of-Life for a given Major release occurs 18 months after the formal release of that version.\nVersion - Release Date End of Life (EOL) 6.2 – August 2021 August 31, 2023* 6.0 – May 2020 May 31, 2022 5.6 – April 2020 October 31, 2021 5.4 – December 2018 December 31, 2020 5.2 – June 2018 December 31, 2019 Note: On June 28th, 2022, release 6.2 EOL was extended by 6 months; from Feb 28, 2023 to Aug 31, 2023. ","categories":["RS"]},{"uri":"/modules/redisearch/release-notes/redisearch-1.0-release-notes/","uriRel":"/modules/redisearch/release-notes/redisearch-1.0-release-notes/","title":"RediSearch 1.0 release notes","tags":[],"keywords":[],"description":"Hamming distance scoring. Wildcard queries. Optional deletion of documents in FT.DEL. Optionally keep document hashes in FT.DROP. Delete geo-sets when dropping an index. Tag prefix completion support.","content":"Requirements RediSearch v1.0.10 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 RediSearch 1.0.10 (April 2018) This is a bug-fix release with some stability fixes, a few processing bug fixes, and a few small additions:\nFixes Fixed false positives in NOT iterator (5fe948e)\nPrevent crashing with uninitialized sorting vectors (96f2473)\nFixed overflow bug in triemap, and limited tag length to 4KB (09e682f)\nFixed false positives in intersects and negative intersects (4998772)\nFixed bug in union of id lists (b8e74ef3)\nNew features Added support for tag prefix completions (137b346) RediSearch 1.0.9 (March 2018) This is a maintenance release from the 1.0 branch, that does not yet contain the aggregation engine that\u0026rsquo;s already in master. It includes a few small fixes and API additions that were backported from master.\nChanges 1. Hamming distance scoring RediSearch can now sort the search result by the inverse Hamming Distance between document payloads and the query payloads (provided they are both the same length). This can be used as a nearest neighbor search ranking when a feature vector is encoded as a bitmap and the distance metric is Hamming distance.\n2. Wildcard queries It\u0026rsquo;s now possible to search for just *, scanning all the documents in the index. This is useful in conjunction with the Hamming Distance scorer.\n3. Optional deletion of documents in FT.DEL Up until now, FT.DEL did not delete the Hash key containing the actual document. As of this version, you can call it with an optional DD (Delete Document) argument, and it will also delete the document.\nFor example: FT.DEL myIndex myDoc DD.\n4. Optionally keep document hashes in FT.DROP Up until now, FT.DROP always deleted the document Hash keys. As of this version, you can specify the KEEPDOCS argument to FT.DROP and it will not touch them.\n5. Delete geo-sets when dropping an index Until now we did not delete the geo-set keys where documents were geographically indexed. This version fixes this (#295)\n","categories":["Modules"]},{"uri":"/modules/redisjson/release-notes/redisjson-1.0-release-notes/","uriRel":"/modules/redisjson/release-notes/redisjson-1.0-release-notes/","title":"RedisJSON 1.0 release notes","tags":[],"keywords":[],"description":"Serialization cache for JSON.GET.","content":"Requirements RedisJSON v1.0.8 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v1.0.8 (August 2021) This is a maintenance release for version 1.0.\nUpdate urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nEnhancements:\n#257 Support inter shards TLS capability Bug fixes:\n#282 Change JSON.DEBUG first key index to 2 v1.0.7 (December 2020) This is a maintenance release for version 1.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\n#222 Validate path is not empty. #253 NULL de-reference after error. v1.0.5 (September 2020) This is a maintenance release for version 1.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nEnhancements: Republish docker image based on Redis 6 Bugfixes: #200 Following a call to JSON.ARRPOP and JSON.STRAPPEND, the LRU cache should be invalidated. v1.0.4 (February 2019) This release updates the version to 1.0.4.\nv1.0.3 (12 September 2018) This release disables the cache by default, and allows an explicit CACHE \u0026lt;ON|OFF\u0026gt; module argument to enable it.\nv1.0.2 (10 September 2018) This release contains some bug fixes over 1.0.2. It also includes some ci changes as well.\nv1.0.1 (December 2017) This version provides a serialization cache for JSON.GET (when used with the default parameters). The cache policy is LRU (last recently used), and extremely small items are not cached.\nIn the future, options may be added to configure the cache, but for the time being the cache is intended to be small and transparent.\n","categories":["Modules"]},{"uri":"/modules/redistimeseries/release-notes/redistimeseries-1.0-release-notes/","uriRel":"/modules/redistimeseries/release-notes/redistimeseries-1.0-release-notes/","title":"RedisTimeSeries 1.0 release rotes","tags":[],"keywords":[],"description":"Downsampling/compaction. Secondary indexing. Aggregation at read time. Integration with Prometheus, Grafana, and Telegraph.","content":"Requirements RedisTimeSeries v1.0.3 requires:\nMinimum Redis compatibility version (database): 5.0.0 Minimum Redis Enterprise Software version (cluster): 5.4.0 v1.0.3 (September 2019) Update urgency: Medium\nThis is a maintenance release for version 1.0.\nThis release improves overall stability and provides fixes for issues found after the previous release.\nMain features:\n#143 Standard Deviation for Aggregations #163 TS.RANGE and TS.MRANGE can limit results via optional COUNT flag #161 Support for ARM architectures #160 Optional TIMESTAMP in TS.INCRBY and TS.DECRBY Main Fixes:\n#199 RETENTION is now 64bit #211 write commands to return OOM error when redis reaches max memory Main Performance improvements:\n#3651 Do not use _union if there\u0026rsquo;s only 1 leaf in the index #0a68 Make _difference faster by iterating over the left dict (which is always smaller) v1.0.1 (July 2019) Update urgency: Minor\nThis is a maintenance release for version 1.0.\nSecondary index should work faster when a filter consists of a list of k=v predicates.\nv1.0.0 (June 2019) This is the General Availability release of RedisTimeSeries! Read the full story here\nFeatures In RedisTimeSeries, we are introducing a new data type that uses chunks of memory of fixed size for time series samples, indexed by the same Radix Tree implementation as Redis Streams. With Streams, you can create a capped stream, effectively limiting the number of messages by count. In RedisTimeSeries, you can apply a retention policy in milliseconds. This is better for time series use cases, because they are typically interested in the data during a given time window, rather than a fixed number of samples.\nDownsampling/compaction If you want to keep all of your raw data points indefinitely, your data set will grow linearly over time. However, if your use case allows you to have less fine-grained data further back in time, downsampling can be applied. This allows you to keep fewer historical data points by aggregating raw data for a given time window using a given aggregation function. RedisTimeSeries supports downsampling with the following aggregations: avg, sum, min, max, range, count, first and last.\nSecondary indexing When using Redis’ core data structures, you can only retrieve a time series by knowing the exact key holding the time series. Unfortunately, for many time series use cases (such as root cause analysis or monitoring), your application won’t know the exact key it’s looking for. These use cases typically want to query a set of time series that relate to each other in a couple of dimensions to extract the insight you need. You could create your own secondary index with core Redis data structures to help with this, but it would come with a high development cost and require you to manage edge cases to make sure the index is correct.\nRedisTimeSeries does this indexing for you based on field value pairs (a.k.a labels) you can add to each time series, and use to filter at query time (a full list of these filters is available in our documentation). Here’s an example of creating a time series with two labels (sensor_id and area_id are the fields with values 2 and 32 respectively) and a retention window of 60,000 milliseconds:\nTS.CREATE temperature RETENTION 60000 LABELS sensor_id 2 area_id 32 Aggregation at read time When you need to query a time series, it’s cumbersome to stream all raw data points if you’re only interested in, say, an average over a given time interval. RedisTimeSeries follows the Redis philosophy to only transfer the minimum required data to ensure lowest latency. Below is an example of aggregation query over time buckets of 5,000 milliseconds with an aggregation function:\n127.0.0.1:6379\u0026gt; TS.RANGE temperature:3:32 1548149180000 1548149210000 AGGREGATION avg 5000 1) 1) (integer) 1548149180000 2) \u0026#34;26.199999999999999\u0026#34; 2) 1) (integer) 1548149185000 2) \u0026#34;27.399999999999999\u0026#34; 3) 1) (integer) 1548149190000 2) \u0026#34;24.800000000000001\u0026#34; 4) 1) (integer) 1548149195000 2) \u0026#34;23.199999999999999\u0026#34; 5) 1) (integer) 1548149200000 2) \u0026#34;25.199999999999999\u0026#34; 6) 1) (integer) 1548149205000 2) \u0026#34;28\u0026#34; 7) 1) (integer) 1548149210000 2) \u0026#34;20\u0026#34; Integrations RedisTimeSeries comes with several integrations into existing time series tools. One such integration is our RedisTimeSeries adapter for Prometheus, which keeps all your monitoring metrics inside RedisTimeSeries while leveraging the entire Prometheus ecosystem. Furthermore, we also created direct integrations for Grafana and Telegraph. This repository contains a docker-compose setup of RedisTimeSeries, its remote write adaptor, Prometheus and Grafana. It also comes with a set of data generators and pre-built Grafana dashboards.\n","categories":["Modules"]},{"uri":"/modules/redisai/release-notes/redisai-1.0-release-notes/","uriRel":"/modules/redisai/release-notes/redisai-1.0-release-notes/","title":"RedisAI 1.0 release notes","tags":[],"keywords":[],"description":"Auto-batching support. Added AI.DAGRUN and AI.DAGRUN_RO commands. AI.MODELSET allows you to provide a model in chunks. Standardized GET methods (TENSORGET,MODELGET,SCRIPTGET) replies (breaking change for clients). Cache model blobs.","content":"Requirements RedisAI v1.0.2 requires:\nMinimum Redis compatibility version (database): 5.0.7 Minimum Redis Enterprise Software version (cluster): 5.4.11 v1.0.2 (October 2020) This is a maintenance release for version 1.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nDetails:\nMinor updates:\n#383 Enable AI.SCRIPTRUN inside AI.DAGRUN #395 Add support for variadic arguments to AI.SCRIPTRUN #400 Low level API additions for use in other modules (e.g. RedisGears) #396 Add relevant RedisAI config entries to the Redis INFO output. Helpful for standard monitoring systems Bug Fixes:\n#403 Atomic ref count #406 Avoid splitting outputs in batches when nbatches == 1 #438 Fixed flagged as \u0026ldquo;getkeys-api\u0026rdquo; during the registration (AI.DAGRUN, AI.DAGRUN_RO, AI.MODELRUN, AI.SCRIPTRUN) #449 Safely add to arrays #443 Segfault for AI.DAGRUN + AI.TENSORSET v1.0.1 (July 2020) This is a maintenance release for version 1.0.\nHeadlines:\nThis release improves overall stability and provides fixes for issues found after the previous release. Details:\nBug Fixes:\n7f87f85 Allow inconsistent zero batch outputs. #385,#382 AI.SCRIPTRUN results were being replicated twice. #384 AI.MODELGET to return inputs, outputs, batchsize, and minbatchsize. #412 Several memory leaks. v1.0.0 (May 2020) Supported Backends:\nTensorFlow Lite 2.0 TensorFlow 1.15.0 PyTorch 1.5 ONXXRuntime 1.2.0 New features:\n#241, #270 auto-batching support. Requests from multiple clients can be automatically and transparently batched in a single request for increased CPU/GPU efficiency during serving. #322 Add AI.DAGRUN. With the new AI.DAGRUN (DAG as in direct acycilc graph) command we support the prescription of combinations of other AI.* commands in a single execution pass, where intermediate keys are never materialised to Redis. #334 Add AI.DAGRUN_RO command, a read-only variant of AI.DAGRUN #338 AI.MODELSET Added the possibility to provide a model in chunks. #332 Standardized GET methods (TENSORGET,MODELGET,SCRIPTGET) replies (breaking change for clients) #331 Cache model blobs for faster serialization and thread-safety. Minor Enhancements:\n#289 Memory access and leak fixes. #319 Documentation improvements. Build Enhancements:\n#299 Coverage info. #273 Enable running valgrind/callgrind on test platform #277, #296 tests extension and refactoring per backend. ","categories":["Modules"]},{"uri":"/modules/redisgears/release-notes/redisgears-1.0-release-notes/","uriRel":"/modules/redisgears/release-notes/redisgears-1.0-release-notes/","title":"RedisGears 1.0 release notes","tags":[],"keywords":[],"description":"First GA release of RedisGears. Built-in C-API and Python interpreter. Run a serverless engine in memory next to your Redis data.","content":"Here\u0026rsquo;s what\u0026rsquo;s changed recently in RedisGears.\nTo learn more, see the linked pull requests.\nRequirements RedisGears v1.0.9 requires:\nMinimum Redis compatibility version (database): 6.0.0 Minimum Redis Enterprise Software version (cluster): 6.0.0 v1.0.9 (January 2022) This is a maintenance release for RedisGears 1.0\nUpdate urgency: LOW : No need to upgrade unless there are new features you want to use.\nDetails:\nImprovements:\n#652 Support for keys space notifications coming from other modules. v1.0.8 (November 2021) This is a maintenance release for RedisGears 1.0.\nUpdate urgency: MODERATE - Program an upgrade of the server, but it\u0026rsquo;s not urgent.\nDetails:\nMinor features:\n#624, #626 Update Python interpreter version to 3.7.12 Bug fixes:\n#610 Fix symbol collision with RediSearch and RedisGraph that causes deadlock\n#609, #611 Crash on stream reader where stream is deleted during read\n#612 Return error when GearsBuilder is used after registered or run\n#613 Rare issue where stream reader might trigger executions on replica\n#629 Rare deadlock on stream reader\nv1.0.7 (August 2021) This is a maintenance release for version 1.0.\nUpdate urgency: LOW - No need to upgrade unless there are new features you want to use.\nDetails:\nMinor Features:\n#594 Added RedisGears info section to Redis info command\n#587 inorder option for the command reader\n#592, #586 Background executions will now appear on Redis slowlog, available in Redis 6.2 and above.\nv1.0.6 (March 2021) This is a maintenance release for version 1.0.\nUpdate urgency: Medium\nHighlights:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBug fixes:\n#496 Allow parallel execution on Command Reader.\n#505 Crash on a client that sends cluster set and disconnects.\nv1.0.5 (18 January 2021) This is a maintenance release for version 1.0.\nUpdate urgency: High\nHighlights:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBug fixes:\n#480 Crash when map/accumulate/accumulateby raises an error which causes the same pyobject to be freed twice.\n#480 \u0026lsquo;Cluster set\u0026rsquo; happened before resending hello request, which caused a crash.\n#480 Memory leaks on \u0026lsquo;cluster set\u0026rsquo;.\nv1.0.4 (17 January 2021) This is a maintenance release for version 1.0.\nHighights:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBug fixes:\n#458 Crash on StreamReader when the stream is dropped during processing of the function.\n#477 Rare crash on accumulate step.\nv1.0.3 (November 2020) This is a maintenance release for version 1.0.\nHighlights:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nBug fixes:\n#427 Rare issue where messages might get lost and cause executions timeouts.\n#434 Triggering an execution on uninitialized cluster state should raise a cluster uninitialized error.\nv1.0.2 (October 2020) This is a maintenance release for version 1.0.\nHighlights:\nThis release improves overall stability and provides fixes for issues found after the previous release.\nDetails:\nMinor Features:\n#368 Support for buffer input on createTensorFromBlob when integrating with RedisAI API.\n#373 Registered execution plans will not re-send their metadata when they are triggered (this feature reduces network overhead and improve performance of distributed registrations).\n#371 Full details on import requirement failure.\nBug fixes:\n#374 Fix RedisAI toFlatList function to handle long long encoding.\n#375 Call OnRegister on RDBLoad.\n#388 Fix crash on hello request failure (internal protocol between shards), a retry will be triggered after 1 second.\n#395 Fix rare wrong results on aggregate and aggregateby. Use deep copy on the zero value to avoid those incorrect results.\nv1.0.1 (July 2020) This is a maintenance release for version 1.0.\nHighlights:\nImport and Export requirements - With this capability, you can export the python requirements present in RedisGears and import them into another instance using gears-cli.\nThis enables, amongst others, to import requirements to environments without internet access. Notice that the requirement should be exported from and imported into machines with the same OS and version. This is the recommended tool to use for productionising RedisGears functions (for example to be used in your CI).\nOptimised requirements installation mechanism - requirements will now be distributed once to each shard and not each time an execution is triggered.\nRedisAI API fixes and additions - fixes/additions related to the integration with RedisAI.\nDetails:\nFeatures:\n#330 SendMsgRetries configuration parameter indicating how many times RedisGears should try sending messages between shards.\n#330 PythonInstallReqMaxIdleTime configuration parameter for the maximum amount of time to wait for requirements to be installed.\n#330 RG.PYDUMPREQ command that output all the currently available python requirements.\nAdded the following API functionality to the RedisAI integration:\n#346 Script run will now return a list of tensors.\n#347 Allow tensor creation from bytes.\n#350 Added mget and mset to get/set multiple tensors from/to the keyspace.\n#345 Free python GIL before calling redisAI model/script run.\nBug fixes:\n#325 Circular reference that caused a logical memory leak.\n#330 Crash on wrong reply status.\n#345 Crash on python session (private data on rg.dumpregistration) \u0026rsquo;tostr\u0026rsquo; function when there is no requirements.\n#334 Type check to RedisAI API to prevent potential crashes.\nv1.0.0 (May 2020) This is the first general availability (GA) release of RedisGears (v1.0.0).\nHighlights:\nProgram everything you want in Redis - With a built-in C-API and Python interpreter, RedisGears lets you use full-fledged Python scripts and libraries to process data inside Redis.\nWrite once, deploy anywhere - Write your functions against a standalone Redis database and deploy them to production clusters—with no changes required.\nRun your serverless engine where your data lives - RedisGears lets you process events and streams faster by running in memory next to your data in Redis.\nTo get started please check out redisgears.io. We plan to release a blogpost soon and will cross link it here.\nBug fixes (compared to RC1):\n#288 dependencies with version will not crash the server.\n#309, #313 Prevent crashes on RedisAI intergration and update the low level C api of RedisAI.\n#311 Fix out-of-order reply in certain situations.\n#321 Return error when unknown argument is given.\nNote: The version inside Redis will be 10000 or 1.0.0 in semantic versioning. This version requires the Redis version to be 6.0 and above. ","categories":["Modules"]},{"uri":"/modules/redisgraph/release-notes/redisgraph-1.0-release-notes/","uriRel":"/modules/redisgraph/release-notes/redisgraph-1.0-release-notes/","title":"RedisGraph 1.0 release notes","tags":[],"keywords":[],"description":"Fixed memory leaks. Support ‘*’ within RETURN clause. Added TYPE function. Initial support for UNWIND clause.","content":"Requirements RedisGraph v1.0.15 requires:\nMinimum Redis compatibility version (database): 4.0.0 Minimum Redis Enterprise Software version (cluster): 5.0.0 v1.0.15 (March 2019) GraphBLAS 2.3.0 release notes WITH clause: Allows query parts to be chained together, piping the results from one to be used as starting points or criteria of the next. ref p.78 Enhancements #393 Discard distinct when performing aggregation Minor bugfixes #359 Adding ORDER BY changes the number of returned hits when used in combination with LIMIT #363 Remove graph entity property when it is set to null #386 Return updated values on queries that modify data v1.0.14 (21 February 2019) License update, REDIS SOURCE AVAILABLE LICENSE AGREEMENT.\nv1.0.13 (6 February 2019) Reuse attribute name to avoid duplication\nv1.0.12 (28 January 2019) Traverse direction optimization to reduce number of matrix transpose\nv1.0.11 (17 January 2019) Removed OpenMP requirement Traverse from multiple nodes concurrently v1.0.10 (9 January 2019) Update indices when MERGE creates new entities\nv1.0.9 (6 January 2019) Support for mixing MERGE and SET clauses Granular writer locking Fix graph serialization error in duplicate edge handling v1.0.8 (24 December 2018) Index utilization when performing cartesian product Increase usage of rm_malloc functions in module Allow serialization of NULL-valued properties Support for multiple relationship types v1.0.7 (18 December 2018) Bulk-insert support unicode Bulk-insert better progress reporting Multiple relationship types v1.0.5 (15 December 2018) Bulk insert supports ID specifying + relationships attributes\nv1.0.4 (9 December 2018) Compact GraphBLAS, using structural semiring v1.0.3 (2 December 2018) GraphBLAS 2.2 Multiple CREATE clauses Multiple MATCH clauses Bug fixes: Replace operations appropriately when rewriting execution plan Entity returned from datablock should have its internals cleared Loaded triemap strings are not guaranteed space for a null terminator v1.0.2 (25 November 2018) Bug fix #249 reset operation within execution plan should propagate upwards v1.0.1 (22 November 2018) Resolved a number of memory leaks Support \u0026lsquo;*\u0026rsquo; within RETURN clause Add TYPE function Initial support for UNWIND clause ","categories":["Modules"]},{"uri":"/modules/redisbloom/release-notes/redisbloom-1.0-release-notes/","uriRel":"/modules/redisbloom/release-notes/redisbloom-1.0-release-notes/","title":"RedisBloom 1.0 release notes","tags":[],"keywords":[],"description":"First GA release of RedisBloom.","content":"Requirements RedisBloom v1.0.3 requires:\nMinimum Redis compatibility version (database): 4.0 Minimum Redis Enterprise Software version (cluster): 5.0 v1.0.3 (December 2017) This contains a single fix, issue #19.\nFrom this version onwards, EXISTS/MEXISTS returns 0 if the (Redis) key does not exist in the database. Earlier versions returned an error.\nv1.0.2 (November 2017) This fixes a build issue (fixed s3 config in circle yaml).\nv1.0.0 (September 2017) This is the first GA release of ReBloom.\n","categories":["Modules"]},{"uri":"/ri/using-redisinsight/java-serialized-objects/","uriRel":"/ri/using-redisinsight/java-serialized-objects/","title":"View Java Serialized Objects in Redis","tags":[],"keywords":[],"description":"","content":"RedisInsight detects java serialized objects and converts them to a nicely formatted JSON object, along with the fully qualified class name.\nIt doesn\u0026rsquo;t matter what you store. Whether it is a hibernate object, or a user session or a plain old java object, RedisInsight reverse-engineers and show it to you nicely.\nJust for fun, we tried out how such an object would look without the formatting.\n","categories":["RI"]},{"uri":"/ri/using-redisinsight/troubleshooting/","uriRel":"/ri/using-redisinsight/troubleshooting/","title":"Troubleshooting RedisInsight","tags":[],"keywords":[],"description":"","content":"When RedisInsight doesn\u0026rsquo;t behave as expected, use these steps to see what the problem is.\nFor additional configuration options, such as changing the default port, go to: https://docs.redislabs.com/latest/ri/installing/configurations/\nLogs To get detailed information about errors in RedisInsight, you can review the log files with the .log extension in:\nDocker: In the /db/ directory inside the container. Mac: In the /Users/\u0026lt;your-username\u0026gt;/.redisinsight directory. Windows: In the C:\\Users\\\u0026lt;your-username\u0026gt;\\.redisinsight directory. Linux: In the /home/\u0026lt;your-username\u0026gt;/.redisinsight directory. Note: You can install RedisInsight on operating systems that are not officially supported, but it may not behave as expected. We are happy to receive your feedback at redisinsight@redis.com.\nUsing behind a reverse proxy When you configure RedisInsight to run behind a reverse proxy like NGINX:\nSince some requests can be long-running, we recommend that the request timeout is set to over 30 seconds on the reverse proxy. Hosting RedisInsight behind a prefix path (path-rewriting) is not supported at this time. ","categories":["RI"]},{"uri":"/ri/release-notes/archive/","uriRel":"/ri/release-notes/archive/","title":"RDBTools releases, 2019","tags":[],"keywords":[],"description":"Release notes for the predecessor of RedisInsight","content":"RDBTools was the predecessor to RedisInsight.\nHere are its release notes:\nVersion (Date) Release notes v0.9.42 (22 Jul 2019) RDBTools v0.9.42, 22 July 2019 v0.9.41 (4 Jul 2019) RDBTools v0.9.41, 4 July 2019 v0.9.40.1 (Jun 2019) RDBTools v0.9.40.1, June 2019 v0.9.40 (17 May 2019) RDBTools v0.9.40, 17 May 2019 v0.9.39 (8 May 2019) RDBTools v0.9.39, 8 May 2019 v0.9.38 (3 May 2019) RDBTools v0.9.38, 3 May 2019 v0.9.37 (10 Apr 2019) RDBTools v0.9.37, 10 April 2019 v0.9.36 (Mar 2019) RDBTools v0.9.36, March 2019 v0.9.35 (Feb 2019) RDBTools v0.9.35, February 2019 v0.9.34.2 (11 Feb 2019) RDBTools v0.9.34.2, 11 February 2019 v0.9.34.1 (7 Feb 2019) RDBTools v0.9.34.1, 7 February 2019 v0.9.34.0 (4 Feb 2019) RDBTools v0.9.34.0, 4 February 2019 v0.9.34.0 (23 Jan 2019) RDBTools v0.9.34.0, 23 January 2019 v0.9.32 (7 Jan 2019) RDBTools v0.9.32, 7 Jan 2019 ","categories":["RI"]},{"uri":"/_header/","uriRel":"/_header/","title":"","tags":[],"keywords":[],"description":"","content":"\n","categories":[]},{"uri":"/embeds/backup-locations/","uriRel":"/embeds/backup-locations/","title":"","tags":[],"keywords":[],"description":"","content":"FTP server Before you choose to backup to an FTP server, make sure that:\nThe RS cluster has network connectivity to the FTP server. The user that you specify in the FTP server location has read and write priviledges. To backup to an FTP server, enter the FTP server location in the format:\nftp://user:password@host\u0026lt;:custom_port\u0026gt;/path/ For example: ftp://username:password@10.1.1.1/home/backups/\nSFTP server Before you choose to backup to an SFTP server, make sure that:\nThe RS cluster has network connectivity to the SFTP server. The user that you specify in the SFTP server location has read and write priviledges. The RS server and SFTP server have the correct TLS certificates. You can select either: Use the cluster auto generated key - Go to settings and copy the Cluster SSH Public Key to the SFTP server. Use a custom key - Generate a TLS key pair for the SFTP server, copy the private key to the SSH Private Key box, and copy the public key to the location required by the SFTP server. To backup to an SFTP server, enter the SFTP server location in the format:\nsftp://user:password@host:\u0026lt;:custom_port\u0026gt;/path/ For example: sftp://username:password@10.1.1.1/home/backups/\nAWS S3 Before you choose to backup to Amazon AWS S3, make sure that you have:\nStorage location path in the format: s3://bucketname/path/ Access key ID Secret access key You can also connect to a storage service that uses the S3 protocol but is not hosted by Amazon AWS. The storage service must have a valid SSL certificate. To connect to an S3-compatible storage location, run: rladmin cluster config s3_url \u0026lt;url\u0026gt;\nLocal mount point Before you choose to backup to a local mount point, make sure that:\nThe node has network connectivity to the destination server of the mount point. The redislabs:redislabs user has read and write priviledges on the local mount point and on the destination server. The backup location has enough disk space for your backup files. The backup files are saved with filenames that include the timestamp so that backup files are not overwritten. To backup to a local mount point for a node:\nOn each node in the cluster, create the mount point:\nConnect to the terminal of the RS server that the node is running on.\nMount the remote storage to a local mount point.\nFor example:\nsudo mount -t nfs 192.168.10.204:/DataVolume/Public /mnt/Public In the path for the backup location, enter the mount point.\nFor example: /mnt/Public\nAzure Blob Storage Before you choose to backup to Azure Blob Storage, make sure that you have:\nStorage location path in the format: /container_name/[path/]/ Account name An authentication token, either an account key or an Azure shared access signature (SAS). Azure SAS support requires Redis Software version 6.0.20. To learn more about Azure SAS, see Grant limited access to Azure Storage resources using shared access signatures.\nGoogle Cloud Storage Before you choose to backup to Google Cloud Storage, make sure that you have:\nStorage location path in the format: /bucket_name/[path/]/ Client ID Client email Private key ID Private key Note: You can find the client and key details in your service account in the GCP console (API \u0026amp; Services \u0026gt; Credentials \u0026gt; Create Credentials).\nMake sure that the service account has the Storage Legacy Bucket Writer permission on the target bucket. Make sure that the bucket doesn\u0026rsquo;t use a retention policy because it can interfere with the process. The format of the private key from the downloaded JSON is in a single string where new lines are marked with \\n characters. When you paste the key into the RS admin console, replace each \\n character with a new line. ","categories":[]},{"uri":"/embeds/cluster-dns-embed/","uriRel":"/embeds/cluster-dns-embed/","title":"","tags":[],"keywords":[],"description":"","content":"By default, Redis Enterprise Software deployments use DNS to communicate between nodes. You can also use the Discovery Service, which uses IP addresses to connect and complies with the Redis Sentinel API supported by open source Redis.\nEach node in a Redis Enterprise cluster includes a small DNS server to manage internal functions, such as high availability, automatic failover, automatic migration, and so on. Nodes should only run the DNS server included with the software. Running additional DNS servers can lead to unexpected behavior.\nCluster name and connection management Whether you\u0026rsquo;re administering Redis Enterprise Software or accessing databases, there are two ways to connect:\nURL-based connections - URL-based connections use DNS to resolve the fully qualified cluster domain name (FQDN). This means that DNS records might need to be updated when topology changes, such as adding (or removing) nodes from the cluster.\nBecause apps and other clients connections rely on the URL (rather than the address), they do not need to be modified when topology changes.\nIP-based connections - IP-based connections do not require DNS setup, as they rely on the underlying TCP/IP addresses. As long as topology changes do not change the address of the cluster nodes, no configuration changes are needed, DNS or otherwise.\nHowever, changes to IP addresses (or changes to IP address access) impact all connections to the node, including apps and clients. IP address changes can therefore be unpredictable or time-consuming.\nURL-based connections The fully qualified domain name (FQDN) is the unique cluster identifier that enables clients to connect to the different components of Redis Enterprise Software. The FQDN is a crucial component of the high-availability mechanism because it\u0026rsquo;s used internally to enable and implement automatic and transparent failover of nodes, databases shards, and endpoints.\nNote: Setting the cluster\u0026rsquo;s FQDN is a one-time operation, one that cannot be changed after being set. The FQDN must always comply with the IETF\u0026rsquo;s RFC 952 standard and section 2.1 of the RFC 1123 standard.\nIdentify the cluster To identify the cluster, either use DNS to define a fully qualified domain name or use the IP addresses of each node.\nDefine domain using DNS Use DNS if you:\nhave your own domain want to integrate the cluster into that domain can access and update the DNS records for that domain Make sure that the cluster and at least one node (preferably all nodes) in the cluster are correctly configured in the DNS with the appropriate NS entries.\nFor example:\nYour domain is: mydomain.com You would like to name the Redis Enterprise Software cluster mycluster You have three nodes in the cluster: node1 (IP address 1.1.1.1) node2 (2.2.2.2) node3 (3.3.3.3) In the FQDN field, enter the value mycluster.mydomain.com and add the following records in the DNS table for mydomain.com:\nmycluster.mydomain.com NS node1.mycluster.mydomain.com node2.mycluster.mydomain.com node3.mycluster.mydomain.com node1.mycluster.mydomain.com A 1.1.1.1 node2.mycluster.mydomain.com A 2.2.2.2 node3.mycluster.mydomain.com A 3.3.3.3 Zero-configuration using mDNS Development and test environments can use Multicast DNS (mDNS), a zero-configuration service designed for small networks. Production environments should not use mDNS.\nmDNS is a standard protocol that provides DNS-like name resolution and service discovery capabilities to machines on local networks with minimal to no configuration.\nBefore adopting mDNS, verify that it\u0026rsquo;s supported by each client you wish to use to connect to your Redis databases. Also make sure that your network infrastructure permits mDNS/multi-casting between clients and cluster nodes.\nConfiguring the cluster to support mDNS requires you to assign the cluster a .local name.\nFor example, if you want to name the Redis Enterprise Software cluster rediscluster, specify the FQDN name as rediscluster.local.\nWhen using the DNS or mDNS option, failover can be done transparently and the DNS is updated automatically to point to the IP address of the new primary node.\nIP-based connections When you use the IP-based connection option, the FQDN does not need to have any special format because clients use IP addresses instead of hostnames to access the databases so you are free to choose whatever name you want. Using the IP-based connection option does not require any DNS configuration either.\nTo administer the cluster you do need to know the IP address of at least one of the nodes in the cluster. Once you have the IP address, you can simply connect to port number 8443 (for example: https://10.0.0.12:8443). However, as the topology of the cluster changes and node with the given IP address is removed, you need to remember the IP address of another node participating in this cluster to connect to the admin console and manage the cluster.\nApplications connecting to Redis Software databases have the same constraints. When using the IP-based connection method, you can use the Discovery Service to discover the database endpoint for a given database name as long as you have an IP address for at least one of the nodes in the cluster. The API used for discovery service is compliant with the Redis Sentinel API.\nTo test your connection, try pinging the service. For help, see Connect to your database.\n","categories":[]},{"uri":"/embeds/cluster-setup/","uriRel":"/embeds/cluster-setup/","title":"","tags":[],"keywords":[],"description":"","content":" In the web browser on the host machine, go to https://localhost:8443 to see the Redis Enterprise Software admin console.\nNote: If your browser displays a certificate error, you can safely proceed. If the server does not show the login screen, try again after a few minutes. Choose Setup to begin configuring the node.\nIn the Node Configuration settings, enter a cluster FQDN such as `cluster.local\u0026rsquo; and then select Next.\nIf you have a license key, enter it and then select Next.\nIf you do not have a license key, a trial version is installed.\nEnter an email and password for the administrator account.\nThese credentials are also used for connections to the REST API.\nSelect OK to acknowledge the replacement of the HTTPS TLS certificate on the node. If you receive a browser warning, you can proceed safely.\n","categories":[]},{"uri":"/embeds/compatible-with-oss/","uriRel":"/embeds/compatible-with-oss/","title":"","tags":[],"keywords":[],"description":"","content":"Yes we are. Not only are we are the home of Redis, but most of the core engineers on open source Redis also work here. We contribute extensively to the open source Redis project. As a rule, we adhere to the open source’s specifications and make every effort to update our service with its latest versions.\nThat said, the following Redis features are not applicable in the context of our service:\nShared databases aren’t supported in our service given their potential negative impact on performance. We recommend using dedicated databases instead (read this post for more information). Therefore, the following commands are blocked and show an error when used: MOVE SELECT Data persistence and backups are managed from the service’s web interface, so the following commands are blocked: BGREWRITEAOF BGSAVE LASTSAVE SAVE Since replication is managed automatically by the service and since it could present a security risk, the following commands are blocked: MIGRATE REPLCONF SLAVEOF SYNC/PSYNC Redis Enterprise clustering technology is different than the open source Redis Cluster and supports clustering in a seamless manner that works with all standard Redis clients. As a result, all Cluster related commands are blocked and show an error when used. Redis Enterprise clustering technology allows multiple active proxies. As a result, the CLIENT ID command cannot guarantee incremental IDs between clients who connect to different nodes under multi proxy policies. Commands that aren’t relevant for a hosted Redis service are blocked: CONFIG RESETSTAT DEBUG OBJECT/SEGFAULT OBJECT SHUTDOWN CLIENT PAUSE COMMAND INFO COMMAND COUNT COMMAND GETKEYS LATENCY LATEST LATENCY HISTORY LATENCY RESET LATENCY GRAPH LATENCY DOCTOR MEMORY STATS MEMORY DOCTOR MEMORY MALLOC-STATS MEMORY PURGE MODULE LOAD MODULE UNLOAD MODULE LIST Additionally, only a subset of Redis’ configuration settings (via CONFIG GET/SET) is applicable to Redis Cloud. Attempts to get or set a configuration parameter that isn’t included in the following list show an error when used: hash-max-ziplist-entries hash-max-ziplist-value list-max-ziplist-entries list-max-ziplist-value notify-keyspace-events set-max-intset-entries slowlog-log-slower-than (value must be larger than 1000) slowlog-max-len (value must be between 128 and 1024) zset-max-ziplist-entries zset-max-ziplist-value Lastly, unlike Redis’ 512MB limit, the maximum size of key names in our service is 64KB (key values, however, can have sizes up to 512MB). ","categories":[]},{"uri":"/embeds/create-db/","uriRel":"/embeds/create-db/","title":"","tags":[],"keywords":[],"description":"","content":" In your web browser, open the admin console of the cluster that you want to connect to in order to create the { { \u0026lt; field \u0026ldquo;db_type\u0026rdquo; \u0026gt; } }. By default, the address is: `https://\u0026lt;RS_address\u0026gt;:8443` In databases, click .\nIf you do not have any databases on the node, you are prompted to create a database.\n","categories":[]},{"uri":"/embeds/create-subscription-next-steps/","uriRel":"/embeds/create-subscription-next-steps/","title":"","tags":[],"keywords":[],"description":"","content":"The subscription shows a \u0026ldquo;Pending\u0026rdquo; status and takes approximately 10 to 15 minutes to provision. You receive an email when your subscription is ready to use.\nNext steps We recommend that you set up VPC peering with your application VPC. VPC peering lets you route traffic between your VPCs using private IP addresses for improved security and performance.\nYou can also edit these subscription settings after the subscription is created:\nSubscription name Payment information ","categories":[]},{"uri":"/embeds/discovery-clients/","uriRel":"/embeds/discovery-clients/","title":"","tags":[],"keywords":[],"description":"","content":" Redis-py (Python redis client) HiRedis (C redis client) Jedis (Java redis client) Ioredis (NodeJS redis client) If you need to use another client, consider using Sentinel Tunnel to discover the current Redis master with Sentinel and create a TCP tunnel between a local port on the client and the master.\n","categories":[]},{"uri":"/embeds/docker-memory-limitation/","uriRel":"/embeds/docker-memory-limitation/","title":"","tags":[],"keywords":[],"description":"","content":"If you cannot activate the database because of a memory limitation, make sure that Docker has enough memory allocated in the Docker Settings.\n","categories":[]},{"uri":"/embeds/hardware-requirements-embed/","uriRel":"/embeds/hardware-requirements-embed/","title":"","tags":[],"keywords":[],"description":"","content":"The hardware requirements for Redis Enterprise Software are different for development and production environments.\nIn a development environment, you can test your application with a live database.\nIf you want to test your application under production conditions, use the production environment requirements.\nIn a production environment you must have enough resources to handle the load on the database and recover from failures.\nDevelopment environment You can build your development environment with non-production hardware, such as a laptop, desktop, or small VM or instance, and with these hardware requirements:\nItem Description Minimum Requirements Recommended Nodes per cluster You can install on one node but many features require at least two nodes. 1 node \u0026gt;= 2 nodes RAM per node The amount of RAM for each node. 4GB \u0026gt;= 10GB Storage per node The amount of storage space for each node. 10GB \u0026gt;= 20GB Production environment We recommend these hardware requirements for production systems or for development systems that are designed to demonstrate production use cases:\nItem Description Minimum Requirements Recommended Nodes per cluster* At least three nodes are required to support a reliable, highly available deployment that handles process failure, node failure, and network split events in a consistent manner. 3 nodes \u0026gt;= 3 nodes (Must be an odd number of nodes) Cores* per node Redis Enterprise Software is based on a multi-tenant architecture and can run multiple Redis processes (or shards) on the same core without significant performance degradation. 4 cores \u0026gt;=8 cores RAM* per node Defining your RAM size must be part of the capacity planning for your Redis usage. 15GB \u0026gt;=30GB Ephemeral Storage Used for storing replication files (RDB format) and cluster log files. RAM x 2 \u0026gt;= RAM x 4 Persistent Storage Used for storing snapshot (RDB format) and AOF files over a persistent storage media, such as AWS Elastic Block Storage (EBS) or Azure Data Disk. RAM x 3 In-memory \u0026gt;= RAM x 6 (except for extreme \u0026lsquo;write\u0026rsquo; scenarios); Redis on Flash \u0026gt;= (RAM + Flash) x 5. Network We recommend using multiple NICs per node where each NIC is \u0026gt;100Mbps, but Redis Enterprise Software can also run over a single 1Gbps interface network used for processing application requests, inter-cluster communication, and storage access. 1G \u0026gt;=10G *Additional considerations:\nNodes per Cluster:\nTo ensure synchronization and consistency, Active-Active deployments with three node clusters are strongly discouraged from using quorum nodes. Because quorum nodes do not store data shards, they cannot support replication. In case of a node failure, replica shards aren\u0026rsquo;t available for Active-Active synchronization. Cores:\nWhen the CPU load reaches a certain level, Redis Enterprise Software sends an alert to the operator.\nIf your application is designed to put a lot of load on your Redis database, make sure that you have at least one available core for each shard of your database.\nIf some of the cluster nodes are utilizing more than 80% of the CPU, consider migrating busy resources to less busy nodes.\nIf all the cluster nodes are utilizing over 80% of the CPU, consider scaling out the cluster by adding a node.\nRAM:\nRedis uses a relatively large number of buffers, which enable replica communication, client communication, pub/sub commands, and more. As a result, you should ensure that 30% of the RAM is available on each node at any given time.\nIf one or more cluster nodes utilizes more than 65% of the RAM, consider migrating resources to less active nodes.\nIf all cluster nodes are utilizing more than 70% of available RAM, consider adding a node.\nDo not run any other memory-intensive processes on the Redis Software node.\n","categories":[]},{"uri":"/embeds/how-many-databases-cloud/","uriRel":"/embeds/how-many-databases-cloud/","title":"","tags":[],"keywords":[],"description":"","content":"Each subscription plan (except for our free plans) enables multiple dedicated databases, each running in a dedicated process and in a non-blocking manner.\nA 1GB plan for example, enables 16 dedicated databases.\n","categories":[]},{"uri":"/embeds/how-many-databases-software/","uriRel":"/embeds/how-many-databases-software/","title":"","tags":[],"keywords":[],"description":"","content":"The number of databases is unlimited. The limiting factor is the available memory in the cluster, and the number of shards in the subscription.\nNote that the impact of the specific database configuration on the number of shards it consumes. For example:\nEnabling database replication, without enabling database clustering, creates two shards: a master shard and a replica shard. Enabling database clustering creates as many database shards as you configure. Enabling both database replication and database clustering creates double the number of database shards you configure. ","categories":[]},{"uri":"/embeds/oss-cluster-api-intro/","uriRel":"/embeds/oss-cluster-api-intro/","title":"","tags":[],"keywords":[],"description":"","content":"Redis OSS Cluster API reduces access times and latency with near-linear scalability. The Redis OSS Cluster API provides a simple mechanism for Redis clients to know the cluster topology.\nClients must first connect to the master node to get the cluster topology, and then they connect directly to the Redis proxy on each node that hosts a master shard.\nNote: You must use a client that supports the OSS cluster API to connect to a database that has the OSS cluster API enabled. ","categories":[]},{"uri":"/embeds/port-configurations-embed/","uriRel":"/embeds/port-configurations-embed/","title":"","tags":[],"keywords":[],"description":"","content":"To make sure that Redis Enterprise Software (RS) servers can pass necessary communications between them, we recommend that all RS servers have all of the ports listed here open between them.\nBy default, the cluster assigns ports in the range of 10,000 - 19,999 to database endpoints. If you assign a specific port for a database when you create it, even outside of this range, the cluster only verifies that the assigned port is not already in use. You must manually update your firewall with the port for that new database endpoint.\nPorts and port ranges used by Redis Enterprise Software Protocol Port Connection Source Description ICMP * Internal For connectivity checking between nodes TCP 1968 Internal Proxy traffic TCP 3333, 3334, 3335, 3336, 3337, 3338, 3339, 36379, 36380 Internal Cluster traffic TCP 8001 Internal, External Traffic from application to RS Discovery Service TCP 8002, 8004 Internal System health monitoring TCP 8443 Internal, External Secure (HTTPS) access to the management web UI TCP 8444, 9080 Internal For nginx \u0026lt;-\u0026gt; cnm_http/cm traffic TCP 9081 Internal, Active-Active For Active-Active management TCP 8070, 8071 Internal, External For metrics exported and managed by nginx TCP 9443 (Recommended), 8080 Internal, External, Active-Active REST API traffic, including cluster management and node bootstrap TCP 10000-19999 Internal, External, Active-Active Database traffic TCP 20000-29999 Internal Database shard traffic UDP 53, 5353 Internal, External DNS/mDNS traffic Connection sources are:\nInternal - The traffic is from other cluster nodes External - The traffic is from client applications or external monitoring resources Active-Active - The traffic is from clusters that host Active-Active databases Changing the management web UI port If for any reason you want to use a custom port for the RS Web UI instead of the default port (8443), you can change the port. Before you change the RS Web UI port, make sure that the new port is not in use by another process.\nNote: After you change the RS Web UI port, when you add a new node to the cluster you must connect to the web UI with the custom port number:\nhttps://newnode.mycluster.example.com:\u0026lt;nonstandard-port-number\u0026gt;\nTo change the default port for the RS Web UI, on any node in the cluster run:\nrladmin cluster config cm_port \u0026lt;new-port\u0026gt; Disabling HTTP support for API endpoints To harden deployments, you can disable the HTTP support for API endpoints that is supported by default. Before you disable HTTP support, make sure that you migrate any scripts or proxy configurations that use HTTP to the encrypted API endpoint to prevent broken connections. After you disable HTTP support, traffic sent to the unencrypted API endpoint is blocked.\nTo disable HTTP support for API endpoints, run:\nrladmin cluster config http_support disabled ","categories":[]},{"uri":"/embeds/reset-password/","uriRel":"/embeds/reset-password/","title":"","tags":[],"keywords":[],"description":"","content":"To reset a user password from the CLI, run:\nrladmin cluster reset_password \u0026lt;username\u0026gt;\nYou are asked to enter and confirm the new password.\n","categories":[]},{"uri":"/embeds/sample/","uriRel":"/embeds/sample/","title":"","tags":[],"keywords":[],"description":"","content":" heading 1 heading 2 heading 3 cell 1x1 cell 1x2 cell 1x3 cell 2x1 cell 2x2 cell 2x3 Example Shortcode Inside an embedded markdown file. Tip - Another example of shortcode inside an embedded file. ","categories":[]},{"uri":"/embeds/shard-placement-intro/","uriRel":"/embeds/shard-placement-intro/","title":"","tags":[],"keywords":[],"description":"","content":"In addition to the shard placement policy, considerations that determine shard placement are:\nSeparation of master and replica shards Available persistence and Redis on Flash (RoF) storage Rack awareness Memory available to host the database when fully populated The shard placement policies are:\ndense - Place as many shards as possible on the smallest number of nodes to reduce the latency between the proxy and the database shards; Recommended for Redis on RAM databases to optimize memory resources sparse - Spread the shards across as many nodes in the cluster as possible to spread the traffic across cluster nodes; Recommended for Redis on Flash databases to optimize disk resources When you create a Redis Enterprise Software cluster, the default shard placement policy (dense) is assigned to all databases that you create on the cluster.\nYou can:\nChange the default shard placement policy for the cluster to sparse so that the cluster applies that policy to all databases that you create Change the shard placement policy for each database after the database is created ","categories":[]},{"uri":"/embeds/supported-platforms-embed/","uriRel":"/embeds/supported-platforms-embed/","title":"","tags":[],"keywords":[],"description":"","content":"Redis Enterprise Software is supported on several operating systems, cloud environments, and virtual environments.\nSystem requirements Make sure your system meets these requirements:\nOnly 64-bit operating systems are supported. You must install Redis Enterprise Software directly on the host, not through system cloning. You must install on a clean host with no other applications running so that all RAM is allocated to the operating system and Redis Enterprise Software only. Linux distributions must be installed with at least \u0026ldquo;Minimal Install\u0026rdquo; configuration. Supported platforms Platform Versions/Information Ubuntu 16.04 (deprecated), 18.04\nServer version is recommended for production installations. Desktop version is only recommended for development deployments. Red Hat Enterprise Linux (RHEL) 7, CentOS 7 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9\nRequires OpenSSL 1.0.2 and firewall configuration RHEL 8, CentOS 8 8.0, 8.1, 8.2, 8.3, 8.4, 8.5, and 8.6 Oracle Linux 7, Oracle Linux 8 Based on the corresponding RHEL version Rocky Linux 8 Based on RHEL 8 Amazon Linux Version 1 Docker Docker images of Redis Enterprise Software are certified for development and testing only. Kubernetes See the Redis Enterprise for Kubernetes documentation Operating system limitations Be aware that Redis Enterprise Software relies on certain components that require support from the operating system. You cannot enable support for components, services, protocols, or versions that aren\u0026rsquo;t supported by the operating system running Redis Enterprise Software. In addition, updates to the operating system or to Redis Enterprise Software can impact component support.\nTo illustrate, version 6.2.8 of Redis Enterprise Software removed support for TLS 1.0 and TLS 1.1 on Red Hat Enterprise Linux 8 (RHEL 8) because that operating system does not enable support for these versions by default.\nIf you have trouble enabling specific components, features, or versions, verify that they\u0026rsquo;re supported by your operating system and that they\u0026rsquo;re configured correctly.\nUpgrade RHEL when using modules RHEL 7 clusters cannot be directly upgraded to RHEL 8 when hosting databases using modules. Due to binary differences in modules between the two operating systems, you cannot directly update RHEL 7 clusters to RHEL 8 when those clusters host databases using modules. Instead, you need to create a new cluster on RHEL 8 and then migrate existing data from your RHEL 7 cluster. This does not apply to clusters that do not use modules.\nVMware Redis Enterprise is compatible with VMware, but make sure that you:\nConfigure your memory, CPU, network, and storage settings to allow for optimal Redis Enterprise performance. Pin each Redis Enterprise shard to a specific ESX/ESXi host by setting the appropriate affinity rules. If you must manually migrate a virtual machine to another host, follow the best practices for shard maintenance and contact support if you have questions. Disable VMware VMotion because Redis Enterprise is not compatible with VMotion. Don\u0026rsquo;t use VMware snapshots because Redis Enterprise cluster manages state dynamically, so a snapshot might not have the correct node and cluster state. ","categories":[]},{"uri":"/embeds/tls-configuration-procedure/","uriRel":"/embeds/tls-configuration-procedure/","title":"","tags":[],"keywords":[],"description":"","content":"To encrypt Replica Of synchronization traffic, you must also configure encryption for the replica database (the destination).\nEncrypt source synchronization traffic To enable TLS for Replica Of communication only on the source database:\nIn databases, either:\nCreate a new database. Select a database to configure and then select Edit. Enable TLS.\nSelect the communication that you want to secure:\nFor a new database - Require TLS for Replica Of communications only is selected by default.\nFor an existing database that is configured to Require TLS for all communications - Select Require TLS for Replica Of communications only.\nBy default, client authentication is enforced. This means you must enter the syncer certificates of the clusters hosting the replicas (the destination databases).\nTo enter the syncer certificates:\nCopy the syncer certificates for each cluster with a destination database:\nSign in to the cluster. Go to Settings. In the syncer certificates box, copy the full text of the certificate to the Clipboard. Select the Add button to open the certificate dialog.\nEnter the copied certificate text into the text box below the Enforce client authentication checkbox.\nUse the Save button to save the certificates.\nYou can also clear Enforce client authentication so that all clusters or clients can connect to your database without authentication.\nTo encrypt Replica Of synchronization traffic, you must also configure encryption for the replica database (the destination).\nEncrypt all source communication To enable TLS for Replica Of and client communication on the source database:\nFrom the Databases menu of the admin console, either:\nCreate a new database.\nSelect an existing database and then select the Edit button.\nEnable TLS and select Require TLS for all communications.\nBy default, client authentication is enforced so you must enter the syncer certificates of the clusters that host the destination databases.\nYou also need to add the certificates of the clients that connect to the database.\nTo enter the syncer and client certificates:\nCopy the entire text of the syncer and client certificates.\nFor each cluster with a destination database:\nSign in to the cluster. Go to Settings. In the syncer certificates box, copy the full text of the certificate to the Clipboard. Use the Add button to open the certificate dialog.\nEnter the copied certificate text into the text box below the Enforce client authentication checkbox.\nUse the Save button to save your changes.\nYou can also clear the Enforce client authentication checkbox to allow client connections without authentication.\n","categories":[]},{"uri":"/embeds/tryout-redisai/","uriRel":"/embeds/tryout-redisai/","title":"","tags":[],"keywords":[],"description":"","content":"Getting started You can connect to RedisAI using any Redis client. Better yet, some languages already have client implementations for RedisAI - the list can be found at the Clients page. RedisAI clients wrap the core API and simplify the interaction with the module.\nWe\u0026rsquo;ll begin by using the official redis-cli Redis client. If you have it locally installed feel free to use that, but it is also available from the container:\nredis-cli Using RedisAI tensors A tensor is an n-dimensional array and is the standard representation for data in DL/ML workloads. RedisAI adds to Redis a Tensor data structure that implements the tensor type. Like any datum in Redis, RedisAI\u0026rsquo;s Tensors are identified by key names.\nCreating new RedisAI tensors is done with the AI.TENSORSET command. For example, consider the tensor:\nWe can create the RedisAI Tensor with the key name \u0026rsquo;tA\u0026rsquo; with the following command:\nAI.TENSORSET tA FLOAT 2 VALUES 2 3 Copy the command to your cli and hit the \u0026lt;ENTER\u0026gt; on your keyboard to execute it. It should look as follows:\n$ redis-cli 127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK The reply \u0026lsquo;OK\u0026rsquo; means that the operation was successful. We\u0026rsquo;ve called the AI.TENSORSET command to set the key named \u0026rsquo;tA\u0026rsquo; with the tensor\u0026rsquo;s data, but the name could have been any string value. The FLOAT argument specifies the type of values that the tensor stores, and in this case a single-precision floating-point. After the type argument comes the tensor\u0026rsquo;s shape as a list of its dimensions, or just a single dimension of 2.\nThe VALUES argument tells RedisAI that the tensor\u0026rsquo;s data will be given as a sequence of numeric values and in this case the numbers 2 and 3. This is useful for development purposes and creating small tensors, however for practical purposes the AI.TENSORSET command also supports importing data in binary format.\nThe Redis key \u0026rsquo;tA\u0026rsquo; now stores a RedisAI Tensor. We can verify that using standard Redis commands such as EXISTS and TYPE:\n127.0.0.1:6379\u0026gt; EXISTS tA (integer) 1 127.0.0.1:6379\u0026gt; TYPE tA AI_TENSOR Using AI.TENSORSET with the same key name, as long as it already stores a RedisAI Tensor, will overwrite the existing data with the new. To delete a RedisAI tensor, use the Redis DEL command.\nRedisAI Tensors are used as inputs and outputs in the execution of models and scripts. For reading the data from a RedisAI Tensor value there is the AI.TENSORGET command:\n127.0.0.1:6379\u0026gt; AI.TENSORGET tA VALUES 1) INT8 2) 1) (integer) 2 3) 1) (integer) 2 1) (integer) 3 Loading models A Model is a Deep Learning or Machine Learning frozen graph that was generated by some framework. The RedisAI Model data structure represents a DL/ML model that is stored in the database and can be run.\nModels, like any other Redis and RedisAI data structures, are identified by keys. A Model\u0026rsquo;s key is created using the AI.MODELSET command and requires the graph payload serialized as protobuf for input.\nIn our examples, we\u0026rsquo;ll use one of the graphs that RedisAI uses in its tests, namely \u0026lsquo;graph.pb\u0026rsquo;, which can be downloaded from here. This graph was created using TensorFlow with this script.\nUse a web browser or the command line to download \u0026lsquo;graph.pb\u0026rsquo;:\nwget https://github.com/RedisAI/RedisAI/raw/master/test/test_data/graph.pb You can view the computation graph using Netron, which supports all frameworks supported by RedisAI.\nThis is a great way to inspect a graph and find out node names for inputs and outputs.\nredis-cli doesn\u0026rsquo;t provide a way to read files\u0026rsquo; contents, so to load the model with it we\u0026rsquo;ll use the command line and output pipes:\n$ cat graph.pb | redis-cli -x \\ AI.MODELSET mymodel TF CPU INPUTS a b OUTPUTS c OK Note: For practical purposes, you are encouraged to use a programmatic Redis or RedisAI client in the language of your choice for interacting with RedisAI. Refer to the following pages for further information:\nRedis clients page RedisAI clients page Like most commands, AI.MODELSET\u0026rsquo;s first argument is a key\u0026rsquo;s name, which is \u0026lsquo;mymodel\u0026rsquo; in the example. The next two arguments are the model\u0026rsquo;s DL/ML backend and the device it will be executed on. \u0026lsquo;graph.pb\u0026rsquo; in the example is a TensorFlow graph and is denoted by TF argument. The model will be executed on the CPU as instructed by the CPU argument.\nTensorFlow models also require declaring the names of their inputs and outputs. The inputs for \u0026lsquo;graph.pb\u0026rsquo; are called \u0026lsquo;a\u0026rsquo; and \u0026lsquo;b\u0026rsquo;, whereas its single output is called \u0026lsquo;c\u0026rsquo;. These names are provided as additional arguments after the \u0026lsquo;INPUTS\u0026rsquo; and \u0026lsquo;OUTPUTS\u0026rsquo; arguments, respectively.\nRunning models Once a RedisAI Model key has been set with AI.MODELSET it can be run with any Tensor keys from the database as its input. The model\u0026rsquo;s output, after it was executed, is stored in RedisAI Tensors as well.\nThe model stored at \u0026lsquo;mymodel\u0026rsquo; expects two input tensors so we\u0026rsquo;ll use the previously-created \u0026rsquo;tA\u0026rsquo; and create another input tensor:\nwith the following command:\nAI.TENSORSET tB FLOAT 2 VALUES 3 5 The model can now be run with the AI.MODELRUN command as follows:\nAI.MODELRUN mymodel INPUTS tA tB OUTPUTS tResult For example:\n127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK 127.0.0.1:6379\u0026gt; AI.TENSORSET tB FLOAT 2 VALUES 3 5 OK 127.0.0.1:6379\u0026gt; AI.MODELRUN mymodel INPUTS tA tB OUTPUTS tModel OK The first argument to AI.MODELRUN is the name of the key at which the RedisAI Model is stored. The names of RedisAI Tensor keys that follow the INPUTS argument are used as input for the model. Similarly, following the OUTPUT argument are the key names of RedisAI Tensors that the model outputs.\nThe inputs for the example are the tensors stored under the \u0026rsquo;tA\u0026rsquo; and \u0026rsquo;tB\u0026rsquo; keys. Once the model\u0026rsquo;s run had finished, a new RedisAI Tensor key called \u0026rsquo;tResult\u0026rsquo; is created and stores the model\u0026rsquo;s output.\nFor example:\n127.0.0.1:6379\u0026gt; AI.TENSORGET tModel VALUES 1) FLOAT 2) 1) (integer) 2 3) 1) \u0026#34;6\u0026#34; 2) \u0026#34;15\u0026#34; The model we\u0026rsquo;ve imported from \u0026lsquo;graph.pb\u0026rsquo; takes two input tensors as input and outputs a tensor that is the product of multiplying them. In the case of the example above it looks like this:\nModel management The AI.MODELGET command can be used for retrieving information about a model and its serialized blob. The AI.INFO command shows runtime statistics about the model\u0026rsquo;s runs. Lastly, RedisAI Model keys can be deleted with the AI.MODELDEL command.\nScripting RedisAI makes it possible to run TorchScript with the PyTorch backend. Scripts are useful for performing pre- and post-processing operations on tensors.\nThe RedisAI Script data structure is managed via a set of dedicated commands, similarly to the models. A RedisAI Script key is:\nCreated with the AI.SCRIPTSET command Run with the AI.SCRIPTRUN command Deleted with the AI.SCRIPTSEL command We can create a RedisAI Script that performs the same computation as the \u0026lsquo;graph.pb\u0026rsquo; model. The script can look like this:\ndef multiply(a, b): return a * b Assuming that the script is stored in the \u0026lsquo;myscript.py\u0026rsquo; file it can be uploaded via command line and the AI.SCRIPTSET command as follows:\ncat myscript.py | redis-cli -x AI.SCRIPTSET myscript CPU This will store the PyTorch Script from \u0026lsquo;myscript.py\u0026rsquo; under the \u0026lsquo;myscript\u0026rsquo; key and will associate it with the CPU device for execution. Once loaded, the script can be run with the following:\nAI.SCRIPTRUN myscript multiply INPUTS tA tB OUTPUTS tScript For example:\n127.0.0.1:6379\u0026gt; AI.TENSORSET tA FLOAT 2 VALUES 2 3 OK 127.0.0.1:6379\u0026gt; AI.TENSORSET tB FLOAT 2 VALUES 3 5 OK 127.0.0.1:6379\u0026gt; AI.SCRIPTRUN myscript multiply INPUTS tA tB OUTPUTS tScript OK 127.0.0.1:6379\u0026gt; AI.TENSORGET tScript VALUES 1) FLOAT 2) 1) (integer) 2 3) 1) \u0026#34;6\u0026#34; 2) \u0026#34;15\u0026#34; ","categories":[]},{"uri":"/embeds/tryout-redisbloom/","uriRel":"/embeds/tryout-redisbloom/","title":"","tags":[],"keywords":[],"description":"","content":"Trying it out You can play with it a bit using redis-cli:\nConnect to redis.\n$ redis-cli -p 12543 127.0.0.1:12543\u0026gt; Run these commands:\n127.0.0.1:12543\u0026gt; BF.ADD bloom kirk 1) (integer) 1 127.0.0.1:12543\u0026gt; BF.ADD bloom redis 1) (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom kirk (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom redis (integer) 1 127.0.0.1:12543\u0026gt; BF.EXISTS bloom nonexist (integer) 0 127.0.0.1:12543\u0026gt; BF.EXISTS bloom que? (integer) 0 127.0.0.1:12543\u0026gt; 127.0.0.1:12543\u0026gt; BF.MADD bloom elem1 elem2 elem3 1) (integer) 1 2) (integer) 1 3) (integer) 1 127.0.0.1:12543\u0026gt; BF.MEXISTS bloom elem1 elem2 elem3 1) (integer) 1 2) (integer) 1 3) (integer) 1 You can also create a custom Bloom filter. The BF.ADD command creates a new Bloom filter suitable for a small-ish number of items. This consumes less memory but may not be ideal for large filters. In that case:\n127.0.0.1:12543\u0026gt; BF.RESERVE largebloom 0.0001 1000000 OK 127.0.0.1:12543\u0026gt; BF.ADD largebloom kirk 1) (integer) 1 Using Cuckoo filters in Redis Enterprise Software Cuckoo filters can also be used as part of the RedisBloom module. You can play with it using redis-cli:\n127.0.0.1:12543\u0026gt; CF.ADD newcuckoo redis (integer) 1 127.0.0.1:12543\u0026gt; CF.EXISTS newcuckoo redis (integer) 1 127.0.0.1:12543\u0026gt; CF.EXISTS newcuckoo notpresent (integer) 0 127.0.0.1:12543\u0026gt; CF.DEL newcuckoo redis (integer) 1 ","categories":[]},{"uri":"/embeds/tryout-redisearch-2/","uriRel":"/embeds/tryout-redisearch-2/","title":"","tags":[],"keywords":[],"description":"","content":"Creating indexes Let\u0026rsquo;s create a new index called \u0026ldquo;database_idx\u0026rdquo;. When you define the index, you must pass in the structure of the data you are adding to the index. In this example, we have four fields: title (TEXT), body (TEXT), url (TEXT) and visits (NUMERIC), with the title field having a higher weight than the others (5.0).\nConnect to Redis (replace 12543 with the port number your RediSearch is running on).\n$ redis-cli -p 12543 127.0.0.1:12543\u0026gt; Create the schema:\n127.0.0.1:12543\u0026gt; FT.CREATE database_idx PREFIX 1 \u0026#34;doc:\u0026#34; SCORE 0.5 SCORE_FIELD \u0026#34;doc_score\u0026#34; SCHEMA title TEXT body TEXT url TEXT visits NUMERIC This command indexes all of the hashes with the prefix \u0026ldquo;doc:\u0026rdquo; as well as all the future ones that will be created with that prefix.\nBy default, all hashes get a score of 1.0, but we can configure that value through the SCORE directive. If we need to assign a document-specific score to documents, we can do that too. We only need to specify the name of the document element in which we will define the document score which will override the default. This is done by using the SCORE_FIELD directive.\nIn our example the default document score is 0.5 and we can have a document-specific score by setting a value between 0 and 1 in the \u0026ldquo;doc_score\u0026rdquo; field of the hash.\nTesting the index Now add some data to this index. Here we add a hash with the key \u0026ldquo;doc:1\u0026rdquo; and the fields:\ntitle: \u0026ldquo;Redis\u0026rdquo; body: \u0026ldquo;Primary and caching\u0026rdquo; url: https://redislabs.com/primary-caching visits: 108 127.0.0.1:12543\u0026gt; HSET doc:1 title \u0026#34;Redis\u0026#34; body \u0026#34;Primary and caching\u0026#34; url \u0026#34;\u0026lt;https://redislabs.com/primary-caching\u0026gt;\u0026#34; visits 108 OK To add a document specific score, that causes the document to appear higher or lower in results, set a value for the doc_score field. We specified the SCORE_FIELD in the schema definition to hold the document weight value.\n127.0.0.1:12543\u0026gt; HSET doc:2 title \u0026#34;Redis\u0026#34; body \u0026#34;Modules\u0026#34; url \u0026#34;\u0026lt;https://redislabs.com/modules\u0026gt;\u0026#34; visits 102 doc_score 0.8 OK Search the index Do a search on this index for any documents with the word \u0026ldquo;primary\u0026rdquo;:\n127.0.0.1:12543\u0026gt; FT.SEARCH database_idx \u0026#34;primary\u0026#34; LIMIT 0 10 1) (integer) 1 2) \u0026#34;doc:1\u0026#34; 3) 1) \u0026#34;title\u0026#34; 2) \u0026#34;Redis\u0026#34; 3) \u0026#34;body\u0026#34; 4) \u0026#34;primary and caching\u0026#34; 5) \u0026#34;url\u0026#34; 6) \u0026#34;https://redislabs.com/\u0026#34; 7) \u0026#34;value\u0026#34; 8) \u0026#34;10\u0026#34; Drop the index You can drop the index without deleting the underlying hash with the FT.DROPINDEX command:\n127.0.0.1:12543\u0026gt; FT.DROPINDEX database_idx OK Auto-complete You can use RediSearch suggestion commands to implement auto-complete.\nNote: Active-Active databases do not support RediSearch suggestions. Let\u0026rsquo;s add a suggestion for the search engine to use:\n127.0.0.1:12543\u0026gt; FT.SUGADD autocomplete \u0026#34;primary and caching\u0026#34; 100 \u0026#34;(integer)\u0026#34; 1 Make sure the suggestion is there:\n127.0.0.1:12543\u0026gt; FT.SUGGET autocomplete \u0026#34;pri\u0026#34; 1) \u0026#34;primary and caching\u0026#34; ","categories":[]},{"uri":"/embeds/tryout-redisgraph/","uriRel":"/embeds/tryout-redisgraph/","title":"","tags":[],"keywords":[],"description":"","content":"Give it a try After you load RedisGraph, you can interact with it using redis-cli.\nHere we\u0026rsquo;ll quickly create a small graph representing a subset of motorcycle riders and teams taking part in the MotoGP league. Once created, we\u0026rsquo;ll start querying our data.\nWith redis-cli Connect to redis.\n$ redis-cli -p 12543 127.0.0.1:12543\u0026gt; Run these commands:\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;CREATE (:Rider {name:\u0026#39;Valentino Rossi\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Yamaha\u0026#39;}), (:Rider {name:\u0026#39;Dani Pedrosa\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Honda\u0026#39;}), (:Rider {name:\u0026#39;Andrea Dovizioso\u0026#39;})-[:rides]-\u0026gt;(:Team {name:\u0026#39;Ducati\u0026#39;})\u0026#34; 1) (empty list or set) 2) 1) Labels added: 2 2) Nodes created: 6 3) Properties set: 6 4) Relationships created: 3 5) \u0026#34;Query internal execution time: 0.399000 milliseconds\u0026#34; Now that our MotoGP graph is created, we can start asking questions. For example: Who\u0026rsquo;s riding for team Yamaha?\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider)-[:rides]-\u0026gt;(t:Team) WHERE t.name = \u0026#39;Yamaha\u0026#39; RETURN r,t\u0026#34; 1) 1) 1) \u0026#34;r.name\u0026#34; 2) \u0026#34;t.name\u0026#34; 2) 1) \u0026#34;Valentino Rossi\u0026#34; 2) \u0026#34;Yamaha\u0026#34; 2) 1) \u0026#34;Query internal execution time: 0.122000 milliseconds\u0026#34; How many riders represent team Ducati?\n127.0.0.1:12543\u0026gt; GRAPH.QUERY MotoGP \u0026#34;MATCH (r:Rider)-[:rides]-\u0026gt;(t:Team {name:\u0026#39;Ducati\u0026#39;}) RETURN count(r)\u0026#34; 1) 1) 1) \u0026#34;count(r)\u0026#34; 2) 1) \u0026#34;1.000000\u0026#34; 2) 1) \u0026#34;Query internal execution time: 0.129000 milliseconds\u0026#34; ","categories":[]},{"uri":"/embeds/tryout-redistimeseries/","uriRel":"/embeds/tryout-redistimeseries/","title":"","tags":[],"keywords":[],"description":"","content":"Quick start with redis-cli After you set up RedisTimeSeries, you can interact with it using redis-cli.\nHere we\u0026rsquo;ll create a time series representing sensor temperature measurements. After you create the time series, you can send temperature measurements. Then you can query the data for a time range on some aggregation rule.\nWith redis-cli Connect to redis.\n$ redis-cli -p 12543 127.0.0.1:12543\u0026gt; Run these commands:\n127.0.0.1:12543\u0026gt; TS.CREATE temperature RETENTION 60 LABELS sensor_id 2 area_id 32 OK 127.0.0.1:12543\u0026gt; TS.ADD temperature:3:11 1548149181 30 OK 127.0.0.1:12543\u0026gt; TS.ADD temperature:3:11 1548149191 42 OK 127.0.0.1:12543\u0026gt; TS.RANGE temperature:3:11 1548149180 1548149210 AGGREGATION avg 5 1) 1) (integer) 1548149180 2) \u0026#34;30\u0026#34; 2) 1) (integer) 1548149190 2) \u0026#34;42\u0026#34; Client libraries Some languages have client libraries that provide support for RedisTimeSeries commands:\nProject Language License Author URL JRedisTimeSeries Java BSD-3 RedisLabs Github redistimeseries-go Go Apache-2 RedisLabs Github redistimeseries-py Python BSD-3 RedisLabs Github ","categories":[]},{"uri":"/embeds/tryout-redsigears/","uriRel":"/embeds/tryout-redsigears/","title":"","tags":[],"keywords":[],"description":"","content":"","categories":[]},{"uri":"/embeds/what-is-redis-enterprise/","uriRel":"/embeds/what-is-redis-enterprise/","title":"","tags":[],"keywords":[],"description":"","content":"Redis has enhanced open source Redis with a technology layer that encapsulates open source Redis, while fully supporting all its commands, data structures and modules. It adds exceptional flexibility, stable high performance and unmatched resilience, as well as multiple deployment choices (public and private clouds, on-premises, hybrid, RAM-Flash combination), topology (active-active, active-passive, active-replica) and support for very large dataset sizes. This enhanced and exponentially more powerful database platform is Redis Enterprise.\nLearn more about Redis Enterprise.\n","categories":[]},{"uri":"/rs/release-notes/legacy-release-notes/rlec-0-99-5-11-january-2015/","uriRel":"/rs/release-notes/legacy-release-notes/rlec-0-99-5-11-january-2015/","title":"RLEC 0.99.5-11 Release Notes (January 5, 2015)","tags":[],"keywords":[],"description":"","content":"New features Initial release, everything is new!\nChanges Initial release, no changes!\nFixed issues None.\nKnown issues Issue: When taking a node offline or removing a node, if the node being taken offline or removed is currently serving as the web server for the web browser being used to view the management UI, the management UI appears down while the node is down. Workaround: If you are using the cluster name in order to connect to the management UI in the browser, and the cluster name is registered in your external DNS or you are using the mDNS option, then the DNS entries will be updated to point to another node in the cluster after a few seconds and the UI will open properly. If you are not using the cluster name but rather the node IP in order to connect to the management UI in the web browser, you have to use the IP of another node in the cluster to access the management UI. ","categories":["RS"]},{"uri":"/rc/accounts/account-settings/","uriRel":"/rc/accounts/account-settings/","title":"Manage account settings","tags":[],"keywords":[],"description":"Describes the settings for a Redis Cloud account.","content":"To review or manage the settings associated with your Redis Cloud account, sign in to the admin console and then select Account Settings from the menu.\nThis displays the Account Settings screen:\nThe available tabs depend on your subscription type and may include:\nThe Account tab displays basic information associated with your account, including general info, address details, time zone setting, security settings, and provider integration details.\nThe Cloud Account tab is displayed for Flexible (or Annual) subscriptions hosted on Amazon Web Services (AWS). To learn more, see Manage AWS cloud accounts.\nAccount info settings The Account Info section provides basic details about your account, including:\nSetting Description Owner name Person associated with the Redis Cloud account Account name Organization associated with the Redis Cloud account Date created Date the user\u0026rsquo;s Redis Cloud account was created, which may differ from the organization account creation date Owner email address Email address used to create the owner\u0026rsquo;s account Account number Internal ID of the owner\u0026rsquo;s account Last updated Date of the last administrative change to the owner\u0026rsquo;s account, typically reflects access changes or other administrative updates You cannot change the email address associated with a Redis Cloud account. Instead, create a new account with the updated email address, assign it as an administrator to the organization account, and then use the new account to delete the account with the invalid email address.\nAccount address settings The Account address section shows the address associated with the current Redis Cloud account and the current time zone.\nTo update the time zone, select the desired time zone from the Time zone drop-down.\nIn addition, this section may include fields unique to your location. For example, certain regions require tax IDs or other regulatory details.\nWhen updating details in this section, pay particular attention to prompts and error messages.\nSecurity settings The Security section lets you:\nManage multi-factor authentication (MFA) for your Redis Cloud account.\nDownload the Redis Cloud certificate authority (CA) bundle as a PEM file, which contains the certificates associated with your Redis Cloud account.\nIntegration settings The Integration section includes settings that help you manage the integration of your Redis Cloud account with your underlying cloud provider. Specific settings vary according to the cloud provider.\nIf this section doesn\u0026rsquo;t appear on the Account Settings screen, it generally means that there aren\u0026rsquo;t any integration settings to manage.\nSave or discard changes Few account settings can be changed; however, you can update a few details, such as Time Zone and MFA enforcement. Available settings vary according to your subscription and the underlying cloud provider.\nUse the Save changes button to save changes or Discard changes to revert them.\nFor help changing other settings, contact Support.\n","categories":["RC"]},{"uri":"/rs/references/rest-api/objects/action/","uriRel":"/rs/references/rest-api/objects/action/","title":"Action object","tags":[],"keywords":[],"description":"An object that represents cluster actions","content":"The cluster allows you to invoke general maintenance actions such as rebalancing or taking a node offline by moving all of its entities to other nodes.\nActions are implemented as tasks in the cluster. Every task has a unique task_id assigned by the cluster, a task name which describes the task, a status, and additional task-specific parameters.\nThe REST API provides a simplified interface that allows callers to invoke actions and query their status without a specific task_id.\nThe action lifecycle is based on the following status and status transitions:\nName Type/Value Description progress integer (range: 0-100) Represents percent completed status queued Requested operation and added it to the queue to await processing starting Picked up operation from the queue and started processing running Currently executing operation cancelling Operation cancellation is in progress cancelled Operation cancelled completed Operation completed failed Operation failed When a task fails, the error_code and error_message fields describe the error.\nPossible error_code values:\nCode Description internal_error An internal error that cannot be mapped to a more precise error code insufficient_resources The cluster does not have sufficient resources to complete the required operation ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/actions/","uriRel":"/rs/references/rest-api/requests/actions/","title":"Actions requests","tags":[],"keywords":[],"description":"Actions requests","content":" Method Path Description GET /v1/actions Get all actions GET /v1/actions/{uid} Get a single action Get all actions GET /v1/actions Get the status of all actions (executing, queued, or completed) on all entities (clusters, nodes, and databases). This API tracks long-lived API requests that return either a task_id or an action_uid.\nRequired permissions Permission name view_status_of_cluster_action Request Example HTTP request GET /actions Response Returns a JSON array of action objects and an array of state-machine objects.\nRegardless of an action’s source, each action in the response contains the following attributes: name, action_uid, status, and progress.\nExample JSON body { \u0026#34;actions\u0026#34;: [ { \u0026#34;action_uid\u0026#34;: \u0026#34;159ca2f8-7bf3-4cda-97e8-4eb560665c28\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;retry_bdb\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;progress\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;task_id\u0026#34;: \u0026#34;159ca2f8-7bf3-4cda-97e8-4eb560665c28\u0026#34; }, { \u0026#34;action_uid\u0026#34;: \u0026#34;661697c5-c747-41bd-ab81-ffc8fd13c494\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;retry_bdb\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;progress\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;task_id\u0026#34;: \u0026#34;661697c5-c747-41bd-ab81-ffc8fd13c494\u0026#34; } ], \u0026#34;state-machines\u0026#34;: [ { \u0026#34;action_uid\u0026#34;: \u0026#34;a10586b1-60bc-428e-9bc6-392eb5f0d8ae\u0026#34;, \u0026#34;heartbeat\u0026#34;: 1650378874, \u0026#34;name\u0026#34;: \u0026#34;SMCreateBDB\u0026#34;, \u0026#34;object_name\u0026#34;: \u0026#34;bdb:1\u0026#34;, \u0026#34;progress\u0026#34;: 100, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34; } ] } Status codes Code Description 200 OK No error, response provides info about an ongoing action 404 Not Found Action does not exist (i.e. not currently running and no available status of last run). Get a specific action GET /v1/actions/{uid} Get the status of a currently executing, queued, or completed action.\nRequired permissions Permission name view_status_of_cluster_action Request Example HTTP request GET /actions/{uid} URL parameters Field Type Description uid string The action_uid to check Response Returns an action object.\nRegardless of an action’s source, each action contains the following attributes: name, action_uid, status, and progress.\nExample JSON body { \u0026#34;action_uid\u0026#34;: \u0026#34;159ca2f8-7bf3-4cda-97e8-4eb560665c28\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;retry_bdb\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;progress\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;task_id\u0026#34;: \u0026#34;159ca2f8-7bf3-4cda-97e8-4eb560665c28\u0026#34; } Status codes Code Description 200 OK No error, response provides info about an ongoing action 404 Not Found Action does not exist (i.e. not currently running and no available status of last run) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/","title":"Database actions requests","tags":[],"keywords":[],"description":"Database action requests","content":"Backup Method Path Description PUT /v1/bdbs/{uid}/actions/backup_reset_status Reset database backup status Export Method Path Description PUT /v1/bdbs/{uid}/actions/export_reset_status Reset database export status POST /v1/bdbs/{uid}/actions/export Initiate database export Import Method Path Description PUT /v1/bdbs/{uid}/actions/import_reset_status Reset database import status POST /v1/bdbs/{uid}/actions/import Initiate manual dataset import Optimize shards placement Method Path Description GET /v1/bdbs/{uid}/actions/optimize_shards_placement Get optimized shards placement for a database ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/actions/","uriRel":"/rs/references/rest-api/requests/cluster/actions/","title":"Cluster actions requests","tags":[],"keywords":[],"description":"Cluster action requests","content":" Method Path Description GET /v1/cluster/actions Get the status of all actions GET /v1/cluster/actions/{action} Get the status of a specific action POST /v1/cluster/actions/{action} Initiate a cluster-wide action DELETE /v1/cluster/actions/{action} Cancel action or remove action status Get all cluster actions GET /v1/cluster/actions Get the status of all currently executing, queued, or completed cluster actions.\nRequired permissions Permission name view_status_of_cluster_action Request Example HTTP request GET /cluster/actions Response Returns a JSON array of action objects.\nExample JSON body { \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;action_name\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;queued\u0026#34;, \u0026#34;progress\u0026#34;: 0.0 } ] } Status codes Code Description 200 OK No error, response provides info about an ongoing action. 404 Not Found Action does not exist (i.e. not currently running and no available status of last run). Get cluster action GET /v1/cluster/actions/{action} Get the status of a currently executing, queued, or completed cluster action.\nRequired permissions Permission name view_status_of_cluster_action Request Example HTTP request GET /cluster/actions/action_name URL parameters Field Type Description action string The action to check. Response Returns an action object.\nExample JSON body { \u0026#34;name\u0026#34;: \u0026#34;action_name\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;queued\u0026#34;, \u0026#34;progress\u0026#34;: 0.0 } Status codes Code Description 200 OK No error, response provides info about an ongoing action. 404 Not Found Action does not exist (i.e. not currently running and no available status of last run). Initiate cluster-wide action POST /v1/cluster/actions/{action} Initiate a cluster-wide action.\nThe API allows only a single instance of any action type to be invoked at the same time, and violations of this requirement will result in a 409 CONFLICT response.\nThe caller is expected to query and process the results of the previously executed instance of the same action, which will be removed as soon as the new one is submitted.\nRequired permissions Permission name start_cluster_action Request Example HTTP request POST /cluster/actions/action_name URL parameters Field Type Description action string The name of the action required. Response The body content may provide additional action details. Currently, it is not used.\nStatus codes Code Description 200 OK No error, action was initiated. 400 Bad Request Bad action or content provided. 409 Conflict A conflicting action is already in progress. Cancel action DELETE /v1/cluster/actions/{action} Cancel a queued or executing cluster action, or remove the status of a previously executed and completed action.\nRequired permissions Permission name cancel_cluster_action Request Example HTTP request DELETE /v1/cluster/actions/action_name URL parameters Field Type Description action string The name of the action to cancel, currently no actions are supported. Response Returns a status code.\nStatus codes Code Description 200 OK Action will be cancelled when possible. 404 Not Found Action unknown or not currently running. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/actions/","uriRel":"/rs/references/rest-api/requests/nodes/actions/","title":"Node actions requests","tags":[],"keywords":[],"description":"Node action requests","content":" Method Path Description GET /v1/nodes/actions Get status of all actions on all nodes GET /v1/nodes/{node_uid}/actions Get status of all actions on a specific node GET /v1/nodes/{node_uid}/actions/{action} Get status of an action on a specific node POST /v1/nodes/{node_uid}/actions/{action} Initiate node action DELETE /v1/nodes/{node_uid}/actions/{action} Cancel action or remove action status Get all actions GET /v1/nodes/actions Get the status of all currently executing, pending, or completed actions on all nodes.\nPermissions Permission name Roles view_status_of_all_node_actions admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /nodes/actions Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a list of action objects.\nStatus codes Code Description 200 OK No error, response provides details about an ongoing action. Get node actions statuses GET /v1/nodes/{node_uid}/actions Get the status of all actions on a specific node.\nRequired permissions Permission name Roles view_status_of_node_action admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /nodes/1/actions Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description action string The action to check. Response Returns a JSON object that includes a list of action objects for the specified node.\nIf no actions are available, the response will include an empty array.\nExample JSON body { \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;remove_node\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;progress\u0026#34;: 10 } ] } Error codes Code Description internal_error An internal error that cannot be mapped to a more precise error code has been encountered. insufficient_resources The cluster does not have sufficient resources to complete the required operation. Status codes Code Description 200 OK No error, response provides details about an ongoing action. 404 Not Found Action does not exist (i.e. not currently running and no available status of last run). Get node action status GET /v1/nodes/{node_uid}/actions/{action} Get the status of a currently executing, queued, or completed action on a specific node.\nRequest Example HTTP request GET /nodes/1/actions/remove Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns an action object for the specified node.\nError codes Code Description internal_error An internal error that cannot be mapped to a more precise error code has been encountered. insufficient_resources The cluster does not have sufficient resources to complete the required operation. Status codes Code Description 200 OK No error, response provides details about an ongoing action. 404 Not Found Action does not exist (i.e. not currently running and no available status of last run). Initiate node action POST /v1/nodes/{node_uid}/actions/{action} Initiate a node action.\nThe API allows only a single instance of any action type to be invoked at the same time, and violations of this requirement will result in a 409 CONFLICT response.\nThe caller is expected to query and process the results of the previously executed instance of the same action, which will be removed as soon as the new one is submitted.\nPermissions Permission name Roles start_node_action admin Request Example HTTP request POST /nodes/1/actions/remove Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description action string The name of the action required. Currently supported actions are:\nremove: Removes the node from the cluster after migrating all bound resources to other nodes. As soon as a successful remove request is received, the cluster will no longer automatically migrate resources (shards/endpoints) to the node, even if the remove task fails at some point. maintenance_on: Creates a snapshot of the node, migrates shards to other nodes, and prepares the node for maintenance. See maintenance mode for more information. If there aren\u0026rsquo;t enough resources to migrate shards out of the maintained node, set \u0026quot;keep_slave_shards\u0026quot;: true in the request body to keep the replica shards in place but demote any master shards. maintenance_off: Restores node to its previous state before maintenance started. See maintenance mode for more information. By default, it uses the latest node snapshot. Use \u0026quot;snapshot_name\u0026quot;: \u0026quot;...\u0026quot; in the request body to restore the state from a specific snapshot. To avoid restoring shards at the node, use \u0026quot;skip_shards_restore\u0026quot;: true. enslave_node: Turn node into a replica. Response The body content may provide additional action details.\nStatus codes Code Description 200 OK Action initiated successfully. 409 Conflict Only a single instance of any action type can be invoked at the same time. Example requests cURL $ curl -k -X POST -u \u0026#34;[username]:[password]\u0026#34; -d \u0026#34;{}\u0026#34; https://[host][:port]/v1/nodes/1/actions/remove Python import requests import json url = \u0026#34;https://[host][port]/v1/nodes/1/actions/remove\u0026#34; payload = json.dumps({}) headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, } auth = (\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;POST\u0026#34;, url, auth=auth, headers=headers, data=payload) print(response.text) Cancel action DELETE /v1/nodes/{node_uid}/actions/{action} Cancel a queued or executing node action, or remove the status of a previously executed and completed action.\nPermissions Permission name cancel_node_action Request Example HTTP request DELETE /nodes/1/actions/remove Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description action string The name of the action to cancel. Response Returns a status code.\nStatus codes Code Description 200 OK Action will be cancelled when possible. 404 Not Found Action unknown or not currently running. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/add-instance/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/add-instance/","title":"crdb-cli crdb add-instance","tags":[],"keywords":[],"description":"Adds a peer replica to an Active-Active database.","content":"Adds a peer replica to an existing Active-Active database in order to host the database on another cluster. This creates an additional active instance of the database on the specified cluster.\ncrdb-cli crdb add-instance --crdb-guid \u0026lt;guid\u0026gt; --instance fqdn=\u0026lt;cluster_fqdn\u0026gt;,username=\u0026lt;username\u0026gt;,password=\u0026lt;password\u0026gt;[,url=\u0026lt;url\u0026gt;,replication_endpoint=\u0026lt;endpoint\u0026gt;] [ --compression \u0026lt;0-6\u0026gt; ] [ --no-wait ] Parameters Parameter Value Description crdb-guid string The GUID of the database (required) instance strings The connection information for the new participating cluster (required) compression 0-6 The level of data compression: 0=Compression disabled 6=High compression and resource load (Default: 3) no-wait Does not wait for the task to complete Returns Returns the task ID of the task that is adding the new instance.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the instance to be added and return finished.\nExample $ crdb-cli crdb add-instance --crdb-guid db6365b5-8aca-4055-95d8-7eb0105c0b35 \\ --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin-password Task f809fae7-8e26-4c8f-9955-b74dbbd47949 created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/addr/","uriRel":"/rs/references/cli-utilities/rladmin/node/addr/","title":"rladmin node addr set","tags":[],"keywords":[],"description":"Sets a node&#39;s internal IP address.","content":"Sets the internal IP address of a node. You can only set the internal IP address when the node is down.\nrladmin node \u0026lt;ID\u0026gt; addr set \u0026lt;IP address\u0026gt; Parameters Parameter Type/Value Description node integer Sets the internal IP address of the specified node addr IP address Sets the node\u0026rsquo;s internal IP address to the specified IP address Returns Returns Updated successfully if the IP address was set. Otherwise, it returns an error.\nUse rladmin status nodes to verify the internal IP address was changed.\nExample $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 3d99db1fdf4b 5/100 6 16.06GB/19.54GB 12.46GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.3 fc7a3d332458 0/100 6 -/19.54GB -/16.02GB 6.2.12-37 DOWN, last seen 33s ago node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 16.06GB/19.54GB 12.46GB/16.02GB 6.2.12-37 OK $ rladmin node 2 addr set 192.0.2.5 Updated successfully. $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 3d99db1fdf4b 5/100 6 14.78GB/19.54GB 11.18GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.5 fc7a3d332458 0/100 6 14.78GB/19.54GB 11.26GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 14.78GB/19.54GB 11.18GB/16.02GB 6.2.12-37 OK ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/alert/","uriRel":"/rs/references/rest-api/objects/alert/","title":"Alert object","tags":[],"keywords":[],"description":"An object that contains alert info","content":"You can view, configure, and enable various alerts for the cluster.\nAlerts are bound to a cluster object (such as a BDB or node), and the cluster\u0026rsquo;s state determines whether the alerts turn on or off.\nName Type/Value Description Writable change_time string Timestamp when alert state last changed change_value object Contains data relevant to the evaluation time when the alert went on/off (thresholds, sampled values, etc.) enabled boolean If true, alert is enabled x severity \u0026lsquo;DEBUG\u0026rsquo;\n\u0026lsquo;INFO\u0026rsquo;\n\u0026lsquo;WARNING\u0026rsquo;\n\u0026lsquo;ERROR\u0026rsquo;\n\u0026lsquo;CRITICAL\u0026rsquo; The alert\u0026rsquo;s severity state boolean If true, alert is currently triggered threshold string Represents an alert threshold when applicable x ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/cluster/alert_settings/","uriRel":"/rs/references/rest-api/objects/cluster/alert_settings/","title":"Alert settings object","tags":[],"keywords":[],"description":"Documents the alert_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cluster_certs_about_to_expire cluster_alert_settings_with_threshold object Cluster certificate will expire in x days cluster_even_node_count boolean (default: false) True high availability requires an odd number of nodes in the cluster cluster_flash_overcommit boolean (default: false) Flash memory committed to databases is larger than cluster total flash memory cluster_inconsistent_redis_sw boolean (default: false) Some shards in the cluster are running different versions of Redis software cluster_inconsistent_rl_sw boolean (default: false) Some nodes in the cluster are running different versions of Redis Enterprise software cluster_internal_bdb boolean (default: false) Issues with internal cluster databases cluster_multiple_nodes_down boolean (default: false) Multiple cluster nodes are down (this might cause data loss) cluster_node_joined boolean (default: false) New node joined the cluster cluster_node_remove_abort_completed boolean (default: false) Cancel node remove operation completed cluster_node_remove_abort_failed boolean (default: false) Cancel node remove operation failed cluster_node_remove_completed boolean (default: false) Node removed from the cluster cluster_node_remove_failed boolean (default: false) Failed to remove a node from the cluster cluster_ocsp_query_failed boolean (default: false) Failed to query the OCSP server cluster_ocsp_status_revoked boolean (default: false) OCSP certificate status is REVOKED cluster_ram_overcommit boolean (default: false) RAM committed to databases is larger than cluster total RAM cluster_too_few_nodes_for_replication boolean (default: false) Replication requires at least 2 nodes in the cluster node_aof_slow_disk_io boolean (default: false) AOF reaching disk I/O limits node_checks_error boolean (default: false) Some node checks have failed node_cpu_utilization cluster_alert_settings_with_threshold object Node CPU utilization has reached the threshold value (% of the utilization limit) node_ephemeral_storage cluster_alert_settings_with_threshold object Node ephemeral storage has reached the threshold value (% of the storage limit) node_failed boolean (default: false) Node failed node_free_flash cluster_alert_settings_with_threshold object Node flash storage has reached the threshold value (% of the storage limit) node_insufficient_disk_aofrw boolean (default: false) Insufficient AOF disk space node_internal_certs_about_to_expire cluster_alert_settings_with_threshold object Internal certificate on node will expire in x days node_memory cluster_alert_settings_with_threshold object Node memory has reached the threshold value (% of the memory limit) node_net_throughput cluster_alert_settings_with_threshold object Node network throughput has reached the threshold value (bytes/s) node_persistent_storage cluster_alert_settings_with_threshold object Node persistent storage has reached the threshold value (% of the storage limit) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/alerts/","uriRel":"/rs/references/rest-api/requests/bdbs/alerts/","title":"Database alerts requests","tags":[],"keywords":[],"description":"Database alert requests","content":" Method Path Description GET /v1/bdbs/alerts Get all alert states for all databases GET /v1/bdbs/alerts/{uid} Get all alert states for a specific database GET /v1/bdbs/alerts/{uid}/{alert} Get a specific database alert state POST /v1/bdbs/alerts/{uid} Update a database’s alerts configuration Get all database alerts GET /v1/bdbs/alerts Get all alert states for all databases.\nRequired permissions Permission name view_all_bdbs_alerts Request Example HTTP request GET /bdbs/alerts Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a hash of alert UIDs and the alerts states for each database.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;bdb_size\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error Get database alerts GET /v1/bdbs/alerts/{int: uid} Get all alert states for a database.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/alerts/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;bdb_size\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified bdb does not exist Get database alert GET /v1/bdbs/alerts/{int: uid}/{alert} Get a database alert state.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/alerts/1/bdb_size Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database alert string The alert name Response Returns an alert object.\nExample JSON body { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } } Status codes Code Description 200 OK No error 400 Bad Request Bad request 404 Not Found Specified alert or bdb does not exist Update database alert POST /v1/bdbs/alerts/{int: uid} Updates a database\u0026rsquo;s alerts configuration.\nRequired permissions Permission name update_bdb_alerts Request If passed with the dry_run URL query string, the function will validate the alert thresholds, but not commit them.\nExample HTTP request POST /bdbs/alerts/1 Example JSON body { \u0026#34;bdb_size\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;80\u0026#34;, \u0026#34;enabled\u0026#34;:true }, \u0026#34;bdb_high_syncer_lag\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;enabled\u0026#34;:false }, \u0026#34;bdb_low_throughput\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;enabled\u0026#34;:true }, \u0026#34;bdb_high_latency\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;3000\u0026#34;, \u0026#34;enabled\u0026#34;:true }, \u0026#34;bdb_high_throughput\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;enabled\u0026#34;:true }, \u0026#34;bdb_backup_delayed\u0026#34;:{ \u0026#34;threshold\u0026#34;:\u0026#34;1800\u0026#34;, \u0026#34;enabled\u0026#34;:true } } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer Database ID dry_run string Validate the alert thresholds but do not apply them Request body The request must contain a single JSON object with one or many database alert objects.\nResponse The response includes the updated database alerts.\nStatus codes Code Description 404 Not Found Specified database was not found. 406 Not Acceptable Invalid configuration parameters provided. 200 OK Success, database alerts updated. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/alerts/","uriRel":"/rs/references/rest-api/requests/cluster/alerts/","title":"Cluster alerts requests","tags":[],"keywords":[],"description":"Cluster alert requests","content":" Method Path Description GET /v1/cluster/alerts Get all cluster alerts GET /v1/cluster/alerts/{alert} Get a specific cluster alert Get all cluster alerts GET /v1/cluster/alerts Get all alert states for the cluster object.\nRequired permissions Permission name view_cluster_alerts Request Example HTTP request GET /cluster/alerts Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description ignore_settings boolean Retrieve updated alert state regardless of the cluster’s alert_settings. When not present, a disabled alert will always be retrieved as disabled with a false state. (optional) Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;cluster_too_few_nodes_for_replication\u0026#34;: { \u0026#34;change_time\u0026#34;: \u0026#34;2014-12-22T11:48:00Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: false }, \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: \u0026#34;off\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error Get cluster alert GET /v1/cluster/alerts/{alert} Get a cluster alert state.\nRequired permissions Permission name view_cluster_alerts Request Example HTTP request GET /cluster/alerts/cluster_too_few_nodes_for_replication Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description ignore_settings boolean Retrieve updated alert state regardless of the cluster’s alert_settings. When not present, a disabled alert will always be retrieved as disabled with a false state. (optional) Response Returns an alert object.\nExample JSON body { \u0026#34;change_time\u0026#34;: \u0026#34;2014-12-22T11:48:00Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: false }, \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: \u0026#34;off\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, } Status codes Code Description 200 OK No error 404 Not Found Specified alert does not exist ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/alerts/","uriRel":"/rs/references/rest-api/requests/nodes/alerts/","title":"Node alerts requests","tags":[],"keywords":[],"description":"Node alert requests","content":" Method Path Description GET /v1/nodes/alerts Get all alert states for all nodes GET /v1/nodes/alerts/{uid} Get all alert states for a node GET /v1/nodes/alerts/{uid}/{alert} Get node alert state Get all alert states GET /v1/nodes/alerts Get all alert states for all nodes.\nRequired permissions Permission name view_all_nodes_alerts Request Example HTTP request GET /nodes/alerts Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description ignore_settings boolean Retrieve updated alert state regardless of the cluster\u0026rsquo;s alert_settings. When not present, a disabled alert will always be retrieved as disabled with a false state. (optional) Response Returns a hash of node UIDs and the alert states for each node.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;node_cpu_utilization\u0026#34;: { \u0026#34;change_time\u0026#34;: \u0026#34;2014-12-22T10:42:00Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;cpu_util\u0026#34;: 2.500000000145519, \u0026#34;global_threshold\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;state\u0026#34;: true }, \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34; }, \u0026#34;...\u0026#34; }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error Get node alert states GET /v1/nodes/alerts/{int: uid} Get all alert states for a node.\nRequired permissions Permission name view_node_alerts Request Example HTTP request GET /nodes/alerts/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description ignore_settings boolean Retrieve updated alert state regardless of the cluster\u0026rsquo;s alert_settings. When not present, a disabled alert will always be retrieved as disabled with a false state. (optional) Response Returns a hash of alert objects and their states for a specific node.\nExample JSON body { \u0026#34;node_cpu_utilization\u0026#34;: { \u0026#34;change_time\u0026#34;: \u0026#34;2014-12-22T10:42:00Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;cpu_util\u0026#34;: 2.500000000145519, \u0026#34;global_threshold\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;state\u0026#34;: true }, \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified node does not exist Get node alert state GET /v1/nodes/alerts/{int: uid}/{alert} Get a node alert state.\nRequired permissions Permission name view_node_alerts Request Example HTTP request GET /nodes/alerts/1/node_cpu_utilization Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description ignore_settings boolean Retrieve updated alert state regardless of the cluster\u0026rsquo;s alert_settings. When not present, a disabled alert will always be retrieved as disabled with a false state. (optional) Response Returns an alert object.\nExample JSON body { \u0026#34;change_time\u0026#34;: \u0026#34;2014-12-22T10:42:00Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;cpu_util\u0026#34;: 2.500000000145519, \u0026#34;global_threshold\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;state\u0026#34;: true }, \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, } Status codes Code Description 200 OK No error 404 Not Found Specified alert or node does not exist ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/auditing-db-conns/","uriRel":"/rs/references/rest-api/requests/cluster/auditing-db-conns/","title":"Auditing database connections requests","tags":[],"keywords":[],"description":"Auditing database connections requests","content":" Method Path Description GET /v1/cluster/auditing/db_conns Get database connection auditing settings PUT /v1/cluster/auditing/db_conns Update database connection auditing settings DELETE /v1/cluster/auditing/db_conns Delete database connection auditing settings Get database auditing settings GET /v1/cluster/auditing/db_conns Gets the configuration settings for auditing database connections.\nRequired permissions Permission name view_cluster_info Request Example HTTP request GET /cluster/auditing/db_conns Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a database connection auditing configuration object.\nExample JSON body { \u0026#34;audit_address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;audit_port\u0026#34;: 12345, \u0026#34;audit_protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;audit_reconnect_interval\u0026#34;: 1, \u0026#34;audit_reconnect_max_attempts\u0026#34;: 0 } Error codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description db_conns_auditing_unsupported_by_capability Not all nodes support DB Connections Auditing capability Status codes Code Description 200 OK Success 406 Not Acceptable Feature not supported for all nodes Update database auditing PUT /v1/cluster/auditing/db_conns Updates the configuration settings for auditing database connections.\nRequired permissions Permission name update_cluster Request Example HTTP request PUT /cluster/auditing/db_conns Example JSON body { \u0026#34;audit_protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;audit_address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;audit_port\u0026#34;: 12345, \u0026#34;audit_reconnect_interval\u0026#34;: 1, \u0026#34;audit_reconnect_max_attempts\u0026#34;: 0 } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a database connection auditing configuration object with updated fields in the request body.\nResponse Returns the updated database connection auditing configuration object.\nExample JSON body { \u0026#34;audit_address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;audit_port\u0026#34;: 12345, \u0026#34;audit_protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;audit_reconnect_interval\u0026#34;: 1, \u0026#34;audit_reconnect_max_attempts\u0026#34;: 0 } Error codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description db_conns_auditing_unsupported_by_capability Not all nodes support DB Connections Auditing capability Status codes Code Description 200 OK Success 406 Not Acceptable Feature not supported for all nodes Delete database auditing settings DELETE /v1/cluster/auditing/db_conns Resets the configuration settings for auditing database connections.\nRequired permissions Permission name update_cluster Request Example HTTP request DELETE /cluster/auditing/db_conns Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a status code that indicates whether the database connection auditing settings reset successfully or failed to reset.\nError codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description db_conns_audit_config_not_found Unable to find the auditing configuration cannot_delete_audit_config_when_policy_enabled Auditing cluster policy is \u0026rsquo;enabled\u0026rsquo; when trying to delete the auditing configuration cannot_delete_audit_config_when_bdb_enabled One of the databases has auditing configuration \u0026rsquo;enabled\u0026rsquo; when trying to delete the auditing configuration Status codes Code Description 200 OK Success 404 Not Found Configuration not found 406 Not Acceptable Feature not supported for all nodes ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/users/authorize/","uriRel":"/rs/references/rest-api/requests/users/authorize/","title":"Authorize user requests","tags":[],"keywords":[],"description":"Users authorization requests","content":" Method Path Description POST /v1/users/authorize Authorize a user Authorize user POST /v1/users/authorize Generate a JSON Web Token (JWT) for a user to use as authorization to access the REST API.\nRequest Example HTTP request POST /users/authorize Example JSON body { \u0026#34;username\u0026#34;: \u0026#34;user@redislabs.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my_password\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a JWT authorize object with a valid username and password in the request body.\nResponse Returns a JSON object that contains the generated access token.\nExample JSON body { \u0026#34;access_token\u0026#34;: \u0026#34;eyJ5bGciOiKIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXViOjE0NjU0NzU0ODYsInVpZFI1IjEiLCJleHAiOjE0NjU0Nz30OTZ9.2xYXumd1rDoE0edFzcLElMOHsshaqQk2HUNgdsUKxMU\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description password_expired The password has expired and must be changed. Status codes Code Description 200 OK The user is authorized. 400 Bad Request The request could not be understood by the server due to malformed syntax. 401 Unauthorized The user is unauthorized. ","categories":["RS"]},{"uri":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/aws-console/","uriRel":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/aws-console/","title":"Create IAM resources using AWS console","tags":[],"keywords":[],"description":"","content":"To manually create IAM resources using the AWS console, follow these steps.\nStep 1: Create the IAM instance policy First, create a policy to use for the new instance role:\nIn the AWS IAM console, go to Policies \u0026gt; Create policy.\nIn the JSON tab, paste the contents of the RedisLabsInstanceRolePolicy.json policy file, shown here:\nView RedisLabsInstanceRolePolicy.json \u0026lt;div class=\u0026quot;highlight\u0026quot;\u0026gt;\u0026lt;pre tabindex=\u0026quot;0\u0026quot; style=\u0026quot;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;language-js\u0026quot; data-lang=\u0026quot;js\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Version\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;2012-10-17\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Statement\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;EC2\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeAvailabilityZones\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeRegions\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeSecurityGroups\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeTags\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeVolumes\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;EC2Tagged\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:AuthorizeSecurityGroupEgress\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:AuthorizeSecurityGroupIngress\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DeleteSecurityGroup\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:RevokeSecurityGroupEgress\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:RevokeSecurityGroupIngress\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Condition\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;StringEquals\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:ResourceTag/RedisLabsIdentifier\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Redislabs-VPC\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;EBSVolumeActions\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:AttachVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateTags\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DescribeTags\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;S3Object\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:PutObject\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:PutObjectAcl\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:GetObject\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:GetObjectAcl\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:DeleteObject\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:ListBucket\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;s3:GetBucketLocation\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;IAM\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:GetPolicy\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListPolicies\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\t\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;]\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt; Validate it and then select Review Policy.\nEnter RedisLabsInstanceRolePolicy as the policy name and then select Create Policy.\nStep 2: Create the service role To create the role that uses the policy:\nIn the AWS IAM console, go to Roles and click Create Role. Select AWS Service as the trusted entity, EC2 as the service and use case, and click Next: Permissions. Enter RedisLabsInstanceRolePolicy in the search box to look up the policy we just created, select it, and click Next: Review. Name the role redislabs-cluster-node-role and click Create Role. Step 3: Create the user policy Now create a policy to assign to the user:\nIn the AWS IAM console, go to Policies \u0026gt; Create policy.\nIn the JSON tab, paste the contents of the RedisLabsIAMUserRestrictedPolicy.json policy file.\nView RedislabsIAMUserRestrictedPolicy.json \u0026lt;div class=\u0026quot;highlight\u0026quot;\u0026gt;\u0026lt;pre tabindex=\u0026quot;0\u0026quot; style=\u0026quot;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;language-js\u0026quot; data-lang=\u0026quot;js\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Version\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;2012-10-17\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Statement\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Ec2DescribeAll\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:Describe*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;CloudWatchReadOnly\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;cloudwatch:Describe*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;cloudwatch:Get*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;cloudwatch:List*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;IamUserOperations\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:GetUser\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:GetUserPolicy\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ChangePassword\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;arn:aws:iam::*:user/${aws:username}\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;RolePolicyUserReadActions\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:GetRole\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:GetPolicy\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListUsers\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListPolicies\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListRolePolicies\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListAttachedRolePolicies\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListInstanceProfiles\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:ListInstanceProfilesForRole\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:SimulatePrincipalPolicy\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;KeyPairActions\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateKeyPair\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DeleteKeyPair\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:ImportKeyPair\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;CreateInstancesSnapshotsVolumesAndTags\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:AttachVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:StartInstances\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:RunInstances\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateSnapshot\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:CreateTags\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:ModifyInstanceAttribute\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;PassRlClusterNodeRole\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;iam:PassRole\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;arn:aws:iam::*:role/redislabs-cluster-node-role\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;NetworkAccess\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Vpc*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*VpcPeering*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Subnet*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Gateway*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Vpn*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Route*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*Address*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*SecurityGroup*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*NetworkAcl*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:*DhcpOptions*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;DeleteInstancesVolumesSnapshotsAndTagsWithIdentiferTag\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:RebootInstances\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:StopInstances\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:TerminateInstances\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DeleteVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DeleteSnapshot\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DetachVolume\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:DeleteTags\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;],\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Condition\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;StringEquals\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;ec2:ResourceTag/RedisLabsIdentifier\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Redislabs-VPC\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;},\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Sid\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Support\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Effect\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Allow\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Action\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;support:*\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Resource\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;*\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;]\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt; Validate the policy and click Review Policy.\nEnter RedislabsIAMUserRestrictedPolicy as the policy name and click Create Policy.\nStep 4: Create the programmatic access user Create a user and attach the policy you created:\nIn the AWS IAM console, go to Users \u0026gt; select Add user. Name it redislabs-user and check only the Programmatic access checkbox. Click Next: Permissions. Select Attach existing policies directly and select RedislabsIAMUserRestrictedPolicy from the list. Click Next: Review. Click Create user. Download the user credentials and store them in a secure location. Step 5: Create the console access role Last, create a role and attach the policy you created:\nIn the AWS IAM console, go to Roles \u0026gt; select Create role. Select Another AWS account. Under Account ID, enter account number 168085023892 (Redis Cloud\u0026rsquo;s AWS account). Under Options, check the Require MFA checkbox only. Do not check Require external ID. Click Next: Permissions. Attach the policy RedisLabsIAMUserRestrictedPolicy to the role. Click Next: Review. Name the role redislabs-role and then click Create role. ","categories":["RC"]},{"uri":"/rc/cloud-integrations/aws-marketplace/","uriRel":"/rc/cloud-integrations/aws-marketplace/","title":"Flexible subscriptions with AWS Marketplace","tags":[],"keywords":[],"description":"Shows how to subscribe to Redis Cloud using AWS Marketplace","content":"You can use AWS Marketplace to subscribe to Redis Enterprise Cloud through AWS Marketplace. This lets you provision according to your needs and pay using your AWS account.\nHere\u0026rsquo;s how to create a new Flexible subscription as part of your AWS Marketplace commitment.\nSign in to the AWS console.\nSearch AWS Marketplace for Redis Enterprise Cloud - Flexible plan.\nSubscribe to Redis Enterprise Cloud - Flexible plan, locate the Set Up Your Account button, and then select it to begin mapping your Redis Cloud account with your AWS Marketplace account.\nSign in to the Redis Cloud admin console.\nUse the AWS Marketplace dialog to select the Redis account to be mapped to your AWS Marketplace account.\nUse the Map account button to confirm your choice.\nOnce your Redis account is mapped to your AWS Marketplace account, a message appears in the upper, left corner of the account panel.\nIn addition, AWS Marketplace is reported as the selected payment method.\nAt this point, you can create a new Flexible subscription using the standard workflow, with one important change. You don\u0026rsquo;t need to enter a payment method, as it\u0026rsquo;s automatically assigned to your AWS Marketplace account.\nTo confirm this, review the payment method associated with your subscription.\nIf, for whatever reason, your AWS Marketplace account is disabled or otherwise unavailable, you won\u0026rsquo;t be able to use your Flexible subscription until the billing method is updated. For help, contact support.\n","categories":["RC"]},{"uri":"/rs/networking/configuring-aws-route53-dns-redis-enterprise/","uriRel":"/rs/networking/configuring-aws-route53-dns-redis-enterprise/","title":"AWS Route53 DNS management","tags":[],"keywords":[],"description":"How to configure AWS Route 53 DNS","content":"Redis Enterprise Software requires DNS to be properly configured to achieve high availability and fail-over regardless of where it is installed.\nHere, you learn how to configure AWS Route53 DNS resolution.\nPrerequisites You need to have a domain name registered. Then, either you need to have Amazon\u0026rsquo;s Route53 as the primary/master nameserver (NS) for this domain or for a delegated zone under this domain. Finally, you need to have the zone (either the whole domain or a sub-zone) defined in AWS Route53.\nHow does Redis Enterprise Software achieve failover? When your application wants to connect to a RS database in a three node Redis Enterprise Software cluster, it connects to any node in this cluster using its fully qualified domain name (FQDN), for example \u0026ldquo;node1.mycluster.enterprise.com\u0026rdquo;. It needs to know the IP address associated with this name and asks the top-level nameservers (.com) for the list of name servers in charge of enterprise.com. Then, it asks these name server (one after each other in case of failure) for the name servers in charge of mycluster.enterprise.com, and finally, it asks these name servers (one after each other in case of failure) for the IP of node1.mycluster.enterprise.com. At the end, it connects to this IP address. All this process is obfuscated from the application and is done by the resolver, a system library.\nYour name servers are in charge of enterprise.com. So, they are able to give the name servers in charge of your cluster. RS embeds a name server in each node. The nodes are in charge of the cluster zone resolution and are able to give the IP address of any node in the cluster. When everything is working, whichever is the top-level nameserver asked, it gives the list of name servers for your enterprise domain, then whichever enterprise name server is asked, it gives the list of cluster name servers (the cluster nodes), and whichever is the node asked, it returns the IP address of the requested node name.\nIf the cluster nameserver (node) asked is down, given the resolution process, the resolver tries to ask another name server (node) from the list and gets the requested IP address. That\u0026rsquo;s the standard DNS resolution behavior, in few words.\nNow, when a cluster node dies, for whatever reason, the other nodes can not reach it anymore and replaces the IP address associated with his name by the IP address of another node in the cluster\u0026rsquo;s name servers. It means that the failed over IP address is never returned anymore by the name servers to any requested FQDN, and that two of the FQDN have the same IP address. Basically, it means that whichever node FQDN your application is asking to resolve, it always gets the IP address of one node in the cluster that is healthy, up and running and the connection succeeds.\nConfiguration After the theory, we can go through the hands-on steps to achieve this configuration with AWS\u0026rsquo;s Route53 DNS as your domain or sub-domain official name server.\nIf you would like to watch a video on the process, here you go.\nConnection to AWS Route53 The first step is to connect your browser to AWS and to login into the administration interface. Then, you have to go in the Services menu at the top of the page and click on the Route53 menu item:\nMake sure you have registered a domain, and that you have defined Route53 as the primary/master name server for the whole domain or for one of its sub-domains. You should have at least one zone in Route53. Click on the Hosted zones link to open it:\nRoute53 is now displaying the list of the zones that you defined. You need to click on the zone in which you want to define your cluster, demo-rlec.redislabs.com in my case:\nNameserver records creation The next step is to create the record that returns the IP address of the name servers in your cluster, that is one of the nodes. To create the first name server IP address resolution record, you need to click on the Create Record Set blue button at the top of the list:\nThis record is only used to resolve the IP address of the cluster name server to query, it is not used by the application to connect to the cluster. This kind of record is an A record type and associates an IP address to a name. To avoid the whole resolving process each time that a name is requested, the results are cached in the forwarding DNS and in the application\u0026rsquo;s resolver library. Given that the IP address associated with a node can change when the related hardware fails, the information needs to expire quickly, otherwise, the node fails, the name servers reflects the failover, but they are not queried again and the local resolver still returns the IP of the failed node. This is the Time To Live (TTL) field associated to the record.\nSo, you need to enter the name used as the name server\u0026rsquo;s name, despite that it could be different, I suggest that you use the node name. You need to enter its IP address, to set the TTL to something short, I\u0026rsquo;ll set it to one minute. This is the maximum amount of time that the record is kept in cache of the resolver and of the forwarding DNS. If the node goes down, the resolver still uses this value until the record expires in his cache, but as the node does not answer, the resolver tries the next name server in the list (he has the A record because he needed to know the IP of the first name server, and if he needed this IP, he already had the name server list).\nFinally submit the first name server\u0026rsquo;s A record to Route53, using the Create button at the bottom of the right panel:\nYou want (and need) to have all your cluster nodes acting as name servers, so you have to repeat these steps for all your nodes and you should get a list of A records in Route53 interface:\nNow, the client-side resolver and the forwarding DNS can reach the cluster nameservers by their IP address, if they know what are the names of the cluster\u0026rsquo;s name servers. That\u0026rsquo;s the next point.\nDefining the nameserver list for a subzone Here, the idea is to provide the list of the cluster nameserver\u0026rsquo;s names to the resolvers and the forwarding DNS, so that they can resolve the IP address of one of them and query it for node\u0026rsquo;s IP. To achieve that, we have to define a new record in Route53, an NS record for Name Server record. So, once again, we click on the button to Create [a] Record Set and we enter the relevant information in the right panel.\nThe name is the name of the cluster, if your cluster nodes are nodeX.mycluster.enterprise.com, then the cluster name is mycluster.enterprise.com. Remember, the resolver asks \u0026ldquo;who are the name servers for zone \u0026ldquo;. The name is the searched key, the record type is the field. In our case, the resolver asks for the `NS` record type to get the nameservers list of the clustername, so we have to choose this type. A short TTL, such as one minute, is a good idea here, too. And we have to enter the node name list in the text box. In other DNS, we would have to create one NS record for each item, but *Route53* takes care of that for us. I also have the habit to end these records with a final dot, it is not a typo, because some other DNS require it and it does not seem to be an issue with *Route53*. At the end, we can click on the *Create* button:\nCongratulations, you completed the Route53 DNS configuration for your Redis Enterprise Software. Let\u0026rsquo;s check what you have.\nVerification You should end with several name server A record (one for each cluster node) to be able to reach any of them by its name. You should also have one record that lists the nameservers names for the zone (cluster):\nIf your cluster nodes are healthy, up and running, with DNS network ports unfiltered, you can test the configuration. Who are the nameservers in charge of the resolution in the cluster:\ndig ns demo.francois.demo-rlec.redislabs.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.5-9+deb8u9-Debian \u0026lt;\u0026lt;\u0026gt;\u0026gt; ns demo.francois.demo-rlec.redislabs.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 25061 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;demo.francois.demo-rlec.redislabs.com. IN NS ;; ANSWER SECTION: demo.francois.demo-rlec.redislabs.com. 3409 IN NS ns2.demo.francois.demo-rlec.redislabs.com. demo.francois.demo-rlec.redislabs.com. 3409 IN NS ns1.demo.francois.demo-rlec.redislabs.com. demo.francois.demo-rlec.redislabs.com. 3409 IN NS ns3.demo.francois.demo-rlec.redislabs.com. ;; Query time: 31 msec ;; SERVER: 192.168.1.254#53(192.168.1.254) ;; WHEN: Tue Feb 14 16:49:13 CET 2017 ;; MSG SIZE rcvd: 120 You can see that the name are given a prefix of ns-. This answer does not come from Route53 but from the cluster nameservers themselves.\nNow you can either install and configure your nodes, if not already made, or connect your client, using the cluster name (not the IP address).\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/shard/backup/","uriRel":"/rs/references/rest-api/objects/shard/backup/","title":"Backup object","tags":[],"keywords":[],"description":"Documents the backup object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description progress number, (range: 0-100) Shard backup progress (percentage) status \u0026rsquo;exporting\u0026rsquo;\n\u0026lsquo;succeeded\u0026rsquo;\n\u0026lsquo;failed\u0026rsquo; Status of scheduled periodic backup process ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/backup_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/backup_job_settings/","title":"Backup job settings object","tags":[],"keywords":[],"description":"Documents the backup_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the backup schedule ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/backup_location/","uriRel":"/rs/references/rest-api/objects/bdb/backup_location/","title":"BDB backup/export location object","tags":[],"keywords":[],"description":"Documents the bdb backup_location/export_location object used with Redis Enterprise Software REST API calls.","content":"You can back up or export a database\u0026rsquo;s dataset to the following types of locations:\nFTP/S SFTP Amazon S3 Google Cloud Storage Microsoft Azure Storage NAS/Local Storage Basic parameters For all backup/export location objects, you need to specify the location type via the type field.\nLocation type \u0026ldquo;type\u0026rdquo; value FTP/S \u0026ldquo;url\u0026rdquo; SFTP \u0026ldquo;sftp\u0026rdquo; Amazon S3 \u0026ldquo;s3\u0026rdquo; Google Cloud Storage \u0026ldquo;gs\u0026rdquo; Microsoft Azure Storage \u0026ldquo;abs\u0026rdquo; NAS/Local Storage \u0026ldquo;mount_point\u0026rdquo; Location-specific parameters Any additional required parameters may differ based on the backup/export location type.\nFTP Key name Type Description url string A URI that represents a FTP/S location with the following format: ftp://user:password@host:port/path/. The user and password can be omitted if not needed. SFTP Key name Type Description key string SSH private key to secure the SFTP server connection. If you do not specify an SSH private key, the autogenerated private key of the cluster is used, and you must add the SSH public key of the cluster to the SFTP server configuration. (optional) sftp_url string SFTP URL in the format: sftp://user:password@host[:port][/path/]. The default port number is 22 and the default path is \u0026lsquo;/\u0026rsquo;. AWS S3 Key name Type Description access_key_id string The AWS Access Key ID with access to the bucket bucket_name string S3 bucket name secret_access_key string The AWS Secret Access Key that matches the Access Key ID subdir string Path to the backup directory in the S3 bucket (optional) Google Cloud Storage Key name Type Description bucket_name string Cloud Storage bucket name client_email string Email address for the Cloud Storage client ID client_id string Cloud Storage client ID with access to the Cloud Storage bucket private_key string Cloud Storage private key that matches the private key ID private_key_id string Cloud Storage private key ID with access to the Cloud Storage bucket subdir string Path to the backup directory in the Cloud Storage bucket (optional) Azure Blob Storage Key name Type Description account_key string Access key for the storage account account_name string Storage account name with access to the container container string Blob Storage container name sas_token string Token to authenticate with shared access signature subdir string Path to the backup directory in the Blob Storage container (optional) Note: account_key and sas_token are mutually exclusive NAS/Local Storage Key name Type Description path string Path to the local mount point. You must create the mount point on all nodes, and the redislabs:redislabs user must have read and write permissions on the local mount point. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/backup_reset_status/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/backup_reset_status/","title":"Backup reset status database action requests","tags":[],"keywords":[],"description":"Reset database backup status requests","content":" Method Path Description PUT /v1/bdbs/{uid}/actions/backup_reset_status Reset database backup status Reset database backup status PUT /v1/bdbs/{int: uid}/actions/backup_reset_status Resets the database\u0026rsquo;s backup_status to idle if a backup is not in progress and clears the value of the backup_failure_reason field.\nPermissions Permission name Roles reset_bdb_current_backup_status admin\ncluster_member\ndb_member Request Example HTTP request PUT /bdbs/1/actions/backup_reset_status Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Response Returns a status code.\nStatus codes Code Description 200 OK The request is accepted and is being processed. 404 Not Found Attempting to perform an action on a nonexistent database. 406 Not Acceptable Not all the modules loaded to the database support \u0026lsquo;backup_restore\u0026rsquo; capability 409 Conflict Database is currently busy with another action. In this context, this is a temporary condition and the request should be reattempted later. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/","uriRel":"/rs/references/rest-api/objects/bdb/","title":"BDB object","tags":[],"keywords":[],"description":"An object that represents a database","content":"An API object that represents a managed database in the cluster.\nName Type/Value Description uid integer Cluster unique ID of database. Can be set during creation but cannot be updated. account_id integer SM account ID action_uid string Currently running action\u0026rsquo;s UID (read-only) aof_policy \u0026lsquo;appendfsync-every-sec\u0026rsquo; \u0026lsquo;appendfsync-always\u0026rsquo; Policy for Append-Only File data persistence authentication_admin_pass string Password for administrative access to the BDB (used for SYNC from the BDB) authentication_redis_pass string Redis AUTH password authentication. Use for Redis databases only. Ignored for memcached databases. authentication_sasl_pass string Binary memcache SASL password authentication_sasl_uname string Binary memcache SASL username (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;) authentication_ssl_client_certs [{ \"client_cert\": string }, ...] List of authorized client certificates\nclient_cert: X.509 PEM (base64) encoded certificate authentication_ssl_crdt_certs [{ \"client_cert\": string }, ...] List of authorized CRDT certificates\nclient_cert: X.509 PEM (base64) encoded certificate authorized_names array of strings Additional certified names (deprecated as of Redis Enterprise v6.4.2; use authorized_subjects instead) authorized_subjects [{ \"CN\": string, \"O\": string, \"OU\": string, \"L\": string, \"ST\": string, \"C\": string }, ...] A list of valid subjects used for additional certificate validations during TLS client authentication. All subject attributes are case-sensitive.\nRequired subject fields:\n\u0026ldquo;CN\u0026rdquo; for Common Name\nOptional subject fields:\n\u0026ldquo;O\u0026rdquo; for Organization\n\u0026ldquo;OU\u0026rdquo; for Organizational Unit\n\u0026ldquo;L\u0026rdquo; for Locality (city)\n\u0026ldquo;ST\u0026rdquo; for State/Province\n\u0026ldquo;C\u0026rdquo; for 2-letter country code avoid_nodes array of strings Cluster node UIDs to avoid when placing the database\u0026rsquo;s shards and binding its endpoints background_op [{ \"status\": string, \"name\": string, \"error\": object, \"progress\": number }, ...] (read-only); progress: Percent of completed steps in current operation backup boolean (default: false) Policy for periodic database backup backup_failure_reason \u0026rsquo;no-permission\u0026rsquo;\n\u0026lsquo;wrong-file-path\u0026rsquo;\n\u0026lsquo;general-error\u0026rsquo; Reason of last failed backup process (read-only) backup_history integer (default: 0) Backup history retention policy (number of days, 0 is forever) backup_interval integer Interval in seconds in which automatic backup will be initiated backup_interval_offset integer Offset (in seconds) from round backup interval when automatic backup will be initiated (should be less than backup_interval) backup_location complex object Target for automatic database backups. Call GET /jsonschema to retrieve the object\u0026rsquo;s structure. backup_progress number, (range: 0-100) Database scheduled periodic backup progress (percentage) (read-only) backup_status \u0026rsquo;exporting\u0026rsquo;\n\u0026lsquo;succeeded\u0026rsquo;\n\u0026lsquo;failed\u0026rsquo; Status of scheduled periodic backup process (read-only) bigstore boolean (default: false) Database bigstore option bigstore_ram_size integer (default: 0) Memory size of bigstore RAM part. bigstore_ram_weights [{ \"shard_uid\": integer, \"weight\": number }, ...] List of shard UIDs and their bigstore RAM weights;\nshard_uid: Shard UID;\nweight: Relative weight of RAM distribution client_cert_subject_validation_type disabled\nsan_cn\nfull_subject Enables additional certificate validations that further limit connections to clients with valid certificates during TLS client authentication.\ndisabled: Authenticates clients with valid certificates. No additional validations are enforced.\nsan_cn: A client certificate is valid only if its Common Name (CN) matches an entry in the list of valid subjects. Ignores other Subject attributes.\nfull_subject: A client certificate is valid only if its Subject attributes match an entry in the list of valid subjects. crdt boolean (default: false) Use CRDT-based data types for multi-master replication crdt_causal_consistency boolean (default: false) Causal consistent CRDB. crdt_config_version integer Replica-set configuration version, for internal use only. crdt_featureset_version integer CRDB active FeatureSet version crdt_ghost_replica_ids string Removed replicas IDs, for internal use only. crdt_guid string GUID of CRDB this database belongs to, for internal use only. crdt_protocol_version integer CRDB active Protocol version crdt_repl_backlog_size string Active-Active replication backlog size (\u0026lsquo;auto\u0026rsquo; or size in bytes) crdt_replica_id integer Local replica ID, for internal use only. crdt_replicas string Replica set configuration, for internal use only. crdt_sources array of syncer_sources objects Remote endpoints/peers of CRDB database to sync from. See the \u0026lsquo;bdb -\u0026gt; replica_sources\u0026rsquo; section crdt_sync \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; \u0026lsquo;paused\u0026rsquo;\n\u0026lsquo;stopped\u0026rsquo; Enable, disable, or pause syncing from specified crdt_sources. Applicable only for Active-Active databases. See replica_sync for more details. crdt_sync_dist boolean Enable/disable distributed syncer in master-master crdt_syncer_auto_oom_unlatch boolean (default: true) Syncer automatically attempts to recover synchronisation from peers after this database throws an Out-Of-Memory error. Otherwise, the syncer exits created_time string The date and time the database was created (read-only) data_internode_encryption boolean Should the data plane internode communication for this database be encrypted data_persistence \u0026lsquo;disabled\u0026rsquo; \u0026lsquo;snapshot\u0026rsquo;\n\u0026lsquo;aof\u0026rsquo; Database on-disk persistence policy. For snapshot persistence, a snapshot_policy must be provided dataset_import_sources complex object Array of source file location description objects to import from when performing an import action. This is write-only and cannot be read after set. Call GET /jsonschema to retrieve the object\u0026rsquo;s structure. db_conns_auditing boolean Enables/deactivates database connection auditing default_user boolean (default: true) Allow/disallow a default user to connect disabled_commands string (default: ) Redis commands which are disabled in db dns_address_master string Database private address endpoint FQDN (read-only) (deprecated) email_alerts boolean (default: false) Send email alerts for this DB endpoint string Latest bound endpoint. Used when reconfiguring an endpoint via update endpoint_ip complex object External IP addresses of node hosting the BDB\u0026rsquo;s endpoint. GET /jsonschema to retrieve the object\u0026rsquo;s structure. (read-only) (deprecated) endpoint_node integer Node UID hosting the BDB\u0026rsquo;s endpoint (read-only) (deprecated) endpoints array List of database access endpoints (read-only) enforce_client_authentication \u0026rsquo;enabled\u0026rsquo; \u0026lsquo;disabled\u0026rsquo; Require authentication of client certificates for SSL connections to the database. If set to \u0026rsquo;enabled\u0026rsquo;, a certificate should be provided in either authentication_ssl_client_certs or authentication_ssl_crdt_certs eviction_policy \u0026lsquo;volatile-lru\u0026rsquo;\n\u0026lsquo;volatile-ttl\u0026rsquo;\n\u0026lsquo;volatile-random\u0026rsquo;\n\u0026lsquo;allkeys-lru\u0026rsquo;\n\u0026lsquo;allkeys-random\u0026rsquo;\n\u0026lsquo;noeviction\u0026rsquo;\n\u0026lsquo;volatile-lfu\u0026rsquo;\n\u0026lsquo;allkeys-lfu\u0026rsquo; Database eviction policy (Redis style).\nRedis DB default: \u0026lsquo;volatile-lru\u0026rsquo;\nmemcached DB default: \u0026lsquo;allkeys-lru\u0026rsquo; export_failure_reason \u0026rsquo;no-permission\u0026rsquo;\n\u0026lsquo;wrong-file-path\u0026rsquo;\n\u0026lsquo;general-error\u0026rsquo; Reason of last failed export process (read-only) export_progress number, (range: 0-100) Database manually triggered export progress (percentage) (read-only) export_status \u0026rsquo;exporting\u0026rsquo;\n\u0026lsquo;succeeded\u0026rsquo;\n\u0026lsquo;failed\u0026rsquo; Status of manually triggered export process (read-only) generate_text_monitor boolean Enable/disable generation of syncer monitoring information gradual_src_max_sources integer (default: 1) Sync a maximum N sources in parallel (gradual_src_mode should be enabled for this to take effect) gradual_src_mode \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; Indicates if gradual sync (of sync sources) should be activated gradual_sync_max_shards_per_source integer (default: 1) Sync a maximum of N shards per source in parallel (gradual_sync_mode should be enabled for this to take effect) gradual_sync_mode \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;auto\u0026rsquo; Indicates if gradual sync (of source shards) should be activated (\u0026lsquo;auto\u0026rsquo; for automatic decision) hash_slots_policy \u0026rsquo;legacy\u0026rsquo;\n\u0026lsquo;16k\u0026rsquo; The policy used for hash slots handling\n\u0026rsquo;legacy\u0026rsquo;: slots range is \u0026lsquo;1-4096\u0026rsquo;\n\u0026lsquo;16k\u0026rsquo;: slots range is \u0026lsquo;0-16383\u0026rsquo; implicit_shard_key boolean (default: false) Controls the behavior of what happens in case a key does not match any of the regex rules. true: if a key does not match any of the rules, the entire key will be used for the hashing function false: if a key does not match any of the rules, an error will be returned. import_failure_reason \u0026lsquo;download-error\u0026rsquo;\n\u0026lsquo;file-corrupted\u0026rsquo;\n\u0026lsquo;general-error\u0026rsquo;\n\u0026lsquo;file-larger-than-mem-limit:\u0026lt;n bytes of expected dataset\u0026gt;:\u0026lt;n bytes configured bdb limit\u0026gt;\u0026rsquo;\n\u0026lsquo;key-too-long\u0026rsquo;\n\u0026lsquo;invalid-bulk-length\u0026rsquo;\n\u0026lsquo;out-of-memory\u0026rsquo; Import failure reason (read-only) import_progress number, (range: 0-100) Database import progress (percentage) (read-only) import_status \u0026lsquo;idle\u0026rsquo;\n\u0026lsquo;initializing\u0026rsquo;\n\u0026lsquo;importing\u0026rsquo;\n\u0026lsquo;succeeded\u0026rsquo;\n\u0026lsquo;failed\u0026rsquo; Database import process status (read-only) internal boolean (default: false) Is this a database used by the cluster internally last_backup_time string Time of last successful backup (read-only) last_changed_time string Last administrative configuration change (read-only) last_export_time string Time of last successful export (read-only) max_aof_file_size integer Maximum size for shard\u0026rsquo;s AOF file (bytes). Default 300GB, (on bigstore DB 150GB) max_aof_load_time integer (default: 3600) Maximum time shard\u0026rsquo;s AOF reload should take (seconds). max_connections integer (default: 0) Maximum number of client connections allowed (0 unlimited) memory_size integer (default: 0) Database memory limit (0 is unlimited), expressed in bytes. metrics_export_all boolean Enable/disable exposing all shard metrics through the metrics exporter mkms boolean (default: true) Are MKMS (Multi Key Multi Slots) commands supported? module_list [{ \"module_id\": string, \"module_args\": [u'string', u'null'], \"module_name\": string, \"semantic_version\": string }, ...] List of modules associated with database\nmodule_id: Module UID module_args: Module command line arguments (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;)\nmodule_name: Module\u0026rsquo;s name\nsemantic_version: Module\u0026rsquo;s semantic version mtls_allow_outdated_certs boolean An optional mTLS relaxation flag for certs verification mtls_allow_weak_hashing boolean An optional mTLS relaxation flag for certs verification name string Database name oss_cluster boolean (default: false) OSS Cluster mode option. Cannot be enabled with 'hash_slots_policy': 'legacy' oss_cluster_api_preferred_ip_type \u0026lsquo;internal\u0026rsquo; \u0026lsquo;external\u0026rsquo; Internal/external IP type in OSS cluster API. Default value for new endpoints oss_sharding boolean (default: false) An alternative to shard_key_regex for using the common case of the OSS shard hashing policy port integer TCP port on which the database is available. Generated automatically if omitted and returned as 0 proxy_policy \u0026lsquo;single\u0026rsquo;\n\u0026lsquo;all-master-shards\u0026rsquo;\n\u0026lsquo;all-nodes\u0026rsquo; The default policy used for proxy binding to endpoints rack_aware boolean (default: false) Require the database to always replicate across multiple racks redis_version string Version of the redis-server processes: e.g. 6.0, 5.0-big repl_backlog_size string Redis replication backlog size (\u0026lsquo;auto\u0026rsquo; or size in bytes) replica_sources array of syncer_sources objects Remote endpoints of database to sync from. See the \u0026lsquo;bdb -\u0026gt; replica_sources\u0026rsquo; section replica_sync \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; \u0026lsquo;paused\u0026rsquo;\n\u0026lsquo;stopped\u0026rsquo; Enable, disable, or pause syncing from specified replica_sources replica_sync_dist boolean Enable/disable distributed syncer in replica-of replication boolean (default: false) In-memory database replication mode roles_permissions [{ \"role_uid\": integer, \"redis_acl_uid\": integer }, ...] shard_block_crossslot_keys boolean (default: false) In Lua scripts, prevent use of keys from different hash slots within the range owned by the current shard shard_block_foreign_keys boolean (default: true) In Lua scripts, foreign_keys prevent use of keys which could reside in a different shard (foreign keys) shard_key_regex [{ \"regex\": string }, ...] To use the default rules you should set the value to: [ { “regex”: “.*\\\\{(?\u003c tag \u003e.*)\\\\}.*” }, { “regex”: “(?\u003c tag \u003e.*)” } ] Custom keyname-based sharding rules. shard_list array of integers Cluster unique IDs of all database shards. sharding boolean (default: false) Cluster mode (server-side sharding). When true, shard hashing rules must be provided by either oss_sharding or shard_key_regex shards_count integer, (range: 1-512) (default: 1) Number of database server-side shards shards_placement \u0026lsquo;dense\u0026rsquo; \u0026lsquo;sparse\u0026rsquo; Control the density of shards \u0026lsquo;dense\u0026rsquo;: Shards reside on as few nodes as possible \u0026lsquo;sparse\u0026rsquo;: Shards reside on as many nodes as possible skip_import_analyze \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; Enable/disable skipping the analysis stage when importing an RDB file slave_ha boolean Enable replica high availability mechanism for this database (default takes the cluster setting) slave_ha_priority integer Priority of the BDB in replica high availability mechanism snapshot_policy array of snapshot_policy objects Policy for snapshot-based data persistence. A dataset snapshot will be taken every N secs if there are at least M writes changes in the dataset ssl boolean (default: false) Require SSL authenticated and encrypted connections to the database (deprecated) status \u0026lsquo;pending\u0026rsquo;\n\u0026lsquo;active\u0026rsquo;\n\u0026lsquo;active-change-pending\u0026rsquo;\n\u0026lsquo;delete-pending\u0026rsquo;\n\u0026lsquo;import-pending\u0026rsquo;\n\u0026lsquo;creation-failed\u0026rsquo;\n\u0026lsquo;recovery\u0026rsquo; Database lifecycle status (read-only) sync \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; \u0026lsquo;paused\u0026rsquo;\n\u0026lsquo;stopped\u0026rsquo; (deprecated, use replica_sync or crdt_sync instead) Enable, disable, or pause syncing from specified sync_sources sync_sources [{ \"uid\": integer, \"uri\": string, \"compression\": integer, \"status\": string, \"rdb_transferred\": integer, \"rdb_size\": integer, \"last_update\": string, \"lag\": integer, \"last_error\": string }, ...] (deprecated, instead use replica_sources or crdt_sources) Remote endpoints of database to sync from. See the \u0026lsquo;bdb -\u0026gt; replica_sources\u0026rsquo; section\nuid: Numeric unique identification of this source\nuri: Source Redis URI\ncompression: Compression level for the replication link\nstatus: Sync status of this source\nrdb_transferred: Number of bytes transferred from the source\u0026rsquo;s RDB during the syncing phase\nrdb_size: The source\u0026rsquo;s RDB size to be transferred during the syncing phase\nlast_update: Time last update was received from the source\nlag: Lag in millisec between source and destination (while synced)\nlast_error: Last error encountered when syncing from the source syncer_mode \u0026lsquo;distributed\u0026rsquo;\n\u0026lsquo;centralized\u0026rsquo; The syncer for replication between database instances is either on a single node (centralized) or on each node that has a proxy according to the proxy policy (distributed). (read-only) tags [{ \"key\": string, \"value\": string }, ...] Optional list of tags objects attached to the database\nkey: Represents the tag\u0026rsquo;s meaning and must be unique among tags (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;)\nvalue: The tag\u0026rsquo;s value tls_mode \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; \u0026lsquo;replica_ssl\u0026rsquo; Require TLS-authenticated and encrypted connections to the database type \u0026lsquo;redis\u0026rsquo; \u0026lsquo;memcached\u0026rsquo; Type of database use_nodes array of strings Cluster node UIDs to use for database shards and bound endpoints version string Database compatibility version: full Redis/memcached version number, e.g. 6.0.6 wait_command boolean (default: true) Supports Redis wait command (read-only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/db_alerts_settings/bdb_alert_settings_with_threshold/","uriRel":"/rs/references/rest-api/objects/db_alerts_settings/bdb_alert_settings_with_threshold/","title":"BDB alert settings with threshold object","tags":[],"keywords":[],"description":"Documents the bdb_alert_settings_with_threshold object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description enabled boolean (default: false) Alert enabled or disabled threshold string Threshold for alert going on/off ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb_group/","uriRel":"/rs/references/rest-api/objects/bdb_group/","title":"BDB group object","tags":[],"keywords":[],"description":"An object that represents a group of databases with a shared memory pool","content":"An API object that represents a group of databases that share a memory pool.\nName Type/Value Description uid integer Cluster unique ID of the database group members array of strings A list of UIDs of member databases (read-only) memory_size integer The common memory pool size limit for all databases in the group, expressed in bytes ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/","uriRel":"/rs/references/rest-api/requests/bdbs/","title":"Database requests","tags":[],"keywords":[],"description":"Database requests","content":" Method Path Description GET /v1/bdbs Get all databases GET /v1/bdbs/{uid} Get a single database PUT /v1/bdbs/{uid} Update database configuration PUT /v1/bdbs/{uid}/{action} Update database configuration and perform additional action POST /v1/bdbs Create a new database POST /v2/bdbs Create a new database DELETE /v1/bdbs/{uid} Delete a database Get all databases GET /v1/bdbs Get all databases in the cluster.\nPermissions Permission name Roles view_all_bdbs_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /v1/bdbs?fields=uid,name Headers Key Value Host The domain name or IP of the cluster Accept application/json Query parameters Field Type Description fields string Comma-separated list of field names to return (by default all fields are returned). (optional) Response The response body contains a JSON array with all databases, represented as BDB objects.\nBody [ { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;name of database #1\u0026#34;, \u0026#34;// additional fields...\u0026#34; }, { \u0026#34;uid\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;name of database #2\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] Status codes Code Description 200 OK No error Example requests cURL $ curl -k -X GET -u \u0026#34;[username]:[password]\u0026#34; \\ -H \u0026#34;accept: application/json\u0026#34; \\ https://[host][:port]/v1/bdbs?fields=uid,name Python import requests import json url = \u0026#34;https://[host][:port]/v1/bdbs?fields=uid,name\u0026#34; auth = (\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } response = requests.request(\u0026#34;GET\u0026#34;, url, auth=auth, headers=headers) print(response.text) Get a database GET /v1/bdbs/{int: uid} Get a single database.\nPermissions Permission name Roles view_bdb_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1 Headers Key Value Host The domain name or IP of the cluster Accept application/json URL parameters Field Type Description uid integer The unique ID of the database requested. Query parameters Field Type Description fields string Comma-separated list of field names to return (by default all fields are returned). (optional) Response Returns a BDB object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;name of database #1\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Database UID does not exist Update database configuration PUT /v1/bdbs/{int: uid} Update the configuration of an active database.\nIf called with the dry_run URL query string, the function will validate the BDB object against the existing database, but will not invoke the state machine that will update it.\nThis is the basic version of the update request. See Update database and perform action to send an update request with an additional action.\nTo track this request\u0026rsquo;s progress, poll the /actions/\u0026lt;action_uid\u0026gt; endpoint with the action_uid returned in the response body.\nPermissions Permission name Roles update_bdb admin\ncluster_member\ndb_member Request Example HTTP request PUT /bdbs/1 Headers Key Value Host The domain name or IP of the cluster Accept application/json Content-type application/json Query parameters Field Type Description dry_run Validate the new BDB object but don\u0026rsquo;t apply the update. URL parameters Field Type Description uid integer The unique ID of the database for which update is requested. Body Include a BDB object with updated fields in the request body.\nExample JSON body { \u0026#34;replication\u0026#34;: true, \u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34; } The above request attempts to modify a database configuration to enable in-memory data replication and append-only file data persistence.\nResponse Returns the updated BDB object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;replication\u0026#34;: true, \u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK The request is accepted and is being processed. The database state will be \u0026lsquo;active-change-pending\u0026rsquo; until the request has been fully processed. 404 Not Found Attempting to change a nonexistent database. 406 Not Acceptable The requested configuration is invalid. 409 Conflict Attempting to change a database while it is busy with another configuration change. In this context, this is a temporary condition, and the request should be reattempted later. Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description rack_awareness_violation • Non rack-aware cluster.\n• Not enough nodes in unique racks. invalid_certificate SSL client certificate is missing or malformed. certificate_expired SSL client certificate has expired. duplicated_certs An SSL client certificate appears more than once. insufficient_resources Shards count exceeds shards limit per bdb. not_supported_action_on_crdt reset_admin_pass action is not allowed on CRDT enabled BDB. name_violation CRDT database name cannot be changed. bad_shards_blueprint The sharding blueprint is broken or doesn’t fit the BDB. replication_violation CRDT database must use replication. eviction_policy_violation LFU eviction policy is not supported on bdb version\u0026lt;4 replication_node_violation Not enough nodes for replication. replication_size_violation Database limit too small for replication. invalid_oss_cluster_configuration BDB configuration does not meet the requirements for OSS cluster mode missing_backup_interval BDB backup is enabled but backup interval is missing. crdt_sharding_violation CRDB created without sharding cannot be changed to use sharding invalid_proxy_policy Invalid proxy_policy value. invalid_bdb_tags Tag objects with the same key parameter were passed. unsupported_module_capabilities Not all modules configured for the database support the capabilities needed for the database configuration. redis_acl_unsupported Redis ACL is not supported for this database. Update database and perform action PUT /v1/bdbs/{int: uid}/{action} Update the configuration of an active database and perform an additional action.\nIf called with the dry_run URL query string, the function will validate the BDB object against the existing database, but will not invoke the state machine that will update it.\nPermissions Permission name Roles update_bdb_with_action admin\ncluster_member\ndb_member Request Example HTTP request PUT /bdbs/1/reset_admin_pass The above request resets the admin password after updating the database.\nHeaders Key Value Host The domain name or IP of the cluster Accept application/json Content-type application/json URL parameters Field Type Description uid integer The unique ID of the database to update. action string Additional action to perform. Currently supported actions are: flush, reset_admin_pass. Query parameters Field Type Description dry_run Validate the new BDB object but don\u0026rsquo;t apply the update. Body Include a BDB object with updated fields in the request body.\nExample JSON body { \u0026#34;replication\u0026#34;: true, \u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34; } The above request attempts to modify a database configuration to enable in-memory data replication and append-only file data persistence.\nNote: To change the shard hashing policy, you must flush all keys from the database. Response If the request succeeds, the response body returns the updated BDB object. If an error occurs, the response body may include an error code and message with more details.\nStatus codes Code Description 200 OK The request is accepted and is being processed. The database state will be \u0026lsquo;active-change-pending\u0026rsquo; until the request has been fully processed. 403 Forbidden redislabs license expired. 404 Not Found Attempting to change a nonexistent database. 406 Not Acceptable The requested configuration is invalid. 409 Conflict Attempting to change a database while it is busy with another configuration change. In this context, this is a temporary condition, and the request should be reattempted later. Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description rack_awareness_violation • Non rack-aware cluster.\n• Not enough nodes in unique racks. invalid_certificate SSL client certificate is missing or malformed. certificate_expired SSL client certificate has expired. duplicated_certs An SSL client certificate appears more than once. insufficient_resources Shards count exceeds shards limit per bdb. not_supported_action_on_crdt reset_admin_pass action is not allowed on CRDT enabled BDB. name_violation CRDT database name cannot be changed. bad_shards_blueprint The sharding blueprint is broken or doesn’t fit the BDB. replication_violation CRDT database must use replication. eviction_policy_violation LFU eviction policy is not supported on bdb version\u0026lt;4 replication_node_violation Not enough nodes for replication. replication_size_violation Database limit too small for replication. invalid_oss_cluster_configuration BDB configuration does not meet the requirements for OSS cluster mode missing_backup_interval BDB backup is enabled but backup interval is missing. crdt_sharding_violation CRDB created without sharding cannot be changed to use sharding invalid_proxy_policy Invalid proxy_policy value. invalid_bdb_tags Tag objects with the same key parameter were passed. unsupported_module_capabilities Not all modules configured for the database support the capabilities needed for the database configuration. redis_acl_unsupported Redis ACL is not supported for this database. Create database v1 POST /v1/bdbs Create a new database in the cluster.\nThe request must contain a single JSON BDB object with the configuration parameters for the new database.\nThe following parameters are required to create the database:\nParameter Type/Value Description name string Name of the new database memory_size integer Size of the database, in bytes If passed with the dry_run URL query string, the function will validate the BDB object, but will not invoke the state machine that will create it.\nTo track this request\u0026rsquo;s progress, poll the /actions/\u0026lt;action_uid\u0026gt; endpoint with the action_uid returned in the response body.\nThe cluster will use default configuration for any missing database field. The cluster creates a database UID if it is missing.\nPermissions Permission name Roles create_bdb admin\ncluster_member\ndb_member Request Example HTTP request POST /bdbs Headers Key Value Host The domain name or IP of the cluster Accept application/json Content-type application/json Query parameters Field Type Description dry_run Validate the new BDB object but don\u0026rsquo;t create the database. Body Include a BDB object in the request body.\nThe following parameters are required to create the database:\nParamter Type/Value Description name string Name of the new database memory_size integer Size of the database, in bytes The uid of the database is auto-assigned by the cluster because it was not explicitly listed in this request. If you specify the database ID (uid), then you must specify the database ID for every subsequent database and make sure that the database ID does not conflict with an existing database. If you do not specify the database ID, then the it is automatically assigned in sequential order.\nDefaults are used for all other configuration parameters.\nExample JSON body { \u0026#34;name\u0026#34;: \u0026#34;test-database\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;memory_size\u0026#34;: 1073741824 } The above request is an attempt to create a Redis database with a user-specified name and a memory limit of 1GB.\nResponse The response includes the newly created BDB object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;test-database\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;memory_size\u0026#34;: 1073741824, \u0026#34;// additional fields...\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description uid_exists The specified database UID is already in use. missing_db_name DB name is a required property. missing_memory_size Memory Size is a required property. missing_module Modules missing from the cluster. port_unavailable The specified database port is reserved or already in use. invalid_sharding Invalid sharding configuration was specified. bad_shards_blueprint The sharding blueprint is broken. not_rack_aware Cluster is not rack-aware and a rack-aware database was requested. invalid_version An invalid database version was requested. busy The request failed because another request is being processed at the same time on the same database. invalid_data_persistence Invalid data persistence configuration. invalid_proxy_policy Invalid proxy_policy value. invalid_sasl_credentials SASL credentials are missing or invalid. invalid_replication Not enough nodes to perform replication. insufficient_resources Not enough resources in cluster to host the database. rack_awareness_violation • Rack awareness violation.• Not enough nodes in unique racks. invalid_certificate SSL client certificate is missing or malformed. certificate_expired SSL client certificate has expired. duplicated_certs An SSL client certificate appears more than once. replication_violation CRDT database must use replication. eviction_policy_violation LFU eviction policy is not supported on bdb version\u0026lt;4 invalid_oss_cluster_configuration BDB configuration does not meet the requirements for OSS cluster mode memcached_cannot_use_modules Cannot create a memcached database with modules. missing_backup_interval BDB backup is enabled but backup interval is missing. wrong_cluster_state_id The given CLUSTER-STATE-ID does not match the current one invalid_bdb_tags Tag objects with the same key parameter were passed. unsupported_module_capabilities Not all modules configured for the database support the capabilities needed for the database configuration. redis_acl_unsupported Redis ACL is not supported for this database. Status codes Code Description 403 Forbidden redislabs license expired. 409 Conflict Database with the same UID already exists. 406 Not Acceptable Invalid configuration parameters provided. 200 OK Success, database is being created. Create database v2 POST /v2/bdbs Create a new database in the cluster. See POST /v1/bdbs for more information.\nThe database\u0026rsquo;s configuration should be under the \u0026ldquo;bdb\u0026rdquo; field.\nThis endpoint allows you to specify a recovery_plan to recover a database. If you include a recovery_plan within the request body, the database will be loaded from the persistence files according to the recovery plan. The recovery plan must match the number of shards requested for the database.\nThe persistence files must exist in the locations specified by the recovery plan. The persistence files must belong to a database with the same shard settings as the one being created (slot range distribution and shard_key_regex); otherwise, the operation will fail or yield unpredictable results.\nIf you create a database with a shards_blueprint and a recovery plan, the shard placement may not fully follow the shards_blueprint.\nRequest Example HTTP request POST /v2/bdbs Headers Key Value Host The domain name or IP of the cluster Accept application/json Content-type application/json Query parameters Field Type Description dry_run Validate the new BDB object but don\u0026rsquo;t create the database. Body Include a JSON object that contains a BDB object and an optional recovery_plan object in the request body.\nExample JSON body { \u0026#34;bdb\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test-database\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;memory_size\u0026#34;: 1073741824, \u0026#34;shards_count\u0026#34;: 1 }, \u0026#34;recovery_plan\u0026#34;: { \u0026#34;data_files\u0026#34;: [ { \u0026#34;shard_slots\u0026#34;: \u0026#34;0-16383\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;redis-4.rdb\u0026#34; } ] } } Response The response includes the newly created BDB object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;test-database\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;memory_size\u0026#34;: 1073741824, \u0026#34;shards_count\u0026#34;: 1, \u0026#34;// additional fields...\u0026#34; } Delete database DELETE /v1/bdbs/{int: uid} Delete an active database.\nPermissions Permission name Roles delete_bdb admin\ncluster_member\ndb_member Request Example HTTP request DELETE /bdbs/1 Headers Key Value Host The domain name or IP of the cluster Accept application/json URL parameters Field Type Description uid integer The unique ID of the database to delete. Response Returns a status code that indicates the database deletion success or failure.\nStatus codes Code Description 200 OK The request is accepted and is being processed. The database state will be \u0026lsquo;delete-pending\u0026rsquo; until the request has been fully processed. 403 Forbidden Attempting to delete an internal database. 404 Not Found Attempting to delete a nonexistent database. 409 Conflict Either the database is not in \u0026lsquo;active\u0026rsquo; state and cannot be deleted, or it is busy with another configuration change. In the second case, this is a temporary condition, and the request should be re-attempted later. ","categories":["RS"]},{"uri":"/rs/clusters/optimize/memtier-benchmark/","uriRel":"/rs/clusters/optimize/memtier-benchmark/","title":"Benchmarking Redis Enterprise","tags":[],"keywords":[],"description":"Use the `memtier_benchmark` tool to perform a performance benchmark of Redis Enterprise Software.","content":"Use the memtier_benchmark tool to perform a performance benchmark of Redis Enterprise Software.\nPrerequisites:\nRedis Enterprise Software installed A cluster configured A database created For help with the prerequisites,see Get started with Redis Enterprise Software.\nIt is recommended to run memtier_benchmark on a separate node that is not part of the cluster being tested. If you run it on a node of the cluster, be mindful that it affects the performance of both the cluster and memtier_benchmark.\n/opt/redislabs/bin/memtier_benchmark -s $DB_HOST -p $DB_PORT -a $DB_PASSWORD -t 4 -R --ratio=1:1 This command instructs memtier_benchmark to connect to your Redis Enterprise database and generates a load doing the following:\nA 50/50 Set to Get ratio Each object has random data in the value Populate a database with testing data If you need to populate a database with some test data for a proof of concept, or failover testing, etc. here is an example for you.\n/opt/redislabs/bin/memtier_benchmark -s $DB_HOST -p $DB_PORT -a $DB_PASSWORD -R -n allkeys -d 500 --key-pattern=P:P --ratio=1:0 This command instructs memtier_benchmark to connect to your Redis Enterprise database and generates a load doing the following:\nWrite objects only, no reads A 500 byte object Each object has random data in the value Each key has a random pattern, then a colon, followed by a random pattern. Run this command until it fills up your database to where you want it for testing. The easiest way to check is on the database metrics page.\nAnother use for memtier_benchmark is to populate a database with data for failure testing.\n","categories":["RS"]},{"uri":"/rs/references/memtier-benchmark/","uriRel":"/rs/references/memtier-benchmark/","title":"Benchmark a Redis on Flash enabled database","tags":[],"keywords":[],"description":"","content":"Redis on Flash (RoF) on Redis Enterprise Software lets you use cost-effective Flash memory as a RAM extension for your database.\nBut what does the performance look like as compared to a memory-only database, one stored solely in RAM?\nThese scenarios use the memtier_benchmark utility to evaluate the performance of a Redis Enterprise Software deployment, including the trial version.\nThe memtier_benchmark utility is located in /opt/redislabs/bin/ of Redis Enterprise Software deployments. To test performance for cloud provider deployments, see the memtier-benchmark GitHub project.\nFor additional, such as assistance with larger clusters, contact support.\nBenchmark and performance test considerations These tests assume you\u0026rsquo;re using a trial version of Redis Enterprise Software and want to test the performance of a Redis on Flash enabled database in the following scenarios:\nWithout replication: Four (4) master shards With replication: Two (2) primary and two replica shards With the trial version of Redis Enterprise Software you can create a cluster of up to four shards using a combination of database configurations, including:\nFour databases, each with a single master shard Two highly available databases with replication enabled (each database has one master shard and one replica shard) One non-replicated clustered database with four master shards One highly available and clustered database with two master shards and two replica shards Test environment and cluster setup For the test environment, you need to:\nCreate a cluster with three nodes. Prepare the flash memory. Configure the load generation tool. Creating a three-node cluster This performance test requires a three-node cluster.\nYou can run all of these tests on Amazon AWS with these hosts:\n2 x i3.2xlarge (8 vCPU, 61 GiB RAM, up to 10GBit, 1.9TB NMVe SSD)\nThese nodes serve RoF data\n1 x m4.large, which acts as a quorum node\nFor instructions on how to install RS and set up a cluster, go to either:\nQuick Setup for a test installation Install and Upgrade for a production installation These tests use a quorum node to reduce AWS EC2 instance use while maintaining the three nodes required to support a quorum node in case of node failure. Quorum nodes can be on less powerful instances because they do not have shards or support traffic.\nAs of this writing, i3.2xlarge instances are required because they support NVMe SSDs, which are required to support RoF. Redis on Flash requires Flash-enabled storage, such as NVMe SSDs.\nFor best results, compare performance of a Flash-enabled deployment to the performance in a RAM-only environment, such as a strictly on-premises deployment.\nPrepare the flash memory After you install RS on the nodes, the flash memory attached to the i3.2xlarge instances must be prepared and formatted with the /opt/redislabs/sbin/prepare_flash.sh script.\nSet up the load generation tool The memtier_benchmark load generator tool generates the load on the RoF databases. To use this tool, install RS on a dedicated instance that is not part of the RS cluster but is in the same region/zone/subnet of your cluster. We recommend that you use a relatively powerful instance to avoid bottlenecks at the load generation tool itself.\nFor these tests, the load generation host uses a c4.8xlarge instance type.\nDatabase configuration parameters Create a Redis on Flash test database You can use the RS admin console to create a test database. We recommend that you use a separate database for each test case with these requirements:\nParameter With replication Without replication Description Name test-1 test-2 The name of the test database Memory limit 100 GB 100 GB The memory limit refers to RAM+Flash, aggregated across all the shards of the database, including master and replica shards. RAM limit 0.3 0.3 RoF always keeps the Redis keys and Redis dictionary in RAM and additional RAM is required for storing hot values. For the purpose of these tests 30% RAM was calculated as an optimal value. Replication Enabled Disabled A database with no replication has only master shards. A database with replication has master and replica shards. Data persistence None None No data persistence is needed for these tests. Database clustering Enabled Enabled A clustered database consists of multiple shards. Number of (master) shards 2 4 Shards are distributed as follows:- With replication: One master shard and one replica shard on each node- Without replication: Two master shards on each node Other parameters Default Default Keep the default values for the other configuration parameters. Data population Populate the benchmark dataset The memtier_benchmark load generation tool populates the database. To populate the database with N items of 500 Bytes each in size, on the load generation instance run:\n$ memtier_benchmark -s $DB_HOST -p $DB_PORT --hide-histogram --key-maximum=$N -n allkeys -d 500 --key-pattern=P:P --ratio=1:0 Set up a test database:\nParameter Description Database host(-s) The fully qualified name of the endpoint or the IP shown in the RS database configuration Database port(-p) The endpoint port shown in your database configuration Number of items(–key-maximum) With replication: 75 MillionWithout replication: 150 Million Item size(-d) 500 Bytes Centralize the keyspace With replication To create roughly 20.5 million items in RAM for your highly available clustered database with 75 million items, run:\n$ memtier_benchmark -s $DB_HOST -p $DB_PORT --hide-histogram --key-minimum=27250000 --key-maximum=47750000 -n allkeys --key-pattern=P:P --ratio=0:1 To verify the database values, use Values in RAM metric, which is available from the Metrics tab of your database in the admin console.\nWithout replication To create 41 million items in RAM without replication enabled and 150 million items, run:\n$ memtier_benchmark -s $DB_HOST -p $DB_PORT --hide-histogram --key-minimum=54500000 --key-maximum=95500000 -n allkeys --key-pattern=P:P --ratio=0:1 Test runs Generate load With replication We recommend that you do a dry run and double check the RAM Hit Ratio on the metrics screen in the RS admin console before you write down the test results.\nTo test RoF with an 85% RAM Hit Ratio, run:\n$ memtier_benchmark -s $DB_HOST -p $DB_PORT --pipeline=11 -c 20 -t 1 -d 500 --key-maximum=75000000 --key-pattern=G:G --key-stddev=5125000 --ratio=1:1 --distinct-client-seed --randomize --test-time=600 --run-count=1 --out-file=test.out Without replication Here is the command for 150 million items:\n$ memtier_benchmark -s $DB_HOST -p $DB_PORT --pipeline=24 -c 20 -t 1 -d 500 --key-maximum=150000000 --key-pattern=G:G --key-stddev=10250000 --ratio=1:1 --distinct-client-seed --randomize --test-time=600 --run-count=1 --out-file=test.out Where:\nParameter Description Access pattern (\u0026ndash;key-pattern) and standard deviation (\u0026ndash;key-stddev) Controls the RAM Hit ratio after the centralization process is complete Number of threads (-t and -c)\\ Controls how many connections are opened to the database, whereby the number of connections is the number of threads multiplied by the number of connections per thread (-t) and number of clients per thread (-c) Pipelining (\u0026ndash;pipeline)\\ Pipelining allows you to send multiple requests without waiting for each individual response (-t) and number of clients per thread (-c) Read\\write ratio (\u0026ndash;ratio)\\ A value of 1:1 means that you have the same number of write operations as read operations (-t) and number of clients per thread (-c) Test results Monitor the test results You can either monitor the results in the metrics tab of the admin console or with the memtier_benchmark output. However, be aware that:\nThe memtier_benchmark results include the network latency between the load generator instance and the cluster instances.\nThe metrics shown in the admin console do not include network latency.\nExpected results You should expect to see an average throughput of:\nAround 160,000 ops/sec when testing without replication (i.e. Four master shards) Around 115,000 ops/sec when testing with enabled replication (i.e. Four master and 2 replica shards) In both cases, the average latency should be below one millisecond.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/bind/","uriRel":"/rs/references/cli-utilities/rladmin/bind/","title":"rladmin bind","tags":[],"keywords":[],"description":"Manages the proxy policy for a specified database endpoint.","content":"Manages the proxy policy for a specific database endpoint.\nbind endpoint exclude Defines a list of nodes to exclude from the proxy policy for a specific database endpoint. When you exclude a node, the endpoint cannot bind to the node\u0026rsquo;s proxy.\nEach time you run an exclude command, it overwrites the previous list of excluded nodes.\nrladmin bind [ db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } ] endpoint \u0026lt;id\u0026gt; exclude \u0026lt;proxy_id1 .. proxy_idN\u0026gt; Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Only allows endpoints for the specified database endpoint endpoint ID Changes proxy settings for the specified endpoint proxy list of proxy IDs Proxies to exclude Returns Returns Finished successfully if the list of excluded proxies was successfully changed. Otherwise, it returns an error.\nUse rladmin status endpoints to verify that the policy changed.\nExample $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:2 all-nodes No db:6 tr02 endpoint:6:1 node:1 all-nodes No db:6 tr02 endpoint:6:1 node:3 all-nodes No $ rladmin bind endpoint 6:1 exclude 2 Executing bind endpoint: OOO. Finished successfully $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:1 all-nodes -2 No db:6 tr02 endpoint:6:1 node:3 all-nodes -2 No bind endpoint include Defines a list of nodes to include in the proxy policy for the specific database endpoint.\nEach time you run an include command, it overwrites the previous list of included nodes.\nrladmin bind [ db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } ] endpoint \u0026lt;id\u0026gt; include \u0026lt;proxy_id1 .. proxy_idN\u0026gt; Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Only allows endpoints for the specified database endpoint endpoint ID Changes proxy settings for the specified endpoint proxy list of proxy IDs Proxies to include Returns Returns Finished successfully if the list of included proxies was successfully changed. Otherwise, it returns an error.\nUse rladmin status endpoints to verify that the policy changed.\nExample $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:3 all-master-shards No $ rladmin bind endpoint 6:1 include 3 Executing bind endpoint: OOO. Finished successfully $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:1 all-master-shards +3 No db:6 tr02 endpoint:6:1 node:3 all-master-shards +3 No bind endpoint policy Changes the overall proxy policy for a specific database endpoint.\nrladmin bind [ db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } ] endpoint \u0026lt;id\u0026gt; policy { single | all-master-shards | all-nodes } Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Only allows endpoints for the specified database endpoint endpoint ID Changes proxy settings for the specified endpoint policy \u0026lsquo;all-master-shards\u0026rsquo;\n\u0026lsquo;all-nodes\u0026rsquo;\n\u0026lsquo;single\u0026rsquo; Changes the proxy policy to the specified policy Proxy policy Description all-master-shards Multiple proxies, one on each master node (best for high traffic and multiple master shards) all-nodes Multiple proxies, one on each node of the cluster (increases traffic in the cluster, only used in special cases) single All traffic flows through a single proxy bound to the database endpoint (preferable in most cases) Returns Returns Finished successfully if the proxy policy was successfully changed. Otherwise, it returns an error.\nUse rladmin status endpoints to verify that the policy changed.\nExample $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:1 all-nodes -2 No db:6 tr02 endpoint:6:1 node:3 all-nodes -2 No $ rladmin bind endpoint 6:1 policy all-master-shards Executing bind endpoint: OOO. Finished successfully $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:3 all-master-shards No ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/","uriRel":"/rs/references/rest-api/objects/bootstrap/","title":"Bootstrap object","tags":[],"keywords":[],"description":"An object for bootstrap configuration","content":"A bootstrap configuration object.\nName Type/Value Description action \u0026lsquo;create_cluster\u0026rsquo;\n\u0026lsquo;join_cluster\u0026rsquo;\n\u0026lsquo;recover_cluster\u0026rsquo; Action to perform cluster cluster_identity object Cluster to join or create cnm_https_port integer Port to join a cluster with non-default cnm_https port credentials credentials object Cluster admin credentials dns_suffixes [{ \"name\": string, \"cluster_default\": boolean, \"use_aaaa_ns\": boolean, \"use_internal_addr\": boolean, \"slaves\": array }, ...] Explicit configuration of DNS suffixes\nname: DNS suffix name\ncluster_default: Should this suffix be the default cluster suffix\nuse_aaaa_ns: Should AAAA records be published for NS records\nuse_internal_addr: Should internal cluster IPs be published for databases\nslaves: List of replica servers that should be published as NS and notified license string License string max_retries integer Max number of retries in case of recoverable errors node node_identity object Node description policy policy object Policy object recovery_filename string Name of backup file to recover from required_version string This node can only join the cluster if all nodes in the cluster have a version greater than the required_version retry_time integer Max waiting time between retries (in seconds) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bootstrap/","uriRel":"/rs/references/rest-api/requests/bootstrap/","title":"Bootstrap requests","tags":[],"keywords":[],"description":"Bootstrap requests","content":" Method Path Description GET /v1/boostrap Get the local node\u0026rsquo;s bootstrap status POST /v1/bootstrap/{action} Initiate bootstrapping Get bootstrap status GET /v1/bootstrap Get the local node\u0026rsquo;s bootstrap status.\nThis request is accepted as soon the cluster software is installed and before the node is part of an active cluster.\nOnce the node is part of an active cluster, authentication is required.\nRequest Example HTTP request GET /bootstrap Headers Key Value Description Accept application/json Accepted media type Response The JSON response object contains a bootstrap_status object and a local_node_info object.\nThe bootstrap_status object contains the following information:\nField Description state Current bootstrap state.\nidle: No bootstrapping started.\ninitiated: Bootstrap request received.\ncreating_cluster: In the process of creating a new cluster.\njoining_cluster: In the process of joining an existing cluster.\nerror: The last bootstrap action failed.\ncompleted: The last bootstrap action completed successfully. start_time Bootstrap process start time end_time Bootstrap process end time error_code If state is error, this error code describes the type of error encountered. error_details An error-specific object that may contain additional information about the error. A common field in use is message which provides a more verbose error message. The local_node_info object is a subset of a node object that provides information about the node configuration.\nExample JSON body { \u0026#34;bootstrap_status\u0026#34;: { \u0026#34;start_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34; }, \u0026#34;local_node_info\u0026#34;: { \u0026#34;uid\u0026#34;: 3, \u0026#34;software_version\u0026#34;: \u0026#34;0.90.0-1\u0026#34;, \u0026#34;cores\u0026#34;: 2, \u0026#34;ephemeral_storage_path\u0026#34;: \u0026#34;/var/opt/redislabs/tmp\u0026#34;, \u0026#34;ephemeral_storage_size\u0026#34;: 1018889.8304, \u0026#34;os_version\u0026#34;: \u0026#34;Ubuntu 14.04 LTS\u0026#34;, \u0026#34;persistent_storage_path\u0026#34;: \u0026#34;/var/opt/redislabs/persist/redis\u0026#34;, \u0026#34;persistent_storage_size\u0026#34;: 1018889.8304, \u0026#34;total_memory\u0026#34;: 24137, \u0026#34;uptime\u0026#34;: 50278, \u0026#34;available_addrs\u0026#34;: [{ \u0026#34;address\u0026#34;: \u0026#34;172.16.50.122\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ipv4\u0026#34;, \u0026#34;if_name\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;private\u0026#34;: true }, { \u0026#34;address\u0026#34;: \u0026#34;10.0.3.1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ipv4\u0026#34;, \u0026#34;if_name\u0026#34;: \u0026#34;lxcbr0\u0026#34;, \u0026#34;private\u0026#34;: true }, { \u0026#34;address\u0026#34;: \u0026#34;172.17.0.1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ipv4\u0026#34;, \u0026#34;if_name\u0026#34;: \u0026#34;docker0\u0026#34;, \u0026#34;private\u0026#34;: true }, { \u0026#34;address\u0026#34;: \u0026#34;2001:db8:0:f101::1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ipv6\u0026#34;, \u0026#34;if_name\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;private\u0026#34;: false }] } } Error codes Code Description config_error An error related to the bootstrap configuration provided (e.g. bad JSON). connect_error Failed to connect to cluster (e.g. FQDN DNS could not resolve, no/wrong node IP provided, etc. access_denied Invalid credentials supplied. invalid_license The license string provided is invalid. Additional info can be fetched from the error_details object, which includes the violation code in case the license is valid but its terms are violated. repair_required Cluster is in degraded mode and can only accept replacement nodes. When this happens, error_details contains two fields: failed_nodes and replace_candidate. The failed_nodes field is an array of objects, each describing a failed node with at least a uid field and an optional rack_id. replace_candidate is the UID of the node most suitable for replacement. insufficient_node_memory An attempt to replace a dead node fails because the replaced node does not have enough memory. When this happens, error_details contains a required_memory field which indicates the node memory requirement. insufficient_node_flash An attempt to replace a dead node fails because the replaced node does not have enough flash. When this happens, error_details contains a required_flash field which indicates the node flash requirement. time_not_sync An attempt to join a node with system time not synchronized with the rest of the cluster. rack_id_required An attempt to join a node with no rack_id in a rack-aware cluster. In addition, a current_rack_ids field will include an array of currently used rack ids. socket_directory_mismatch An attempt to join a node with a socket directory setting that differs from the cluster node_config_mismatch An attempt to join a node with a configuration setting (e.g. confdir, osuser, installdir) that differs from the cluster path_error A needed path does not exist or is not accessable. internal_error A different, unspecified internal error was encountered. Status codes Code Description 200 OK No error Start bootstrapping POST /v1/bootstrap/{action} Initiate bootstrapping.\nThe request must contain a bootstrap configuration JSON object, as described in Object attributes or a minimal subset.\nBootstrapping is permitted only when the current bootstrap state is idle or error (in which case the process will restart with the new configuration).\nThis request is asynchronous - once the request has been accepted, the caller is expected to poll bootstrap status while waiting for it to complete.\nRequest Example HTTP request POST /bootstrap/create_cluster Example JSON body Join cluster { \u0026#34;action\u0026#34;: \u0026#34;join_cluster\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;nodes\u0026#34;:[ \u0026#34;1.1.1.1\u0026#34;, \u0026#34;2.2.2.2\u0026#34; ] }, \u0026#34;node\u0026#34;: { \u0026#34;paths\u0026#34;: { \u0026#34;persistent_path\u0026#34;: \u0026#34;/path/to/persistent/storage\u0026#34;, \u0026#34;ephemeral_path\u0026#34;: \u0026#34;/path/to/ephemeral/storage\u0026#34;, \u0026#34;bigstore_path\u0026#34;: \u0026#34;/path/to/bigstore/storage\u0026#34; }, \u0026#34;bigstore_driver\u0026#34;: \u0026#34;rocksdb\u0026#34;, \u0026#34;identity\u0026#34;: { \u0026#34;addr\u0026#34;:\u0026#34;1.2.3.4\u0026#34;, \u0026#34;external_addr\u0026#34;:[\u0026#34;2001:0db8:85a3:0000:0000:8a2e:0370:7334\u0026#34;, \u0026#34;3.4.5.6\u0026#34;] } }, \u0026#34;credentials\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;my_username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my_password\u0026#34; } } Create cluster { \u0026#34;action\u0026#34;: \u0026#34;create_cluster\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;nodes\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;my.cluster\u0026#34; }, \u0026#34;node\u0026#34;: { \u0026#34;paths\u0026#34;: { \u0026#34;persistent_path\u0026#34;: \u0026#34;/path/to/persistent/storage\u0026#34;, \u0026#34;ephemeral_path\u0026#34;: \u0026#34;/path/to/ephemeral/storage\u0026#34;, \u0026#34;bigstore_path\u0026#34;: \u0026#34;/path/to/bigredis/storage\u0026#34; }, \u0026#34;identity\u0026#34;: { \u0026#34;addr\u0026#34;:\u0026#34;1.2.3.4\u0026#34;, \u0026#34;external_addr\u0026#34;:[\u0026#34;2001:0db8:85a3:0000:0000:8a2e:0370:7334\u0026#34;, \u0026#34;3.4.5.6\u0026#34;] }, \u0026#34;bigstore_driver\u0026#34;: \u0026#34;rocksdb\u0026#34; }, \u0026#34;license\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;credentials\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;my_username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my_password\u0026#34; } } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a bootstrap object in the request body.\nResponse Status codes Code Description 200 OK Request received and processing begins. 409 Conflict Bootstrap already in progress (check state) ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/task/cancel/","uriRel":"/rs/references/cli-utilities/crdb-cli/task/cancel/","title":"crdb-cli task cancel","tags":[],"keywords":[],"description":"Attempts to cancel a specified Active-Active database task.","content":"Cancels the Active-Active database task specified by the task ID.\ncrdb-cli task cancel \u0026lt;task_id\u0026gt; Parameters Parameter Value Description task-id \u0026lt;task_id\u0026gt; string An Active-Active database task ID (required) Returns Attempts to cancel an Active-Active database task.\nBe aware that tasks may complete before they can be cancelled.\nExample $ crdb-cli task cancel --task-id 2901c2a3-2828-4717-80c0-6f27f1dd2d7c ","categories":["RS"]},{"uri":"/rs/installing-upgrading/configuring/centos-rhel-7-firewall/","uriRel":"/rs/installing-upgrading/configuring/centos-rhel-7-firewall/","title":"Configure CentOS/RHEL firewall","tags":[],"keywords":[],"description":"","content":"CentOS/RHEL7 distributions have, by default, a restrictive firewall mechanism based on firewalld that in turn configures the standard iptables system. The default configuration assigns the network interfaces to the public zone and blocks all ports, except 22 (SSH).\nRedis Enterprise Software (RS) installation on CentOS/RHEL 7 automatically creates two firewalld system services:\nA service named redislabs, which includes all ports and protocols needed for communications between cluster nodes. A service named redislabs-clients, which includes the ports and protocols needed for communications external to the cluster. These services are defined but not allowed through the firewall by default. As part of the installation process, the installer prompts you to confirm auto-configuration of a default (public) zone to allow the redislabs service.\nWhile this process makes the installation process simple and straightforward, if the machine\u0026rsquo;s network environment is not secured it could be considered to be insecure, for example by means of an external firewall, EC2 Classic security groups. You can use firewalld configuration tools such as firewall-cmd (command line) or firewall-config (UI) to create more specific firewall policies that allow these two services through the firewall, as necessary.\nNote: If databases are created with non-standard RS ports, you need to explicitly configure firewalld to make sure those ports are not blocked. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/cert_rotation_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/cert_rotation_job_settings/","title":"Certificate rotation job settings object","tags":[],"keywords":[],"description":"Documents the cert_rotation_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the certificate rotation schedule expiry_days_before_rotation integer, (range: 1-90) (default: 60) Number of days before a certificate expires before rotation ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/certificate/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/certificate/","title":"rladmin cluster certificate","tags":["configured"],"keywords":[],"description":"Sets the cluster certificate.","content":"Sets a cluster certificate to a specified PEM file.\nrladmin cluster certificate set \u0026lt;certificate name\u0026gt; certificate_file \u0026lt;certificate filepath\u0026gt; [ key_file \u0026lt;key filepath\u0026gt; ] To set a certificate for a specific service, use the corresponding certificate name. See the certificates table for the list of cluster certificates and their descriptions.\nParameters Parameter Type/Value Description certificate name \u0026lsquo;cm\u0026rsquo;\n\u0026lsquo;api\u0026rsquo;\n\u0026lsquo;proxy\u0026rsquo;\n\u0026lsquo;syncer\u0026rsquo;\n\u0026lsquo;metrics_exporter\u0026rsquo; Name of the certificate to update certificate_file filepath Path to the certificate file key_file filepath Path to the key file (optional) Returns Reports that the certificate was set to the specified file. Returns an error message if the certificate fails to update.\nExample $ rladmin cluster certificate set proxy \\ certificate_file /tmp/proxy.pem Set proxy certificate to contents of file /tmp/proxy.pem ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/certificates/","uriRel":"/rs/references/rest-api/requests/cluster/certificates/","title":"Cluster certificates requests","tags":[],"keywords":[],"description":"Cluster certificates requests","content":" Method Path Description GET /v1/cluster/certificates Get cluster certificates DELETE /v1/cluster/certificates/{certificate_name} Delete cluster certificate Get cluster certificates GET /v1/cluster/certificates Get the cluster\u0026rsquo;s certificates.\nRequired permissions Permission name view_cluster_info Request Example HTTP request GET /cluster/certificates Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON object that contains the cluster\u0026rsquo;s certificates and keys.\nExample JSON body { \u0026#34;api_cert\u0026#34;: \u0026#34;-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;-----BEGIN RSA PRIVATE KEY-----...-----END RSA PRIVATE KEY-----\u0026#34; \u0026#34;// additional certificates...\u0026#34; } Status codes Code Description 200 OK No error Delete cluster certificate DELETE /v1/cluster/certificates/{string: certificate_name} Removes the specified cluster certificate from both CCS and disk across all nodes. Only optional certificates can be deleted through this endpoint. See the certificates table for the list of cluster certificates and their descriptions.\nRequest Example HTTP request DELETE /cluster/certificates/\u0026lt;certificate_name\u0026gt; Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a status code that indicates the certificate deletion success or failure.\nStatus codes Code Description 200 OK Operation successful 404 Not Found Failed, requested deletion of an unknown certificate 403 Forbidden Failed, requested deletion of a required certificate 500 Internal Server Error Failed, error while deleting certificate from disk ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/check_result/","uriRel":"/rs/references/rest-api/objects/check_result/","title":"Check result object","tags":[],"keywords":[],"description":"An object that contains the results of a cluster check","content":"Cluster check result\nName Type/Value Description cluster_test_result boolean Indication if any of the tests failed nodes [{ \"node_uid\": integer, \"result\": boolean, \"error\": string }, ...] Nodes results ","categories":["RS"]},{"uri":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/cloudformation/","uriRel":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/cloudformation/","title":"Create IAM resources using CloudFormation","tags":[],"keywords":[],"description":"","content":"The following link uses AWS CloudFormation to create a stack using the AWS console:\nYou can then use the Outputs tab to find the data needed to complete the creation of a Cloud Account. For the accessSecretKey (i.e. user\u0026rsquo;s access key) and consolePassword (user\u0026rsquo;s console password) you\u0026rsquo;ll have to follow the links to the AWS Secrets Manager service, and use that to find the secret values. These values, being secrets, aren\u0026rsquo;t displayed directly by CloudFormation.\nYou can use the AWS command-line interface (CLI) if you prefer:\nexport AWS_PROFILE=YOUR_PROFILE_HERE aws cloudformation create-stack --stack-name RedisCloud --template-url \\ https://s3.amazonaws.com/iam-resource-automation-do-not-delete/RedisCloud.yaml \\ --capabilities CAPABILITY_AUTO_EXPAND CAPABILITY_NAMED_IAM CAPABILITY_IAM Update the values of AWS_PROFILE with your profile credentials.\nAdditional options are described in the AWS CLI docs.\nYou can track the status of the cloud formation with the following command:\naws cloudformation describe-stacks --stack-name RedisCloud The data needed to complete the creation of a Cloud Account is shown as Output Key and Output Value pairs.\nFor the two secrets (accessSecretKey and consolePassword) you\u0026rsquo;ll need to use the AWS secretmanager CLI - the value you\u0026rsquo;ll need has a key of SecretString:\naws secretsmanager get-secret-value --secret-id=/redislabsuser/secret_access_key We recommend using yaml output for the consolePassword, as it makes decoding the required value easier.\naws secretsmanager get-secret-value --secret-id=/redislabsuser/password --output yaml The consolePassword is a JSON object containing a single member whose key is password and whose value is the password. This can be a bit complex to parse out. Here\u0026rsquo;s an example output:\nuser@example-computer ~ % aws secretsmanager get-secret-value --secret-id=/redislabsuser/password --output yaml ARN: arn:aws:secretsmanager:middle-earth-1:913769183952:secret:/redislabsuser/password-qaEMYs CreatedDate: \u0026#39;2021-06-16T06:27:53.402000-06:00\u0026#39; Name: /redislabsuser/password SecretString: \u0026#39;{\u0026#34;password\u0026#34;:\u0026#34;S3cr3tP@$$w0rd\u0026#34;}\u0026#39; VersionId: 00000000-0000-0000-0000-000000000000 VersionStages: - AWSCURRENT The JSON object is the value (less the single quotes) of the SecretString key. i.e. it is {\u0026quot;password\u0026quot;:\u0026quot;S3cr3tP@$$w0rd\u0026quot;}.\nThe password is the value associated with that key (less the double quotes): S3cr3tP@$$w0rd.\n","categories":["RC"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/","title":"rladmin cluster","tags":[],"keywords":[],"description":"Manage cluster.","content":"Manages cluster configuration and administration. Most rladmin cluster commands are only for clusters that are already configured, while a few others are only for new clusters that have not been configured.\nCommands for configured clusters Command Description certificate Sets the cluster certificate. config Updates the cluster\u0026#39;s configuration. debug_info Creates a support package. ocsp Manages OCSP. reset_password Changes the password for a given email. running_actions Lists all active tasks. stats_archiver Enables/deactivates the stats archiver. Commands for non-configured clusters Command Description create Creates a new cluster. join Adds a node to an existing cluster. recover Recovers a cluster from a backup file. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/cluster/","uriRel":"/rs/references/rest-api/objects/cluster/","title":"Cluster object","tags":[],"keywords":[],"description":"An object that represents a cluster","content":"An API object that represents the cluster.\nName Type/Value Description alert_settings alert_settings object Cluster and node alert settings cluster_ssh_public_key string Cluster\u0026rsquo;s autogenerated SSH public key cm_port integer, (range: 1024-65535) UI HTTPS listening port cm_session_timeout_minutes integer (default: 15) The timeout (in minutes) for the session to the CM cnm_http_port integer, (range: 1024-65535) API HTTP listening port cnm_https_port integer, (range: 1024-65535) API HTTPS listening port control_cipher_suites string Specifies the enabled ciphers for the control plane. The ciphers are specified in the format understood by the BoringSSL library. crdt_rest_client_retries integer Maximum number of retries for the REST client used by the Active-Active management API crdt_rest_client_timeout integer Timeout for REST client used by the Active-Active management API created_time string Cluster creation date (read-only) data_cipher_list string Specifies the enabled ciphers for the data plane. The ciphers are specified in the format understood by the OpenSSL library. debuginfo_path string Path to a local directory used when generating support packages default_non_sharded_proxy_policy string (default: single) Default proxy_policy for newly created non-sharded databases\u0026rsquo; endpoints (read-only) default_sharded_proxy_policy string (default: all-master-shards) Default proxy_policy for newly created sharded databases\u0026rsquo; endpoints (read-only) email_alerts boolean (default: false) Send node/cluster email alerts (requires valid SMTP and email_from settings) email_from string Sender email for automated emails encrypt_pkeys boolean (default: false) Enable or turn off encryption of private keys envoy_max_downstream_connections integer, (range: 100-2048) The max downstream connections envoy is allowed to open handle_redirects boolean (default: false) Handle API HTTPS requests and redirect to the master node internally http_support boolean (default: false) Enable or turn off HTTP support min_control_TLS_version \u0026lsquo;1\u0026rsquo; \u0026lsquo;1.1\u0026rsquo; \u0026lsquo;1.2\u0026rsquo; \u0026lsquo;1.3\u0026rsquo; The minimum version of TLS protocol which is supported at the control path min_data_TLS_version \u0026lsquo;1\u0026rsquo; \u0026lsquo;1.1\u0026rsquo; \u0026lsquo;1.2\u0026rsquo; The minimum version of TLS protocol which is supported at the data path min_sentinel_TLS_version \u0026lsquo;1\u0026rsquo; \u0026lsquo;1.1\u0026rsquo; \u0026lsquo;1.2\u0026rsquo; The minimum version of TLS protocol which is supported at the data path name string Cluster\u0026rsquo;s fully qualified domain name (read-only) password_complexity boolean (default: false) Enforce password complexity policy password_expiration_duration integer (default: 0) The number of days a password is valid until the user is required to replace it proxy_certificate string Cluster\u0026rsquo;s proxy certificate proxy_max_ccs_disconnection_time integer Cluster-wide proxy timeout policy between proxy and CCS rack_aware boolean Cluster operates in a rack-aware mode (read-only) s3_url string Specifies the URL for S3 export and import saslauthd_ldap_conf string saslauthd LDAP configuration sentinel_cipher_suites array Specifies the list of enabled ciphers for the sentinel service. The supported ciphers are ones that were implemented by the cipher_suits.go package. sentinel_ssl_policy \u0026lsquo;allowed\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; \u0026lsquo;required\u0026rsquo; Determines whether the discovery service allows, blocks, or requires TLS connections\nallowed: Allows both TLS and non-TLS connections\ndisabled: Allows only non-TLS connections\nrequired: Allows only TLS connections slave_ha boolean (default: false) Enable the replica high-availability mechanism (read-only) slave_ha_bdb_cooldown_period integer (default: 86400) Time in seconds between runs of the replica high-availability mechanism on different nodes on the same database (read-only) slave_ha_cooldown_period integer (default: 3600) Time in seconds between runs of the replica high-availability mechanism on different nodes (read-only) slave_ha_grace_period integer (default: 900) Time in seconds between a node failure and when the replica high-availability mechanism starts relocating shards (read-only) slowlog_in_sanitized_support boolean Whether to include slowlogs in the sanitized support package smtp_host string SMTP server for automated emails smtp_password string SMTP server password smtp_port integer SMTP server port for automated emails smtp_tls_mode \u0026rsquo;none\u0026rsquo;\n\u0026lsquo;starttls\u0026rsquo;\n\u0026lsquo;tls\u0026rsquo; Specifies which TLS mode to use for SMTP access smtp_use_tls boolean (default: false) Use TLS for SMTP access (deprecated, use smtp_tls_mode field instead) smtp_username string SMTP server username (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;) syncer_certificate string Cluster\u0026rsquo;s syncer certificate upgrade_mode boolean (default: false) Is cluster currently in upgrade mode use_ipv6 boolean (default: true) Should redislabs services listen on ipv6 wait_command boolean (default: true) Supports Redis wait command (read-only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/","uriRel":"/rs/references/rest-api/requests/cluster/","title":"Cluster requests","tags":[],"keywords":[],"description":"Cluster settings requests","content":" Method Path Description GET /v1/cluster Get cluster info PUT /v1/cluster Update cluster settings Get cluster info GET /v1/cluster Get cluster info.\nRequired permissions Permission name view_cluster_info Request Example HTTP request GET /cluster Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a cluster object.\nExample JSON body { \u0026#34;name\u0026#34;: \u0026#34;my-rlec-cluster\u0026#34;, \u0026#34;alert_settings\u0026#34;: { \u0026#34;...\u0026#34; }, \u0026#34;created_time\u0026#34;: \u0026#34;2015-04-29T09:09:25Z\u0026#34;, \u0026#34;email_alerts\u0026#34;: false, \u0026#34;email_from\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;rack_aware\u0026#34;: false, \u0026#34;smtp_host\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;smtp_password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;smtp_port\u0026#34;: 25, \u0026#34;smtp_tls_mode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;smtp_username\u0026#34;: \u0026#34;\u0026#34; } Status codes Code Description 200 OK No error Update cluster settings PUT /v1/cluster Update cluster settings.\nIf called with the dry_run URL query string, the function will validate the cluster object, but will not apply the requested changes.\nRequired permissions Permission name update_cluster Request Example HTTP request PUT /cluster Example JSON body { \u0026#34;email_alerts\u0026#34;: true, \u0026#34;alert_settings\u0026#34;: { \u0026#34;node_failed\u0026#34;: true, \u0026#34;node_memory\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34; } } } The above request will enable email alerts and alert reporting for node failures and node removals.\nRequest headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description dry_run string Validate but don\u0026rsquo;t apply the new cluster settings Request body Include a cluster object with updated fields in the request body.\nResponse Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;mycluster.mydomain.com\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;alert_settings\u0026#34;: { \u0026#34;node_failed\u0026#34;: true, \u0026#34;node_memory\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34; } }, \u0026#34;// additional fields...\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description bad_nginx_conf • Designated port is already bound.\n• nginx configuration is illegal. bad_debuginfo_path • Debuginfo path doesn\u0026rsquo;t exist.\n• Debuginfo path is inaccessible. config_edit_conflict Cluster config was edited by another source simultaneously. Status codes Code Description 200 OK No error. 400 Bad Request Bad content provided. ","categories":["RS"]},{"uri":"/rs/networking/cluster-lba-setup/","uriRel":"/rs/networking/cluster-lba-setup/","title":"Set up cluster behind a load balancer","tags":[],"keywords":[],"description":"Set up a cluster using a load balancer instead of DS to direct traffic to cluster nodes.","content":"When you want to setup a Redis Enterprise cluster in an environment that doesn\u0026rsquo;t allow DNS, you can use a load balancer (LB) to direct traffic to the cluster nodes.\nDNS role for databases Normally, Redis Enterprise uses DNS to provide dynamic database endpoints. A DNS name such as redis-12345.clustername.domain gives clients access to the database resource:\nIf multiple proxies are in use, the DNS name resolves to multiple IP addresses so that clients can load balance. On failover or topology changes, the DNS name is automatically updated to reflect the live IP addresses. When DNS cannot be used, clients can still connect to the endpoints with the IP addresses, but the benefits of load balancing and automatic updates to IP addresses won\u0026rsquo;t be available.\nNetwork architecture with load balancer You can compensate for the lack of DNS resolution with load balancers that can expose services and provide service discovery. A load balancer is configured in front of Redis Enterprise cluster, exposing several logical services:\nControl plane services, such as the RS admin console to access cluster administration interface Data plane services, such as a database endpoint to connect from client applications Depending on which Redis Enterprise services you want to access outside the cluster you may need to configure the load balancers separately. One or more Virtual IPs (VIPs) are defined on the load balancer to expose Redis Enterprise services. The architecture is shown in the following diagram with 3 nodes Redis Enterprise cluster with one database (DB1) configured on port 12000:\nSetting up an RS cluster with load balancers Prerequisites Install the latest version of RS on your clusters Configure the cluster with the cluster name (FQDN) even though DNS is not in use. Remember that the same cluster name is used to issue the licence keys. We recommend that you use a “.local” suffix in the FQDN. Configuring the load balancers Make sure that the load balancer is performing TCP health checks on the cluster nodes. Expose the services that you require through a Virtual IP, for example: Web Management Portal (8443) Rest API service (Secured - 9443; Non-secured - 8080) Database ports (In the range of 10000-19999) Other ports are shown in the list of RS network ports.\nNote: Sticky, secured connections are needed only for RS admin console service (provided on port 8443).\nCertain LBAs provide specific logic to close idle connections. Either disable this feature or make sure the applications connecting to Redis use reconnection logic. Make sure the LB is fast enough to resolve connections between two clusters or applications that are connected to Redis databases through LB. Choose the standard LB which is commonly used in your environment so that you have easy access to in-house expertise for troubleshooting issues. RS cluster configuration There are certain recommended settings within the cluster that guarantee a flawless connectivity experience for applications and admin users when they access the cluster through a Load Balancer.\nNote: Run the rladmin commands directly on the cluster. The rladmin commands update the settings on all nodes in the cluster. The following settings are needed to allow inbound connections to be terminated on the relevant node inside the cluster:\n# enable all-node proxy policy by default rladmin tune cluster default_sharded_proxy_policy all-nodes # ensure we redirect where necessary when running behind an LBA rladmin cluster config handle_redirects enabled An additional setting can be done to allow (on average) closer termination of client connection to where the Redis shard is located. This is an optional setting.\n# enable sparse placement by default rladmin tune cluster default_shards_placement sparse RS database configuration After the cluster settings are updated and the LBs are configured you can go to the RS admin console at https://load-balancer-virtual-ip:8443/ and create a new database.\nIf you are creating an Active-Active database, you will need to use the crdb-cli utility. See the crdb-cli reference for more information about creating Active-Active databases from the command line.\nKeep LB configuration updated when the cluster configuration changes When your RS cluster is located behind a load balancer, you must update the LB when the cluster topology and IP addresses change. Some common cases that require you to update the LB are:\nAdding new nodes to the Redis Enterprise cluster Removing nodes from the Redis Enterprise cluster Maintenance for Redis Enterprise cluster nodes IP address changes for Redis Enterprise cluster nodes After these changes, make sure that the redis connections in your applications can connect to the Redis database, especially if they are directly connected on IP addresses that have changed.\nIntercluster communication considerations Redis Enterprise supports several topologies that allow inter cluster replication, these include Active/Passive and Active/Active for deployment options. When your Redis Enterprise software clusters are located behind load balancers, you must allow some network services to be open and defined in the load balancers to allow the replication to work.\nActive Passive For Active Passive communication to work, you will need to expose database port(s) locally in each cluster (as defined above) but also allow these ports through firewalls that may be positioned between the clusters.\nActive-Active For Active-Active communication to work, you need to expose several ports, including every database port and several control plane ports as defined in Network port configurations. Pay attention to services that are marked with Connection Source as \u0026ldquo;Active-Active\u0026rdquo;. These ports should be allowed through firewalls that may be positioned between the clusters.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/statistics/cluster-metrics/","uriRel":"/rs/references/rest-api/objects/statistics/cluster-metrics/","title":"Cluster metrics","tags":[],"keywords":[],"description":"Documents the cluster metrics used with Redis Enterprise Software REST API calls.","content":" Metric name Type Description available_flash float Sum of available flash in all nodes (bytes) available_memory float Sum of available memory in all nodes (bytes) avg_latency float Average latency of requests handled by all cluster endpoints (micro-sec); returned only when there is traffic bigstore_free float Sum of free space of backend flash (used by flash DB\u0026rsquo;s BigRedis) on all cluster nodes (bytes); only returned when BigRedis is enabled bigstore_iops float Rate of I/O operations against backend flash for all shards which are part of a flash-based DB (BigRedis) in the cluster (ops/sec); returned only when BigRedis is enabled bigstore_kv_ops float Rate of value read/write operations against back-end flash for all shards which are part of a flash based DB (BigRedis) in cluster (ops/sec); only returned when BigRedis is enabled bigstore_throughput float Throughput I/O operations against backend flash for all shards which are part of a flash-based DB (BigRedis) in the cluster (bytes/sec); only returned when BigRedis is enabled conns float Total number of clients connected to all cluster endpoints cpu_idle float CPU idle time portion, the value is weighted between all nodes based on number of cores in each node (0-1, multiply by 100 to get percent) cpu_system float CPU time portion spent in kernel on the cluster, the value is weighted between all nodes based on number of cores in each node (0-1, multiply by 100 to get percent) cpu_user float CPU time portion spent by users-pace processes on the cluster. The value is weighted between all nodes based on number of cores in each node (0-1, multiply by 100 to get percent). egress_bytes float Sum of rate of outgoing network traffic on all cluster nodes (bytes/sec) ephemeral_storage_avail float Sum of disk space available to Redis Enterprise processes on configured ephemeral disk on all cluster nodes (bytes) ephemeral_storage_free float Sum of free disk space on configured ephemeral disk on all cluster nodes (bytes) free_memory float Sum of free memory in all cluster nodes (bytes) ingress_bytes float Sum of rate of incoming network traffic on all cluster nodes (bytes/sec) persistent_storage_avail float Sum of disk space available to Redis Enterprise processes on configured persistent disk on all cluster nodes (bytes) persistent_storage_free float Sum of free disk space on configured persistent disk on all cluster nodes (bytes) provisional_flash float Sum of provisional flash in all nodes (bytes) provisional_memory float Sum of provisional memory in all nodes (bytes) total_req float Request rate handled by all endpoints on the cluster (ops/sec) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/cluster/cluster_alert_settings_with_threshold/","uriRel":"/rs/references/rest-api/objects/cluster/cluster_alert_settings_with_threshold/","title":"Cluster alert settings with threshold object","tags":[],"keywords":[],"description":"Documents the cluster_alert_settings_with_threshold object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description enabled boolean (default: false) Alert enabled or disabled threshold string Threshold for alert going on/off ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/cluster_identity/","uriRel":"/rs/references/rest-api/objects/bootstrap/cluster_identity/","title":"Cluster identity object","tags":[],"keywords":[],"description":"Documents the cluster_identity object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description name string Fully qualified cluster name. Limited to 64 characters and must comply with the IETF\u0026rsquo;s RFC 952 standard and section 2.1 of the RFC 1123 standard. nodes array of strings Array of IP addresses of existing cluster nodes wait_command boolean (default: true) Supports Redis wait command ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/cluster_info/","uriRel":"/rs/references/rest-api/objects/crdb/cluster_info/","title":"CRDB cluster info object","tags":[],"keywords":[],"description":"An object that represents Active-Active cluster info","content":"Configuration details for a cluster that is part of an Active-Active database.\nName Type/Value Description credentials { \"username\": string, \"password\": string } Cluster access credentials (required) name string Cluster fully qualified name, used to uniquely identify the cluster. Typically this is the same as the hostname used in the URL, although in some configruations the URL may point to a different name/address. (required) replication_endpoint string Address to use for peer replication. If not specified, it is assumed that standard cluster naming conventions apply. replication_tls_sni string Cluster SNI for TLS connections url string Cluster access URL (required) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/cluster_settings/","uriRel":"/rs/references/rest-api/objects/cluster_settings/","title":"Cluster settings object","tags":[],"keywords":[],"description":"An object for cluster resource management settings","content":"Cluster resources management policy\nName Type/Value Description acl_pubsub_default resetchannels\nallchannels Default pub/sub ACL rule for all databases in the cluster:\n•resetchannels blocks access to all channels (restrictive)\n•allchannels allows access to all channels (permissive) bigstore_migrate_node_threshold integer Minimum free memory (excluding reserved memory) allowed on a node before automatic migration of shards from it to free more memory bigstore_migrate_node_threshold_p integer Minimum free memory (excluding reserved memory) allowed on a node before automatic migration of shards from it to free more memory bigstore_provision_node_threshold integer Minimum free memory (excluding reserved memory) allowed on a node before new shards can no longer be added to it bigstore_provision_node_threshold_p integer Minimum free memory (excluding reserved memory) allowed on a node before new shards can no longer be added to it data_internode_encryption boolean Enable/deactivate encryption of the data plane internode communication db_conns_auditing boolean Audit connections for new databases by default if set to true. default_concurrent_restore_actions integer Default number of restore actions allowed at the same time. Set to 0 to allow any number of simultaneous restore actions. default_fork_evict_ram boolean If true, the bdbs should evict data from RAM to ensure successful replication or persistence default_non_sharded_proxy_policy single\nall-master-shards\nall-nodes Default proxy_policy for newly created non-sharded databases\u0026rsquo; endpoints default_provisioned_redis_version string Default Redis version default_sharded_proxy_policy single\nall-master-shards\nall-nodes Default proxy_policy for newly created sharded databases\u0026rsquo; endpoints default_shards_placement dense\nsparse Default shards_placement for a newly created databases endpoint_rebind_propagation_grace_time integer Time to wait between the addition and removal of a proxy login_lockout_counter_reset_after integer Number of seconds that must elapse between failed sign in attempts before the lockout counter is reset to 0. login_lockout_duration integer Duration (in secs) of account lockout. If set to 0, the account lockout will persist until released by an admin. login_lockout_threshold integer Number of failed sign in attempts allowed before locking a user account max_saved_events_per_type integer Maximum saved events per event type max_simultaneous_backups integer Maximum number of backup processes allowed at the same time parallel_shards_upgrade integer Maximum number of shards to upgrade in parallel rack_aware boolean Cluster operates in a rack-aware mode redis_migrate_node_threshold integer Minimum free memory (excluding reserved memory) allowed on a node before automatic migration of shards from it to free more memory redis_migrate_node_threshold_p integer Minimum free memory (excluding reserved memory) allowed on a node before automatic migration of shards from it to free more memory redis_provision_node_threshold integer Minimum free memory (excluding reserved memory) allowed on a node before new shards can no longer be added to it redis_provision_node_threshold_p integer Minimum free memory (excluding reserved memory) allowed on a node before new shards can no longer be added to it redis_upgrade_policy major latest Create/upgrade Redis Enterprise software on databases in the cluster by compatibility with major versions or latest versions of OSS Redis shards_overbooking boolean If true, all databases\u0026rsquo; memory_size is ignored during shards placement show_internals boolean Show internal databases (and their shards and endpoints) REST APIs slave_ha boolean Enable the replica high-availability mechanism slave_ha_bdb_cooldown_period integer Time in seconds between runs of the replica high-availability mechanism on different nodes on the same database slave_ha_cooldown_period integer Time in seconds between runs of the replica high-availability mechanism on different nodes on the same database slave_ha_grace_period integer Time in seconds between a node failure and when the replica high-availability mechanism starts relocating shards ","categories":["RS"]},{"uri":"/rc/databases/configuration/clustering/","uriRel":"/rc/databases/configuration/clustering/","title":"Clustering Redis Databases","tags":[],"keywords":[],"description":"Redis Enterprise Cloud uses clustering to manage very large databases (25 GB and larger).  Here, you&#39;ll learn how to manage clustering and how to use hashing policies to control how data is managed.","content":"For very large databases, Redis Enterprise Cloud distributes database data to different cloud instances. For example:\nWhen data grows beyond the the RAM resources of a single server.\nMultiple shards should be used when data grows to 25 GB (50 GB for Redis on Flash) to create multiple shards.\nThe operations performed against the database are CPU intensive enough to degrade performance.\nClustering distributes operational load, whether to instances on the same server or across multiple servers.\nThis distribution is called clustering because it manages the way data is distributed throughout the cluster of nodes that support the database.\nHow data is distributed A Redis Cloud cluster is a set of managed Redis processes and cloud instances, with each process managing a subset of the database keyspace. Clustering uses multiple cores and resources of multiple instances to overcome scaling challenges.\nIn a Redis Cloud cluster, the keyspace is partitioned into hash slots. At any given time a slot resides on and is managed by a single Redis server.\nAn instance that belongs to a cluster can manage multiple slots. This division of the key space, known as sharding, is achieved by hashing the key names, or parts of these (key hash tags), in order to obtain the slot in which a key should reside.\nEven when using multiple Redis processes, the use of a Redis Enterprise Cloud cluster is nearly transparent to the application that uses it. The cluster is accessible via a single endpoint that automatically routes all operations to the relevant shards, without the complexity of a cluster-aware Redis client. This allows applications to benefit from using the cluster without performing any code changes, even if they were not designed to use it beforehand.\nWhen creating or editing a Redis database on Redis Enterprise Cloud, the system automatically calculates the number of shards needed based on the database memory limit and required throughput.\nMulti-key operations Operations on multiple keys in a sharded Redis Cloud cluster are supported, with the following limitations:\nMulti-key commands: Redis offers several commands that accept multiple keys as arguments. In a sharded setup, you can run multi-key commands only if all of the affected keys reside in the same slot (and on the same shard). This restriction applies to all mulit-key commands, including BITOP, BLPOP, BRPOP, BRPOPLPUSH, MSETNX, RPOPLPUSH, SDIFF, SDIFFSTORE, SINTER, SINTERSTORE, SMOVE, SORT, SUNION, XREAD, ZINTER, ZINTERSTORE, ZUNION, ZUNIONSTORE, ZDIFF, ZDIFFSTORE Geo commands: In GEORADIUS/GEORADIUSBYMEMBER/GEOSEARCHSTORE commands, the STORE and STOREDIST options can only be used when all affected keys reside in the same slot. Transactions: All operations within a WATCH/MULTI/EXEC block should be performed on keys that are in the same slot. Lua scripts: All keys that are used by the script must reside in the same slot and need to be provided as arguments to the EVAL/EVALSHA commands (as per the Redis specification). Renaming/Copy keys: The use of the RENAME/RENAMENX/COPY commands is allowed only when both the key\u0026rsquo;s original name and its new name are mapped to the same hash slot. Variadic commands: The use of (MGET, MSET, HMGET, HMSET, etc..) and pipelining are supported with Redis Cloud cluster like if it were a non-cluster DB. Changing the hashing policy The clustering configuration of a Redis Cloud instance can be changed. However, hashing policy changes delete existing data (FLUSHDB) before they\u0026rsquo;re applied. These changes include:\nChanging the hashing policy, either from standard to custom or vice versa. Changing the order of custom hashing policy rules. Adding rules before existing ones in the custom hashing policy. Deleting rules from the custom hashing policy. Disabling clustering for the database. Standard hashing policy When using the standard hashing policy, a Redis Cloud cluster behaves like the standard, open-source Redis cluster, and hashing is performed as follows:\nKeys with a hashtag: a key\u0026rsquo;s hashtag is any substring between \u0026lsquo;{\u0026rsquo; and \u0026lsquo;}\u0026rsquo; in the key\u0026rsquo;s name. That means that when a key\u0026rsquo;s name includes the pattern \u0026lsquo;{\u0026hellip;}\u0026rsquo;, the hashtag is used as input for the hashing function. For example, the following key names have the same hashtag and are mapped to the same slot: foo{bar}, {bar}baz \u0026amp; foo{bar}baz. Keys without a hashtag: when a key doesn\u0026rsquo;t contain the \u0026lsquo;{\u0026hellip;}\u0026rsquo; pattern, the entire key\u0026rsquo;s name is used for hashing. You can use the \u0026lsquo;{\u0026hellip;}\u0026rsquo; pattern to direct related keys to the same hash slot, so that multi-key operations are supported on them. On the other hand, not using a hashtag in the key\u0026rsquo;s name results in a (statistically) even distribution of keys across the keyspace\u0026rsquo;s shards. If your application does not perform multi-key operations, you don\u0026rsquo;t need to construct key names with hashtags.\nCustom hashing policy A Redis Cloud cluster can be configured to use a custom hashing policy. A custom hashing policy is required when different keys need to be kept together on the same shard to allow multi-key operations. Redis Cloud\u0026rsquo;s custom hashing policy is provided via a set of Perl Compatible Regular Expressions (PCRE) rules that describe the dataset\u0026rsquo;s key name patterns.\nTo configure a custom hashing policy, enter regular expression (RegEx) rules that identify the substring in the key\u0026rsquo;s name - hashtag - on which hashing will be done. The hashing tag is denoted in the RegEx by the use of the `tag` named subpattern. Different keys that have the same hashtag will be stored and managed in the same slot.\nOnce you enable the custom hashing policy, the Redis Cloud\u0026rsquo;s default RegEx rules that implement the standard hashing policy are:\nRegEx Rule Description .*{(?\u0026lt;tag\u0026gt;.*)}.* Hashing is done on the substring between the curly braces. (?\u0026lt;tag\u0026gt;.*) The entire key\u0026rsquo;s name is used for hashing. You can modify existing rules, add new ones, delete rules, or change their order to suit your application\u0026rsquo;s requirements.\nCustom hashing policy notes and limitations You can define up to 32 RegEx rules, each up to 256 characters. RegEx rules are evaluated by their order. The first rule matched is used; strive to place common key name patterns at the beginning of the rule list. Key names that do not match any of the RegEx rules trigger an error. The \u0026lsquo;.*(?\u0026lt;tag\u0026gt;)\u0026rsquo; RegEx rule forces keys into a single slot as the hash key is always empty. When used, this should be the last catch-all rule. The following flag is enabled in our regular expression parser: PCRE_ANCHORED: the pattern is constrained to match only at the start of the string which is being searched. ","categories":["RC"]},{"uri":"/rs/references/rest-api/objects/services_configuration/cm_server/","uriRel":"/rs/references/rest-api/objects/services_configuration/cm_server/","title":"CM server object","tags":[],"keywords":[],"description":"Documents the cm_server object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the CM server ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/","uriRel":"/rs/references/cli-utilities/","title":"Command-line utilities","tags":[],"keywords":[],"description":"Documents the command-line utilities included with Redis Enterprise Software.","content":"Redis Enterprise Software includes a set of utilities to help you manage and test your cluster. To use a utility, run it from the command line.\nPublic utilities Administrators can use these CLI tools to manage and test a Redis Enterprise cluster. You can find the binaries in the /opt/redislabs/bin/ directory.\nUtility Description crdb-cli (manage Active-Active) Manage Active-Active databases. redis-cli (run Redis commands) Run Redis commands. rladmin (manage cluster) Manage Redis Enterprise clusters and databases. rlcheck (verify nodes) Verify nodes. Internal utilities The /opt/redislabs/bin/ directory also contains utilities used internally by Redis Enterprise Software and for troubleshooting.\nWarning - Do not use these tools for normal operations. Utility Description bdb-cli redis-cli connected to a database. ccs-cli Inspect Cluster Configuration Store. cnm-ctl Manages services for provisioning, migration, monitoring, resharding, rebalancing, deprovisioning, and autoscaling. consistency_checker Checks the consistency of Redis instances. crdbtop Monitor Active-Active databases. debug_mode Enables debug mode. debuginfo Collects cluster information. dmc-cli Configure and monitor the DMC proxy. pdns_control Sends commands to a running PowerDNS nameserver. redis_ctl Stops or starts Redis instances. rl_rdbloader Load RDB backup files to a server. rlutil Maintenance utility. shard-cli redis-cli connected to a shard. supervisorctl Manages the lifecycles of Redis Enterprise services. ","categories":["RS"]},{"uri":"/rs/references/compatibility/commands/","uriRel":"/rs/references/compatibility/commands/","title":"Compatibility with open source Redis commands","tags":[],"keywords":[],"description":"Open source Redis commands compatible with Redis Enterprise.","content":"Learn which open source Redis commands are compatible with Redis Enterprise Software and Redis Enterprise Cloud.\nSelect a command group for more details about compatibility with standard and Active-Active Redis Enterprise.\nCommand group Description Cluster management Cluster management commands compatible with Redis Enterprise. Connection management Connection management commands compatibility. Data types Data type commands compatibility (bitmaps, geospatial indices, hashes, HyperLogLogs, lists, sets, sorted sets, streams, strings). Keys (generic) Generic key commands compatible with Redis Enterprise. Pub/sub Pub/sub commands compatibility. Scripting Scripting commands compatibility. Server management Server management commands compatibility. Transactions Transaction commands compatibility. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/config/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/config/","title":"rladmin cluster config","tags":["configured"],"keywords":[],"description":"Updates the cluster&#39;s configuration.","content":"Updates the cluster configuration.\nrladmin cluster config [ auditing db_conns audit_protocol { TCP | local } audit_address \u0026lt;audit_address\u0026gt; audit_port \u0026lt;audit_port\u0026gt; ] [ cipher_suites \u0026lt;BoringSSL cipher list\u0026gt; ] [ cm_port \u0026lt;number\u0026gt; ] [ cm_session_timeout \u0026lt;minutes\u0026gt; ] [ cnm_http_port \u0026lt;number\u0026gt; ] [ cnm_https_port \u0026lt;number\u0026gt;] [ data_cipher_list \u0026lt;openSSL cipher list\u0026gt; ] [ debuginfo_path \u0026lt;filepath\u0026gt; ] [ encrypt_pkeys { enabled | disabled } ] [ handle_redirects { enabled | disabled } ] [ http_support { enabled | disabled } ] [ ipv6 { enabled | disabled } ] [ min_control_TLS_version \u0026lt;control_tls_version\u0026gt;] [ min_data_TLS_version \u0026lt;data_tls_version\u0026gt; ] [ min_sentinel_TLS_version \u0026lt;sentinel_tls_version\u0026gt; ] [ s3_url \u0026lt;URL\u0026gt; ] [ saslauthd_ldap_conf \u0026lt;/tmp/ldap.conf\u0026gt; ] [ sentinel_ssl_policy { allowed | required | disabled } ] [ sentinel_cipher_suites \u0026lt;golang cipher list\u0026gt; ] [ services { cm_server | crdb_coordinator | crdb_worker | mdns_server | pdns_server | saslauthd | stats_archiver } { enabled | disabled } ] [ upgrade_mode { enabled | disabled } ] Parameters Parameter Type/Value Description audit_address string TCP/IP address where a listener can capture audit event notifications audit_port string Port where a listener can capture audit event notifications audit_protocol tcplocal Protocol used for audit event notificationsFor production systems, only tcp is supported. cipher_suites list of ciphers Cipher suites used for TLS connections to the admin console (specified in the format understood by the BoringSSL library) cm_port integer UI server listening port cm_session_timeout integer Timeout in minutes for the CM session cmn_http_port integer HTTP REST API server listening port cnm_https_port integer HTTPS REST API server listening port data_cipher_list list of ciphers Cipher suites used by the the data plane (specified in the format understood by the OpenSSL library) debuginfo_path filepath Local directory to place generated support package files encrypt_pkeys enabled\ndisabled Enable or turn off encryption of private keys handle_redirects enabled\ndisabled Enable or turn off handling DNS redirects when DNS is not configured and running behind a load balancer http_support enabled\ndisabled Enable or turn off using HTTP for REST API connections ipv6 enabled\ndisabled Enable or turn off IPv6 connections to the admin console min_control_TLS_version TLS protocol version The minimum TLS protocol version that is supported for the control path min_data_TLS_version TLS protocol version The minimum TLS protocol version that is supported for the data path min_sentinel_TLS_version TLS protocol version The minimum TLS protocol version that is supported for the discovery service s3_url string The URL of S3 export and import saslauthd_ldap_conf filepath Updates LDAP authentication configuration for the cluster (see cluster-based LDAP authentication sentinel_cipher_suites list of ciphers Cipher suites used by the discovery service (supported ciphers are implemented by the golang.org cipher suites package) sentinel_ssl_policy allowed\nrequired\ndisabled Define the SSL policy for the discovery service services cm_server\ncrdb_coordinator\ncrdb_worker\nmdns_server\npdns_server\nsaslauthd\nstats_archiver\nenabled\ndisabled Enable or turn off selected cluster services upgrade_mode enabled\ndisabled Enable or turn off upgrade mode on the cluster Returns Reports whether the cluster was configured successfully. Displays an error message if the configuration attempt fails.\nExample $ rladmin cluster config cm_session_timeout_minutes 20 Cluster configured successfully ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/modules/config/","uriRel":"/rs/references/rest-api/requests/bdbs/modules/config/","title":"Database modules config requests","tags":[],"keywords":[],"description":"Configure Redis module requests","content":" Method Path Description POST /v1/bdbs/{uid}/modules/config Configure module Configure module POST /v1/bdbs/{string: uid}/modules/config Use the module runtime configuration command (if defined) to configure new arguments for the module.\nRequired permissions Permission name edit_bdb_module Request Example HTTP request POST /bdbs/1/modules/config Example JSON body { \u0026#34;modules\u0026#34;: [ { \u0026#34;module_name\u0026#34;: \u0026#34;search\u0026#34;, \u0026#34;module_args\u0026#34;: \u0026#34;MINPREFIX 3 MAXEXPANSIONS 1000\u0026#34; } ] } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Field Type Description modules list of JSON objects List of modules (module_name) and their new configuration settings (module_args) module_name string Module\u0026rsquo;s name module_args string Module command line arguments (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,”) Response Returns a status code. If an error occurs, the response body may include an error code and message with more details.\nError codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description db_not_exist Database with given UID doesn\u0026rsquo;t exist in cluster missing_field \u0026ldquo;module_name\u0026rdquo; or \u0026ldquo;module_args\u0026rdquo; are not defined in request invalid_schema JSON object received is not a dict object param_error \u0026ldquo;module_args\u0026rdquo; parameter was not parsed properly module_not_exist Module with given \u0026ldquo;module_name\u0026rdquo; does not exist for the database Status codes Code Description 200 OK Success, module updated on bdb. 404 Not Found bdb not found. 400 Bad Request Bad or missing configuration parameters. 406 Not Acceptable Module does not support runtime configuration of arguments. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/modules/config/","uriRel":"/rs/references/rest-api/requests/modules/config/","title":"Configure module requests","tags":[],"keywords":[],"description":"Configure module requests","content":" Method Path Description POST /v1/modules/config/bdb/{uid} Configure module Configure module POST /v1/modules/config/bdb/{string: uid} Use the module runtime configuration command (if defined) to configure new arguments for the module.\nRequired permissions Permission name edit_bdb_module Request Example HTTP request POST /modules/config/bdb/1 Example JSON body { \u0026#34;modules\u0026#34;: [ { \u0026#34;module_name\u0026#34;: \u0026#34;search\u0026#34;, \u0026#34;module_args\u0026#34;: \u0026#34;MINPREFIX 3 MAXEXPANSIONS 1000\u0026#34; } ] } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Field Type Description modules list of JSON objects List of modules (module_name) and their new configuration settings (module_args) module_name string Module\u0026rsquo;s name module_args string Module command line arguments (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,”) Response Returns a status code. If an error occurs, the response body may include an error code and message with more details.\nError codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description db_not_exist Database with given UID doesn\u0026rsquo;t exist in cluster missing_field \u0026ldquo;module_name\u0026rdquo; or \u0026ldquo;module_args\u0026rdquo; are not defined in request invalid_schema JSON object received is not a dict object param_error \u0026ldquo;module_args\u0026rdquo; parameter was not parsed properly module_not_exist Module with given \u0026ldquo;module_name\u0026rdquo; does not exist for the database Status codes Code Description 200 OK Success, module updated on bdb. 404 Not Found bdb not found. 400 Bad Request Bad or missing configuration parameters. 406 Not Acceptable Module does not support runtime configuration of arguments. ","categories":["RS"]},{"uri":"/rs/references/compatibility/config-settings/","uriRel":"/rs/references/compatibility/config-settings/","title":"Compatibility with open source Redis configuration settings","tags":[],"keywords":[],"description":"Open source Redis configuration settings supported by Redis Enterprise.","content":"Redis Enterprise Software and Redis Enterprise Cloud only support a subset of open source Redis configuration settings. Using CONFIG GET or CONFIG SET with unsupported configuration settings returns an error.\nSetting Redis\nEnterprise Redis\nCloud Notes activerehashing ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active busy-reply-threshold ✅ Standard\n✅ Active-Active ❌ Standard\n❌ Active-Active Value must be between 0 and 60000. hash-max-listpack-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active hash-max-listpack-value ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active hash-max-ziplist-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active hash-max-ziplist-value ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active hll-sparse-max-bytes ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active list-compress-depth ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active list-max-listpack-size ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active list-max-ziplist-size ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active lua-time-limit ✅ Standard\n✅ Active-Active ❌ Standard\n❌ Active-Active Value must be between 0 and 60000. notify-keyspace-events ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active set-max-intset-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active slowlog-log-slower-than ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Value must be larger than 1000. slowlog-max-len ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active Value must be between 128 and 1024. stream-node-max-bytes ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active stream-node-max-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active zset-max-listpack-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active zset-max-listpack-value ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active zset-max-ziplist-entries ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active zset-max-ziplist-value ✅ Standard\n✅ Active-Active ✅ Standard\n✅ Active-Active ","categories":["RS"]},{"uri":"/rs/networking/cluster-dns/","uriRel":"/rs/networking/cluster-dns/","title":"Configure cluster DNS","tags":[],"keywords":[],"description":"Configure DNS to communicate between nodes in your cluster.","content":"By default, Redis Enterprise Software deployments use DNS to communicate between nodes. You can also use the Discovery Service, which uses IP addresses to connect and complies with the Redis Sentinel API supported by open source Redis.\nEach node in a Redis Enterprise cluster includes a small DNS server to manage internal functions, such as high availability, automatic failover, automatic migration, and so on. Nodes should only run the DNS server included with the software. Running additional DNS servers can lead to unexpected behavior.\nCluster name and connection management Whether you\u0026rsquo;re administering Redis Enterprise Software or accessing databases, there are two ways to connect:\nURL-based connections - URL-based connections use DNS to resolve the fully qualified cluster domain name (FQDN). This means that DNS records might need to be updated when topology changes, such as adding (or removing) nodes from the cluster.\nBecause apps and other client connections rely on the URL (rather than the address), they do not need to be modified when topology changes.\nIP-based connections - IP-based connections do not require DNS setup, as they rely on the underlying TCP/IP addresses. As long as topology changes do not change the address of the cluster nodes, no configuration changes are needed, DNS or otherwise.\nHowever, changes to IP addresses (or changes to IP address access) impact all connections to the node, including apps and clients. IP address changes can therefore be unpredictable or time-consuming.\nURL-based connections The fully qualified domain name (FQDN) is the unique cluster identifier that enables clients to connect to the different components of Redis Enterprise Software. The FQDN is a crucial component of the high-availability mechanism because it\u0026rsquo;s used internally to enable and implement automatic and transparent failover of nodes, databases, shards, and endpoints.\nNote: Setting the cluster\u0026rsquo;s FQDN is a one-time operation, one that cannot be changed after being set. The FQDN must always comply with the IETF\u0026rsquo;s RFC 952 standard and section 2.1 of the RFC 1123 standard.\nIdentify the cluster To identify the cluster, either use DNS to define a fully qualified domain name or use the IP addresses of each node.\nDefine domain using DNS Use DNS if you:\nhave your own domain want to integrate the cluster into that domain can access and update the DNS records for that domain Make sure that the cluster and at least one node (preferably all nodes) in the cluster are correctly configured in the DNS with the appropriate NS entries.\nFor example:\nYour domain is: mydomain.com You would like to name the Redis Enterprise Software cluster mycluster You have three nodes in the cluster: node1 (IP address 1.1.1.1) node2 (2.2.2.2) node3 (3.3.3.3) In the FQDN field, enter the value mycluster.mydomain.com and add the following records in the DNS table for mydomain.com:\nmycluster.mydomain.com NS node1.mycluster.mydomain.com node2.mycluster.mydomain.com node3.mycluster.mydomain.com node1.mycluster.mydomain.com A 1.1.1.1 node2.mycluster.mydomain.com A 2.2.2.2 node3.mycluster.mydomain.com A 3.3.3.3 Zero-configuration using mDNS Development and test environments can use Multicast DNS (mDNS), a zero-configuration service designed for small networks. Production environments should not use mDNS.\nmDNS is a standard protocol that provides DNS-like name resolution and service discovery capabilities to machines on local networks with minimal to no configuration.\nBefore adopting mDNS, verify that it\u0026rsquo;s supported by each client you wish to use to connect to your Redis databases. Also make sure that your network infrastructure permits mDNS/multi-casting between clients and cluster nodes.\nConfiguring the cluster to support mDNS requires you to assign the cluster a .local name.\nFor example, if you want to name the Redis Enterprise Software cluster rediscluster, specify the FQDN name as rediscluster.local.\nWhen using the DNS or mDNS option, failover can be done transparently and the DNS is updated automatically to point to the IP address of the new primary node.\nIP-based connections When you use the IP-based connection option, the FQDN does not need to have any special format because clients use IP addresses instead of hostnames to access the databases so you are free to choose whatever name you want. Using the IP-based connection option does not require any DNS configuration either.\nTo administer the cluster you do need to know the IP address of at least one of the nodes in the cluster. Once you have the IP address, you can simply connect to port number 8443 (for example: https://10.0.0.12:8443). However, as the topology of the cluster changes and node with the given IP address is removed, you need to remember the IP address of another node participating in this cluster to connect to the admin console and manage the cluster.\nApplications connecting to Redis Software databases have the same constraints. When using the IP-based connection method, you can use the Discovery Service to discover the database endpoint for a given database name as long as you have an IP address for at least one of the nodes in the cluster. The API used for discovery service is compliant with the Redis Sentinel API.\nTo test your connection, try pinging the service. For help, see Connect to your database.\n","categories":["RS"]},{"uri":"/contribution-guide/","uriRel":"/contribution-guide/","title":"Contribution Guide","tags":[],"keywords":[],"description":"How to contribute to the Redis documentation","content":"Redis documentation is an open source project and we welcome edits of all types.\nJust to get you started, here is a simple explanation of how to contribute content to the docs.\nEdit in GitHub vs. open an issue If you see a problem on a page, either with content or formatting, and you think you can fix it, you can click on the Edit on GitHub link, edit the page, and submit the change.\nBranches vs. forks The redislabs-docs repository is public but only members of the repository can create new branches in the repo. New branches in the repo are automatically built into staging sites at: http://docs.redis.com/staging/\u0026lt;branch\u0026gt; After every commit to a branch, the site is re-built within about 1 minute so you can see the live updates.\nIf you are not a member of the repository, you can fork the repository to a branch in your account and make your changes on your private branch. You must use a new branch for every change. If the changes that you make resolve two separate issues, make two separate PRs.\nAfter you commit your changes to the public repo or your forked repo, you can open a pull request to submit your changes for consideration.\nServing the docs site locally After you clone the repository to your local machine, you can serve the site locally with Hugo and then browse to http://localhost:1313 to see how it looks.\nThe Redis documentation site does not run on the latest version of Hugo. You can go to the readme page of the redislabs-docs repository to find the version of Hugo that is currently supported.\nTo run an older version of Hugo on your local machine:\nGo to the Hugo releases and find the release version you want. Download your selected binary — by default, it goes to your Downloads folder. In terminal, navigate to: /usr/local/bin Create a directory for this version of Hugo and name it something sensible, like “hugo-0.57.2”. In your terminal, cd to your newly created Hugo-version folder, and run: tar zxf ~/Downloads/\u0026lt;hugo_archive\u0026gt; Confirm it’s working from the command line now: ./hugo version Make a shell config file alias, for example: alias hugo57='/usr/local/bin/hugo-0.57.2/hugo' Go to your redislabs-docs repo and use the alias to build the docs, for example: hugo57 serve Adding pages Every article in the docs is an individual markdown file. To add a new article in the docs, you must add a markdown file in the hierarchy of the content directory.\nThe markdown file must include these metadata in the header of the file:\nTitle: - The title of the article description: - A short description of the article (Currently not used) weight: - A number that indicates the location of the article among the articles in the directory (0 is top) alwaysopen: false - The article is by default hidden in the table of contents in the directory categories: - The product that the article relates to: RC, RS, Platforms, Modules For example:\n--- Title: Usage Reports description: weight: 70 alwaysopen: false categories: [\u0026#34;RC\u0026#34;] --- Adding sections To add a section to the docs that includes multiple articles, you must add a directory with a _index.md file. The _index.md file is the landing page for the section that should contain the main information for the section.\nStart the article with the header information as shown above and fill the body with information that is relevant to all articles in the section. If you do not have content that can serve as the landing page for the section, you can use the children shortcode to show all of the sub-topics for the section.\n{{% children style=\u0026#34;h2\u0026#34; description=\u0026#34;true\u0026#34; %}} Writing article content The structure of an article is:\nA short description (2-4 sentences) of the article. Concepts Prerequisites (if necessary) Informational notices (if necessary) Procedure Every article must relate to only one procedure, and the concepts must explain any background information that is needed to know when and why to use the procedure.\nWriting structure The concept section consists of: 2-4 paragraphs in the section or subsection 2-4 sentences in each paragraph 10-15 words in each sentence section Break lines at logical sentence breaks (end of sentence, comma) The prerequisites are listed as bullet points List notes and warnings before the procedure For procedures: No more than 10 steps\nSub-procedures must be one step with multiple sub-steps\nFor example: 1. To create service accounts, on each participating cluster: 1. In your web browser, open the admin console of the cluster that you want to connect to in order to create the CRDB. By default, the address is: `https://\u0026lt;RS_address\u0026gt;:8443` 1. Go to **settings \u0026gt; team** and click ![Add](/images/rs/icon_add.png#no-click \u0026quot;Add\u0026quot;). 1. Enter the name, email, and password for the user, select the **Admin** role, and click ![Save](/images/rs/icon_save.png#no-click \u0026quot;Save\u0026quot;). ![Service Account Creation](/images/rs/create-service-account.png) 1. To make sure that there is network connectivity between the participating clusters, telnet on port 9443 from each participating cluster to each of the other participating clusters. ```sh telnet \u0026lt;target FQDN\u0026gt; 9443 ``` Writing guidelines We recommend that you use a markdown linter in your IDE to maintain good markdown syntax. Format names of UI controls in bold - OK Format commands, filenames, and input text in code - rladmin Introduce a procedure with the goal of the procedure and a colon (:) - To authenticate to the Swagger UI: Lead the sentence with the subject Use “must” instead of - needs, wants, has to, desire, \u0026hellip; Subject and verb must agree in number - blueprint exists, blueprints exist Don’t start a sentence with “also” Instead of writing “In this section” write what is in the section: Incorrect - In this section detailed description of available development tools, like built-in features, widget objects, functions, template mechanism and available libraries is provided. Correct - The widget development tools include built-in features, widget objects, functions, template mechanism and available libraries. Link from text itself instead of “see …”. For example: Incorrect - Using the React Utility is the recommended method, and it requires a build operation. You must use the build system described in [Widget building](/rs/) section. Correct - Using the React Utility is the recommended method, and it requires a build operation in the [widget build system](/rs/). Terminology for APIs: Your code calls an API, which usually returns something. A call to a REST API sends an HTTP Request to the API URL and the server that hosts that URL usually returns an HTTP Response. The Request and the Response usually consist of Headers and a Body. Query and Form parameters can also be part of the Request. The Request Method determines the action that the Server should perform to process the Request. The CRUD actions are what the server does when it processes the POST, GET, PATCH/PUT, DELETE methods. Writing tone We use a friendly but functional tone. Use simple language in your writing so that it is easy for non-English speakers to understand your writing. At the same time, do not skip words just to save space.\nHere are some examples:\nText type Wrong Correct Explanation Procedure Sign up for Redis Cloud Pro account. Sign up for a Redis Cloud Pro account. Do not use please. Do not skip a, an, or the. Procedure Enter the Deployment CIDR that you will need to use Enter the required Deployment CIDR. Do not use future tense or any other complex verbs Common syntax gotchas Lists\nList procedures in numbered lists preceded with a blank line -\nTo add your name: 1. Click here. 1. Enter your name. 1. Click **OK**. List items in bulleted lists preceded with a blank line -\n```sh The ingredients are: - Potatoes - Chocolate ``` To add more information about a step in a procedure, add a blank line and a tab before the sentence -\n```sh 1. Click here. Here means where you need to click. 1. Enter your name. ``` When you list types of items, put the name of the item in bold followed by a dash (-) and then describe the item:\n```sh Some API operations require input, such as: - **Parameters** - When an API operation requires URI parameters, such as \u0026quot;get subscription by subscription id, you can enter the values for the parameters. - **JSON Request Body** - For API operations that require a JSON request body, you can either: - Use the **model display** to write the request based on the expected JSON structure and parameters. - Use the **Try it now** sample JSON created by Swagger as a base template that you can edit and execute. ``` Images and notices Add images below text at the same tab level as the text -\n```sh The API response is shown in the **Responses** section of the API operation. The results include an example of how you to execute the same operation in a standard command-line utility using `cURL`. ![swagger-query-results](/images/rv/api/swagger-query-results.png) ``` Use info boxes to add additional information and note boxes to add information that can avoid problems.\n```sh FYI - We use `cURL` and Linux shell scripts to provide examples on using the API. Note: The key values are not saved when you refresh the page. ``` When you add info, note or warning boxes below indented text, you must indent the open and close tags and not indent the content.\n```sh Note: The key values are not saved when you refresh the page. ``` Related Information For more about contributing and editing documentation, see:\nEditing guide Markdown cheatsheet ","categories":[]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/","title":"crdb-cli crdb commands","tags":[],"keywords":[],"description":"Manage Active-Active databases.","content":"Use crdb-cli crdb commands to manage Active-Active databases.\ncrdb-cli crdb commands Command Description add-instance Adds a peer replica to an Active-Active database. create Creates an Active-Active database. delete Deletes an Active-Active database. flush Clears all keys from an Active-Active database. get Shows the current configuration of an Active-Active database. health-report Shows the health report of an Active-Active database. list Shows a list of all Active-Active databases. purge-instance Deletes data from a local instance and removes it from the Active-Active database. remove-instance Removes a peer replica from an Active-Active database. update Updates the configuration of an Active-Active database. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/","uriRel":"/rs/references/rest-api/objects/crdb/","title":"CRDB object","tags":[],"keywords":[],"description":"An object that represents an Active-Active database","content":"An object that represents an Active-Active database.\nName Type/Value Description guid string The global unique ID of the Active-Active database causal_consistency boolean Enables causal consistency across CRDT instances default_db_config CRDB database_config object Default database configuration encryption boolean Encrypt communication featureset_version integer Active-Active database active FeatureSet version instances array of CRDB instance_info objects local_databases [{ \"bdb_uid\": string, \"id\": integer }, ...] Mapping of instance IDs for local databases to local BDB IDs name string Name of Active-Active database protocol_version integer Active-Active database active protocol version ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/crdb_coordinator/","uriRel":"/rs/references/rest-api/objects/services_configuration/crdb_coordinator/","title":"CRDB coordinator object","tags":[],"keywords":[],"description":"Documents the crdb_coordinator object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the CRDB coordinator process ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb_task/","uriRel":"/rs/references/rest-api/objects/crdb_task/","title":"CRDB task object","tags":[],"keywords":[],"description":"An object that represents a CRDB task","content":"An object that represents an Active-Active (CRDB) task.\nName Type/Value Description id string CRDB task ID (read only) crdb_guid string Globally unique Active-Active database ID (GUID) (read-only) errors [{ \"cluster_name\": string, \"description\": string, \"error_code\": string }, ...] Details for errors that occurred on a cluster status \u0026lsquo;queued\u0026rsquo; \u0026lsquo;started\u0026rsquo; \u0026lsquo;finished\u0026rsquo; \u0026lsquo;failed\u0026rsquo; CRDB task status (read only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdb_tasks/","uriRel":"/rs/references/rest-api/requests/crdb_tasks/","title":"CRDB tasks requests","tags":[],"keywords":[],"description":"Active-Active database task status requests","content":" Method Path Description GET /v1/crdb_tasks/{task_id} Get the status of an executed task Get task status GET /v1/crdb_tasks/{task_id} Get the status of an executed task.\nThe status of a completed task is kept for 500 seconds by default.\nRequest Example HTTP request GET /crdb_tasks/1 Request headers Key Value Description X-Result-TTL integer Task time to live URL parameters Field Type Description task_id string Task ID Response Returns a CRDB task object.\nStatus codes Code Description 200 OK Task status. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Task not found. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/crdb_worker/","uriRel":"/rs/references/rest-api/objects/services_configuration/crdb_worker/","title":"CRDB worker object","tags":[],"keywords":[],"description":"Documents the crdb_worker object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the CRDB worker processes ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/","uriRel":"/rs/references/cli-utilities/crdb-cli/","title":"crdb-cli","tags":[],"keywords":[],"description":"Manage Active-Active databases.","content":"An Active-Active database (also known as CRDB or conflict-free replicated database) replicates your data across Redis Enterprise Software clusters located in geographically distributed regions. Active-Active databases allow read-write access in all locations, making them ideal for distributed applications that require fast response times and disaster recovery.\nThe Active-Active database on an individual cluster is called an instance. Each cluster that hosts an instance is called a participating cluster.\nAn Active-Active database requires two or more participating clusters. Each instance is responsible for updating the instances that reside on other participating clusters with the transactions it receives. Write conflicts are resolved using conflict-free replicated data types (CRDTs).\nTo programmatically maintain an Active-Active database and its instances, you can use the crdb-cli command-line tool.\ncrdb-cli commands Command Description crdb Manage Active-Active databases. task Manage Active-Active tasks. Use the crdb-cli To use the crdb-cli tool, use SSH to sign in to a Redis Enterprise host with a user that belongs to the group that Redis Enterprise Software was installed with (Default: redislabs). If you sign in with a non-root user, you must add /opt/redislabs/bin/ to your PATH environment variables.\ncrdb-cli commands use the syntax: crdb-cli \u0026lt;command\u0026gt; \u0026lt;arguments\u0026gt; to let you:\nCreate, list, update, flush, or delete an Active-Active database. Add or remove an instance of the Active-Active database on a specific cluster. Each command creates a task.\nBy default, the command runs immediately and displays the result in the output.\nIf you use the --no-wait flag, the command runs in the background so that your application is not delayed by the response.\nUse the crdb-cli task commands to manage Active-Active database tasks.\nFor each crdb-cli command, you can use --help for additional information about the command.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdbs/","uriRel":"/rs/references/rest-api/requests/crdbs/","title":"CRDBs requests","tags":[],"keywords":[],"description":"Active-Active database requests","content":" Method Path Description GET /v1/crdbs Get all Active-Active databases GET /v1/crdbs/{crdb_guid} Get a specific Active-Active database PATCH /v1/crdbs/{crdb_guid} Update an Active-Active database POST /v1/crdbs Create a new Active-Active database DELETE /v1/crdbs/{crdb_guid} Delete an Active-Active database Get all Active-Active databases GET /v1/crdbs Get a list of all Active-Active databases on the cluster.\nRequest Example HTTP request GET /crdbs Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result Response Returns a JSON array of CRDB objects.\nStatus codes Code Description 200 OK A list of Active-Active database. 401 Unauthorized Unauthorized request. Invalid credentials Get an Active-Active database GET /v1/crdbs/{crdb_guid} Get a specific Active-Active database.\nRequest Example HTTP request GET /crdbs/552bbccb-99f3-4142-bd17-93d245f0bc79 Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Query parameters Field Type Description instance_id integer Instance from which to get the Active-Active database information Response Returns a CRDB object.\nStatus codes Code Description 200 OK Active-Active database information is returned. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Database or configuration does not exist. Update an Active-Active database PATCH /v1/crdbs/{crdb_guid} Update an Active-Active database\u0026rsquo;s configuration.\nIn order to add or remove instances, use POST crdbs/{crdb_guid}/updates instead.\nRequest Example HTTP request PATCH /crdbs/552bbccb-99f3-4142-bd17-93d245f0bc79 Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Request body Include a CRDB object with updated fields in the request body.\nResponse Returns a CRDB task object.\nStatus codes Code Description 200 OK The request has been accepted. 400 Bad Request The posted Active-Active database contains invalid parameters. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Configuration or Active-Active database not found. 406 Not Acceptable The posted Active-Active database cannot be accepted. Create an Active-Active database POST /v1/crdbs Create a new Active-Active database.\nRequest Example HTTP request POST /crdbs Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result Request body Include a CRDB object, which defines the Active-Active database, in the request body.\nExample body { \u0026#34;default_db_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sample-crdb\u0026#34;, \u0026#34;memory_size\u0026#34;: 214748365 }, \u0026#34;instances\u0026#34;: [ { \u0026#34;cluster\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://\u0026lt;cluster1_FQDN\u0026gt;:9443\u0026#34;, \u0026#34;credentials\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;password\u0026gt;\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cluster-1\u0026#34; }, \u0026#34;compression\u0026#34;: 6 }, { \u0026#34;cluster\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://\u0026lt;cluster2_FQDN\u0026gt;:9443\u0026#34;, \u0026#34;credentials\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;password\u0026gt;\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cluster-2\u0026#34; }, \u0026#34;compression\u0026#34;: 6 } ], \u0026#34;name\u0026#34;: \u0026#34;sample-crdb\u0026#34; } This JSON body creates an Active-Active database without TLS and with two participating clusters.\nResponse Returns a CRDB task object.\nStatus codes Code Description 200 OK The request has been accepted. 400 Bad Request The request is invalid or malformed. 401 Unauthorized Unauthorized request. Invalid credentials 406 Not Acceptable The posted Active-Active database cannot be accepted. Delete an Active-Active database DELETE /v1/crdbs/{crdb_guid} Delete an Active-Active database.\nRequest Example HTTP request DELETE /crdbs/552bbccb-99f3-4142-bd17-93d245f0bc79 Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Response Returns a CRDB task object.\nStatus codes Code Description 200 OK Action was successful. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Configuration or Active-Active database not found. 406 Not Acceptable The Active-Active GUID is invalid or the Active-Active database was already deleted. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/crdt_sources-alerts/","uriRel":"/rs/references/rest-api/requests/bdbs/crdt_sources-alerts/","title":"Database CRDT sources alerts requests","tags":[],"keywords":[],"description":"Conflict-free replicated data type (CRDT) source alert requests","content":" Method Path Description GET /v1/bdbs/crdt_sources/alerts Get all CRDT sources alert states for all CRDB databases GET /v1/bdbs/crdt_sources/alerts/{uid} Get all CRDT sources alert states for a database GET /v1/bdbs/crdt_sources/alerts/{uid}/{crdt_src_id} Get all alert states for a CRDT source GET /v1/bdbs/crdt_sources/alerts/{uid}/{crdt_src_id}/{alert} Get a database alert state Get all CRDB CRDT source alert states GET /v1/bdbs/crdt_sources/alerts Get all alert states for all CRDT sources of all CRDBs.\nRequired permissions Permission name view_all_bdbs_alerts Request Example HTTP request GET /bdbs/crdt_sources/alerts Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a hash of alert UIDs and the alerts states for each local BDB of CRDB.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;crdt_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error Get all BDB CRDT sources alert states GET /v1/bdbs/crdt_sources/alerts/{int: uid} Get all alert states for all crdt sources for a specific local bdb of a CRDB.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/crdt_sources/alerts/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;crdt_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified bdb does not exist Get all CRDT source alert states GET /v1/bdbs/crdt_sources/alerts/{int: uid}/{int: crdt_src_id} Get all alert states for specific crdt source for a specific local BDB of a CRDB.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/crdt_sources/alerts/1/2 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database crdt_src_id integer The ID of the crdt source in this BDB Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;crdt_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified bdb does not exist Get database alert state GET /v1/bdbs/crdt_sources/alerts/{int: uid}/{int: crdt_src_id}/{alert} Get a BDB alert state.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/crdt_sources/alerts/1/2/crdt_src_syncer_connection_error Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database crdt_src_id integer The ID of the crdt source in this BDB alert string The alert name Response Returns an alert object.\nExample JSON body { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } } Status codes Code Description 200 OK No error 400 Bad Request Bad request 404 Not Found Specified alert or bdb does not exist ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/create/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/create/","title":"crdb-cli crdb create","tags":[],"keywords":[],"description":"Creates an Active-Active database.","content":"Creates an Active-Active database.\ncrdb-cli crdb create --name \u0026lt;name\u0026gt; --memory-size \u0026lt;maximum_memory\u0026gt; --instance fqdn=\u0026lt;cluster_fqdn\u0026gt;,username=\u0026lt;username\u0026gt;,password=\u0026lt;password\u0026gt;[,url=\u0026lt;url\u0026gt;,replication_endpoint=\u0026lt;endpoint\u0026gt;] --instance fqdn=\u0026lt;cluster_fqdn\u0026gt;,username=\u0026lt;username\u0026gt;,password=\u0026lt;password\u0026gt;[,url=\u0026lt;url\u0026gt;,replication_endpoint=\u0026lt;endpoint\u0026gt;] [--port \u0026lt;port_number\u0026gt;] [--no-wait] [--default-db-config \u0026lt;configuration\u0026gt;] [--default-db-config-file \u0026lt;filename\u0026gt;] [--compression \u0026lt;0-6\u0026gt;] [--causal-consistency { true | false } ] [--password \u0026lt;password\u0026gt;] [--replication { true | false } ] [--encryption { true | false } ] [--sharding { false | true } ] [--shards-count \u0026lt;number_of_shards\u0026gt;] [--shard-key-regex \u0026lt;regex_rule\u0026gt;] [--oss-cluster { true | false } ] [--bigstore { true | false }] [--bigstore-ram-size \u0026lt;maximum_memory\u0026gt;] [--with-module name=\u0026lt;module_name\u0026gt;,version=\u0026lt;module_version\u0026gt;,args=\u0026lt;module_args\u0026gt;] Prerequisites Before you create an Active-Active database, you must have:\nAt least two participating clusters Network connectivity between the participating clusters Parameters Parameter \u0026amp; options(s) Value Description name \u0026lt;CRDB_name\u0026gt; string Name of the Active-Active database (required) memory-size \u0026lt;maximum_memory\u0026gt; size in bytes, kilobytes (KB), or gigabytes (GB) Maximum database memory (required) instance fqdn=\u0026lt;cluster_fqdn\u0026gt;, username=\u0026lt;username\u0026gt;, password=\u0026lt;password\u0026gt; strings The connection information for the participating clusters (required for each participating cluster) port \u0026lt;port_number\u0026gt; integer TCP port for the Active-Active database on all participating clusters default-db-config \u0026lt;configuration\u0026gt; string Default database configuration options default-db-config-file \u0026lt;filename\u0026gt; filepath Default database configuration options from a file no-wait Prevents crdb-cli from running another command before this command finishes compression 0-6 The level of data compression: 0 = No compression 6 = High compression and resource load (Default: 3) causal-consistency true false (default) Causal consistency applies updates to all instances in the order they were received password \u0026lt;password\u0026gt; string Password for access to the database replication true false (default) Activates or deactivates database replication where every master shard replicates to a replica shard encryption true false (default) Activates or deactivates encryption sharding true false (default) Activates or deactivates sharding (also known as database clustering). Cannot be updated after the database is created shards-count \u0026lt;number_of_shards\u0026gt; integer If sharding is enabled, this specifies the number of Redis shards for each database instance oss-cluster truefalse (default) Activates OSS cluster API shard-key-regex \u0026lt;regex_rule\u0026gt; string If clustering is enabled, this defines a regex rule (also known as a hashing policy) that determines which keys are located in each shard (defaults to {u'regex': u'.*\\\\{(?\u0026lt;tag\u0026gt;.*)\\\\}.*'}, {u'regex': u'(?\u0026lt;tag\u0026gt;.*)'} ) bigstore true false (default) If true, the database uses Redis on Flash to add flash memory to the database bigstore-ram-size \u0026lt;size\u0026gt; size in bytes, kilobytes (KB), or gigabytes (GB) Maximum RAM limit for the Redis on Flash database with-module name=\u0026lt;module_name\u0026gt;, version=\u0026lt;module_version\u0026gt;, args=\u0026lt;module_args\u0026gt; strings Creates a database with a specific module eviction-policy noeviction (default)allkeys-lruallkeys-lfuallkeys-randomvolatile-lruvolatile-lfuvolatile-randomvolatile-ttl Sets eviction policy proxy-policy all-nodes\nall-master-shards\nsingle Sets proxy policy Returns Returns the task ID of the task that is creating the database.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the database to be created and then return the CRDB GUID.\nExamples $ crdb-cli crdb create --name database1 --memory-size 1GB --port 12000 \\ --instance fqdn=cluster1.redis.local,username=admin@redis.local,password=admin \\ --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin Task 633aaea3-97ee-4bcb-af39-a9cb25d7d4da created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; CRDB GUID Assigned: crdb:d84f6fe4-5bb7-49d2-a188-8900e09c6f66 ---\u0026gt; Status changed: started -\u0026gt; finished To create an Active-Active database with two shards in each instance and with encrypted traffic between the clusters:\ncrdb-cli crdb create --name mycrdb --memory-size 100mb --port 12000 --instance fqdn=cluster1.redis.local,username=admin@redis.local,password=admin --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin --shards-count 2 --encryption true To create an Active-Active database with two shards and with RediSearch 2.0.6 module:\ncrdb-cli crdb create --name mycrdb --memory-size 100mb --port 12000 --instance fqdn=cluster1.redis.local,username=admin@redis.local,password=admin --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin --shards-count 2 --with-module name=search,version=\u0026#34;2.0.6\u0026#34;,args=\u0026#34;PARTITIONS AUTO\u0026#34; To create an Active-Active database with two shards and with encrypted traffic between the clusters:\ncrdb-cli crdb create --name mycrdb --memory-size 100mb --port 12000 --instance fqdn=cluster1.redis.local,username=admin@redis.local,password=admin --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin --encryption true --shards-count 2 To create an Active-Active database with 1 shard in each instance and not wait for the response:\ncrdb-cli crdb create --name mycrdb --memory-size 100mb --port 12000 --instance fqdn=cluster1.redis.local,username=admin@redis.local,password=admin --instance fqdn=cluster2.redis.local,username=admin@redis.local,password=admin --no-wait ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/create/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/create/","title":"rladmin cluster create","tags":["non-configured"],"keywords":[],"description":"Creates a new cluster.","content":"Creates a new cluster. The node where you run rladmin cluster create becomes the first node of the new cluster.\ncluster create name \u0026lt;cluster name\u0026gt; username \u0026lt;admin email\u0026gt; password \u0026lt;admin password\u0026gt; [ node_uid \u0026lt;node UID\u0026gt; ] [ rack_aware ] [ rack_id \u0026lt;node rack ID\u0026gt; ] [ license_file \u0026lt;file\u0026gt; ] [ ephemeral_path \u0026lt;path\u0026gt; ] [ persistent_path \u0026lt;path\u0026gt; ] [ ccs_persistent_path \u0026lt;path\u0026gt; ] [ register_dns_suffix ] [ flash_enabled ] [ flash_path \u0026lt;path\u0026gt; ] [ addr \u0026lt;IP address\u0026gt; ] [ external_addr \u0026lt;IP address\u0026gt; ] Parameters Parameter Type/Value Description addr IP address The node\u0026rsquo;s internal IP address (optional) ccs_persistent_path filepath (default: /var/opt/redislabs/persist) Path to the location of CCS snapshots (optional) ephemeral_path filepath (default: /var/opt/redislabs) Path to the ephemeral storage location (optional) external_addr IP address The node\u0026rsquo;s external IP address (optional) flash_enabled Enables flash storage (optional) flash_path filepath (default: /var/opt/redislabs/flash) Path to the flash storage location (optional) license_file filepath Path to the RLEC license file (optional) name string Cluster name node_uid integer Unique node ID (optional) password string Admin user\u0026rsquo;s password persistent_path filepath (default: /var/opt/redislabs/persist) Path to the persistent storage location (optional) rack_aware Activates or deactivates rack awareness (optional) rack_id string The rack\u0026rsquo;s unique identifier (optional) register_dns_suffix Enables database mapping to both internal and external IP addresses (optional) username email address Admin user\u0026rsquo;s email address Returns Returns ok if the new cluster was created successfully. Otherwise, it returns an error message.\nExample $ rladmin cluster create name cluster.local \\ username admin@example.com \\ password admin-password Creating a new cluster... ok ","categories":["RS"]},{"uri":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/","uriRel":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/","title":"Create IAM resources for AWS cloud accounts","tags":[],"keywords":[],"description":"","content":"For most Redis Enterprise Cloud Flexible or Annual subscriptions deployed to Amazon Web Services (AWS), we manage the supporting infrastructure for you in dedicated AWS accounts.\nYou can manage this infrastructure with your own AWS accounts.\nYou\u0026rsquo;ll want these accounts to be separate from any AWS application accounts and you\u0026rsquo;ll need to create dedicated identity and access management (IAM) resources to allow us to manage the infrastructure.\nIn the new AWS account, you need to create:\nAn instance role A user with an access key A role that grants AWS console access Save the access key in a secure location so that you can enter it when you register the cloud account with your the Redis Enterprise Cloud subscription.\nWarning - We use the provided credentials to configure your AWS environment and provision required resources.\nTo make sure that we can manage your AWS resources, you must not:\nManually change the configurations of provisioned resources, such as security groups Manually stop or terminate provisioned instances For help creating an AWS user, see the AWS IAM documentation.\nYou can use one of the following tools to create IAM resources:\nCloudFormation - The AWS automation tool Terraform - Widely supported in the Redis community for additional automation The AWS Console ","categories":["RC"]},{"uri":"/rs/installing-upgrading/creating-support-package/","uriRel":"/rs/installing-upgrading/creating-support-package/","title":"Creating a Support Package","tags":[],"keywords":[],"description":"","content":"If you encounter any issues that you are not able to resolve yourself and need to contact Redis support for assistance, you can create a support package that gathers all essential information to help us debug your issues.\nNote: The process of creating the support package can take several minutes and generates load on the system. Creating a support package To create a support package:\nClick Support at the top right of the admin console. Click Create Support Package at the bottom of the page, and confirm the action. The package is created and downloaded by your browser. The package creation failed with an error? You can\u0026#39;t access the UI? If package creation fails with internal error or if you cannot access the UI, create a support package for the cluster from the command-line on any of the nodes in the cluster with the command: /opt/redislabs/bin/rladmin cluster debug_info\nIf rladmin cluster debug_info fails for lack of space in the /tmp directory, you can:\nChange the storage location where the support package is saved: rladmin cluster config debuginfo_path \u0026lt;path\u0026gt;\nThe redislabs user must have write access to the storage location on all cluster nodes.\nOn any one of the node in the cluster, run: rladmin cluster debug_info\nIf rladmin cluster debug_info fails for another reason, you can create a support package for the cluster from the command-line on each node in the cluster with the command: /opt/redislabs/bin/debuginfo\nUpload the tar archive to Redis Support. The path to the archive is shown in the command output.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/credentials/","uriRel":"/rs/references/rest-api/objects/bootstrap/credentials/","title":"Credentials object","tags":[],"keywords":[],"description":"Documents the credentials object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description password string Admin password username string Admin username (pattern does not allow special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;) ","categories":["RS"]},{"uri":"/rc/databases/configuration/data-eviction-policies/","uriRel":"/rc/databases/configuration/data-eviction-policies/","title":"Data eviction policies","tags":[],"keywords":[],"description":"Data eviction policies control what happens when new data exceeds the memory limits of a database.  Here, you&#39;ll learn the available policies and how to change which one is used for a database.","content":"The data eviction policy of a database controls what happens when new data exceeds the memory size of a database. Typically, such situations require evicting (or deleting) data previously added to the database.\nTo control this behavior, change the Data eviction policy setting for a database.\nAvailable policies For each database, you can choose from these data eviction policies:\nAvailable policies Description allkeys-lru Keeps most recently used keys; removes least recently used (LRU) keys allkeys-lfu Keeps frequently used keys; removes least frequently used (LFU) keys allkeys-random Randomly removes keys volatile-lru Removes least recently used keys with expire field set to true (Default) volatile-lfu Removes least frequently used keys with expire field set to true volatile-random Randomly removes keys with expire field set to true volatile-ttl Removes least frequently used keys with expire field set to true and the shortest remaining time-to-live (TTL) value no eviction New values aren\u0026rsquo;t saved when memory limit is reachedWhen a database uses replication, this applies to the primary database Redis Cloud supports Redis on Flash (RoF) to prevent data eviction but maintain high performance.\nRoF can extend your database across RAM and Flash Memory and intelligently manage \u0026ldquo;hot\u0026rdquo; (active) data in RAM and \u0026ldquo;cold\u0026rdquo; (less active) data in Flash memory (SSD).\n","categories":["RC"]},{"uri":"/rc/databases/configuration/data-persistence/","uriRel":"/rc/databases/configuration/data-persistence/","title":"Data persistence","tags":[],"keywords":[],"description":"Data persistence enables recovery in the event of memory loss or other catastrophic failure.  Here, you learn data persistence options, when they&#39;re available, and how to apply specific settings to individual databases.","content":"Redis Enterprise Cloud can persist data to enable recovery in the event of memory loss or other catastrophic failure. When you enable data persistence, in-memory data is copied to persistent storage attached to the underlying cloud instance.\nPersistence options Data can be persisted in one of two ways:\nAn Append-Only File (AOF) maintains a record (sometimes called a redo log or journal) of write operations. This allows the data to be restored by using the record to reconstruct the database up to the point of failure.\nThe AOF file records write operations made to the database; it can be updated every second or on every write (Flexible or Annual plans only).\nAOF files allow recovery nearly to the point of failure; however, recovery takes longer as the database is reconstructed from the record.\nSnapshots are copies of the in-memory database, taken at periodic intervals (one, six, or twelve hours).\nSnapshot recovery is faster, however, there\u0026rsquo;s a greater risk of data loss depending on the time between failure and the most recent snapshot.\nAOF files require more resources than snapshots; they provide greater protection (durability) at the cost of resources and recovery time. Snapshots provide faster recovery while risking greater data loss.\nData persistance can be also disabled. In such cases, data is lost when the database goes down.\nConfigure data persistence In Redis Enterprise Cloud, data persistence is a database configuration setting that can be changed by editing your database settings.\nThe availability of the setting depends on your subscription:\nFree subscriptions do not support data persistence; the setting is disabled entirely.\nFor Fixed plans, persistence requires a standard plan and is not available for cache subscriptions.\nUse the Plan description setting to determine your subscription type. You might need to change your subscription type to enable data persistence.\nFlexible and Annual plans enable data persistence settings for every database.\nWhen enabled, you can change the Data persistence setting to one of the following values:\nOptions Description None Data is not persisted to disk at all. Append Only File (AoF) every write (Flexible and Annual subscriptions only) Every write is recorded (synchronized to disk using fsync) Append Only File (AoF) every 1 second Record is updated every second (synchronized to disk using fsync) Snapshot every 1 hour A snapshot of the database is created every hour Snapshot every 6 hours A snapshot of the database is created every 6 hours Snapshot every 12 hours A snapshot of the database is created every 12 hours When you save changes to data persistence settings, the updates are applied in the background. This means there is a brief delay while the new settings are applied.\nWhen replication is enabled for a database, persistence is performed against replicas (copies) to reduce performance impact on the primary (master) database.\n","categories":["RC"]},{"uri":"/rs/references/rest-api/objects/crdb/database_config/","uriRel":"/rs/references/rest-api/objects/crdb/database_config/","title":"CRDB database config object","tags":[],"keywords":[],"description":"An object that represents the database configuration","content":"An object that represents the database configuration.\nName Type/Value Description aof_policy string Policy for Append-Only File data persistence authentication_admin_pass string Administrative databases access token authentication_redis_pass string Redis AUTH password bigstore boolean Database driver is Redis on Flash bigstore_ram_size integer Memory size of RAM size data_persistence string Database on-disk persistence max_aof_file_size integer Hint for maximum AOF file size max_aof_load_time integer Hint for maximum AOF reload time memory_size integer Database memory size limit, in bytes oss_cluster boolean Enables OSS Cluster mode oss_cluster_api_preferred_ip_type string Indicates preferred IP type in OSS cluster API: internal/external oss_sharding boolean An alternative to shard_key_regex for using the common case of the OSS shard hashing policy port integer TCP port for database access proxy_policy string The policy used for proxy binding to the endpoint rack_aware boolean Require the database to be always replicated across multiple racks replication boolean Database replication shard_key_regex [{ \"regex\": string }, ...] To use the default rules you should set the value to: [ { “regex”: “.*\\\\{(?\u003c tag \u003e.*)\\\\}.*” }, { “regex”: “(?\u003c tag \u003e.*)” } ] Custom keyname-based sharding rules (required) shards_count integer Number of database shards shards_placement string Control the density of shards: should they reside on as few or as many nodes as possible snapshot_policy array of snapshot_policy objects Policy for snapshot-based data persistence (required) tls_mode string Encrypt communication ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/dataset_import_sources/","uriRel":"/rs/references/rest-api/objects/bdb/dataset_import_sources/","title":"BDB dataset import sources object","tags":[],"keywords":[],"description":"Documents the bdb dataset_import_sources object used with Redis Enterprise Software REST API calls.","content":"You can import data to a database from the following location types:\nHTTP/S FTP SFTP Amazon S3 Google Cloud Storage Microsoft Azure Storage NAS/Local Storage The source file to import should be in the RDB format. It can also be in a compressed (gz) RDB file.\nSupply an array of dataset import source objects to import data from multiple files.\nBasic parameters For all import location objects, you need to specify the location type via the type field.\nLocation type \u0026ldquo;type\u0026rdquo; value FTP/S \u0026ldquo;url\u0026rdquo; SFTP \u0026ldquo;sftp\u0026rdquo; Amazon S3 \u0026ldquo;s3\u0026rdquo; Google Cloud Storage \u0026ldquo;gs\u0026rdquo; Microsoft Azure Storage \u0026ldquo;abs\u0026rdquo; NAS/Local Storage \u0026ldquo;mount_point\u0026rdquo; Location-specific parameters Any additional required parameters may differ based on the import location type.\nFTP Key name Type Description url string A URI that represents the FTP/S location with the following format: ftp://user:password@host:port/path/. The user and password can be omitted if not needed. SFTP Key name Type Description key string SSH private key to secure the SFTP server connection. If you do not specify an SSH private key, the autogenerated private key of the cluster is used and you must add the SSH public key of the cluster to the SFTP server configuration. (optional) sftp_url string SFTP URL in the format: sftp://user:password@host[:port]/path/filename.rdb. The default port number is 22 and the default path is \u0026lsquo;/\u0026rsquo;. AWS S3 Key name Type Description access_key_id string The AWS Access Key ID with access to the bucket bucket_name string S3 bucket name secret_access_key string The AWS Secret Access that matches the Access Key ID subdir string Path to the backup directory in the S3 bucket (optional) Google Cloud Storage Key name Type Description bucket_name string Cloud Storage bucket name client_email string Email address for the Cloud Storage client ID client_id string Cloud Storage client ID with access to the Cloud Storage bucket private_key string Private key for the Cloud Storage matching the private key ID private_key_id string Cloud Storage private key ID with access to the Cloud Storage bucket subdir string Path to the backup directory in the Cloud Storage bucket (optional) Azure Blob Storage Key name Type Description account_key string Access key for the storage account account_name string Storage account name with access to the container container string Blob Storage container name sas_token string Token to authenticate with shared access signature subdir string Path to the backup directory in the Blob Storage container (optional) Note: account_key and sas_token are mutually exclusive NAS/Local Storage Key name Type Description path string Path to the locally mounted filename to import. You must create the mount point on all nodes, and the redislabs:redislabs user must have read permissions on the local mount point. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/statistics/db-metrics/","uriRel":"/rs/references/rest-api/objects/statistics/db-metrics/","title":"DB metrics","tags":[],"keywords":[],"description":"Documents the DB metrics used with Redis Enterprise Software REST API calls.","content":" Metric name Type Description avg_latency float Average latency of operations on the DB (microseconds). Only returned when there is traffic. avg_other_latency float Average latency of other (non read/write) operations (microseconds). Only returned when there is traffic. avg_read_latency float Average latency of read operations (microseconds). Only returned when there is traffic. avg_write_latency float Average latency of write operations (microseconds). Only returned when there is traffic. big_del_flash float Rate of key deletes for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_del_ram float Rate of key deletes for keys in RAM (BigRedis) (key access/sec); this includes write misses (new keys created). Only returned when BigRedis is enabled. big_fetch_flash float Rate of key reads/updates for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_fetch_ram float Rate of key reads/updates for keys in RAM (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_io_ratio_flash float Rate of key operations on flash. Can be used to compute the ratio of I/O operations (key access/sec). Only returned when BigRedis is enabled. big_io_ratio_redis float Rate of Redis operations on keys. Can be used to compute the ratio of I/O operations (key access/sec). Only returned when BigRedis is enabled. big_write_flash float Rate of key writes for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_write_ram float Rate of key writes for keys in RAM (BigRedis) (key access/sec); this includes write misses (new keys created). Only returned when BigRedis is enabled. bigstore_io_dels float Rate of key deletions from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_io_read_bytes float Throughput of I/O read operations against backend flash bigstore_io_reads float Rate of key reads from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_io_write_bytes float Throughput of I/O write operations against backend flash bigstore_io_writes float Rate of key writes from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_iops float Rate of I/O operations against backend flash for all shards of the DB (BigRedis) (ops/sec). Only returned when BigRedis is enabled. bigstore_kv_ops float Rate of value read/write/del operations against backend flash for all shards of the DB (BigRedis) (key access/sec). Only returned when BigRedis is enabled bigstore_objs_flash float Value count on flash (BigRedis). Only returned when BigRedis is enabled. bigstore_objs_ram float Value count in RAM (BigRedis). Only returned when BigRedis is enabled. bigstore_throughput float Throughput of I/O operations against backend flash for all shards of the DB (BigRedis) (bytes/sec). Only returned when BigRedis is enabled. conns float Number of client connections to the DB’s endpoints disk_frag_ratio float Flash fragmentation ratio (used/required). Only returned when BigRedis is enabled. egress_bytes float Rate of outgoing network traffic to the DB’s endpoint (bytes/sec) evicted_objects float Rate of key evictions from DB (evictions/sec) expired_objects float Rate keys expired in DB (expirations/sec) fork_cpu_system float % cores utilization in system mode for all Redis shard fork child processes of this database fork_cpu_user float % cores utilization in user mode for all Redis shard fork child processes of this database ingress_bytes float Rate of incoming network traffic to the DB’s endpoint (bytes/sec) instantaneous_ops_per_sec float Request rate handled by all shards of the DB (ops/sec) last_req_time date, ISO_8601 format Last request time received to the DB (ISO format 2015-07-05T22:16:18Z). Returns 1/1/1970 when unavailable. last_res_time date, ISO_8601 format Last response time received from DB (ISO format 2015-07-05T22:16:18Z). Returns 1/1/1970 when unavailable. main_thread_cpu_system float % cores utilization in system mode for all Redis shard main threads of this database main_thread_cpu_user float % cores utilization in user mode for all Redis shard main threads of this database mem_frag_ratio float RAM fragmentation ratio (RSS/allocated RAM) mem_not_counted_for_evict float Portion of used_memory (in bytes) not counted for eviction and OOM errors mem_size_lua float Redis Lua scripting heap size (bytes) monitor_sessions_count float Number of client connected in monitor mode to the DB no_of_expires float Number of volatile keys in the DB no_of_keys float Number of keys in the DB other_req float Rate of other (non read/write) requests on DB (ops/sec) other_res float Rate of other (non read/write) responses on DB (ops/sec) pubsub_channels float Count the pub/sub channels with subscribed clients pubsub_patterns float Count the pub/sub patterns with subscribed clients ram_overhead float Non values RAM overhead (BigRedis) (bytes). Only returned when BigRedis is enabled. read_hits float Rate of read operations accessing an existing key (ops/sec) read_misses float Rate of read operations accessing a nonexistent key (ops/sec) read_req float Rate of read requests on DB (ops/sec) read_res float Rate of read responses on DB (ops/sec) shard_cpu_system float % cores utilization in system mode for all Redis shard processes of this database shard_cpu_user float % cores utilization in user mode for the Redis shard process total_connections_received float Rate of new client connections to the DB (connections/sec) total_req float Rate of all requests on DB (ops/sec) total_res float Rate of all responses on DB (ops/sec) used_bigstore float Flash used by DB (BigRedis) (bytes). Only returned when BigRedis is enabled. used_memory float Memory used by DB (in BigRedis this includes flash) (bytes) used_ram float RAM used by DB (BigRedis) (bytes). Only returned when BigRedis is enabled. write_hits float Rate of write operations accessing an existing key (ops/sec) write_misses float Rate of write operations accessing a nonexistent key (ops/sec) write_req float Rate of write requests on DB (ops/sec) write_res float Rate of write responses on DB (ops/sec) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/db_alerts_settings/","uriRel":"/rs/references/rest-api/objects/db_alerts_settings/","title":"Database alerts settings object","tags":[],"keywords":[],"description":"An object for database alerts configuration","content":"An API object that represents the database alerts configuration.\nName Type/Value Description bdb_backup_delayed bdb_alert_settings_with_threshold object Periodic backup has been delayed for longer than specified threshold value (minutes) bdb_crdt_src_high_syncer_lag bdb_alert_settings_with_threshold object CRDB source sync lag is higher than specified threshold value (seconds) bdb_crdt_src_syncer_connection_error bdb_alert_settings_with_threshold object CRDB source sync had a connection error while trying to connect to replica source bdb_crdt_src_syncer_general_error bdb_alert_settings_with_threshold object CRDB sync encountered in general error bdb_high_latency bdb_alert_settings_with_threshold object Latency is higher than specified threshold value (microsec) bdb_high_syncer_lag bdb_alert_settings_with_threshold object Replica of sync lag is higher than specified threshold value (seconds) (deprecated) bdb_high_throughput bdb_alert_settings_with_threshold object Throughput is higher than specified threshold value (requests / sec) bdb_long_running_action bdb_alert_settings_with_threshold object An alert for state machines that are running for too long bdb_low_throughput bdb_alert_settings_with_threshold object Throughput is lower than specified threshold value (requests / sec) bdb_ram_dataset_overhead bdb_alert_settings_with_threshold object Dataset RAM overhead of a shard has reached the threshold value (% of its RAM limit) bdb_ram_values bdb_alert_settings_with_threshold object Percent of values kept in a shard\u0026rsquo;s RAM is lower than (% of its key count) bdb_replica_src_high_syncer_lag bdb_alert_settings_with_threshold object Replica of source sync lag is higher than specified threshold value (seconds) bdb_replica_src_syncer_connection_error bdb_alert_settings_with_threshold object Replica of source sync has connection error while trying to connect replica source bdb_replica_src_syncer_general_error bdb_alert_settings_with_threshold object Replica of sync encountered in general error bdb_shard_num_ram_values bdb_alert_settings_with_threshold object Number of values kept in a shard\u0026rsquo;s RAM is lower than (values) bdb_size bdb_alert_settings_with_threshold object Dataset size has reached the threshold value (% of the memory limit) bdb_syncer_connection_error bdb_alert_settings_with_threshold object Replica of sync has connection error while trying to connect replica source (deprecated) bdb_syncer_general_error bdb_alert_settings_with_threshold object Replica of sync encountered in general error (deprecated) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/db-conns-auditing-config/","uriRel":"/rs/references/rest-api/objects/db-conns-auditing-config/","title":"Database connection auditing configuration object","tags":[],"keywords":[],"description":"An object for database connection auditing settings","content":"Database connection auditing configuration\nName Type/Value Description audit_address string TCP/IP address where one can listen for notifications. audit_port integer Port where one can listen for notifications. audit_protocol TCP\nlocal Protocol used to process notifications. For production systems, TCP is the only valid value. audit_reconnect_interval integer Interval (in seconds) between attempts to reconnect to the listener. Default is 1 second. audit_reconnect_max_attempts integer Maximum number of attempts to reconnect. Default is 0 (infinite). ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/debug_info/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/debug_info/","title":"rladmin cluster debug_info","tags":["configured"],"keywords":[],"description":"Creates a support package.","content":"Downloads a support package to the specified path. If you do not specify a path, it downloads the package to the default path specified in the cluster configuration file.\nrladmin cluster debug_info [ node \u0026lt;ID\u0026gt; ] [ path \u0026lt;path\u0026gt; ] [ sanitized ] Parameters Parameter Type/Value Description node integer Downloads a support package for the specified node path filepath Specifies the location where the support package should download sanitized Removes sensitive data (passwords, certificates, etc.) from the support package Returns Reports the progress of the support package download.\nExample $ rladmin cluster debug_info node 1 sanitized Preparing the debug info files package Downloading... [==================================================] Downloading complete. File /tmp/debuginfo.20220511-215637.node-1.tar.gz is saved. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/delete/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/delete/","title":"crdb-cli crdb delete","tags":[],"keywords":[],"description":"Deletes an Active-Active database.","content":"Deletes an Active-Active database.\ncrdb-cli crdb delete --crdb-guid \u0026lt;guid\u0026gt; [ --no-wait ] This command is irreversible. If the data in your database is important, back it up before you delete the database.\nParameters Parameter Value Description crdb-guid string The GUID of the database (required) no-wait Does not wait for the task to complete Returns Returns the task ID of the task that is deleting the database.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the database to be deleted and return finished.\nExample $ crdb-cli crdb delete --crdb-guid db6365b5-8aca-4055-95d8-7eb0105c0b35 Task dfe6cacc-88ff-4667-812e-938fd05fe359 created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/clusters/optimize/disk-sizing-heavy-write-scenarios/","uriRel":"/rs/clusters/optimize/disk-sizing-heavy-write-scenarios/","title":"Disk sizing for heavy write scenarios","tags":[],"keywords":[],"description":"Sizing considerations for persistent disk space for heavy throughput databases.","content":"In extreme write scenarios, when AOF is enabled, the AOF rewrite process may require considerably more disk space for database persistence.\nTo estimate the required persistent disk space in such cases, use the formula described below.\nThe required persistent disk space for AOF rewrite purposes in extreme write scenarios, assuming identical shard sizes:\nX (1 + 3Y +Y²) where: X = each shard size Y = number of shards\nFollowing are examples of database configurations and the persistence disk space they would require in this scenario:\nExample 1 Example 2 Example 3 Example 4 Database size (GB) 10 10 40 40 Number of shards 4 16 5 15 Shard size (GB) 2.5 0.625 8 2.67 Required disk space (GB) 73 191 328 723 For disk size requirements in standard usage scenarios, refer to the Hardware requirements section.\n","categories":["RS"]},{"uri":"/editing-guide/","uriRel":"/editing-guide/","title":"Editing Guide","tags":[],"keywords":[],"description":"How to edit the Redis documentation","content":"Redis documentation is an open source project and we welcome edits of all types.\nJust to get you started, here is a simple explanation of how to edit the docs.\nFind the source Find a page with something to edit.\nEdits can be anything from a simple typo or grammar error to adding additional information.\nOn that page, click Edit on GitHub.\nEnter your GitHub credentials.\nIf you don\u0026rsquo;t have an account, it is easy and free to open one.\nHere\u0026rsquo;s what this process looks like: Make the change Fork the repository.\nA fork is a copy of the entire contents of the documentation site that is stored in your account.\nIn the source file of the page, edit the text as you like.\nDon\u0026rsquo;t worry. This is your copy of the file, so you can\u0026rsquo;t do much damage.\nYou can use the markdown cheatsheet for help with more complex editing.\nHere\u0026rsquo;s what this process looks like: Propose the change When you finish with your changes, scroll down to the bottom of the page.\nEnter a meaningful name for the change that you made.\nThe default is \u0026ldquo;Update \u0026rdquo; but we like a little more detail than that if you can.\nClick Propose file change.\nHere\u0026rsquo;s what this process looks like: Open a pull request Review your changes.\nYou can see the before and after of the file. If you see a mistake or want to add to the change, just go back.\nClick Create pull request.\nA pull request is a recommendation that includes the changes that you made. We are notified of all request and can review or change the request before we include it in the documentation.\nEnter a meaningful name for the pull request.\nBy default the pull request has the same name as your file change, which is usually fine.\nClick Create pull request.\nNow your pull request is in our system and ready for us to handle.\nYou can add information in the comments box to help us understand why you think the change is important.\nMaybe we\u0026rsquo;ll even write back!\nHere\u0026rsquo;s what this process looks like: Your edits improve the quality of the documentation and help every Redis customer get the most out of Redis products!\nRelated Info For more info about editing and writing our documents:\nMarkdown cheatsheet Contribution guide ","categories":[]},{"uri":"/embeds/","uriRel":"/embeds/","title":"Embeds","tags":[],"keywords":[],"description":"","content":"","categories":[]},{"uri":"/rs/references/rest-api/requests/endpoints-stats/","uriRel":"/rs/references/rest-api/requests/endpoints-stats/","title":"Endpoints stats requests","tags":[],"keywords":[],"description":"Endpoint statistics requests","content":" Method Path Description GET /v1/endpoints/stats Get stats for all endpoints Get all endpoints stats GET /v1/endpoints/stats Get statistics for all endpoint-proxy links.\nNote: This method will return both endpoints and listeners stats for backwards compatability. Required permissions Permission name view_endpoint_stats Request Example HTTP request GET /endpoints/stats?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response The uid format in the response is: \u0026quot;BDB_UID:ENDPOINT_UID:PROXY_UID\u0026quot;\nFor example: {\u0026quot;uid\u0026quot;: \u0026quot;1:2:3\u0026quot;} means BDB_UID=1, ENDPOINT_UID=2, and PROXY_UID=3\nExample JSON body [ { \u0026#34;uid\u0026#34; : \u0026#34;365:1:1\u0026#34;, \u0026#34;intervals\u0026#34; : [ { \u0026#34;interval\u0026#34; : \u0026#34;10sec\u0026#34;, \u0026#34;total_req\u0026#34; : 0, \u0026#34;egress_bytes\u0026#34; : 0, \u0026#34;cmd_get\u0026#34; : 0, \u0026#34;monitor_sessions_count\u0026#34; : 0, \u0026#34;auth_errors\u0026#34; : 0, \u0026#34;acc_latency\u0026#34; : 0, \u0026#34;stime\u0026#34; : \u0026#34;2017-01-12T14:59:50Z\u0026#34;, \u0026#34;write_res\u0026#34; : 0, \u0026#34;total_connections_received\u0026#34; : 0, \u0026#34;cmd_set\u0026#34; : 0, \u0026#34;read_req\u0026#34; : 0, \u0026#34;max_connections_exceeded\u0026#34; : 0, \u0026#34;acc_write_latency\u0026#34; : 0, \u0026#34;write_req\u0026#34; : 0, \u0026#34;other_res\u0026#34; : 0, \u0026#34;cmd_flush\u0026#34; : 0, \u0026#34;acc_read_latency\u0026#34; : 0, \u0026#34;other_req\u0026#34; : 0, \u0026#34;conns\u0026#34; : 0, \u0026#34;write_started_res\u0026#34; : 0, \u0026#34;cmd_touch\u0026#34; : 0, \u0026#34;read_res\u0026#34; : 0, \u0026#34;auth_cmds\u0026#34; : 0, \u0026#34;etime\u0026#34; : \u0026#34;2017-01-12T15:00:00Z\u0026#34;, \u0026#34;total_started_res\u0026#34; : 0, \u0026#34;ingress_bytes\u0026#34; : 0, \u0026#34;last_res_time\u0026#34; : 0, \u0026#34;read_started_res\u0026#34; : 0, \u0026#34;acc_other_latency\u0026#34; : 0, \u0026#34;total_res\u0026#34; : 0, \u0026#34;last_req_time\u0026#34; : 0, \u0026#34;other_started_res\u0026#34; : 0 } ] }, { \u0026#34;intervals\u0026#34; : [ { \u0026#34;acc_read_latency\u0026#34; : 0, \u0026#34;other_req\u0026#34; : 0, \u0026#34;etime\u0026#34; : \u0026#34;2017-01-12T15:00:00Z\u0026#34;, \u0026#34;auth_cmds\u0026#34; : 0, \u0026#34;total_started_res\u0026#34; : 0, \u0026#34;write_started_res\u0026#34; : 0, \u0026#34;cmd_touch\u0026#34; : 0, \u0026#34;conns\u0026#34; : 0, \u0026#34;read_res\u0026#34; : 0, \u0026#34;total_res\u0026#34; : 0, \u0026#34;other_started_res\u0026#34; : 0, \u0026#34;last_req_time\u0026#34; : 0, \u0026#34;read_started_res\u0026#34; : 0, \u0026#34;last_res_time\u0026#34; : 0, \u0026#34;ingress_bytes\u0026#34; : 0, \u0026#34;acc_other_latency\u0026#34; : 0, \u0026#34;egress_bytes\u0026#34; : 0, \u0026#34;interval\u0026#34; : \u0026#34;10sec\u0026#34;, \u0026#34;total_req\u0026#34; : 0, \u0026#34;acc_latency\u0026#34; : 0, \u0026#34;auth_errors\u0026#34; : 0, \u0026#34;cmd_get\u0026#34; : 0, \u0026#34;monitor_sessions_count\u0026#34; : 0, \u0026#34;read_req\u0026#34; : 0, \u0026#34;max_connections_exceeded\u0026#34; : 0, \u0026#34;total_connections_received\u0026#34; : 0, \u0026#34;cmd_set\u0026#34; : 0, \u0026#34;acc_write_latency\u0026#34; : 0, \u0026#34;write_req\u0026#34; : 0, \u0026#34;stime\u0026#34; : \u0026#34;2017-01-12T14:59:50Z\u0026#34;, \u0026#34;write_res\u0026#34; : 0, \u0026#34;cmd_flush\u0026#34; : 0, \u0026#34;other_res\u0026#34; : 0 } ], \u0026#34;uid\u0026#34; : \u0026#34;333:1:2\u0026#34; } ] Status codes Code Description 200 OK No error ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/enslave/","uriRel":"/rs/references/cli-utilities/rladmin/node/enslave/","title":"rladmin node enslave","tags":[],"keywords":[],"description":"Changes a node&#39;s resources to replicas.","content":"Changes the resources of a node to replicas.\nnode enslave Changes all of the node\u0026rsquo;s endpoints and shards to replicas.\nrladmin node \u0026lt;ID\u0026gt; enslave [demote_node] [retry_timeout_seconds \u0026lt;seconds\u0026gt;] Parameters Parameter Type/Value Description node integer Changes all of the node\u0026rsquo;s endpoints and shards to replicas demote_node If the node is a primary node, changes the node to replica retry_timeout_seconds integer Retries on failure until the specified number of seconds has passed. Returns Returns OK if the roles were successfully changed. Otherwise, it returns an error.\nUse rladmin status shards to verify that the roles were changed.\nExample $ rladmin status shards node 2 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:14 node:2 master 0-4095 3.2MB OK db:6 tr02 redis:16 node:2 master 4096-8191 3.12MB OK db:6 tr02 redis:18 node:2 master 8192-12287 3.16MB OK db:6 tr02 redis:20 node:2 master 12288-16383 3.12MB OK $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 slave 192.0.2.12 198.51.100.1 3d99db1fdf4b 1/100 6 14.43GB/19.54GB 10.87GB/16.02GB 6.2.12-37 OK node:2 master 192.0.2.13 198.51.100.2 fc7a3d332458 4/100 6 14.43GB/19.54GB 10.88GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.14 b87cc06c830f 5/120 6 14.43GB/19.54GB 10.83GB/16.02GB 6.2.12-37 OK $ rladmin node 2 enslave demote_node Performing enslave_node action on node:2: 100% OK $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.12 198.51.100.1 3d99db1fdf4b 1/100 6 14.72GB/19.54GB 10.91GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.13 198.51.100.2 fc7a3d332458 4/100 6 14.72GB/19.54GB 11.17GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.14 b87cc06c830f 5/120 6 14.72GB/19.54GB 10.92GB/16.02GB 6.2.12-37 OK $ rladmin status shards node 2 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:14 node:2 slave 0-4095 2.99MB OK db:6 tr02 redis:16 node:2 slave 4096-8191 3.01MB OK db:6 tr02 redis:18 node:2 slave 8192-12287 2.93MB OK db:6 tr02 redis:20 node:2 slave 12288-16383 3.06MB OK node enslave endpoints_only Changes the role for all endpoints on a node to replica.\nrladmin node \u0026lt;ID\u0026gt; enslave endpoints_only [retry_timeout_seconds \u0026lt;seconds\u0026gt;] Parameters Parameter Type/Value Description node integer Changes all of the node\u0026rsquo;s endpoints to replicas retry_timeout_seconds integer Retries on failure until the specified number of seconds has passed. Returns Returns OK if the roles were successfully changed. Otherwise, it returns an error.\nUse rladmin status endpoints to verify that the roles were changed.\nExample $ rladmin status endpoints ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:5 tr01 endpoint:5:1 node:1 single No db:6 tr02 endpoint:6:1 node:3 all-master-shards No $ rladmin node 1 enslave endpoints_only Performing enslave_node action on node:1: 100% OK $ rladmin status endpoints ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:5 tr01 endpoint:5:1 node:3 single No db:6 tr02 endpoint:6:1 node:3 all-master-shards No node enslave shards_only Changes the role for all shards of a node to replica.\nrladmin node \u0026lt;ID\u0026gt; enslave shards_only [retry_timeout_seconds \u0026lt;seconds\u0026gt;] Parameters Parameter Type/Value Description node integer Changes all of the node\u0026rsquo;s shards to replicas retry_timeout_seconds integer Retries on failure until the specified number of seconds has passed. Returns Returns OK if the roles were successfully changed. Otherwise, it returns an error.\nUse rladmin status shards to verify that the roles were changed.\nExample $ rladmin status shards node 3 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:3 master 0-16383 3.04MB OK db:6 tr02 redis:15 node:3 master 0-4095 4.13MB OK db:6 tr02 redis:17 node:3 master 4096-8191 4.13MB OK db:6 tr02 redis:19 node:3 master 8192-12287 4.13MB OK db:6 tr02 redis:21 node:3 master 12288-16383 4.13MB OK $ rladmin node 3 enslave shards_only Performing enslave_node action on node:3: 100% OK $ rladmin status shards node 3 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:3 slave 0-16383 2.98MB OK db:6 tr02 redis:15 node:3 slave 0-4095 4.23MB OK db:6 tr02 redis:17 node:3 slave 4096-8191 4.11MB OK db:6 tr02 redis:19 node:3 slave 8192-12287 4.19MB OK db:6 tr02 redis:21 node:3 slave 12288-16383 4.27MB OK ","categories":["RS"]},{"uri":"/rs/clusters/optimize/optimization/","uriRel":"/rs/clusters/optimize/optimization/","title":"Cluster environment optimization","tags":[],"keywords":[],"description":"Optimize your cluster environments for better performance.","content":"Redis Enterprise Software employs various algorithms to optimize performance. As part of this process, Redis Enterprise Software examines usage characteristics and load to adjust its run-time configuration accordingly. Depending on your specific usage characteristics and load, it might take Redis Enterprise Software some time to adjust itself to optimal performance.\nTo ensure optimal performance, you must run your workload several times and for a long duration until performance stabilizes.\nIn addition, you can optimize your cluster for two different environments:\nLocal-network environment Cloud environment Depending on which configuration you choose, Redis Enterprise Software uses different thresholds to make operation related decisions.\nThe default configuration is for local-network environments. If you are running in a cloud environment, it is advisable that you change the configuration.\nHow to change the cluster environment configuration In the rladmin command-line interface (CLI), run the following command:\nrladmin tune cluster watchdog_profile [cloud | local-network] If after following all of the instructions above, you find that RS still does not meet your performance expectations, contact us at support@redislabs.com to help you optimize RS to your specific needs.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/export/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/export/","title":"Export database action requests","tags":[],"keywords":[],"description":"Export database requests","content":" Method Path Description POST /v1/bdbs/{uid}/actions/export Initiate database export Initiate database export POST /v1/bdbs/{int: uid}/actions/export Initiate a database export.\nPermissions Permission name Roles start_bdb_export admin\ncluster_member\ndb_member Request Example HTTP request POST /bdbs/1/actions/export Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Body The request body should contain a JSON object with the following export parameters:\nField Type Description export_location backup_location/export_location object Details for the export destination. Call GET /jsonschema on the bdb object and review the backup_location field to retrieve the object\u0026rsquo;s structure. email_notification boolean Enable/disable an email notification on export failure/ completion. (optional) Example JSON body { \u0026#34;export_location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;ftp://...\u0026#34; }, \u0026#34;email_notification\u0026#34;: true } The above request initiates an export operation to the specified location.\nResponse Returns a status code.\nStatus codes Code Description 200 OK The request is accepted and is being processed. In order to monitor progress, the BDB\u0026rsquo;s export_status, export_progress, and export_failure_reason attributes can be consulted. 404 Not Found Attempting to perform an action on a nonexistent database. 406 Not Acceptable Not all the modules loaded to the database support \u0026lsquo;backup_restore\u0026rsquo; capability 409 Conflict Database is currently busy with another action. In this context, this is a temporary condition and the request should be reattempted later. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/export_reset_status/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/export_reset_status/","title":"Export resets status database action requests","tags":[],"keywords":[],"description":"Reset database export status requests","content":" Method Path Description PUT /v1/bdbs/{uid}/actions/export_reset_status Reset database export status Reset database export status PUT /v1/bdbs/{int: uid}/actions/export_reset_status Resets the database\u0026rsquo;s export_status to idle if an export is not in progress and clears the value of the export_failure_reason field.\nPermissions Permission name Roles reset_bdb_current_export_status admin\ncluster_member\ndb_member Request Example HTTP request PUT /bdbs/1/actions/export_reset_status Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Response Returns a status code.\nStatus codes Code Description 200 OK The request is accepted and is being processed. 404 Not Found Attempting to perform an action on a nonexistent database. 406 Not Acceptable Not all the modules loaded to the database support \u0026lsquo;backup_restore\u0026rsquo; capability 409 Conflict Database is currently busy with another action. In this context, this is a temporary condition and the request should be reattempted later. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/external-addr/","uriRel":"/rs/references/cli-utilities/rladmin/node/external-addr/","title":"rladmin node external_addr","tags":[],"keywords":[],"description":"Configures a node&#39;s external IP addresses.","content":"Configures a node\u0026rsquo;s external IP addresses.\nnode external_addr add Adds an external IP address that accepts inbound user connections for the node.\nrladmin node \u0026lt;ID\u0026gt; external_addr add \u0026lt;IP address\u0026gt; Parameters Parameter Type/Value Description node integer Adds an external IP address for the specified node IP address IP address External IP address of the node Returns Returns Updated successfully if the IP address was added. Otherwise, it returns an error.\nUse rladmin status nodes to verify the external IP address was added.\nExample $ rladmin node 1 external_addr add 198.51.100.1 Updated successfully. $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 198.51.100.1 3d99db1fdf4b 5/100 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.3 fc7a3d332458 0/100 6 14.75GB/19.54GB 11.24GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK node external_addr set Sets one or more external IP addresses that accepts inbound user connections for the node.\nrladmin node \u0026lt;ID\u0026gt; external_addr set \u0026lt;IP address 1\u0026gt; ... \u0026lt;IP address N\u0026gt; Parameters Parameter Type/Value Description node integer Sets external IP addresses for the specified node IP address list of IP addresses Sets specified IP addresses as external addresses Returns Returns Updated successfully if the IP addresses were set. Otherwise, it returns an error.\nUse rladmin status nodes to verify the external IP address was set.\nExample $ rladmin node 2 external_addr set 198.51.100.2 198.51.100.3 Updated successfully. $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 198.51.100.1 3d99db1fdf4b 5/100 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.3 198.51.100.2,198.51.100.3 fc7a3d332458 0/100 6 14.75GB/19.54GB 11.23GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK node external_addr remove Removes the specified external IP address from the node.\nrladmin node \u0026lt;ID\u0026gt; external_addr remove \u0026lt;IP address\u0026gt; Parameters Parameter Type/Value Description node integer Removes an external IP address for the specified node IP address IP address Removes the specified IP address of the node Returns Returns Updated successfully if the IP address was removed. Otherwise, it returns an error.\nUse rladmin status nodes to verify the external IP address was removed.\nExample $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 198.51.100.1 3d99db1fdf4b 5/100 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.3 198.51.100.2,198.51.100.3 fc7a3d332458 0/100 6 14.75GB/19.54GB 11.23GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 14.75GB/19.54GB 11.15GB/16.02GB 6.2.12-37 OK $ rladmin node 2 external_addr remove 198.51.100.3 Updated successfully. $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.2 198.51.100.1 3d99db1fdf4b 5/100 6 14.74GB/19.54GB 11.14GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.3 198.51.100.2 fc7a3d332458 0/100 6 14.74GB/19.54GB 11.22GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.4 b87cc06c830f 5/120 6 14.74GB/19.54GB 11.14GB/16.02GB 6.2.12-37 OK ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/failover/","uriRel":"/rs/references/cli-utilities/rladmin/failover/","title":"rladmin failover","tags":[],"keywords":[],"description":"Fail over primary shards of a database to their replicas.","content":"Fails over one or more primary (also known as master) shards of a database and promotes their respective replicas to primary shards.\nrladmin failover [db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; }] shard \u0026lt;id1 ... idN\u0026gt; [immediate] Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Fail over shards for the specified database shard one or more primary shard IDs Primary shard or shards to fail over immediate Perform failover without verifying the replica shards are in full sync with the master shards Returns Returns Finished successfully if the failover completed. Otherwise, it returns an error.\nUse rladmin status shards to verify that the failover completed.\nExample $ rladmin status shards SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:1 slave 0-16383 3.02MB OK db:5 tr01 redis:13 node:2 master 0-16383 3.09MB OK $ rladmin failover shard 13 Executing shard fail-over: OOO. Finished successfully $ rladmin status shards SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:1 master 0-16383 3.12MB OK db:5 tr01 redis:13 node:2 slave 0-16383 2.99MB OK ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/flush/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/flush/","title":"crdb-cli crdb flush","tags":[],"keywords":[],"description":"Clears all keys from an Active-Active database.","content":"Clears all keys from an Active-Active database.\ncrdb-cli crdb flush --crdb-guid \u0026lt;guid\u0026gt; [ --no-wait ] This command is irreversible. If the data in your database is important, back it up before you flush the database.\nParameters Parameter Value Description crdb-guid string The GUID of the database (required) no-wait Does not wait for the task to complete Returns Returns the task ID of the task clearing the database.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the database to be cleared and return finished.\nExample $ crdb-cli crdb flush --crdb-guid d84f6fe4-5bb7-49d2-a188-8900e09c6f66 Task 53cdc59e-ecf5-4564-a8dd-448d71f9e568 created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdbs/flush/","uriRel":"/rs/references/rest-api/requests/crdbs/flush/","title":"CRDB flush requests","tags":[],"keywords":[],"description":"Flush Active-Active database requests","content":" Method Path Description PUT /v1/crdbs/{crdb_guid}/flush Flush an Active-Active database Flush an Active-Active database PUT /v1/crdbs/{crdb_guid}/flush Flush an Active-Active database.\nRequest Example HTTP request PUT /crdbs/552bbccb-99f3-4142-bd17-93d245f0bc79/flush Headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Response Returns a CRDB task object.\nStatus codes Code Description 200 OK Action was successful. 400 Bad Request The request is invalid or malformed. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Configuration or Active-Active database not found. 406 Not Acceptable Configuration cannot be accepted, typically because it was already committed. ","categories":["RS"]},{"uri":"/rs/clusters/optimize/turn-off-services/","uriRel":"/rs/clusters/optimize/turn-off-services/","title":"Turn off services to free system memory","tags":[],"keywords":[],"description":"Turn off services to free memory and improve performance.","content":"The Redis Enterprise Software cluster nodes host a range of services that support the cluster processes. In most deployments, either all of these services are required, or there are enough memory resources on the nodes for the database requirements.\nIn a deployment with limited memory resources, certain services can be disabled from API endpoint to free system memory or using the rladmin command. Before you turn off a service, make sure that your deployment does not depend on that service. After you turn off a service, you can re-enable in the same way.\nThe services that you can turn off are:\nRS Admin Console - cm_server Logs in CSV format - stats_archiver LDAP authentication - saslauthd Discovery service- mdns_server, pdns_server Active-Active databases - crdb_coordinator, crdb_worker Alert Manager - alert_mgr (For best results, disable only if you have an alternate alert system.) To turn off a service with the rladmin cluster config command, use the services parameter and the name of the service, followed by disabled.\nrladmin cluster config [ services \u0026lt;service_name\u0026gt; \u0026lt;enabled | disabled\u0026gt; ] To turn off a service with the API, use the PUT /v1/services_configuration endpoint with the name of the service and the operating mode (enabled/disabled) in JSON format.\nFor example:\nTo turn off the Redis Enterprise admin console, use this PUT request:\nPUT https://[host][:9443]/v1/cluster/services_configuration \u0026#39;{ \u0026#34;cm_server\u0026#34;:{ \u0026#34;operating_mode\u0026#34;:\u0026#34;disabled\u0026#34; } }\u0026#39; To turn off the CRDB services and enable the stats_archiver for cluster component statistics, use this PUT request:\nPUT https://[host][:9443]/v1/cluster/services_configuration \u0026#39;{ \u0026#34;crdb_coordinator\u0026#34;:{ \u0026#34;operating_mode\u0026#34;:\u0026#34;disabled\u0026#34; }, \u0026#34;crdb_worker\u0026#34;:{ \u0026#34;operating_mode\u0026#34;:\u0026#34;disabled\u0026#34; }, \u0026#34;stats_archiver\u0026#34;:{ \u0026#34;operating_mode\u0026#34;:\u0026#34;enabled\u0026#34; } }\u0026#39; ","categories":["RS"]},{"uri":"/rc/cloud-integrations/gcp-marketplace/","uriRel":"/rc/cloud-integrations/gcp-marketplace/","title":"Flexible subscriptions with GCP Marketplace","tags":[],"keywords":[],"description":"Shows how to subscribe to Redis Cloud using GCP Marketplace","content":"You can use Google Cloud Platform (GCP) Marketplace to subscribe to Redis Enterprise Cloud. This lets you provision according to your needs and pay using your GCP account.\nHere\u0026rsquo;s how to create a new Flexible subscription as part of your GCP Marketplace commitment:\nSign in to the GCP console.\nSearch GCP Marketplace for Redis Enterprise Cloud Flexible - Pay as You Go.\nSelect the Subscribe button. This redirects you to the subscription details page.\nReview the subscription details, accept the terms, and select Subscribe.\nWhen you subscribe for the first time, select Register with Redis. This will redirect you to the Redis Cloud admin console.\nCreate a Redis Cloud admin account or sign in to an existing account.\nUse the GCP Marketplace dialog to select the Redis account you want to map to your GCP Marketplace account. You only need to do this once.\nSelect Map account to confirm your choice.\nAfter you map your Redis account to your GCP Marketplace account, a message appears in the upper left corner of the account panel.\nOn the GCP Marketplace listing, select Manage on provider to go to the Redis Cloud admin console.\nAt this point, you can create a new Flexible subscription using the standard workflow, with one important change. You don\u0026rsquo;t need to enter a payment method, as it automatically uses your GCP Marketplace account.\nTo confirm this, review the payment method associated with your subscription.\nIf your GCP Marketplace account is deactivated or otherwise unavailable, you can\u0026rsquo;t use your Flexible subscription until you update the billing method. For help, contact support.\n","categories":["RC"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/get/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/get/","title":"crdb-cli crdb get","tags":[],"keywords":[],"description":"Shows the current configuration of an Active-Active database.","content":"Shows the current configuration of an Active-Active database.\ncrdb-cli crdb get --crdb-guid \u0026lt;guid\u0026gt; Parameters Parameter Value Description crdb-guid string The GUID of the database (required) Returns Returns the current configuration of the database.\nExample $ crdb-cli crdb get --crdb-guid d84f6fe4-5bb7-49d2-a188-8900e09c6f66 CRDB-GUID: d84f6fe4-5bb7-49d2-a188-8900e09c6f66 Name: database1 Encryption: False Causal consistency: False Protocol version: 1 FeatureSet version: 5 Modules: [] Default-DB-Config: memory_size: 1073741824 port: 12000 replication: True shard_key_regex: [{\u0026#39;regex\u0026#39;: \u0026#39;.*\\\\{(?\u0026lt;tag\u0026gt;.*)\\\\}.*\u0026#39;}, {\u0026#39;regex\u0026#39;: \u0026#39;(?\u0026lt;tag\u0026gt;.*)\u0026#39;}] sharding: True shards_count: 1 tls_mode: disabled rack_aware: None data_persistence: None authentication_redis_pass: None authentication_admin_pass: None oss_sharding: None oss_cluster: None proxy_policy: None shards_placement: None oss_cluster_api_preferred_ip_type: None bigstore: None bigstore_ram_size: None aof_policy: None snapshot_policy: None max_aof_load_time: None max_aof_file_size: None Instance: Id: 1 Cluster: FQDN: cluster1.redis.local URL: https://cluster1.redis.local:9443 Replication-Endpoint: \u0026lt;Default\u0026gt; Replication TLS SNI: \u0026lt;Default\u0026gt; Compression: 3 DB-Config: authentication_admin_pass: replication: None rack_aware: None memory_size: None data_persistence: None tls_mode: None authentication_redis_pass: None port: None shards_count: None shard_key_regex: None oss_sharding: None oss_cluster: None proxy_policy: None shards_placement: None oss_cluster_api_preferred_ip_type: None bigstore: None bigstore_ram_size: None aof_policy: None snapshot_policy: None max_aof_load_time: None max_aof_file_size: None Instance: Id: 2 Cluster: FQDN: cluster2.redis.local URL: https://cluster2.redis.local:9443 Replication-Endpoint: \u0026lt;Default\u0026gt; Replication TLS SNI: \u0026lt;Default\u0026gt; Compression: 3 DB-Config: authentication_admin_pass: replication: None rack_aware: None memory_size: None data_persistence: None tls_mode: None authentication_redis_pass: None port: None shards_count: None shard_key_regex: None oss_sharding: None oss_cluster: None proxy_policy: None shards_placement: None oss_cluster_api_preferred_ip_type: None bigstore: None bigstore_ram_size: None aof_policy: None snapshot_policy: None max_aof_load_time: None max_aof_file_size: None ","categories":["RS"]},{"uri":"/rs/installing-upgrading/hardware-requirements/","uriRel":"/rs/installing-upgrading/hardware-requirements/","title":"Hardware requirements","tags":[],"keywords":[],"description":"","content":" The hardware requirements for Redis Enterprise Software are different for development and production environments.\nIn a development environment, you can test your application with a live database.\nIf you want to test your application under production conditions, use the production environment requirements.\nIn a production environment you must have enough resources to handle the load on the database and recover from failures.\nDevelopment environment You can build your development environment with non-production hardware, such as a laptop, desktop, or small VM or instance, and with these hardware requirements:\nItem Description Minimum Requirements Recommended Nodes per cluster You can install on one node but many features require at least two nodes. 1 node \u0026gt;= 2 nodes RAM per node The amount of RAM for each node. 4GB \u0026gt;= 10GB Storage per node The amount of storage space for each node. 10GB \u0026gt;= 20GB Production environment We recommend these hardware requirements for production systems or for development systems that are designed to demonstrate production use cases:\nItem Description Minimum Requirements Recommended Nodes per cluster* At least three nodes are required to support a reliable, highly available deployment that handles process failure, node failure, and network split events in a consistent manner. 3 nodes \u0026gt;= 3 nodes (Must be an odd number of nodes) Cores* per node Redis Enterprise Software is based on a multi-tenant architecture and can run multiple Redis processes (or shards) on the same core without significant performance degradation. 4 cores \u0026gt;=8 cores RAM* per node Defining your RAM size must be part of the capacity planning for your Redis usage. 15GB \u0026gt;=30GB Ephemeral Storage Used for storing replication files (RDB format) and cluster log files. RAM x 2 \u0026gt;= RAM x 4 Persistent Storage Used for storing snapshot (RDB format) and AOF files over a persistent storage media, such as AWS Elastic Block Storage (EBS) or Azure Data Disk. RAM x 3 In-memory \u0026gt;= RAM x 6 (except for extreme \u0026lsquo;write\u0026rsquo; scenarios); Redis on Flash \u0026gt;= (RAM + Flash) x 5. Network We recommend using multiple NICs per node where each NIC is \u0026gt;100Mbps, but Redis Enterprise Software can also run over a single 1Gbps interface network used for processing application requests, inter-cluster communication, and storage access. 1G \u0026gt;=10G *Additional considerations:\nNodes per Cluster:\nTo ensure synchronization and consistency, Active-Active deployments with three node clusters are strongly discouraged from using quorum nodes. Because quorum nodes do not store data shards, they cannot support replication. In case of a node failure, replica shards aren\u0026rsquo;t available for Active-Active synchronization. Cores:\nWhen the CPU load reaches a certain level, Redis Enterprise Software sends an alert to the operator.\nIf your application is designed to put a lot of load on your Redis database, make sure that you have at least one available core for each shard of your database.\nIf some of the cluster nodes are utilizing more than 80% of the CPU, consider migrating busy resources to less busy nodes.\nIf all the cluster nodes are utilizing over 80% of the CPU, consider scaling out the cluster by adding a node.\nRAM:\nRedis uses a relatively large number of buffers, which enable replica communication, client communication, pub/sub commands, and more. As a result, you should ensure that 30% of the RAM is available on each node at any given time.\nIf one or more cluster nodes utilizes more than 65% of the RAM, consider migrating resources to less active nodes.\nIf all cluster nodes are utilizing more than 70% of available RAM, consider adding a node.\nDo not run any other memory-intensive processes on the Redis Software node.\n","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/hashes/","uriRel":"/rs/databases/active-active/develop/data-types/hashes/","title":"Hashes in an Active-Active databases","tags":[],"keywords":[],"description":"Information about using hashes with an Active-Active database.","content":"Hashes are great for structured data that contain a map of fields and values. They are used for managing distributed user or app session state, user preferences, form data and so on. Hash fields contain string type and string types operate just like the standard Redis string types when it comes to CRDTs. Fields in hashes can be initialized as a string using HSET or HMSET or can be used to initialize counter types that are numeric integers using HINCRBY or floats using HINCRBYFLOAT.\nHashes in Active-Active databases behave the same and maintain additional metadata to achieve an \u0026ldquo;OR-Set\u0026rdquo; behavior to handle concurrent conflicting writes. With the OR-Set behavior, writes to add new fields across multiple Active-Active database instances are typically unioned except in cases of conflicts. Conflicting instance writes can happen when an Active-Active database instance deletes a field while the other adds the same field. In this case and observed remove rule is followed. That is, remove can only remove fields it has already seen and in all other cases element add/update wins.\nField values behave just like CRDT strings. String values can be types string, counter integer based on the command used for initialization of the field value. See \u0026ldquo;String Data Type in Active-Active databases\u0026rdquo; and \u0026ldquo;String Data Type with Counter Value in Active-Active databases\u0026rdquo; for more details.\nHere is an example of an \u0026ldquo;add wins\u0026rdquo; case:\nTime CRDB Instance1 CRDB Instance2 t1 HSET key1 field1 “a” t2 HSET key1 field2 “b” t4 - Sync - - Sync - t5 HGETALL key11) “field2”2) “b”3) “field1”4) “a” HGETALL key11) “field2”2) “b”3) “field1”4) “a” ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/health_report/","uriRel":"/rs/references/rest-api/objects/crdb/health_report/","title":"CRDB health report object","tags":[],"keywords":[],"description":"An object that represents an Active-Active database health report.","content":"An object that represents an Active-Active database health report.\nName Type/Value Description active_config_version integer Active configuration version cluster_name string Name of local Active-Active cluster configurations array of health_report_configuration objects Stored database configurations connection_error string Error string if remote cluster is not available connections [{ \"name\": string, \"status\": string }, ...] Connections to other clusters and their statuses name string Name of the Active-Active database ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdbs/health_report/","uriRel":"/rs/references/rest-api/requests/crdbs/health_report/","title":"CRDB health report requests","tags":[],"keywords":[],"description":"Active-Active database health report requests","content":" Method Path Description GET /v1/crdbs/{crdb_guid}/health_report Get a health report for an Active-Active database Get health report GET /v1/crdbs/{crdb_guid}/health_report Get a health report for an Active-Active database.\nRequest Example HTTP request GET /crdbs/{crdb_guid}/health_report URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Query parameters Field Type Description instance_id integer The request retrieves information from the specified Active-Active database instance. If this instance doesn’t exist, the request retrieves information from all instances. (optional) Response Returns a JSON array of CRDB health report objects.\nStatus codes Code Description 200 OK Action was successful. 404 Not Found Configuration or Active-Active database not found. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/health_report/health_report_configuration/","uriRel":"/rs/references/rest-api/objects/crdb/health_report/health_report_configuration/","title":"CRDB health report configuration object","tags":[],"keywords":[],"description":"An object that represents the database configuration to include in an Active-Active database health report.","content":"An object that represents the database configuration to include in an Active-Active database health report.\nName Type/Value Description causal_consistency boolean Enables causal consistency across Active-Active replicas encryption boolean Intercluster encryption featureset_version integer CRDB active FeatureSet version instances [{ \"id\": integer, // Unique instance ID \"db_uid\": string, // Local database instance ID \"cluster\": { \"name\": string // Cluster FQDN \"url\": string // Cluster access URL } }, ...] Local database instances name string Name of database protocol_version integer CRDB active protocol version status string Current status of the configuration.Possible values:posted: Configuration was posted to all replicasready: All replicas have finished processing posted configuration (create a database)committed: Posted configuration is now active on all replicascommit-completed: All replicas have finished processing committed configuration (database is active)failed: Configuration failed to post version integer Database configuration version ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/health-report/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/health-report/","title":"crdb-cli crdb health-report","tags":[],"keywords":[],"description":"Shows the health report of an Active-Active database.","content":"Shows the health report of the API management layer of an Active-Active database.\ncrdb-cli crdb health-report --crdb-guid \u0026lt;guid\u0026gt; Parameters Parameter Value Description crdb-guid string The GUID of the database (required) Returns Returns the health report of the API management layer of the database.\nExample $ crdb-cli crdb health-report --crdb-guid d84f6fe4-5bb7-49d2-a188-8900e09c6f66 [ { \u0026#34;active_config_version\u0026#34;:1, \u0026#34;cluster_name\u0026#34;:\u0026#34;cluster2.redis.local\u0026#34;, \u0026#34;configurations\u0026#34;:[ { \u0026#34;causal_consistency\u0026#34;:false, \u0026#34;encryption\u0026#34;:false, \u0026#34;featureset_version\u0026#34;:5, \u0026#34;instances\u0026#34;:[ { \u0026#34;cluster\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;cluster1.redis.local\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/cluster1.redis.local:9443\u0026#34; }, \u0026#34;db_uid\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;id\u0026#34;:1 }, { \u0026#34;cluster\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;cluster2.redis.local\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/cluster2.redis.local:9443\u0026#34; }, \u0026#34;db_uid\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;id\u0026#34;:2 } ], \u0026#34;name\u0026#34;:\u0026#34;database1\u0026#34;, \u0026#34;protocol_version\u0026#34;:1, \u0026#34;status\u0026#34;:\u0026#34;commit-completed\u0026#34;, \u0026#34;version\u0026#34;:1 } ], \u0026#34;connections\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;cluster1.redis.local\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34; }, { \u0026#34;name\u0026#34;:\u0026#34;cluster2.redis.local\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34; } ], \u0026#34;guid\u0026#34;:\u0026#34;d84f6fe4-5bb7-49d2-a188-8900e09c6f66\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;database1\u0026#34;, \u0026#34;connection_error\u0026#34;:null }, { \u0026#34;active_config_version\u0026#34;:1, \u0026#34;cluster_name\u0026#34;:\u0026#34;cluster1.redis.local\u0026#34;, \u0026#34;configurations\u0026#34;:[ { \u0026#34;causal_consistency\u0026#34;:false, \u0026#34;encryption\u0026#34;:false, \u0026#34;featureset_version\u0026#34;:5, \u0026#34;instances\u0026#34;:[ { \u0026#34;cluster\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;cluster1.redis.local\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/cluster1.redis.local:9443\u0026#34; }, \u0026#34;db_uid\u0026#34;:\u0026#34;4\u0026#34;, \u0026#34;id\u0026#34;:1 }, { \u0026#34;cluster\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;cluster2.redis.local\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/cluster2.redis.local:9443\u0026#34; }, \u0026#34;db_uid\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;id\u0026#34;:2 } ], \u0026#34;name\u0026#34;:\u0026#34;database1\u0026#34;, \u0026#34;protocol_version\u0026#34;:1, \u0026#34;status\u0026#34;:\u0026#34;commit-completed\u0026#34;, \u0026#34;version\u0026#34;:1 } ], \u0026#34;connections\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;cluster1.redis.local\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34; }, { \u0026#34;name\u0026#34;:\u0026#34;cluster2.redis.local\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34; } ], \u0026#34;guid\u0026#34;:\u0026#34;d84f6fe4-5bb7-49d2-a188-8900e09c6f66\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;database1\u0026#34;, \u0026#34;connection_error\u0026#34;:null } ] ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/help/","uriRel":"/rs/references/cli-utilities/rladmin/help/","title":"rladmin help","tags":[],"keywords":[],"description":"Shows available commands or specific command usage.","content":"Lists all options and parameters for rladmin commands.\nrladmin help [command] Parameters Parameter Description command Display help for this rladmin command (optional) Returns Returns a list of available rladmin commands.\nIf a command is specified, returns a list of all the options and parameters for that rladmin command.\nExample $ rladmin help usage: rladmin [options] [command] [command args] Options: -y Assume Yes for all required user confirmations. Commands: bind Bind an endpoint cluster Cluster management commands exit Exit admin shell failover Fail-over master to slave help Show available commands, or use help \u0026lt;command\u0026gt; for a specific command info Show information about tunable parameters migrate Migrate elements between nodes node Node management commands placement Configure shards placement policy recover Recover databases restart Restart database shards status Show status information suffix Suffix management tune Tune system parameters upgrade Upgrade entity version verify Cluster verification reports Use \u0026#34;rladmin help [command]\u0026#34; to get more information on a specific command. ","categories":["RS"]},{"uri":"/rc/databases/configuration/high-availability/","uriRel":"/rc/databases/configuration/high-availability/","title":"High availability and replication","tags":[],"keywords":[],"description":"Describes database replication and high availability as it affects Redis Enterprise Cloud.","content":"Database replication helps ensure high availability.\nWhen replication is enabled, your dataset is duplicated to create a replica that is synchronized with the primary dataset.\nReplication allows for automatic failover and greater fault tolerance. It can prevent data loss in the event of a hardware or zone failure.\nOptions and plan support Redis Enterprise Cloud supports three levels of replication:\nNo replication means that you will have a single copy of your database.\nSingle-zone replication means that your database will have a primary and a replica located in the same cloud zone. If anything happens to the primary, the replica takes over and becomes the new primary.\nMulti-zone replication means that the primary and its replicas are stored in different zones. This means that your database can remain online even if an entire zone becomes unavailable.\nYour replication options depend on your subscription plan:\nFree plans do not support replication. Fixed plans let you choose between no replication, single-zone replication, or multi-zone replication when creating a subscription. Flexible and Annual plans allow multi-zone or single-zone subscriptions by default. You can also disable replication. Performance impact Replication can affect performance as traffic increases to synchronize all copies.\nDatabase storage costs also increase:\nFor Fixed plans, single-zone and multi-zone replication effectively doubles storage costs\nFor Flexible and Annual plans, replication requires additional shards and can affect subscription costs\nZone setting maintenance Zone settings can only be defined when a subscription is created. You cannot change these settings once the subscription becomes active.\nThis means you can\u0026rsquo;t convert a multi-zone subscription to a single zone (or vice-versa).\nTo use different zone settings, create a new subscription with the preferred settings and then migrate data from the original subscription.\nMore info To learn more about high availability and replication, see:\nHighly Available Redis Database replication ","categories":["RC"]},{"uri":"/rs/databases/active-active/develop/data-types/hyperloglog/","uriRel":"/rs/databases/active-active/develop/data-types/hyperloglog/","title":"HyperLogLog in Active-Active databases","tags":[],"keywords":[],"description":"Information about using hyperloglog with an Active-Active database.","content":"HyperLogLog is an algorithm that addresses the count-distinct problem. To do this it approximates the numbers of items in a set. Determining the exact cardinality of a set requires memory according to the cardinality of the set. Because it estimates the cardinality by probability, the HyperLogLog algorithm can run with more reasonable memory requirements.\nHyperLogLog in Redis Open source Redis implements HyperLogLog (HLL) as a native data-structure. It supports adding elements (PFADD) to an HLL, counting elements (PFCOUNT) of HLLs, and merging of (PFMERGE) HLLs.\nHere is an example of a simple write case:\nTime Replica 1 Replica 2 t1 PFADD hll x t2 \u0026mdash; sync \u0026mdash; t3 PFADD hll y t4 \u0026mdash; sync \u0026mdash; t5 PFCOUNT hll \u0026ndash;\u0026gt; 2 PFCOUNT hll \u0026ndash;\u0026gt; 2 Here is an example of a concurrent add case:\nTime Replica 1 Replica 2 t1 PFADD hll x PFADD hll y t2 PFCOUNT hll \u0026ndash;\u0026gt; 1 PFCOUNT hll \u0026ndash;\u0026gt; 1 t3 \u0026mdash; sync \u0026mdash; t4 PFCOUNT hll \u0026ndash;\u0026gt; 2 PFCOUNT hll \u0026ndash;\u0026gt; 2 The DEL-wins approach Other collections in the Redis-CRDT implementation use the observed remove method to resolve conflicts. The CRDT-HLL uses the DEL-wins method. If a DEL request is received at the same time as any other request (ADD/MERGE/EXPIRE) on the HLL-key the replicas consistently converge to delete key. In the observed remove method used by other collections (sets, lists, sorted-sets and hashes), only the replica that received the DEL request removes the elements, but elements added concurrently in other replicas exist in the consistently converged collection. We chose to use the DEL-wins method for the CRDT-HLL to maintain the original time and space complexity of the HLL in open source Redis.\nHere is an example of a DEL-wins case:\nHLL | Set | Time Replica 1 Replica 2 | Time Replica 1 Replica 2 | t1 PFADD h e1 | t1 SADD s e1 t2 \u0026mdash; sync \u0026mdash; | t2 \u0026mdash; sync \u0026mdash; t3 PFCOUNT h \u0026ndash;\u0026gt; 1 PFCOUNT h \u0026ndash;\u0026gt; 1 | t3 SCARD s \u0026ndash;\u0026gt; 1 SCARD s \u0026ndash;\u0026gt; 1 t4 PFADD h e2 Del h | t4 SADD s e2 Del S t5 PFCOUNT h \u0026ndash;\u0026gt; 2 PFCOUNT h \u0026ndash;\u0026gt; 0 | t5 SCARD s \u0026ndash;\u0026gt; 2 SCARD s \u0026ndash;\u0026gt; 0 t6 \u0026mdash; sync \u0026mdash; | t6 \u0026mdash; sync \u0026mdash; t7 PFCOUNT h \u0026ndash;\u0026gt; 0 PFCOUNT h \u0026ndash;\u0026gt; 0 | t7 SCARD s \u0026ndash;\u0026gt; 1 SCARD s \u0026ndash;\u0026gt; 1 t8 Exists h \u0026ndash;\u0026gt; 0 Exists h \u0026ndash;\u0026gt; 0 | t8 Exists s \u0026ndash;\u0026gt; 1 Exists s \u0026ndash;\u0026gt; 1 | t9 SMEMBERS s \u0026ndash;\u0026gt; {e2} SMEMBERS s \u0026ndash;\u0026gt; {e2} HLL in Active-Active databases versus HLL in Open Source Redis In Active-Active databases, we implemented HLL within the CRDT on the basis of the Redis implementation with a few exceptions:\nRedis keeps the HLL data structure as an encoded string object such that you can potentially run any string request can on a key that contains an HLL. In CRDT, only get and set are supported for HLL. In CRDT, if you do SET on a key that contains a value encoded as an HLL, then the value will remain an HLL. If the value is not encoded as HLL, then it will be a register. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/identity/","uriRel":"/rs/references/rest-api/objects/bootstrap/identity/","title":"Identity object","tags":[],"keywords":[],"description":"Documents the identity object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description uid integer Assumed node\u0026rsquo;s UID to join cluster. Used to replace a dead node with a new one. accept_servers boolean (default: true) If true, no shards will be created on the node addr string Internal IP address of node external_addr complex object External IP addresses of node. GET /jsonschema to retrieve the object\u0026rsquo;s structure. name string Node\u0026rsquo;s name override_rack_id boolean When replacing an existing node in a rack-aware cluster, allows the new node to be located in a different rack rack_id string Rack ID, overrides cloud config ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/import/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/import/","title":"Import database action requests","tags":[],"keywords":[],"description":"Import database requests","content":" Method Path Description POST /v1/bdbs/{uid}/actions/import Initiate manual dataset import Initiate manual dataset import POST /v1/bdbs/{int: uid}/actions/import Initiate a manual import process.\nPermissions Permission name Roles start_bdb_import admin\ncluster_member\ndb_member Request Example HTTP request POST /bdbs/1/actions/import Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Content-Length 0 Length of the request body in octets URL parameters Field Type Description uid integer The unique ID of the database Body The request may contain a subset of the BDB JSON object, which includes the following import-related attributes:\nField Type Description dataset_import_sources array of dataset_import_sources objects Details for the import sources. Call GET /jsonschema on the bdb object and review the dataset_import_sources field to retrieve the object\u0026rsquo;s structure. email_notification boolean Enable/disable an email notification on import failure/ completion. (optional) Note: Other attributes are not allowed and will cause the request to fail. Example JSON Body { \u0026#34;dataset_import_sources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://...\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;redis://...\u0026#34; } ], \u0026#34;email_notification\u0026#34;: true } This request initiates an import process using dataset_import_sources values that were previously configured for the database.\nResponse Returns a status code.\nStatus codes Code Description 200 OK The request is accepted and is being processed. In order to monitor progress, the import_status, import_progress, and import_failure_reason attributes can be consulted. 404 Not Found Attempting to perform an action on a nonexistent database. 406 Not Acceptable Not all the modules loaded to the database support \u0026lsquo;backup_restore\u0026rsquo; capability. 409 Conflict Database is currently busy with another action. In this context, this is a temporary condition and the request should be reattempted later. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/import_reset_status/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/import_reset_status/","title":"Import reset status database action requests","tags":[],"keywords":[],"description":"Reset database import status requests","content":" Method Path Description PUT /v1/bdbs/{uid}/actions/import_reset_status Reset database import status Reset database import status PUT /v1/bdbs/{int: uid}/actions/import_reset_status Reset the database’s import_status to idle if a backup is not in progress and clears the value of the import_failure_reason field.\nPermissions Permission name Roles reset_bdb_current_import_status admin\ncluster_member\ndb_member Request Example HTTP request PUT /bdbs/1/actions/import_reset_status Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Response Returns a status code.\nStatus codes Code Description 200 OK The request is accepted and is being processed. 404 Not Found Attempting to perform an action on a nonexistent database. 406 Not Acceptable Not all the modules loaded to the database support \u0026lsquo;backup_restore\u0026rsquo; capability 409 Conflict Database is currently busy with another action. In this context, this is a temporary condition and the request should be reattempted later. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/info/","uriRel":"/rs/references/cli-utilities/rladmin/info/","title":"rladmin info","tags":[],"keywords":[],"description":"Shows the current configuration of a cluster, database, node, or proxy.","content":"Shows the current configuration of specified databases, proxies, clusters, or nodes.\ninfo cluster Lists the current configuration for the cluster.\nrladmin info cluster Parameters None\nReturns Returns the current configuration for the cluster.\nExample $ rladmin info cluster Cluster configuration: repl_diskless: enabled shards_overbooking: disabled default_non_sharded_proxy_policy: single default_sharded_proxy_policy: single default_shards_placement: dense default_fork_evict_ram: enabled default_provisioned_redis_version: 6.0 redis_migrate_node_threshold: 0KB (0 bytes) redis_migrate_node_threshold_percent: 4 (%) redis_provision_node_threshold: 0KB (0 bytes) redis_provision_node_threshold_percent: 12 (%) max_simultaneous_backups: 4 slave_ha: enabled slave_ha_grace_period: 600 slave_ha_cooldown_period: 3600 slave_ha_bdb_cooldown_period: 7200 parallel_shards_upgrade: 0 show_internals: disabled expose_hostnames_for_all_suffixes: disabled login_lockout_threshold: 5 login_lockout_duration: 1800 login_lockout_counter_reset_after: 900 default_concurrent_restore_actions: 10 endpoint_rebind_propagation_grace_time: 15 data_internode_encryption: disabled redis_upgrade_policy: major db_conns_auditing: disabled watchdog profile: local-network http support: enabled upgrade mode: disabled cm_session_timeout_minutes: 15 cm_port: 8443 cnm_http_port: 8080 cnm_https_port: 9443 info db Shows the current configuration for databases.\nrladmin info db [ {db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt;} ] Parameters Parameter Description db:id ID of the specified database (optional) name Name of the specified database (optional) Returns Returns the current configuration for all databases.\nIf db:\u0026lt;id\u0026gt; or \u0026lt;name\u0026gt; is specified, returns the current configuration for the specified database.\nExample $ rladmin info db db:1 db:1 [database1]: client_buffer_limits: 1GB (hard limit)/512MB (soft limit) in 30 seconds slave_buffer: auto pubsub_buffer_limits: 32MB (hard limit)/8MB (soft limit) in 60 seconds proxy_client_buffer_limits: 0KB (hard limit)/0KB (soft limit) in 0 seconds proxy_slave_buffer_limits: 1GB (hard limit)/512MB (soft limit) in 60 seconds proxy_pubsub_buffer_limits: 32MB (hard limit)/8MB (soft limit) in 60 seconds repl_backlog: 1.02MB (1073741 bytes) repl_timeout: 360 seconds repl_diskless: default master_persistence: disabled maxclients: 10000 conns: 5 conns_type: per-thread sched_policy: cmp max_aof_file_size: 300GB max_aof_load_time: 3600 seconds dedicated_replicaof_threads: 5 max_client_pipeline: 200 max_shard_pipeline: 2000 max_connections: 0 oss_cluster: disabled oss_cluster_api_preferred_ip_type: internal gradual_src_mode: disabled gradual_src_max_sources: 1 gradual_sync_mode: auto gradual_sync_max_shards_per_source: 1 slave_ha: disabled (database) mkms: enabled oss_sharding: disabled mtls_allow_weak_hashing: disabled mtls_allow_outdated_certs: disabled data_internode_encryption: disabled proxy_policy: single db_conns_auditing: disabled syncer_mode: centralized info node Lists the current configuration for all nodes.\nrladmin info node [ \u0026lt;id\u0026gt; ] Parameters Parameter Description id ID of the specified node Returns Returns the current configuration for all nodes.\nIf \u0026lt;id\u0026gt; is specified, returns the current configuration for the specified node.\nExample $ rladmin info node 3 Command Output: node:3 address: 198.51.100.17 external addresses: N/A recovery path: N/A quorum only: disabled max redis servers: 100 max listeners: 100 info proxy Lists the current configuration for a proxy.\nrladmin info proxy { \u0026lt;id\u0026gt; | all } Parameters Parameter Description id ID of the specified proxy all Show the current configuration for all proxies (optional) Returns If no parameter is specified or the all option is specified, returns the current configuration for all proxies.\nIf \u0026lt;id\u0026gt;is specified, returns the current configuration for the specified proxy.\nExample $ rladmin info proxy proxy:1 mode: dynamic scale_threshold: 80 (%) scale_duration: 30 (seconds) max_threads: 8 threads: 3 ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/instance_info/","uriRel":"/rs/references/rest-api/objects/crdb/instance_info/","title":"CRDB instance info object","tags":[],"keywords":[],"description":"An object that represents Active-Active instance info","content":"An object that represents Active-Active instance info.\nName Type/Value Description id integer Unique instance ID cluster CRDB cluster_info object compression integer Compression level when syncing from this source db_config CRDB database_config object Database configuration db_uid string ID of local database instance. This field is likely to be empty for instances other than the local one. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/","uriRel":"/rs/references/rest-api/objects/job_scheduler/","title":"Job scheduler object","tags":[],"keywords":[],"description":"An object for job scheduler settings","content":"An API object that represents the job scheduler settings in the cluster.\nName Type/Value Description backup_job_settings backup_job_settings object Backup job settings cert_rotation_job_settings cert_rotation_job_settings object Job settings for internal certificate rotation log_rotation_job_settings log_rotation_job_settings object Log rotation job settings node_checks_job_settings node_checks_job_settings object Node checks job settings redis_cleanup_job_settings redis_cleanup_job_settings object Redis cleanup job settings rotate_ccs_job_settings rotate_ccs_job_settings object Rotate CCS job settings ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/join/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/join/","title":"rladmin cluster join","tags":["non-configured"],"keywords":[],"description":"Adds a node to an existing cluster.","content":"Adds a node to an existing cluster.\nrladmin cluster join nodes \u0026lt;node IP address\u0026gt; username \u0026lt;admin user email\u0026gt; password \u0026lt;admin password\u0026gt; [ ephemeral_path \u0026lt;path\u0026gt; ] [ persistent_path \u0026lt;path\u0026gt; ] [ ccs_persistent_path \u0026lt;path\u0026gt; ] [ rack_id \u0026lt;node rack ID\u0026gt; ] [ override_rack_id ] [ replace_node \u0026lt;node UID\u0026gt; ] [ flash_enabled ] [ flash_path \u0026lt;path\u0026gt; ] [ addr \u0026lt;IP address\u0026gt; ] [ external_addr \u0026lt;IP address\u0026gt; ] [ override_repair ] [ accept_servers { enabled | disabled } ] [ cmn_http_port \u0026lt;port\u0026gt; ] Parameters Parameter Type/Value Description accept_servers \u0026rsquo;enabled\u0026rsquo;\n\u0026lsquo;disabled\u0026rsquo; Allows allocation of resources on the new node when enabled (optional) addr IP address Sets a node\u0026rsquo;s internal IP address. If not provided, the node sets the address automatically. (optional) ccs_persistent_path filepath (default: /var/opt/redislabs/persist) Path to the CCS snapshot location (the default is the same as persistent_path) (optional) cmn_http_port integer Joins a cluster that has a non-default cnm_http_port (optional) ephemeral_path filepath Path to the ephemeral storage location (optional) external_addr IP address Sets a node\u0026rsquo;s external IP address. If not provided, the node sets the address automatically. (optional) flash_enabled Enables flash capabilities for a database (optional) flash_path filepath (default: /var/opt/redislabs/flash) Path to the flash storage location in case the node does not support CAPI (required if flash_enabled) nodes IP address Internal IP address of an existing node in the cluster override_rack_id Changes to a new rack, specified by rack_id (optional) override_repair Enables joining a cluster with a dead node (optional) password string Admin user\u0026rsquo;s password persistent_path filepath (default: /var/opt/redislabs/persist) Path to the persistent storage location (optional) rack_id string Moves the node to the specified rack (optional) replace_node integer Replaces the specified node with the new node (optional) username email address Admin user\u0026rsquo;s email address Returns Returns ok if the node joined the cluster successfully. Otherwise, it returns an error message.\nExample $ rladmin cluster join nodes 192.0.2.2 \\ username admin@example.com \\ password admin-password Joining cluster... ok ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/jsonschema/","uriRel":"/rs/references/rest-api/requests/jsonschema/","title":"JSON schema requests","tags":[],"keywords":[],"description":"API object JSON schema requests","content":" Method Path Description GET /v1/jsonschema Get JSON schema of API objects Get object JSON schema GET /v1/jsonschema Get the JSON schema of various Redis Enterprise REST API objects.\nRequest Example HTTP request GET /jsonschema?object=bdb Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description object string Optional. The API object name: \u0026lsquo;cluster\u0026rsquo;, \u0026rsquo;node\u0026rsquo;, \u0026lsquo;bdb\u0026rsquo; etc. Response Returns the JSON schema of the specified API object.\nExample JSON body { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;An API object that represents a managed database in the cluster.\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;....\u0026#34; }, \u0026#34;....\u0026#34; } Status codes Code Description 200 OK Success. 406 Not Acceptable Invalid object. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/jwt_authorize/","uriRel":"/rs/references/rest-api/objects/jwt_authorize/","title":"JWT authorize object","tags":[],"keywords":[],"description":"An object for user authentication or a JW token refresh request","content":"An API object for user authentication or a JW token refresh request.\nName Type/Value Description password string The user’s password (required) ttl integer (range: 1-86400) (default: 300) Time to live - The amount of time in seconds the token will be valid username string The user’s username (required) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/stats/last/","uriRel":"/rs/references/rest-api/requests/bdbs/stats/last/","title":"Latest database stats requests","tags":[],"keywords":[],"description":"Most recent database statistics requests","content":" Method Path Description GET /v1/bdbs/stats/last Get most recent stats for all databases GET /v1/bdbs/{uid} Get most recent stats for a specific database Get latest stats for all databases GET /v1/bdbs/stats/last Get the most recent statistics for all databases.\nRequired permissions Permission name Roles view_all_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request Without metrics filter (returns all metrics by default)\nGET /bdbs/stats/last With metrics filter\nGET /bdbs/stats/last?metrics=no_of_keys,used_memory Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description metrics string Comma-separated list of metric names for which we want statistics (default is all). (optional) Response Returns statistics for all databases.\nExample JSON body Without metrics filter (returns all metrics by default)\n{ \u0026#34;1\u0026#34;: { \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:06:37Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:06:44Z\u0026#34;, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:06:44Z\u0026#34;, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;instantaneous_ops_per_sec\u0026#34;: 0.0, \u0026#34;last_req_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;last_res_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;used_memory\u0026#34;: 5651336.0, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;monitor_sessions_count\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;other_req\u0026#34;: 0.0, \u0026#34;other_res\u0026#34;: 0.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;read_req\u0026#34;: 0.0, \u0026#34;read_res\u0026#34;: 0.0, \u0026#34;total_connections_received\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;total_res\u0026#34;: 0.0, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;write_req\u0026#34;: 0.0, \u0026#34;write_res\u0026#34;: 0.0 }, \u0026#34;2\u0026#34;: { \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:06:37Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:06:44Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; }, \u0026#34;// Additional BDBs...\u0026#34; } With metrics filter\n{ \u0026#34;1\u0026#34;: { \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:06:44Z\u0026#34;, \u0026#34;used_memory\u0026#34;: 5651576.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:06:37Z\u0026#34; }, \u0026#34;2\u0026#34;: { \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:06:44ZZ\u0026#34;, \u0026#34;used_memory\u0026#34;: 5651440.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:06:37Z\u0026#34; }, \u0026#34;// Additional BDBs..\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found No bdbs exist Get latest database stats GET /v1/bdbs/stats/last/{int: uid} Get the most recent statistics for a specific database.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/stats/last/1?metrics=no_of_keys,used_memory Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the requested BDB. Query parameters Field Type Description metrics string Comma-separated list of metric names for which we want statistics (default is all). (optional) Response Returns the most recent statistics for a specific database.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;etime\u0026#34;: \u0026#34;2015-06-23T12:05:08Z\u0026#34;, \u0026#34;used_memory\u0026#34;: 5651576.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-06-23T12:05:03Z\u0026#34; } } Status codes Code Description 200 OK No error 404 Not Found bdb does not exist 406 Not Acceptable bdb isn\u0026rsquo;t currently active 503 Service Unavailable bdb is in recovery state ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/stats/last/","uriRel":"/rs/references/rest-api/requests/cluster/stats/last/","title":"Cluster last stats requests","tags":[],"keywords":[],"description":"Most recent cluster statistics requests","content":" Method Path Description GET /v1/cluster/stats/last Get most recent cluster stats Get latest cluster stats GET /v1/cluster/stats/last Get the most recent cluster statistics.\nRequired permissions Permission name view_cluster_stats Request Example HTTP request GET /cluster/stats/last?interval=1sec\u0026amp;stime=2015-10-14T06:44:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week. Default: 1sec. (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns the most recent statistics for the cluster.\nExample JSON body { \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.8424999999988358, \u0026#34;cpu_system\u0026#34;: 0.01749999999992724, \u0026#34;cpu_user\u0026#34;: 0.08374999999978172, \u0026#34;egress_bytes\u0026#34;: 7403.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 151638712320.0, \u0026#34;ephemeral_storage_free\u0026#34;: 162375925760.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-10-14T06:44:01Z\u0026#34;, \u0026#34;free_memory\u0026#34;: 5862400000.0, \u0026#34;ingress_bytes\u0026#34;: 7469.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;persistent_storage_avail\u0026#34;: 151638712320.0, \u0026#34;persistent_storage_free\u0026#34;: 162375925760.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-10-14T06:44:00Z\u0026#34;, \u0026#34;total_req\u0026#34;: 0.0 } Status codes Code Description 200 OK No error 500 Internal Server Error Internal server error ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/stats/last/","uriRel":"/rs/references/rest-api/requests/nodes/stats/last/","title":"Latest node stats requests","tags":[],"keywords":[],"description":"Most recent node statistics requests","content":" Method Path Description GET /v1/nodes/stats/last Get latest stats for all nodes GET /v1/nodes/stats/last/{uid} Get latest stats for a single node Get latest stats for all nodes GET /v1/nodes/stats/last Get latest statistics for all nodes.\nRequired permissions Permission name view_all_nodes_stats Request Example HTTP request GET /nodes/stats/last?interval=1sec\u0026amp;stime=2015-10-14T06:29:43Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week. Default: 1sec. (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns most recent statistics for all nodes.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.922500000015134, \u0026#34;cpu_system\u0026#34;: 0.007499999999708962, \u0026#34;cpu_user\u0026#34;: 0.01749999999810825, \u0026#34;cur_aof_rewrites\u0026#34;: 0.0, \u0026#34;egress_bytes\u0026#34;: 7887.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 75821363200.0, \u0026#34;ephemeral_storage_free\u0026#34;: 81189969920.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-10-14T06:29:44Z\u0026#34;, \u0026#34;free_memory\u0026#34;: 2956963840.0, \u0026#34;ingress_bytes\u0026#34;: 4950.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;persistent_storage_avail\u0026#34;: 75821363200.0, \u0026#34;persistent_storage_free\u0026#34;: 81189969920.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-10-14T06:29:43Z\u0026#34;, \u0026#34;total_req\u0026#34;: 0.0 }, \u0026#34;2\u0026#34;: { \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.922500000015134, \u0026#34;// additional fields...\u0026#34; } } Status codes Code Description 200 OK No error 404 Not Found No nodes exist Get latest node stats GET /v1/nodes/stats/last/{int: uid} Get the latest statistics of a node.\nRequired permissions Permission name view_node_stats Request Example HTTP request GET /nodes/stats/last/1?interval=1sec\u0026amp;stime=2015-10-13T09:01:54Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the node requested. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week. Default: 1sec. (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601format (optional) Response Returns the most recent statistics for the specified node.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.8049999999930151, \u0026#34;cpu_system\u0026#34;: 0.02750000000014552, \u0026#34;cpu_user\u0026#34;: 0.12000000000080036, \u0026#34;cur_aof_rewrites\u0026#34;: 0.0, \u0026#34;egress_bytes\u0026#34;: 2169.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 75920293888.0, \u0026#34;ephemeral_storage_free\u0026#34;: 81288900608.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-10-13T09:01:55Z\u0026#34;, \u0026#34;free_memory\u0026#34;: 3285381120.0, \u0026#34;ingress_bytes\u0026#34;: 3020.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;persistent_storage_avail\u0026#34;: 75920293888.0, \u0026#34;persistent_storage_free\u0026#34;: 81288900608.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-10-13T09:01:54Z\u0026#34;, \u0026#34;total_req\u0026#34;: 0.0 } } Error codes Code Description 200 OK No error 404 Not Found Node does not exist 406 Not Acceptable Node isn\u0026rsquo;t currently active 503 Service Unavailable Mode is in recovery state ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/shards-stats/last/","uriRel":"/rs/references/rest-api/requests/shards-stats/last/","title":"Latest shards stats requests","tags":[],"keywords":[],"description":"Most recent shard statistics requests","content":" Method Path Description GET /v1/shards/stats/last Get most recent stats for all shards GET /v1/shards/stats/last/{uid} Get most recent stats for a specific shard Get latest stats for all shards GET /v1/shards/stats/last Get most recent statistics for all shards.\nRequired permissions Permission name view_all_shard_stats Request Example HTTP request GET /shards/stats/last?interval=1sec\u0026amp;stime=015-05-27T08:27:35Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week. Default: 1sec (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns most recent statistics for all shards.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:35Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:28:36Z\u0026#34;, \u0026#34;used_memory_peak\u0026#34;: 5888264.0, \u0026#34;used_memory_rss\u0026#34;: 5888264.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;pubsub_patterns\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;last_save_time\u0026#34;: 1432541051.0, \u0026#34;sync_partial_ok\u0026#34;: 0.0, \u0026#34;connected_clients\u0026#34;: 9.0, \u0026#34;avg_ttl\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;used_memory\u0026#34;: 5651440.0, \u0026#34;sync_full\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;blocked_clients\u0026#34;: 0.0, \u0026#34;pubsub_channels\u0026#34;: 0.0, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;no_of_expires\u0026#34;: 0.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;sync_partial_err\u0026#34;: 0.0, \u0026#34;rdb_changes_since_last_save\u0026#34;: 0.0 }, \u0026#34;2\u0026#34;: { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:40Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:28:45Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } } Status codes Code Description 200 OK No error 404 Not Found No shards exist Get latest shard stats GET /v1/shards/stats/last/{int: uid} Get most recent statistics for a specific shard.\nRequired permissions Permission name view_shard_stats Request Example HTTP request GET /shards/stats/last/1?interval=1sec\u0026amp;stime=2015-05-28T08:27:35Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the shard requested. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week. Default: 1sec. (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns the most recent statistics for the specified shard.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:35Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:27:36Z\u0026#34;, \u0026#34;used_memory_peak\u0026#34;: 5888264.0, \u0026#34;used_memory_rss\u0026#34;: 5888264.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;pubsub_patterns\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;last_save_time\u0026#34;: 1432541051.0, \u0026#34;sync_partial_ok\u0026#34;: 0.0, \u0026#34;connected_clients\u0026#34;: 9.0, \u0026#34;avg_ttl\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;used_memory\u0026#34;: 5651440.0, \u0026#34;sync_full\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;blocked_clients\u0026#34;: 0.0, \u0026#34;pubsub_channels\u0026#34;: 0.0, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;no_of_expires\u0026#34;: 0.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;sync_partial_err\u0026#34;: 0.0, \u0026#34;rdb_changes_since_last_save\u0026#34;: 0.0 } } Status codes Code Description 200 OK No error 404 Not Found Shard does not exist 406 Not Acceptable Shard isn\u0026rsquo;t currently active ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/ldap/","uriRel":"/rs/references/rest-api/objects/ldap/","title":"LDAP object","tags":[],"keywords":[],"description":"An object that contains the cluster&#39;s LDAP configuration","content":"An API object that represents the cluster\u0026rsquo;s LDAP configuration.\nName Type/Value Description bind_dn string DN used when binding with the LDAP server to run queries bind_pass string Password used when binding with the LDAP server to run queries ca_cert string PEM-encoded CA certificate(s) used to validate TLS connections to the LDAP server cache_ttl integer (default: 300) Maximum TTL (in seconds) of cached entries control_plane boolean (default: false) Use LDAP for user authentication/authorization in the control plane data_plane boolean (default: false) Use LDAP for user authentication/authorization in the data plane dn_group_attr string The name of an attribute of the LDAP user entity that contains a list of the groups that user belongs to. (Mutually exclusive with \u0026ldquo;dn_group_query\u0026rdquo;) dn_group_query complex object An LDAP search query for mapping from a user DN to the groups the user is a member of. The substring \u0026ldquo;%D\u0026rdquo; in the filter will be replaced with the user\u0026rsquo;s DN. (Mutually exclusive with \u0026ldquo;dn_group_attr\u0026rdquo;) starttls boolean (default: false) Use StartTLS negotiation for the LDAP connection uris array of strings URIs of LDAP servers that only contain the schema, host, and port user_dn_query complex object An LDAP search query for mapping from a username to a user DN. The substring \u0026ldquo;%u\u0026rdquo; in the filter will be replaced with the username. (Mutually exclusive with \u0026ldquo;user_dn_template\u0026rdquo;) user_dn_template string A string template that maps between the username, provided to the cluster for authentication, and the LDAP DN. The substring \u0026ldquo;%u\u0026rdquo; will be replaced with the username. (Mutually exclusive with \u0026ldquo;user_dn_query\u0026rdquo;) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/ldap/","uriRel":"/rs/references/rest-api/requests/cluster/ldap/","title":"Cluster LDAP requests","tags":[],"keywords":[],"description":"LDAP configuration requests","content":" Method Path Description GET /v1/cluster/ldap Get LDAP configuration PUT /v1/cluster/ldap Set/update LDAP configuration DELETE /v1/cluster/ldap Delete LDAP configuration Get LDAP configuration GET /v1/cluster/ldap Get the LDAP configuration.\nRequired permissions Permission name view_ldap_config Request Example HTTP request GET /cluster/ldap Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns an LDAP object.\nExample JSON body { \u0026#34;bind_dn\u0026#34;: \u0026#34;rl_admin\u0026#34;, \u0026#34;bind_pass\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;ca_cert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;control_plane\u0026#34;: false, \u0026#34;data_plane\u0026#34;: false, \u0026#34;dn_group_attr\u0026#34;: \u0026#34;MemberOf\u0026#34;, \u0026#34;dn_group_query\u0026#34;: {}, \u0026#34;starttls\u0026#34;: false, \u0026#34;uris\u0026#34;: [\u0026#34;ldap://ldap.example.org:636\u0026#34;], \u0026#34;user_dn_query\u0026#34;: {}, \u0026#34;user_dn_template\u0026#34;: \u0026#34;cn=%u, ou=users,dc=example,dc=org\u0026#34; } Status codes Code Description 200 OK Success Update LDAP configuration PUT /v1/cluster/ldap Set or update the cluster LDAP configuration.\nRequired permissions Permission name config_ldap Request Example HTTP request POST /cluster/ldap Example JSON body { \u0026#34;uris\u0026#34;: [ \u0026#34;ldap://ldap.redislabs.com:389\u0026#34; ], \u0026#34;bind_dn\u0026#34;: \u0026#34;rl_admin\u0026#34;, \u0026#34;bind_pass\u0026#34;: \u0026#34;secret\u0026#34;, \u0026#34;user_dn_template\u0026#34;: \u0026#34;cn=%u,dc=example,dc=org\u0026#34;, \u0026#34;dn_group_attr\u0026#34;: \u0026#34;MemberOf\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include an LDAP object with updated fields in the request body.\nResponse Returns a status code. If an error occurs, the response body may include an error code and message with more details.\nError codes Possible error_code values:\nCode Description illegal_fields_combination An unacceptable combination of fields was specified for the configuration object (e.g.: two mutually-exclusive fields), or a required field is missing. Status codes Code Description 200 OK Success, LDAP config has been set. 400 Bad Request Bad or missing configuration parameters. Delete LDAP configuration DELETE /v1/cluster/ldap Clear the LDAP configuration.\nRequired permissions Permission name config_ldap Request Example HTTP request DELETE /cluster/ldap Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a status code.\nStatus codes Code Description 200 OK Success ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/ldap_mapping/","uriRel":"/rs/references/rest-api/objects/ldap_mapping/","title":"LDAP mapping object","tags":[],"keywords":[],"description":"An object that represents a mapping between an LDAP group and roles","content":"An API object that represents an LDAP mapping between an LDAP group and roles.\nName Type/Value Description uid integer LDAP mapping\u0026rsquo;s unique ID account_id integer SM account ID action_uid string Action UID. If it exists, progress can be tracked by the GET /actions/{uid} API (read-only) bdbs_email_alerts complex object UIDs of databases that associated email addresses will receive alerts for cluster_email_alerts boolean Activate cluster email alerts for an associated email dn string An LDAP group\u0026rsquo;s distinguished name email string Email address used for alerts (if set) email_alerts boolean (default: true) Activate email alerts for an associated email name string Role\u0026rsquo;s name role_uids array of integers List of role UIDs associated with the LDAP group ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/ldap_mappings/","uriRel":"/rs/references/rest-api/requests/ldap_mappings/","title":"LDAP mappings requests","tags":[],"keywords":[],"description":"LDAP mappings requests","content":" Method Path Description GET /v1/ldap_mappings Get all LDAP mappings GET /v1/ldap_mappings/{uid} Get a single LDAP mapping PUT /v1/ldap_mappings/{uid} Update an LDAP mapping POST /v1/ldap_mappings Create a new LDAP mapping DELETE /v1/ldap_mappings/{uid} Delete an LDAP mapping Get all LDAP mappings GET /v1/ldap_mappings Get all LDAP mappings.\nRequired permissions Permission name view_all_ldap_mappings_info Request Example HTTP request GET /ldap_mappings Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON array of LDAP mapping objects.\nExample JSON body [ { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Admins\u0026#34;, \u0026#34;dn\u0026#34;: \u0026#34;OU=ops.group,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops.group@redislabs.com\u0026#34;, \u0026#34;role_uids\u0026#34;: [\u0026#34;1\u0026#34;], \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;cluster_email_alerts\u0026#34;: true } ] Status codes Code Description 200 OK No error Get LDAP mapping GET /v1/ldap_mappings/{int: uid} Get a specific LDAP mapping.\nRequired permissions Permission name view_ldap_mapping_info Request Example HTTP request GET /ldap_mappings/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The object\u0026rsquo;s unique ID. Response Returns an LDAP mapping object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Admins\u0026#34;, \u0026#34;dn\u0026#34;: \u0026#34;OU=ops.group,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops.group@redislabs.com\u0026#34;, \u0026#34;role_uids\u0026#34;: [\u0026#34;1\u0026#34;], \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;cluster_email_alerts\u0026#34;: true } Error codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. ldap_mapping_not_exist An object does not exist Status codes Code Description 200 OK Success. 403 Forbidden Operation is forbidden. 404 Not Found ldap_mapping does not exist. 501 Not Implemented Cluster doesn\u0026rsquo;t support LDAP mappings yet. Update LDAP mapping PUT /v1/ldap_mappings/{int: uid} Update an existing ldap_mapping object.\nRequired permissions Permission name update_ldap_mapping Request Example HTTP request PUT /ldap_mappings/17 Example JSON body { \u0026#34;dn\u0026#34;: \u0026#34;OU=ops,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops@redislabs.com\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;cluster_email_alerts\u0026#34;: true } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include an LDAP mapping object with updated fields in the request body.\nResponse Example JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Admins\u0026#34;, \u0026#34;dn\u0026#34;: \u0026#34;OU=ops,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops@redislabs.com\u0026#34;, \u0026#34;role_uids\u0026#34;: [\u0026#34;1\u0026#34;], \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;cluster_email_alerts\u0026#34;: true } Error codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists ldap_mapping_not_exist An object does not exist invalid_dn_param A dn parameter has an illegal value invalid_name_param A name parameter has an illegal value invalid_role_uids_param A role_uids parameter has an illegal value invalid_account_id_param An account_id parameter has an illegal value Status codes Code Description 200 OK Success, LDAP mapping is created. 400 Bad Request Bad or missing configuration parameters. 404 Not Found Attempting to change a non-existing LDAP mapping. 501 Not Implemented Cluster doesn\u0026rsquo;t support LDAP mapping yet. Create LDAP mapping POST /v1/ldap_mappings Create a new LDAP mapping.\nRequired permissions Permission name create_ldap_mapping Request Example HTTP request POST /ldap_mappings Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;Admins\u0026#34;, \u0026#34;dn\u0026#34;: \u0026#34;OU=ops.group,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops.group@redislabs.com\u0026#34;, \u0026#34;role_uids\u0026#34;: [\u0026#34;1\u0026#34;] } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include an LDAP mapping object in the request body.\nResponse Example JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Admins\u0026#34;, \u0026#34;dn\u0026#34;: \u0026#34;OU=ops.group,DC=redislabs,DC=com\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ops.group@redislabs.com\u0026#34;, \u0026#34;role_uids\u0026#34;: [\u0026#34;1\u0026#34;] } Error codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists missing_field A needed field is missing invalid_dn_param A dn parameter has an illegal value invalid_name_param A name parameter has an illegal value invalid_role_uids_param A role_uids parameter has an illegal value Status codes Code Description 200 OK Success, an LDAP-mapping object is created. 400 Bad Request Bad or missing configuration parameters. 501 Not Implemented Cluster doesn\u0026rsquo;t support LDAP mappings yet. Delete LDAP mapping DELETE /v1/ldap_mappings/{int: uid} Delete an LDAP mapping object.\nRequired permissions Permission name delete_ldap_mapping Request Example HTTP request DELETE /ldap_mappings/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The ldap_mapping unique ID. Response Returns a status code. If an error occurs, the response body may include a more specific error code and message.\nError codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. ldap_mapping_not_exist An object does not exist Status codes Code Description 200 OK Success, the ldap_mapping is deleted. 406 Not Acceptable The request is not acceptable. 501 Not Implemented Cluster doesn\u0026rsquo;t support LDAP mappings yet. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/license/","uriRel":"/rs/references/rest-api/requests/license/","title":"License requests","tags":[],"keywords":[],"description":"License requests","content":" Method Path Description GET /v1/license Get license details PUT /v1/license Update the license Get license GET /v1/license Returns the license details, including license string, expiration, and supported features.\nRequired permissions Permission name view_license Request Example HTTP request GET /license Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON object that contains the license details.\nExample JSON body { \u0026#34;license\u0026#34;: \u0026#34;----- LICENSE START -----\\ndi+iK...KniI9\\n----- LICENSE END -----\\n\u0026#34;, \u0026#34;expired\u0026#34;: true, \u0026#34;activation_date\u0026#34;:\u0026#34;2018-12-31T00:00:00Z\u0026#34;, \u0026#34;expiration_date\u0026#34;:\u0026#34;2019-12-31T00:00:00Z\u0026#34;, \u0026#34;shards_limit\u0026#34;: 300, \u0026#34;features\u0026#34;: [\u0026#34;bigstore\u0026#34;] } Status codes Code Description 200 OK License is returned in the response body. 404 Not Found No license is installed. Update license PUT /v1/license Validate and install a new license string.\nRequired permissions Permission name install_new_license Request The request must be a JSON object with a single key named \u0026ldquo;license\u0026rdquo;.\nExample HTTP request PUT /license Example JSON body { \u0026#34;license\u0026#34;: \u0026#34;----- LICENSE START -----\\ndi+iK...KniI9\\n----- LICENSE END -----\\n\u0026#34; } Request headers Key Value Description Accept application/json Accepted media type Request body Include a JSON object that contains the new license string in the request body.\nResponse Returns an error if the new license is not valid.\nStatus codes Code Description 200 OK License installed successfully. 400 Bad Request Invalid request, either bad JSON object or corrupted license was supplied. 406 Not Acceptable License violation. A response body provides more details on the specific cause. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/limits/","uriRel":"/rs/references/rest-api/objects/bootstrap/limits/","title":"Limits object","tags":[],"keywords":[],"description":"Documents the limits object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description max_listeners integer (default: 100) Max allowed listeners on node max_redis_servers integer (default: 100) Max allowed Redis servers on node ","categories":["RS"]},{"uri":"/rs/installing-upgrading/configuring/linux-swap/","uriRel":"/rs/installing-upgrading/configuring/linux-swap/","title":"Configure Swap for Linux","tags":[],"keywords":[],"description":"","content":"Swap space is used by the Linux OS to help manage memory (pages) by copying pages from RAM to disk and the OS is configured by default to be fairly aggressive. For Redis Enterprise Software (RS) with the way it utilizes and manages memory, it is best to eliminate the likelihood of the OS swapping. If you would like to understand why, please read more on memory limits for best functionality and performance. The formal recommendation is to disable Linux swap completely in the OS.\nDisabling swap To disable the swap in the OS of an existing server/VM/instance, you must have sudo access or be root to run the following command:\n$ sudo swapoff -a $ sudo sed -i.bak \u0026#39;/ swap / s/^(.*)$/#1/g\u0026#39; /etc/fstab The first command turns swap off immediately and the second command comments out the swap partitions configured in the OS so swap being off survives a reboot.\nIf you are able to, it is best when you install/build the OS on the server/VM/instance to be used in your RS cluster, to simply not configure swap partitions at all.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/list/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/list/","title":"crdb-cli crdb list","tags":[],"keywords":[],"description":"Shows a list of all Active-Active databases.","content":"Shows a list of all Active-Active databases.\ncrdb-cli crdb list Parameters None\nReturns Returns a list of all Active-Active databases that the cluster participates in. Each database is represented with a unique GUID, the name of the database, an instance ID, and the FQDN of the cluster that hosts the instance.\nExample $ crdb-cli crdb list CRDB-GUID NAME REPL-ID CLUSTER-FQDN d84f6fe4-5bb7-49d2-a188-8900e09c6f66 database1 1 cluster1.redis.local d84f6fe4-5bb7-49d2-a188-8900e09c6f66 database1 2 cluster2.redis.local ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/task/list/","uriRel":"/rs/references/cli-utilities/crdb-cli/task/list/","title":"crdb-cli task list","tags":[],"keywords":[],"description":"Lists active and recent Active-Active database tasks.","content":"Lists active and recent Active-Active database tasks.\ncrdb-cli task list Parameters None\nReturns A table listing current and recent Active-Active tasks. Each entry includes the following:\nColumn Description Task ID String containing the unique ID associated with the taskExample: e1c49470-ae0b-4df8-885b-9c755dd614d0 CRDB-GUID String containing the unique ID associated with the Active-Active database affected by the taskExample: 1d7741cc-1110-4e2f-bc6c-935292783d24 Operation String describing the task actionExample: create_crdb Status String indicating the task statusExample: finished Worker name String identifying the process handling the taskExample: crdb_worker:1:0 Started TimeStamp value indicating when the task started (UTC)Example: 2022-10-12T09:33:41Z Ended TimeStamp value indicating when the task ended (UTC)Example: 2022-10-12T09:33:55Z Example $ crdb-cli task list TASK-ID CRDB-GUID OPERATION STATUS WORKER-NAME STARTED ENDED \u0026lt;task-ID\u0026gt; \u0026lt;crdb-ID\u0026gt; \u0026lt;operation\u0026gt; \u0026lt;result\u0026gt; \u0026lt;worker-ID\u0026gt; \u0026lt;started\u0026gt; \u0026lt;ended\u0026gt; ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/lists/","uriRel":"/rs/databases/active-active/develop/data-types/lists/","title":"Lists in Active-Active databases","tags":[],"keywords":[],"description":"Information about using list with an Active-Active database.","content":"Redis lists are simply lists of strings, sorted by insertion order. It is possible to add elements to a Redis List that push new elements to the head (on the left) or to the tail (on the right) of the list. Redis lists can be used to easily implement queues (using LPUSH and RPOP, for example) and stacks (using LPUSH and LPOP, for example).\nLists in Active-Active databases are just the same as regular Redis Lists. See the following examples to get familiar with Lists\u0026rsquo; behavior in an Active-Active database.\nSimple Lists example:\nTime CRDB Instance 1 CRDB Instance 2 t1 LPUSH mylist “hello” t2 — Sync — — Sync — t3 LPUSH mylist “world” t4 — Sync — — Sync — t5 LRANGE mylist 0 -1 =\u0026gt;“world” “hello” LRANGE mylist 0 -1 =\u0026gt; “world” “hello” Explanation: The final list contains both the \u0026ldquo;world\u0026rdquo; and \u0026ldquo;hello\u0026rdquo; elements, in that order (Instance 2 observed \u0026ldquo;hello\u0026rdquo; when it added \u0026ldquo;world\u0026rdquo;).\nExample of Lists with Concurrent Insertions:\nTime CRDB Instance 1 CRDB Instance 2 t1 LPUSH L x t2 — Sync — — Sync — t3 LINSERT L AFTER x y1 t4 LINSERT L AFTER x y2 t5 LRANGE L 0 -1 =\u0026gt; x y1 LRANGE L 0 -1 =\u0026gt; x y2 t6 — Sync — — Sync — t7 LRANGE L 0 -1 =\u0026gt; x y1 y2 LRANGE L 0 -1 =\u0026gt; x y1 y2 Explanation: Instance 1 added an element y1 after x, and then Instance 2 added element y2 after x. The final List contains all three elements: x is the first element, after it y1 and then y2. The Active-Active database resolves the conflict arbitrarily but applies the resolution consistently across all Active-Active database instances.\nExample of Deleting a List while Pushing a New Element:\nTime CRDB Instance 1 CRDB Instance 2 t1 LPUSH L x t2 — Sync — — Sync — t3 LRANGE L 0 -1 =\u0026gt; x LRANGE L 0 -1 =\u0026gt; x t4 LPUSH L y DEL L t5 — Sync — — Sync — t6 LRANGE L 0 -1 =\u0026gt; y LRANGE L 0 -1 =\u0026gt; y Explanation At t4 - t6, DEL deletes only observed elements. This is why L still contains y.\nExample of Popping Elements from a List:\nTime CRDB Instance 1 CRDB Instance 2 t1 LPUSH L x y z t2 — Sync — — Sync — t3 RPOP L =\u0026gt; x t4 — Sync — — Sync — t5 RPOP L =\u0026gt; y t6 — Sync — — Sync — t7 RPOP L =\u0026gt; z RPOP L =\u0026gt; z Explanation: At t1, the operation pushes elements x, y, z to List L. At t3, the sequential pops behave as expected from a queue. At t7, the concurrent pop in both instances might show the same result. The instance was not able to sync regarding the z removal so, from the point of view of each instance, z is located in the List and can be popped. After syncing, both lists are empty.\nBe aware of the behavior of Lists in Active-Active databases when using List as a stack or queue. As seen in the above example, two parallel RPOP operations performed by two different Active-Active database instances can get the same element in the case of a concurrent operation. Lists in Active-Active databases guarantee that each element is POP-ed at least once, but cannot guarantee that each element is POP-ed only once. Such behavior should be taken into account when, for example, using Lists in Active-Active databases as building blocks for inter-process communication systems.\nIn that case, if the same element cannot be handled twice by the applications, it\u0026rsquo;s recommended that the POP operations be performed by one Active-Active database instance, whereas the PUSH operations can be performed by multiple instances.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/shard/loading/","uriRel":"/rs/references/rest-api/objects/shard/loading/","title":"Loading object","tags":[],"keywords":[],"description":"Documents the loading object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description progress number, (range: 0-100) Percentage of bytes already loaded status \u0026lsquo;in_progress\u0026rsquo;\n\u0026lsquo;idle\u0026rsquo; Status of the load of a dump file (read-only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/log_rotation_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/log_rotation_job_settings/","title":"Log rotation job settings object","tags":[],"keywords":[],"description":"Documents the log_rotation_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the log rotation schedule ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/logs/","uriRel":"/rs/references/rest-api/requests/logs/","title":"Logs requests","tags":[],"keywords":[],"description":"Cluster event logs requests","content":" Method Path Description GET /v1/logs Get cluster events log Get cluster events log GET /v1/logs Get cluster events log.\nRequired permissions Permission name view_logged_events Request Example HTTP request GET /logs?order=desc Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description stime ISO_8601 Start time before which we don\u0026rsquo;t want events. (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want events. (optional) order string desc/asc - get events in descending or ascending order. Defaults to asc. limit integer Maximum number of events to return. (optional) offset integer Skip offset events before returning first one (useful for pagination). (optional) Response Returns a JSON array of events.\nExample JSON body [ { \u0026#34;time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bdb_name_updated\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;bdb_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;old_val\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;new_val\u0026#34;: \u0026#34;test123\u0026#34; }, { \u0026#34;time\u0026#34;: \u0026#34;2014-08-29T11:18:48Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cluster_bdb_created\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;bdb_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;bdb_name\u0026#34;: \u0026#34;test\u0026#34; }, { \u0026#34;time\u0026#34;: \u0026#34;2014-08-29T11:17:49Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cluster_node_joined\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;node_uid\u0026#34;: 2 } ] Event object Field Description time Timestamp when event happened. type Event type. Additional fields may be available for certain event types. additional fields Additional fields may be present based on event type. Status codes Code Description 200 OK No error ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/maintenance-mode/","uriRel":"/rs/references/cli-utilities/rladmin/node/maintenance-mode/","title":"rladmin node maintenance_mode","tags":[],"keywords":[],"description":"Turns quorum-only mode on or off for a node.","content":"Configures quorum-only mode on a node.\nnode maintenance_mode on Migrates shards out of the node and turns the node into a quorum node to prevent shards from returning to it.\nrladmin node \u0026lt;ID\u0026gt; maintenance_mode on [ keep_slave_shards ] [ demote_node ] [ max_concurrent_actions \u0026lt;integer\u0026gt; ] Parameters Parameter Type/Value Description node integer Turns the specified node into a quorum node demote_node If the node is a primary node, changes the node to replica keep_slave_shards Keeps replica shards in the node and demotes primary shards to replicas max_concurrent_actions integer Maximum number of concurrent actions during node maintenance Returns Returns OK if the node was converted successfully. If the cluster does not have enough resources to migrate the shards, the process returns a warning.\nUse rladmin status nodes to verify the node became a quorum node.\nExample $ rladmin node 2 maintenance_mode on Performing maintenance_on action on node:2: 0% created snapshot NodeSnapshot\u0026lt;name=maintenance_mode_2022-05-12_20-25-37,time=None,node_uid=2\u0026gt; node:2 will not accept any more shards Performing maintenance_on action on node:2: 100% OK $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.12 198.51.100.1 3d99db1fdf4b 5/100 6 14.21GB/19.54GB 10.62GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.13 198.51.100.2 fc7a3d332458 0/0 6 14.21GB/19.54GB 0KB/0KB 6.2.12-37 OK node:4 slave 192.0.2.14 6d754fe12cb9 5/100 6 14.21GB/19.54GB 10.62GB/16.02GB 6.2.12-37 OK node maintenance_mode off Turns maintenance mode off and returns the node to its previous state.\nrladmin node \u0026lt;ID\u0026gt; maintenance_mode off [ { snapshot_name \u0026lt;name\u0026gt; | skip_shards_restore } ] [ max_concurrent_actions \u0026lt;integer\u0026gt; ] Parameters Parameter Type/Value Description node integer Restores the node back to the previous state max_concurrent_actions integer Maximum number of concurrent actions during node maintenance skip_shards_restore Does not restore shards back to the node snapshot_name string Restores the node back to a state stored in the specified snapshot Returns Returns OK if the node was restored successfully.\nUse rladmin status nodes to verify the node was restored.\nExample $ rladmin node 2 maintenance_mode off Performing maintenance_off action on node:2: 0% Found snapshot: NodeSnapshot\u0026lt;name=maintenance_mode_2022-05-12_20-25-37,time=2022-05-12T20:25:37Z,node_uid=2\u0026gt; Performing maintenance_off action on node:2: 0% migrate redis:12 to node:2: executing Performing maintenance_off action on node:2: 0% migrate redis:12 to node:2: finished Performing maintenance_off action on node:2: 0% migrate redis:17 to node:2: executing migrate redis:15 to node:2: executing Performing maintenance_off action on node:2: 0% migrate redis:17 to node:2: finished migrate redis:15 to node:2: finished Performing maintenance_off action on node:2: 0% failover redis:16: executing failover redis:14: executing Performing maintenance_off action on node:2: 0% failover redis:16: finished failover redis:14: finished Performing maintenance_off action on node:2: 0% failover redis:18: executing Performing maintenance_off action on node:2: 0% failover redis:18: finished migrate redis:21 to node:2: executing migrate redis:19 to node:2: executing Performing maintenance_off action on node:2: 0% migrate redis:21 to node:2: finished migrate redis:19 to node:2: finished failover redis:20: executing Performing maintenance_off action on node:2: 0% failover redis:20: finished Performing maintenance_off action on node:2: 0% rebind endpoint:6:1: executing Performing maintenance_off action on node:2: 0% rebind endpoint:6:1: finished Performing maintenance_off action on node:2: 100% OK $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.12 198.51.100.1 3d99db1fdf4b 5/100 6 14.2GB/19.54GB 10.61GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.13 198.51.100.2 fc7a3d332458 5/100 6 14.2GB/19.54GB 10.61GB/16.02GB 6.2.12-37 OK node:4 slave 192.0.2.14 6d754fe12cb9 0/100 6 14.2GB/19.54GB 10.69GB/16.02GB 6.2.12-37 OK ","categories":["RS"]},{"uri":"/cheatsheet/","uriRel":"/cheatsheet/","title":"Markdown Cheatsheet","tags":[],"keywords":[],"description":"Syntax instructions for markdown formatting","content":"Here you can find examples of style and formatting elements that you can use in your pages.\nNote: % is used when the inner content of the shortcode is markdown, and \u0026lt; is used when the inner content is HTML. Basic content formatting Tabbed paragraphs A tabbed paragraph create scrolling code blocks like this:\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris efficitur, velit sit amet tempus commodo, orci ipsum laoreet turpis, eu ullamcorper orci enim ut dui. A tabbed paragraph in a bulleted or numbered list create indented paragraphs like this:\nLorem\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris efficitur, velit sit amet tempus commodo, orci ipsum laoreet turpis, eu ullamcorper orci enim ut dui.\nLinks to internal pages To link to another page in the content directory:\nSyntax:\n[Redis Cloud Quick Start]({{\u0026lt; relref \u0026#34;/rc/rc-quickstart.md\u0026#34; \u0026gt;}})` Output:\nRedis Cloud Quick Start\nTo link to an anchor on another page in the content directory:\nSyntax:\n`[Sign up for Redis Cloud]({{\u0026lt; relref \u0026#34;/rc/rc-quickstart#step-1-sign-up-for-redis-cloud-pro-account\u0026#34; \u0026gt;}})` Output:\nSign up for Redis Cloud\nText formatting Text styles\nStyle Syntax Output Emphasized text *emphasized* emphasized Bold text **bold** bold Comments\nText in the comments shortcode is not published in the output.\nSyntax:\n{{% comment %}}Do not publish!{{% /comment %}} Ouput:\nDefinitions A series of definitions, as you\u0026rsquo;d find in a glossary, should go into a definition list (e.g., \u0026lt;dl\u0026gt;\u0026lt;/dl\u0026gt;).\nFor the entires, use the following shortcode:\n{{\u0026lt;definition \u0026#34;term\u0026#34;\u0026gt;}} The definition of \u0026#34;term\u0026#34; goes here. {{\u0026lt;/definition\u0026gt;}} This produces the following HTML output:\n\u0026lt;dt id=\u0026#34;term\u0026#34;\u0026gt;term\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;The definition of \u0026#34;term\u0026#34; goes here.\u0026lt;/dd\u0026gt; Code formatting Embedded code snippets\nCode snippets that need to pass automation must be located in /static/code and included in the article with:\n```json {{\u0026lt;/* embed-code \u0026#34;sample.json\u0026#34; \u0026gt;}} ``` If the code snippet is more than 30 lines, add it within an expanding block:\n{{% expand %}} ```json {{\u0026lt; embed-code \u0026#34;sample.json\u0026#34; \u0026gt;}} ``` {{% /expand %}} Code blocks\nTo add code blocks of a specific language, e.g. python, type this:\n```python ##this is python code def hello_world(): print \u0026#34;Hello World!\u0026#34; ``` Output:\n##this is python code def hello_world(): print \u0026#34;Hello World!\u0026#34; Inline Code\nTo indicate text entry, commands or code paramters inline use a single backtick (``).\nSyntax:\nUse the `TRUE` value to enable the feature. Output:\nUse the TRUE value to enable the feature.\nCode shortcode\nCode can be shown like code fences using the code shortcode also.\nSyntax:\n{{\u0026lt; code \u0026gt;}}var x = 123;{{\u0026lt; /code \u0026gt;}} Ouput:\nvar x = 123; Tables Syntax:\n| heading 1 | heading 2 | heading 3 | | :-----| :-----: |-----: | | cell 1x1 | cell 1x2 | cell 1x3 | | cell 2x1 | cell 2x2 | cell 2x3 | Ouput:\nheading 1 heading 2 heading 3 cell 1x1 cell 1x2 cell 1x3 cell 2x1 cell 2x2 cell 2x3 Tables from CSV The first parameter when using this shortcode is the name of the CSV file located in /static/tables directory and the second is the number of columns in the table.\nSyntax: {{\u0026lt; table-csv \u0026quot;test_table.csv\u0026quot; 3 \u0026gt;}}\nFile:\nName,Description,Detail cell 1x1,cell 1x2,cell 1x3 cell 2x1,cell 2x2,cell 2x3 cell 3x1,cell 3x2,cell 3x3 Output:\nName Description Detail cell 1x1 cell 1x2 cell 1x3 cell 2x1 cell 2x2 cell 2x3 cell 3x1 cell 3x2 cell 3x3 Tabs Source: https://github.com/rvanhorn/hugo-dynamic-tabs\nSyntax:\n{{% tabs tabTotal=\u0026#34;3\u0026#34; tabID=\u0026#34;1\u0026#34; tabName1=\u0026#34;Tab 1\u0026#34; tabName2=\u0026#34;Tab 2\u0026#34; tabName3=\u0026#34;Tab 3\u0026#34; %}} {{% tab tabNum=\u0026#34;1\u0026#34; %}} Tab 1 Content {{% /tab %}} {{% tab tabNum=\u0026#34;2\u0026#34; %}} Tab 2 Content {{% /tab %}} {{% tab tabNum=\u0026#34;3\u0026#34; %}} Tab 3 Content {{% /tab %}} {{% /tabs %}} Output:\nTab 1 Tab 2 Tab 3 Tab 1 Content\nContent in the first tab Tab 2 Content Tab 3 Content Images and videos Adding an image Copy the image to a directory in: /static/images In the markdown page, add the alt text and path to the image like this: Syntax:\n`![Alt text]( /images/path/image.png )` Output:\n![Redis Enterpise Cluster]( /images/rs/rp_stack.png ) shows:\nTo make an image appear on the next line in a list:\nWrite your instruction. Add 2 spaces at the end of the line and put the image on the next line with a tab indentation. Adding a video Embed a local video\nTo embed a video, use the video shortcode with the location of the video and a video title.\n{{\u0026lt; video \u0026quot;/images/\u0026lt;path\u0026gt;/\u0026lt;video\u0026gt;.mp4\u0026quot; \u0026quot;Video title\u0026quot; \u0026gt;}}\nYouTube\nEmbed YouTube video.\nSyntax:\n{{\u0026lt; youtube Bi1T3toQfF4 \u0026gt;}} Ouput:\nYoutube with start time\nEmbed YouTube video and start playback from specific timestamp.\nSyntax:\n{{\u0026lt; youtube_start Bi1T3toQfF4 10 \u0026gt;}} Ouput:\nSingle-sourcing Expanding blocks Syntax:\n{{\u0026lt; expand \u0026#34;How do you make expanding blocks?\u0026#34; \u0026gt;}} This is how you make expanding blocks. {{\u0026lt; /expand \u0026gt;}} Output:\nHow do you make expanding blocks? This is how you make expanding blocks. Embedded partials A partial markdown or HTML file can be included in other files using the embed-md or embed-html shortcodes. Partials must be placed in content/embeds directory.\nEmbed a markdown partial\nSyntax:\n{{\u0026lt; excerpt \u0026gt;}}The Redis OSS Cluster API support in Redis Enterprise Software (RS) provides a simple mechanism for cluster-aware Redis clients to learn and know the cluster topology. This enables clients to connect directly to an RS proxy on the node hosting the master shard for the data being operated on.{{\u0026lt; /excerpt \u0026gt;}} Output:\nThe Redis OSS Cluster API support in Redis Enterprise Software (RS) provides a simple mechanism for cluster-aware Redis clients to learn and know the cluster topology. This enables clients to connect directly to an RS proxy on the node hosting the master shard for the data being operated on. Embed an HTML partial\nSyntax:\n{{\u0026lt; excerpt-include filename=\u0026#34;rs/clusters/optimize/oss-cluster-api.md\u0026#34; \u0026gt;}} Output:\nExcerpts Warning - In most cases, use embedded partials instead of excerpts. Defining an excerpt\nSyntax:\n{{% excerpt %}}The Redis OSS Cluster API support in Redis Enterprise Software (RS) provides a simple mechanism for cluster-aware Redis clients to learn and know the cluster topology. This enables clients to connect directly to an RS proxy on the node hosting the master shard for the data being operated on.{{% /excerpt %}} Output:\nThe Redis OSS Cluster API support in Redis Enterprise Software (RS) provides a simple mechanism for cluster-aware Redis clients to learn and know the cluster topology. This enables clients to connect directly to an RS proxy on the node hosting the master shard for the data being operated on.\nIncluding an excerpt\nSyntax:\n{{% excerpt-include filename=\u0026#34;rs/clusters/optimize/oss-cluster-api.md\u0026#34; %}} Output:\nInformative notices Info\nInfo boxes give background information that does not prevent proper use of the product.\nSyntax:\n{{\u0026lt; info \u0026gt;}}After you do this the first time, it gets easier.{{\u0026lt; /info \u0026gt;}} Ouput:\nFYI - After you do this the first time, it gets easier. Tip\nTips give additional information for improved use of the product.\nSyntax:\n{{\u0026lt; tip */%}}Eating on time prevents hunger.{{\u0026lt;/* /tip \u0026gt;}} Ouput:\nTip - Eating on time prevents hunger. Note\nNotes suggest steps that prevent errors that do not cause data loss.\nSyntax:\n{{\u0026lt; note \u0026gt;}}Make sure you have enough disk space.{{\u0026lt; /note \u0026gt;}} Ouput:\nNote: Make sure you have enough disk space. Warning\nWarnings suggest that users think carefully before doing steps that can cause irresversible data loss.\nSyntax:\n{{\u0026lt; warning \u0026gt;}}Backup your data before erasing the hard disk!{{\u0026lt; /warning \u0026gt;}} Ouput:\nWarning - Backup your data before erasing the hard disk! Label (Not used)\nLabel displays a label. The type parameter can be passed to the shortcode in order to display the label in a different color, eg. success for a green label, warning for orange, info for blue and danger for red.\nSyntax:\n{{\u0026lt; label type=\u0026#34;info\u0026#34; \u0026gt;}}This is a label{{\u0026lt; /label \u0026gt;}} Ouput:\nThis is a label Well\nWell displays content inside a container.\nSyntax:\n{{\u0026lt; well \u0026gt;}} Inside a well {{\u0026lt; /well \u0026gt;}} Ouput:\nInside a well Contents lists All children Allchildren displays all the child pages of current page.\nSyntax:\n{{\u0026lt; allchildren style=\u0026#34;h2\u0026#34; description=\u0026#34;true\u0026#34; \u0026gt;}} Ouput:\nSee example here.\nRecently updated This shortcode can be used to display recently updated articles.\nSyntax:\n{{\u0026lt; recently-updated \u0026gt;}} Recently updated articles {{\u0026lt; /recently-updated \u0026gt;}} Ouput:\nRecently updated articles RedisInsight v2.16.0, December 2022 RedisInsight v2.14.0, November 2022 RedisInsight v2.18.0, January 2023 RedisInsight v2.12.0, October 2022 Get started with Redis Enterprise Software Modules quick start RedisInsight v2.10.0, September 2022 Get started with Redis Enterprise Software using Docker RedisInsight v2.8.0, August 2022 RedisInsight v1.13, Aug 2022 Other shortcodes Attachments Mermaid Children Related Info For more info about editing and writing our documents:\nEditing guide Contribution guide ","categories":[]},{"uri":"/rs/networking/mdns/","uriRel":"/rs/networking/mdns/","title":"Client prerequisites for mDNS","tags":[],"keywords":[],"description":"Requirements for using the mDNS protocol in development and testing environments.","content":" Note: mDNS is only supported for development and testing environments. If you choose to use the mDNS protocol when you set the cluster name, make sure that the configurations and prerequisites for resolving database endpoints are met on the client machines. If you have Replica Of databases on the cluster, the configurations and prerequisites are also required for the Redis Enterprise Software nodes.\nTo prepare a client or node for mDNS:\nMake sure that the clients and cluster nodes are on the same physical network or have the network infrastructure configured to allow multicasting between them.\nInstall these prerequisite packages:\nFor Ubuntu:\napt-get install libnss-mdns For RHEL/CentOS 6.x:\n$ rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm $ yum install nss-mdns $ service avahi-daemon start For RHEL/CentOS 7:\n$ rpm -ivh https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-12.noarch.rpm $ yum install nss-mdns $ service avahi-daemon start If you are using mDNS with IPv6 addresses, update the hosts line in /etc/nsswitch.conf to:\nhosts: files mdns4_minimal \\[NOTFOUND=return\\] mdns ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/mdns_server/","uriRel":"/rs/references/rest-api/objects/services_configuration/mdns_server/","title":"MDNS server object","tags":[],"keywords":[],"description":"Documents the mdns_server object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the multicast DNS server ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/migrate/","uriRel":"/rs/references/cli-utilities/rladmin/migrate/","title":"rladmin migrate","tags":[],"keywords":[],"description":"Moves Redis Enterprise Software shards or endpoints to a new node in the same cluster.","content":"Moves Redis Enterprise shards or endpoints to a new node in the same cluster.\nmigrate all_master_shards Moves all primary shards of a specified database or node to a new node in the same cluster.\nrladmin migrate { db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } | node \u0026lt;origin node ID\u0026gt; } all_master_shards target_node \u0026lt;id\u0026gt; [ override_policy ] Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Limits migration to a specific database node integer Limits migration to a specific origin node target_node integer Migration target node override_policy Overrides the rack aware policy and allows primary and replica shards on the same node Returns Returns Done if the migration completed successfully. Otherwise, returns an error.\nUse rladmin status shards to verify the migration completed.\nExample $ rladmin status shards db db:6 sort ROLE SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:14 node:3 master 0-4095 3.01MB OK db:6 tr02 redis:16 node:3 master 4096-8191 3.2MB OK db:6 tr02 redis:18 node:3 master 8192-12287 3.2MB OK db:6 tr02 redis:20 node:3 master 12288-16383 3.01MB OK $ rladmin migrate db db:6 all_master_shards target_node 1 Monitoring 8b0f28e2-4342-427a-a8e3-a68cba653ffe queued - migrate_shards running - migrate_shards Executing migrate_redis with shards_uids [\u0026#39;18\u0026#39;, \u0026#39;14\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;16\u0026#39;] Ocompleted - migrate_shards Done $ rladmin status shards node 1 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:14 node:1 master 0-4095 3.22MB OK db:6 tr02 redis:16 node:1 master 4096-8191 3.22MB OK db:6 tr02 redis:18 node:1 master 8192-12287 3.22MB OK db:6 tr02 redis:20 node:1 master 12288-16383 2.99MB OK migrate all_shards Moves all shards on a specified node to a new node in the same cluster.\nrladmin migrate node \u0026lt;origin node ID\u0026gt; [ max_concurrent_bdb_migrations \u0026lt;value\u0026gt; ] all_shards target_node \u0026lt;id\u0026gt; [ override_policy ] Parameters Parameter Type/Value Description node integer Limits migration to a specific origin node max_concurrent_bdb_migrations integer Sets the maximum number of concurrent endpoint migrations override_policy Overrides the rack aware policy and allows primary and replica shards on the same node Returns Returns Done if the migration completed successfully. Otherwise, returns an error.\nUse rladmin status shards to verify the migration completed.\nExample $ rladmin status shards node 1 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:1 master 0-16383 3.04MB OK db:6 tr02 redis:15 node:1 slave 0-4095 2.93MB OK db:6 tr02 redis:17 node:1 slave 4096-8191 2.93MB OK db:6 tr02 redis:19 node:1 slave 8192-12287 3.08MB OK db:6 tr02 redis:21 node:1 slave 12288-16383 3.08MB OK $ rladmin migrate node 1 all_shards target_node 2 Monitoring 71a4f371-9264-4398-a454-ce3ff4858c09 queued - migrate_shards .running - migrate_shards Executing migrate_redis with shards_uids [\u0026#39;21\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;19\u0026#39;] OExecuting migrate_redis with shards_uids [\u0026#39;12\u0026#39;] Ocompleted - migrate_shards Done $ rladmin status shards node 2 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:2 master 0-16383 3.14MB OK db:6 tr02 redis:15 node:2 slave 0-4095 2.96MB OK db:6 tr02 redis:17 node:2 slave 4096-8191 2.96MB OK db:6 tr02 redis:19 node:2 slave 8192-12287 2.96MB OK db:6 tr02 redis:21 node:2 slave 12288-16383 2.96MB OK migrate all_slave_shards Moves all replica shards of a specified database or node to a new node in the same cluster.\nrladmin migrate { db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } | node \u0026lt;origin node ID\u0026gt; } all_slave_shards target_node \u0026lt;id\u0026gt; [ override_policy ] Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Limits migration to a specific database node integer Limits migration to a specific origin node target_node integer Migration target node override_policy Overrides the rack aware policy and allows primary and replica shards on the same node Returns Returns Done if the migration completed successfully. Otherwise, returns an error.\nUse rladmin status shards to verify the migration completed.\nExample $ rladmin status shards db db:6 node 2 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:15 node:2 slave 0-4095 3.06MB OK db:6 tr02 redis:17 node:2 slave 4096-8191 3.06MB OK db:6 tr02 redis:19 node:2 slave 8192-12287 3.06MB OK db:6 tr02 redis:21 node:2 slave 12288-16383 3.06MB OK $ rladmin migrate db db:6 all_slave_shards target_node 3 Monitoring 5d36a98c-3dc8-435f-8ed9-35809ba017a4 queued - migrate_shards .running - migrate_shards Executing migrate_redis with shards_uids [\u0026#39;15\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;21\u0026#39;, \u0026#39;19\u0026#39;] Ocompleted - migrate_shards Done $ rladmin status shards db db:6 node 3 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:6 tr02 redis:15 node:3 slave 0-4095 3.04MB OK db:6 tr02 redis:17 node:3 slave 4096-8191 3.04MB OK db:6 tr02 redis:19 node:3 slave 8192-12287 3.04MB OK db:6 tr02 redis:21 node:3 slave 12288-16383 3.04MB OK migrate endpoint_to_shards Moves database endpoints to the node where the majority of primary shards are located.\nrladmin migrate [ db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } ] endpoint_to_shards [ restrict_target_node \u0026lt;id\u0026gt; ] [ commit ] [ max_concurrent_bdb_migrations \u0026lt;value\u0026gt; ] Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Limits migration to a specific database restrict_target_node integer Moves the endpoint only if the target node matches the specified node commit Performs endpoint movement max_concurrent_bdb_migrations integer Sets the maximum number of concurrent endpoint migrations Returns Returns a list of steps to perform the migration. If the commit flag is set, the steps will run and return Finished successfully if they were completed. Otherwise, returns an error.\nUse rladmin status endpoints to verify that the endpoints were moved.\nExample $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:3 all-master-shards No $ rladmin migrate db db:6 endpoint_to_shards * Going to bind endpoint:6:1 to node 1 Dry-run completed, add \u0026#39;commit\u0026#39; argument to execute $ rladmin migrate db db:6 endpoint_to_shards commit * Going to bind endpoint:6:1 to node 1 Executing bind endpoint:6:1: OOO. Finished successfully $ rladmin status endpoints db db:6 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL db:6 tr02 endpoint:6:1 node:1 all-master-shards No migrate shard Moves one or more shards to a new node in the same cluster.\nrladmin migrate shard \u0026lt;id1.. idN\u0026gt; [ preserve_roles ] target_node \u0026lt;id\u0026gt; [ override_policy ] Parameters Parameter Type/Value Description shard list of shard IDs Shards to migrate preserve_roles Performs an additional failover to guarantee the primary shards\u0026rsquo; roles are preserved target_node integer Migration target node override_policy Overrides the rack aware policy and allows primary and replica shards on the same node Returns Returns Done if the migration completed successfully. Otherwise, returns an error.\nUse rladmin status shards to verify the migration completed.\nExample rladmin\u0026gt; status shards db db:5 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:2 master 0-16383 3.01MB OK db:5 tr01 redis:13 node:3 slave 0-16383 3.1MB OK rladmin\u0026gt; migrate shard 13 target_node 1 Monitoring d2637eea-9504-4e94-a70c-76df087efcb2 queued - migrate_shards .running - migrate_shards Executing migrate_redis with shards_uids [\u0026#39;13\u0026#39;] Ocompleted - migrate_shards Done rladmin\u0026gt; status shards db db:5 SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:5 tr01 redis:12 node:2 master 0-16383 3.01MB OK db:5 tr01 redis:13 node:1 slave 0-16383 3.04MB OK ","categories":["RS"]},{"uri":"/rs/databases/import-export/migrate-to-active-active/","uriRel":"/rs/databases/import-export/migrate-to-active-active/","title":"Migrate a database to Active-Active","tags":[],"keywords":[],"description":"Use Replica Of to migrate your database to an Active-Active database.","content":"With Active-Active databases, applications can read and write to the same dataset from different geographical locations seamlessly and with latency less than 1 ms, without changing the way the application connects to the database. Active-Active databases also provide disaster recovery and accelerated data read-access for geographically distributed users.\nIf you have data in a single-region Redis Enterprise Software database that you want to migrate to an Active-Active database, you\u0026rsquo;ll need to create a new Active-Active database and migrate the data into the new database as a Replica Of the existing database. This process will gradually populate the data in the Active-Active database.\nBefore data migration starts, all data is flushed from the Active-Active database. The data is migrated to the Active-Active instance where you enabled Replica Of, and the data from that instance is copied to the other Active-Active instances. When data migration is finished, disable Replica Of and connect your applications to the Active-Active database.\nNote: During the migration, make sure that any applications that connect to the Active-Active database are read-only to make sure the dataset is identical to the source database during the migration process. You may continue to write to the source database during the migration process. To migrate an RS database to Active-Active:\nCreate a new Active-Active database.\nAfter the Active-Active database is activated, you see the database\u0026rsquo;s configuration.\nClick Edit at the bottom of the database configuration.\nEnable Migration using Replica Of.\nClick Continue to confirm that you want to flush the data from the Active-Active database.\nEnter the URL of the source database endpoint (the order has no impact on replication).\nFor a source database in the same RS cluster - When you click on the box, the available databases are shown in the correct format for the URL of the source endpoint:\n\u0026lt;database name\u0026gt;: redis://admin:\u0026lt;database_password\u0026gt;@\u0026lt;database_endpoint\u0026gt;:\u0026lt;database_port\u0026gt; You can select the database that you want to use as the source.\nFor a source database in a different RS cluster:\nLog in to the Web UI of the cluster that hosts the source database.\nIn databases, click on the database and go to configuration.\nUnder Endpoint, click on Get Replica Of source URL.\nClick Copy to Clipboard to copy the URL of the source endpoint.\nIf you want a different internal password, you can click Regenerate Password.\nWarning - If you regenerate the password, replication to existing destinations fails until you update their configuration with the new password. In the destination database, paste the URL of the source endpoint in the Replica Of box, and click .\nNote: For a source database on a different Redis Enterprise Software cluster, you can compress the replication data to save bandwidth. For a source database in an OSS Redis cluster - Enter the URL of the source endpoint in the format:\nIf the database has a password -\nredis://:\u0026lt;redis_password\u0026gt;@\u0026lt;hostname\u0026gt;:\u0026lt;database_port\u0026gt; Where the password is the Redis password represented with URL encoding escape characters.\nIf the database has no password -\nredis://\u0026lt;hostname\u0026gt;:\u0026lt;database_port\u0026gt; Note: If you used the mDNS protocol for the cluster name (FQDN), the client mDNS prerequisites must be met in order to communicate with other clusters. Click Update at the bottom of the page.\nWhen the synchronization icon turns green , the migration is complete. Note that migration can take minutes to hours to complete depending on the dataset size and network quality.\nEdit the configuration of the Active-Active database and select the Stop button to disable Migration using Replica Of. Redirect your database connections to the Active-Active database.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/crdb/modify_request/","uriRel":"/rs/references/rest-api/objects/crdb/modify_request/","title":"CRDB modify request object","tags":[],"keywords":[],"description":"An object to update an Active-Active database","content":"An object to update an Active-Active database.\nName Type/Value Description add_instances array of CRDB instance_info objects List of new CRDB instances crdb CRDB object An object that represents an Active-Active database force_update boolean (Warning: This flag can cause unintended and dangerous changes) Force the configuration update and increment the configuration version even if there is no change to the configuration parameters. If you use force, you can mistakenly cause the other instances to update to the configuration version even though it was not changed. remove_instances array of integers List of unique instance IDs remove_instances.force_remove boolean Force removal of instance from the Active-Active database. Before we remove an instance from an Active-Active database, all of the operations that the instance received from clients must be propagated to the other instances. This is the safe method to remove an instance from the Active-Active database. If the instance does not have connectivity to other instances, the propagation fails and removal fails. To remove an instance that does not have connectivity to other instances, you must use the force flag. The removed instance keeps its data and configuration for the instance. After you remove an instance by force, you must use the purge_instances API on the removed instance. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/module/","uriRel":"/rs/references/rest-api/objects/module/","title":"Module object","tags":[],"keywords":[],"description":"An object that represents a Redis module","content":"Represents a Redis module.\nName Type/Value Description uid string Cluster unique ID of module architecture string Architecture used to compile the module author string Module creator capabilities array of strings List of capabilities supported by this module command_line_args string Command line arguments passed to the module config_command string Name of command to configure module arguments at runtime dependencies object dependencies Module dependencies description string Short description of the module display_name string Name of module for display purposes email string Author\u0026rsquo;s email address homepage string Module\u0026rsquo;s homepage is_bundled boolean Whether module came bundled with a version of Redis Enterprise license string Module is distributed under this license min_redis_pack_version string Minimum Redis pack version required by this module min_redis_version string Minimum Redis version required by this module module_file string Module filename module_name string Module\u0026rsquo;s name os string Operating system used to compile the module os_list array of strings List of supported operating systems semantic_version string Module\u0026rsquo;s semantic version sha256 string SHA256 of module binary version integer Module\u0026rsquo;s version ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/module-capabilities/","uriRel":"/rs/references/rest-api/requests/cluster/module-capabilities/","title":"Cluster module capabilities requests","tags":[],"keywords":[],"description":"Redis module capabilities requests","content":" Method Path Description GET /v1/cluster/module-capabilities List possible Redis module capabilities List Redis module capabilities GET /v1/cluster/module-capabilities List possible Redis module capabilities.\nRequired permissions Permission name view_cluster_modules Request Example HTTP request GET /cluster/module-capabilities Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept */* Accepted media type Response Returns a JSON object that contains a list of capability names and descriptions.\nExample JSON body { \u0026#34;all_capabilities\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;types\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;module has its own types and not only operate on existing redis types\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;no_multi_key\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;module has no methods that operate on multiple keys\u0026#34;} \u0026#34;// additional capabilities...\u0026#34; ] } Status codes Code Description 200 OK No error ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/modules/","uriRel":"/rs/references/rest-api/requests/bdbs/modules/","title":"Database modules requests","tags":[],"keywords":[],"description":"Redis module requests","content":"Configure module Method Path Description POST /v1/bdbs/{uid}/modules/config Configure module Upgrade module Method Path Description POST /v1/bdbs/{uid}/modules/upgrade Upgrade module ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/modules/","uriRel":"/rs/references/rest-api/requests/modules/","title":"Modules requests","tags":[],"keywords":[],"description":"Redis modules requests","content":" Method Path Description GET /v1/modules List available modules GET /v1/modules/{uid} Get a specific module POST /v1/modules Upload a new module POST /v2/modules Upload a new module and its dependencies DELETE /v1/modules/{uid} Delete a module without dependencies DELETE /v2/modules/{uid} Delete a module with dependencies List modules GET /v1/modules List available modules, i.e. modules stored within the CCS.\nPermissions Permission name Roles view_cluster_modules admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /modules Headers Key Value Description Host 127.0.0.1:9443 Domain name Accept */* Accepted media type Response Returns a JSON array of module objects.\nStatus codes Code Description 200 OK No error Get module GET /v1/modules/{string: uid} Get specific available modules, i.e. modules stored within the CCS.\nPermissions Permission name Roles view_cluster_modules admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /modules/1 Headers Key Value Description Host 127.0.0.1:9443 Domain name Accept */* Accepted media type URL parameters Field Type Description uid integer The module\u0026rsquo;s unique ID. Response Returns a module object.\nStatus codes Code Description 200 OK No error 404 Not Found Module does not exist. Upload module v1 POST /v1/modules Uploads a new module to the cluster.\nThe request must contain a Redis module, bundled using RedisModule Packer. For modules in Redis Stack, download the module from the download center.\nSee Install a module on a cluster for more information.\nPermissions Permission name Roles update_cluster admin Request Example HTTP request POST /v1/modules Headers Key Value Description Host string Domain name Accept */* Accepted media type Content-Length integer Length of the request body in octets Expect 100-continue Requires particular server behaviors Content-Type multipart/form-data Media type of request/response body Response Returns a status code. If an error occurs, the response body may include an error code and message with more details.\nError codes The server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description no_module Module wasn\u0026rsquo;t provided or could not be found invalid_module Module either corrupted or packaged files are wrong module_exists Module already in system min_redis_pack_version Module isn\u0026rsquo;t supported yet in this Redis pack unsupported_module_capabilities The module does not support required capabilities os_not_supported This module is not supported for this operating system dependencies_not_supported This endpoint does not support dependencies, see v2 Status codes Code Description 400 Bad Request Either missing module file or an invalid module file. Examples cURL $ curl -k -u \u0026#34;[username]:[password]\u0026#34; -X POST -F \u0026#34;module=@/tmp/rejson.Linux-ubuntu18.04-x86_64.2.0.8.zip\u0026#34; https://[host][:port]/v1/modules Python import requests url = \u0026#34;https://[host][:port]/v1/modules\u0026#34; files=[ (\u0026#39;module\u0026#39;, (\u0026#39;rejson.Linux-ubuntu18.04-x86_64.2.0.8.zip\u0026#39;, open(\u0026#39;/tmp/rejson.Linux-ubuntu18.04-x86_64.2.0.8.zip\u0026#39;,\u0026#39;rb\u0026#39;), \u0026#39;application/zip\u0026#39;) ) ] auth=(\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;POST\u0026#34;, url, auth=auth, files=files, verify=False) print(response.text) Upload module v2 POST /v2/modules Asynchronously uploads a new module and its dependencies to the cluster.\nThe request must contain a Redis module bundled using RedisModule Packer. If the module\u0026rsquo;s metadata includes a dependencies section, a /v2/modules request automatically uploads the dependencies.\nFor modules in Redis Stack, download the module from the Download Center. See Install a module on a cluster for more information.\nPermissions Permission name Roles update_cluster admin Request Example HTTP request POST /v2/modules Headers Key Value Description Host string Domain name Accept */* Accepted media type Content-Length integer Length of the request body in octets Expect 100-continue Requires particular server behaviors Content-Type multipart/form-data; Media type of request/response body Response Returns a module object with an additional action_uid field.\nYou can use the action_uid to track the progress of the module upload.\nExample JSON body { \u0026#34;action_uid\u0026#34;:\u0026#34;dfc0152c-8449-4b1c-9184-480ea7cb526c\u0026#34;, \u0026#34;author\u0026#34;:\u0026#34;RedisLabs\u0026#34;, \u0026#34;capabilities\u0026#34;:[ \u0026#34;types\u0026#34;, \u0026#34;crdb\u0026#34;, \u0026#34;failover_migrate\u0026#34;, \u0026#34;persistence_aof\u0026#34;, \u0026#34;persistence_rdb\u0026#34;, \u0026#34;clustering\u0026#34;, \u0026#34;backup_restore\u0026#34; ], \u0026#34;command_line_args\u0026#34;:\u0026#34;Plugin gears_python CreateVenv 1\u0026#34;, \u0026#34;config_command\u0026#34;:\u0026#34;RG.CONFIGSET\u0026#34;, \u0026#34;dependencies\u0026#34;:{ \u0026#34;gears_jvm\u0026#34;:{ \u0026#34;sha256\u0026#34;:\u0026#34;b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;http://example.com/redisgears_plugins/jvm_plugin/gears-jvm.linux-centos7-x64.0.1.0.tgz\u0026#34; }, \u0026#34;gears_python\u0026#34;:{ \u0026#34;sha256\u0026#34;:\u0026#34;22dca9cd75484cb15b8130db37f5284e22e3759002154361f72f6d2db46ee682\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;http://example.com/redisgears-python.linux-centos7-x64.1.2.1.tgz\u0026#34; } }, \u0026#34;description\u0026#34;:\u0026#34;Dynamic execution framework for your Redis data\u0026#34;, \u0026#34;display_name\u0026#34;:\u0026#34;RedisGears\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;user@example.com\u0026#34;, \u0026#34;homepage\u0026#34;:\u0026#34;http://redisgears.io\u0026#34;, \u0026#34;is_bundled\u0026#34;:false, \u0026#34;license\u0026#34;:\u0026#34;Redis Source Available License Agreement\u0026#34;, \u0026#34;min_redis_pack_version\u0026#34;:\u0026#34;6.0.0\u0026#34;, \u0026#34;min_redis_version\u0026#34;:\u0026#34;6.0.0\u0026#34;, \u0026#34;module_name\u0026#34;:\u0026#34;rg\u0026#34;, \u0026#34;semantic_version\u0026#34;:\u0026#34;1.2.1\u0026#34;, \u0026#34;sha256\u0026#34;:\u0026#34;2935ea53611803c8acf0015253c5ae1cd81391bbacb23e14598841e1edd8d28b\u0026#34;, \u0026#34;uid\u0026#34;:\u0026#34;98f255d5d33704c8e4e97897fd92e32d\u0026#34;, \u0026#34;version\u0026#34;:10201 } Error codes The server may return a JSON object with error_code and message fields that provide additional information.\nPossible error_code values include /v1/modules error codes and the following:\nCode Description invalid_dependency_data Provided dependencies have an unexpected format Status codes Code Description 200 OK Success, scheduled module upload. 400 Bad Request Module name or version does not exist. 404 Not Found Dependency not found. 500 Internal Server Error Failed to get dependency. Delete module v1 DELETE /v1/modules/{string: uid} Delete a module.\nIf the module has dependencies, use the v2 request instead.\nPermissions Permission name Roles update_cluster admin Request Example HTTP request DELETE /v1/modules/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The module\u0026rsquo;s unique ID. Response Returns a status code to indicate module deletion success or failure.\nError codes Code Description dependencies_not_supported You can use the following API endpoint to delete this module with its dependencies: /v2/modules/\u0026lt;uid\u0026gt; Status codes Code Description 200 OK Success, the module is deleted. 404 Not Found Attempting to delete a nonexistent module. 406 Not Acceptable The request is not acceptable. Delete module v2 DELETE /v2/modules/{string: uid} Delete a module with dependencies.\nPermissions Permission name Roles update_cluster admin Request Example HTTP request DELETE /v2/modules/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The module\u0026rsquo;s unique ID. Response Returns a JSON object with an action_uid that allows you to track the progress of module deletion.\nStatus codes Code Description 200 OK Success, scheduled module deletion. 404 Not Found Attempting to delete a nonexistent module. ","categories":["RS"]},{"uri":"/rs/networking/multi-ip-ipv6/","uriRel":"/rs/networking/multi-ip-ipv6/","title":"Multi-IP and IPv6","tags":[],"keywords":[],"description":"Information and requirements for using multiple IP addresses or IPv6 addresses with Redis Enterprise Software.","content":"Redis Enterprise Software (RS) supports server/instances/VMs with multiple IP addresses, as well as IPv6 addresses.\nRS related traffic can be logically and physically divided into internal traffic and external traffic:\n\u0026ldquo;Internal traffic\u0026rdquo; refers to internal cluster communications, such as communications between the nodes for cluster management purposes. \u0026ldquo;External traffic\u0026rdquo; refers to communications between the clients and the databases, as well as connections to the management UI in the browser. When only one IP address exists on a machine that serves as an RS node, it is used for both internal and external traffic.\nWhen more than one IP address exists on an RS node:\nOne of the IPv4 addresses is used for internal traffic Other IP addresses may only be used for external traffic As part of the node configuration process, if the machine has multiple IP addresses, you are required to assign one of the machine\u0026rsquo;s IPv4 addresses for internal traffic use, and assign one or more IPv4/IPv6 addresses for external traffic.\nIf at a later stage you would like to update the IP address allocation, run the relevant commands in rladmin command-line interface (CLI).\nIf you need to update the internal IP address in the OS, you must remove that node from the RS cluster, make the IP change, and then add the node back into the cluster.\nWhen manually configuring an internal address for a node, make sure the address is valid and bound to an active interface on the node. Failure to do so prevents the node from coming back online and rejoining the cluster.\nWhen configuring external addresses, it is possible to list external addresses that are not bound to an active interface, but are otherwise mapped or configured to route traffic to the node (AWS Elastic IPs, a load balancer VIP and so on).\nrladmin node address commands syntax: node addr set node external_addr set node external_addr [ add | remove ]\nWhere:\naddr - the internal address (can be used only when the node is offline) external_addr - external addresses Note: While joining a new node to a cluster during the node bootstrap process, when prompted to provide an IP of an existing node in the cluster, if you use the node\u0026rsquo;s IP, provide the node\u0026rsquo;s internal IP address. ","categories":["RS"]},{"uri":"/rs/networking/port-configurations/","uriRel":"/rs/networking/port-configurations/","title":"Network port configurations","tags":[],"keywords":[],"description":"This document describes the various network port ranges and their uses.","content":"All Redis Enterprise Software deployments span multiple physical/virtual nodes. You\u0026rsquo;ll need to keep several ports open between these nodes. This document describes the various port ranges and their uses.\nNote: Whenever you create a new database, you must verify that the ports assigned to the new database\u0026rsquo;s endpoints are open. The cluster will not perform this verification for you. Ports and port ranges used by Redis Enterprise Software Redis Enterprise Software\u0026rsquo;s port usage falls into three general categories:\nInternal: For traffic between or within cluster nodes External: For traffic from client applications or external monitoring resources Active-Active: For traffic to and from clusters hosting Active-Active databases Protocol Port Connection Source Description TCP 8001 Internal, External Traffic from application to Redis Enterprise Software Discovery Service TCP 8070, 8071 Internal, External Metrics exported and managed by the web proxy TCP 8443 Internal, External Secure (HTTPS) access to the management web UI TCP 9081 Internal Active-Active management (internal) TCP 9443 (Recommended), 8080 Internal, External, Active-Active REST API traffic, including cluster management and node bootstrap TCP 10000-19999 Internal, External, Active-Active Database traffic UDP 53, 5353 Internal, External DNS/mDNS traffic ICMP * Internal Connectivity checking between nodes TCP 1968 Internal Proxy traffic TCP 3333-3341, 3342-3344, 36379, 36380 Internal Internode communication TCP 20000-29999 Internal Database shard traffic TCP 8002, 8004, 8006 Internal System health monitoring TCP 8444, 9080 Internal Traffic between web proxy and cnm_http/cm Change the admin console port The Redis Enterprise Software admin console uses port 8443, by default. You can change this to a custom port as long as the new port is not in use by another process.\nTo change this port, run:\nrladmin cluster config cm_port \u0026lt;new-port\u0026gt; After changing the Redis Enterprise Software web UI port, you must connect any new node added to the cluster to the UI with the custom port number: https://newnode.mycluster.example.com:\u0026lt;nonstandard-port-number\u0026gt;\nChange the REST API port For the REST API, Redis Enterprise Software uses port 9443 (secure) and port 8080 (unsecure), by default. You can change this to a custom port as long as the new port is not in use by another process.\nTo change these ports, run:\nrladmin cluster config cnm_http_port \u0026lt;new-port\u0026gt; rladmin cluster config cnm_https_port \u0026lt;new-port\u0026gt; Disable HTTP support for API endpoints To harden deployments, you can disable the HTTP support for API endpoints that is supported by default. Before you disable HTTP support, make sure you migrate any scripts or proxy configurations that use HTTP to the encrypted API endpoint to prevent broken connections.\nTo disable HTTP support for API endpoints, run:\nrladmin cluster config http_support disabled After you disable HTTP support, traffic sent to the unencrypted API endpoint is blocked.\nHTTP to HTTPS redirection Starting with version 6.0.12, the automatic HTTP to HTTPS redirection is disabled. To poll metrics from the metrics_exporter or to access the admin console, use HTTPS in your request. HTTP requests won\u0026rsquo;t be automatically redirected to HTTPS for those services.\nNodes on different VLANs Nodes in the same cluster must reside on the same VLAN. If you can\u0026rsquo;t host the nodes on the same VLAN, then you must open all ports between them.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/","uriRel":"/rs/references/cli-utilities/rladmin/node/","title":"rladmin node","tags":[],"keywords":[],"description":"Manage nodes.","content":"rladmin node commands manage nodes in the cluster.\nCommand Description addr Sets a node\u0026#39;s internal IP address. enslave Changes a node\u0026#39;s resources to replicas. external_addr Configures a node\u0026#39;s external IP addresses. maintenance_mode Turns quorum-only mode on or off for a node. recovery_path Sets a node\u0026#39;s local recovery path. remove Removes a node from the cluster. snapshot Manages snapshots of the state of a node\u0026#39;s resources. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/node/","uriRel":"/rs/references/rest-api/objects/node/","title":"Node object","tags":[],"keywords":[],"description":"An object that represents a node in the cluster","content":"An API object that represents a node in the cluster.\nName Type/Value Description uid integer Cluster unique ID of node (read-only) accept_servers boolean (default: true) If true, no shards will be created on the node addr string Internal IP address of node architecture string Hardware architecture (read-only) bigredis_storage_path string Flash storage path (read-only) bigstore_driver \u0026lsquo;ibm-capi-ga1\u0026rsquo;\n\u0026lsquo;ibm-capi-ga2\u0026rsquo;\n\u0026lsquo;ibm-capi-ga4\u0026rsquo;\n\u0026lsquo;rocksdb\u0026rsquo; Bigstore driver name or none bigstore_size integer Storage size of bigstore storage (read-only) cores integer Total number of CPU cores (read-only) ephemeral_storage_path string Ephemeral storage path (read-only) ephemeral_storage_size number Ephemeral storage size (bytes) (read-only) external_addr complex object External IP addresses of node. GET /jsonschema to retrieve the object\u0026rsquo;s structure. max_listeners integer Maximum number of listeners on the node max_redis_servers integer Maximum number of shards on the node os_name string OS name (read-only) os_semantic_version string Full version number (read-only) os_version string Installed OS version (human-readable) (read-only) persistent_storage_path string Persistent storage path (read-only) persistent_storage_size number Persistent storage size (bytes) (read- only) public_addr string Public IP address of node rack_id string Rack ID where node is installed recovery_path string Recovery files path shard_count integer Number of shards on the node (read-only) shard_list array of integers Cluster unique IDs of all node shards software_version string Installed Redis Enterprise cluster software version (read-only) status \u0026lsquo;active\u0026rsquo;\n\u0026lsquo;decommissioning\u0026rsquo;\n\u0026lsquo;down\u0026rsquo;\n\u0026lsquo;provisioning\u0026rsquo; Node status (read-only) supported_database_versions [{ \"db_type\": string, \"version\": string }, ...] Versions of open source databases supported by Redis Enterprise Software on the node (read-only)\ndb_type: Type of database\nversion: Version of database system_time string System time (UTC) (read-only) total_memory integer Total memory of node (bytes) (read-only) uptime integer System uptime (seconds) (read-only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/statistics/node-metrics/","uriRel":"/rs/references/rest-api/objects/statistics/node-metrics/","title":"Node metrics","tags":[],"keywords":[],"description":"Documents the node metrics used with Redis Enterprise Software REST API calls.","content":" Metric name Type Description available_flash float Available flash on the node (bytes) available_memory float Available RAM on the node (bytes) avg_latency float Average latency of requests handled by endpoints on the node (micro-sec); returned only when there is traffic bigstore_free float Free space of backend flash (used by flash DB\u0026rsquo;s BigRedis) (bytes); returned only when BigRedis is enabled bigstore_iops float Rate of I/O operations against backend flash for all shards which are part of a flash-based DB (BigRedis) on the node (ops/sec); returned only when BigRedis is enabled bigstore_kv_ops float Rate of value read/write operations against backend flash for all shards which are part of a flash-based DB (BigRedis) on the node (ops/sec); returned only when BigRedis is enabled bigstore_throughput float Throughput of I/O operations against backend flash for all shards which are part of a flash-based DB (BigRedis) on the node (bytes/sec); returned only when BigRedis is enabled conns float Number of clients connected to endpoints on the node cpu_idle float CPU idle time portion (0-1, multiply by 100 to get percent) cpu_system float CPU time portion spent in kernel (0-1, multiply by 100 to get percent) cpu_user float CPU time portion spent by users-pace processes (0-1, multiply by 100 to get percent) cur_aof_rewrites float Number of current AOF rewrites by shards on this node egress_bytes float Rate of outgoing network traffic to the node (bytes/sec) ephemeral_storage_avail float Disk space available to Redis Enterprise processes on configured ephemeral disk (bytes) ephemeral_storage_free float Free disk space on configured ephemeral disk (bytes) free_memory float Free memory on the node (bytes) ingress_bytes float Rate of incoming network traffic to the node (bytes/sec) persistent_storage_avail float Disk space available to Redis Enterprise processes on configured persistent disk (bytes) persistent_storage_free float Free disk space on configured persistent disk (bytes) provisional_flash float Amount of flash available for new shards on this node, taking into account overbooking, max Redis servers, reserved flash, and provision and migration thresholds (bytes) provisional_memory float Amount of RAM available for new shards on this node, taking into account overbooking, max Redis servers, reserved memory, and provision and migration thresholds (bytes) total_req float Request rate handled by endpoints on the node (ops/sec) ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/node_checks_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/node_checks_job_settings/","title":"Node checks job settings object","tags":[],"keywords":[],"description":"Documents the node_checks_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the node checks schedule ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/node_identity/","uriRel":"/rs/references/rest-api/objects/bootstrap/node_identity/","title":"Node identity object","tags":[],"keywords":[],"description":"Documents the node_identity object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description bigstore_driver \u0026lsquo;ibm-capi-ga1\u0026rsquo;\n\u0026lsquo;ibm-capi-ga2\u0026rsquo;\n\u0026lsquo;ibm-capi-ga4\u0026rsquo;\n\u0026lsquo;rocksdb\u0026rsquo; Bigstore driver name or none identity identity object Node identity limits limits object Node limits paths paths object Storage paths object ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/","uriRel":"/rs/references/rest-api/requests/nodes/","title":"Nodes requests","tags":[],"keywords":[],"description":"Node requests","content":" Method Path Description GET /v1/nodes Get all cluster nodes GET /v1/nodes/{uid} Get a single cluster node PUT /v1/nodes/{uid} Update a node Get all nodes GET /v1/nodes Get all cluster nodes.\nPermissions Permission name Roles view_all_nodes_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /nodes Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON array of node objects.\nExample JSON body [ { \u0026#34;uid\u0026#34;: 1, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;uptime\u0026#34;: 262735, \u0026#34;total_memory\u0026#34;: 6260334592, \u0026#34;software_version\u0026#34;: \u0026#34;0.90.0-1\u0026#34;, \u0026#34;ephemeral_storage_size\u0026#34;: 20639797248, \u0026#34;persistent_storage_path\u0026#34;: \u0026#34;/var/opt/redislabs/persist\u0026#34;, \u0026#34;persistent_storage_size\u0026#34;: 20639797248, \u0026#34;os_version\u0026#34;: \u0026#34;Ubuntu 14.04.2 LTS\u0026#34;, \u0026#34;ephemeral_storage_path\u0026#34;: \u0026#34;/var/opt/redislabs/tmp\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;shard_count\u0026#34;: 23, \u0026#34;public_addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cores\u0026#34;: 4, \u0026#34;rack_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;supported_database_versions\u0026#34;: [ { \u0026#34;db_type\u0026#34;: \u0026#34;memcached\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.4.17\u0026#34; }, { \u0026#34;db_type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.6.16\u0026#34; }, { \u0026#34;db_type\u0026#34;: \u0026#34;redis\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.8.19\u0026#34; } ], \u0026#34;shard_list\u0026#34;: [1, 3, 4], \u0026#34;addr\u0026#34;: \u0026#34;10.0.3.61\u0026#34; }, { \u0026#34;uid\u0026#34;: 1, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] Status codes Code Description 200 OK No error Get node GET /v1/nodes/{int: uid} Get a single cluster node.\nPermissions Permission name Roles view_node_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /nodes/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the node requested. Response Returns a node object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;node:1\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Node UID does not exist Update node PUT /v1/nodes/{int: uid} Update a node object.\nCurrently, you can edit the following attributes:\naddr\nexternal_addr\nrecovery_path\naccept_servers\nNote: You can only update the addr attribute for offline nodes. Otherwise, the request returns an error. Permissions Permission name Roles update_node admin Request Example HTTP request PUT /nodes/1 Example JSON body { \u0026#34;addr\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;external_addr\u0026#34; : [ \u0026#34;192.0.2.24\u0026#34; ] } Request headers Key Value Description Host cluster.fqdn Domain name Accept application/json Accepted media type Content-Type application/json Media type of request/response body URL parameters Field Type Description uid integer The unique ID of the updated node. Body Field Type Description addr string Internal IP address of node external_addr JSON array External IP addresses of the node recovery_path string Path for recovery files accept_servers boolean If true, no shards will be created on the node Response If the request is successful, the body will be empty. Otherwise, it may contain a JSON object with an error code and error message.\nStatus codes Code Description 200 OK No error, the request has been processed. 406 Not Acceptable Update request cannot be processed. 400 Bad Request Bad content provided. Error codes Code Description node_not_found Node does not exist node_not_offline Attempted to change node address while it is online node_already_populated The node contains shards or endpoints, cannot disable accept_servers invalid_oss_cluster_port_mapping Cannot enable \u0026ldquo;accept_servers\u0026rdquo; since there are databases with \u0026ldquo;oss_cluster_port_mapping\u0026rdquo; that do not have a port configuration for the current node node_already_has_rack_id Attempted to change node\u0026rsquo;s rack_id when it already has one ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/ocsp/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/ocsp/","title":"rladmin cluster ocsp","tags":["configured"],"keywords":[],"description":"Manages OCSP.","content":"Manages OCSP configuration and verifies the status of a server certificate maintained by a third-party certificate authority (CA).\nocsp certificate_compatible Checks if the proxy certificate contains an OCSP URI.\nrladmin cluster ocsp certificate_compatible Parameters None\nReturns Returns the OCSP URI if it exists. Otherwise, it returns an error.\nExample $ rladmin cluster ocsp certificate_compatible Success. OCSP URI is http://responder.ocsp.url.com ocsp config Displays or updates OCSP configuration. Run the command without the set option to display the current configuration of a parameter.\nrladmin cluster ocsp config \u0026lt;OCSP parameter\u0026gt; [set \u0026lt;value\u0026gt;] Parameters Parameter Type/Value Description ocsp_functionality enabled\ndisabled Enables or turns off OCSP for the cluster query_frequency integer (range: 60-86400) (default: 3600) The time interval in seconds between OCSP queries to check the certificate\u0026rsquo;s status recovery_frequency integer (range: 60-86400) (default: 60) The time interval in seconds between retries after a failed query recovery_max_tries integer (range: 1-100) (default: 5) The number of retries before the validation query fails and invalidates the certificate responder_url string The OCSP server URL embedded in the proxy certificate (you cannot manually set this parameter) response_timeout integer (range: 1-60) (default: 1) The time interval in seconds to wait for a response before timing out Returns If you run the ocsp config command without the set option, it displays the specified parameter\u0026rsquo;s current configuration.\nExample $ rladmin cluster ocsp config recovery_frequency Recovery frequency of the OCSP server is 60 seconds $ rladmin cluster ocsp config recovery_frequency set 30 $ rladmin cluster ocsp config recovery_frequency Recovery frequency of the OCSP server is 30 seconds ocsp status Returns the latest cached status of the certificate\u0026rsquo;s OCSP response.\nrladmin cluster ocsp status Parameters None\nReturns Returns the latest cached status of the certificate\u0026rsquo;s OCSP response.\nExample $ rladmin cluster ocsp status OCSP certificate status is: REVOKED produced_at: Wed, 22 Dec 2021 12:50:11 GMT responder_url: http://responder.ocsp.url.com revocation_time: Wed, 22 Dec 2021 12:50:04 GMT this_update: Wed, 22 Dec 2021 12:50:11 GMT ocsp test_certificate Queries the OCSP server for the certificate\u0026rsquo;s latest status, then caches and displays the response.\nrladmin cluster ocsp test_certificate Parameters None\nReturns Returns the latest status of the certificate\u0026rsquo;s OCSP response.\nExample $ rladmin cluster ocsp test_certificate Initiating a query to OCSP server ...OCSP certificate status is: REVOKED produced_at: Wed, 22 Dec 2021 12:50:11 GMT responder_url: http://responder.ocsp.url.com revocation_time: Wed, 22 Dec 2021 12:50:04 GMT this_update: Wed, 22 Dec 2021 12:50:11 GMT ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/ocsp/","uriRel":"/rs/references/rest-api/objects/ocsp/","title":"OCSP object","tags":[],"keywords":[],"description":"An object that represents the cluster&#39;s OCSP configuration","content":"An API object that represents the cluster\u0026rsquo;s OCSP configuration.\nName Type/Value Description ocsp_functionality boolean (default: false) Enables or turns off OCSP for the cluster query_frequency integer (range: 60-86400) (default: 3600) The time interval in seconds between OCSP queries to check the certificate’s status recovery_frequency integer (range: 60-86400) (default: 60) The time interval in seconds between retries after the OCSP responder returns an invalid status for the certificate recovery_max_tries integer (range: 1-100) (default: 5) The number of retries before the validation query fails and invalidates the certificate responder_url string The OCSP server URL embedded in the proxy certificate (if available) (read-only) response_timeout integer (range: 1-60) (default: 1) The time interval in seconds to wait for a response before timing out ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/ocsp/","uriRel":"/rs/references/rest-api/requests/ocsp/","title":"OCSP requests","tags":[],"keywords":[],"description":"OCSP requests","content":" Method Path Description GET /v1/ocsp Get OCSP configuration PUT /v1/ocsp Update OCSP configuration Get OCSP configuration GET /v1/ocsp Gets the cluster\u0026rsquo;s OCSP configuration.\nRequired permissions Permission name view_ocsp_config Request Example HTTP request GET /ocsp Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns an OCSP configuration object.\nExample JSON body { \u0026#34;ocsp_functionality\u0026#34;: true, \u0026#34;responder_url\u0026#34;: \u0026#34;http://responder.ocsp.url.com\u0026#34;, \u0026#34;query_frequency\u0026#34;: 3800, \u0026#34;response_timeout\u0026#34;: 2, \u0026#34;recovery_frequency\u0026#34;: 80, \u0026#34;recovery_max_tries\u0026#34;: 20 } Error codes When errors occur, the server returns a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description ocsp_unsupported_by_capability Not all nodes support OCSP capability Status codes Code Description 200 OK Success 406 Not Acceptable Feature not supported in all nodes Update OCSP configuration PUT /v1/ocsp Updates the cluster\u0026rsquo;s OCSP configuration.\nRequired permissions Permission name config_ocsp Request Example HTTP request PUT /ocsp Example JSON body { \u0026#34;ocsp_functionality\u0026#34;: true, \u0026#34;query_frequency\u0026#34;: 3800, \u0026#34;response_timeout\u0026#34;: 2, \u0026#34;recovery_frequency\u0026#34;: 80, \u0026#34;recovery_max_tries\u0026#34;: 20 } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include an OCSP configuration object with updated fields in the request body.\nResponse Returns the updated OCSP configuration object.\nError codes When errors occur, the server returns a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description invalid_schema An illegal parameter or a parameter with an illegal value no_responder_url Tried to enable OCSP with no responder URL configured ocsp_unsupported_by_capability Not all nodes support OCSP capability Status codes Code Description 200 OK Success, OCSP config has been set 400 Bad Request Bad or missing configuration parameters 406 Not Acceptable Feature not supported in all nodes ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/ocsp_status/","uriRel":"/rs/references/rest-api/objects/ocsp_status/","title":"OCSP status object","tags":[],"keywords":[],"description":"An object that represents the cluster&#39;s OCSP status","content":"An API object that represents the cluster\u0026rsquo;s OCSP status.\nName Type/Value Description cert_status string Indicates the proxy certificate\u0026rsquo;s status: GOOD/REVOKED/UNKNOWN (read-only) responder_url string The OCSP responder URL this status came from (read-only) next_update string The expected date and time of the next certificate status update (read-only) produced_at string The date and time when the OCSP responder signed this response (read-only) revocation_time string The date and time when the certificate was revoked or placed on hold (read-only) this_update string The most recent time that the responder confirmed the current status (read-only) ","categories":["RS"]},{"uri":"/rs/references/compatibility/","uriRel":"/rs/references/compatibility/","title":"Redis Enterprise compatibility with open source Redis","tags":[],"keywords":[],"description":"Redis Enterprise compatibility with open source Redis.","content":"Both Redis Enterprise Software and Redis Enterprise Cloud are compatible with open source Redis (OSS Redis). Redis contributes extensively to the open source Redis project and uses it inside of Redis Enterprise. As a rule, we adhere to the open source project\u0026rsquo;s specifications and update Redis Enterprise with the latest version of open source Redis.\nRedis commands See Compatibility with open source Redis commands to learn which open source Redis commands are compatible with Redis Enterprise (Software and Cloud).\nConfiguration settings Compatibility with open source Redis configuration settings lists the open source Redis configuration settings supported by Redis Enterprise (Software and Cloud).\nRedis clients You can use any standard Redis client with Redis Enterprise.\nCompatibility with open source Redis Cluster API Redis Enterprise supports Redis OSS Cluster API if it is enabled for a database. For more information, see Enable OSS Cluster API.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/actions/optimize_shards_placement/","uriRel":"/rs/references/rest-api/requests/bdbs/actions/optimize_shards_placement/","title":"Optimize shards placement database action requests","tags":[],"keywords":[],"description":"Optimize shard placement requests","content":" Method Path Description GET /v1/bdbs/{uid}/actions/optimize_shards_placement Get optimized shards placement for a database Get optimized shards placement GET /v1/bdbs/{int: uid}/actions/optimize_shards_placement Get optimized shards placement for the given database.\nRequired permissions Permission name Roles view_bdb_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1/actions/optimize_shards_placement Response To rearrange the database shards, you can submit the blueprint returned in this response body as the shards_blueprint field in the PUT /bdbs/{uid} request.\nExample JSON body [ { \u0026#34;nodes\u0026#34;: [ { \u0026#34;node_uid\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; }, { \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;slave\u0026#34; } ], \u0026#34;slot_range\u0026#34;: \u0026#34;5461-10922\u0026#34; }, { \u0026#34;nodes\u0026#34;: [ { \u0026#34;node_uid\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; }, { \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;slave\u0026#34; } ], \u0026#34;slot_range\u0026#34;: \u0026#34;10923-16383\u0026#34; }, { \u0026#34;nodes\u0026#34;: [ { \u0026#34;node_uid\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; }, { \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;slave\u0026#34; } ], \u0026#34;slot_range\u0026#34;: \u0026#34;0-5460\u0026#34; } ] Headers Key Value Description Content-Length 352 Length of the request body in octets cluster-state-id 30 Cluster state ID Status codes Code Description 200 OK No error 404 Not Found Database UID does not exist 406 Not Acceptable Not enough resources in the cluster to host the database Rearrange database shards Use the blueprint returned by the GET /bdbs/{uid}/actions/optimize_shards_placement request as the value of the shards_blueprint field to rearrange the database shards.\nTo ensure that the optimized shard placement is relevant for the current cluster state, pass the cluster-state-id, taken from the response header of the GET request, in the PUT /bdbs/{uid} request headers.\nThe cluster will reject the update if its state was changed since the optimal shards placement was obtained.\nRequest Example HTTP request PUT /bdbs/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type cluster-state-id 30 Cluster state ID Example JSON body { \u0026#34;shards_blueprint\u0026#34;: [ { \u0026#34;nodes\u0026#34;: [ { \u0026#34;node_uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; } ], \u0026#34;slot_range\u0026#34;: \u0026#34;0-8191\u0026#34; }, \u0026#34;...\u0026#34; ] } Warning - If you submit such an optimized blueprint, it may cause strain on the cluster and its resources. Use with caution. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/users/password/","uriRel":"/rs/references/rest-api/requests/users/password/","title":"User password requests","tags":[],"keywords":[],"description":"User password requests","content":" Method Path Description PUT /v1/users/password Change an existing password POST /v1/users/password Add a new password DELETE /v1/users/password Delete a password Update password PUT /v1/users/password Reset the password list of an internal user to include a new password.\nRequest Example HTTP request PUT /users/password Example JSON body { \u0026#34;username\u0026#34;: \u0026#34;johnsmith\u0026#34;, \u0026#34;old_password\u0026#34;: \u0026#34;a password that exists in the current list\u0026#34;, \u0026#34;new_password\u0026#34;: \u0026#34;the new (single) password\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body The request must contain a single JSON object with the following fields:\nField Type Description username string Affected user (required) old_password string A password that exists in the current list (required) new_password string The new password (required) Response Returns a status code to indicate password update success or failure.\nError codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description password_not_complex The given password is not complex enough (Only work when the password_complexity feature is enabled). new_password_same_as_current The given new password is identical to one of the already existing passwords. Status codes Code Description 200 OK Success, password changed 400 Bad Request Bad or missing parameters. 401 Unauthorized The user is unauthorized. 404 Not Found Attempting to reset password to a non-existing user. Add password POST /v1/users/password Add a new password to an internal user\u0026rsquo;s passwords list.\nRequest Example HTTP request POST /users/password Example JSON body { \u0026#34;username\u0026#34;: \u0026#34;johnsmith\u0026#34;, \u0026#34;old_password\u0026#34;: \u0026#34;an existing password\u0026#34;, \u0026#34;new_password\u0026#34;: \u0026#34;a password to add\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body The request must contain a single JSON object with the following fields:\nField Type Description username string Affected user (required) old_password string A password that exists in the current list (required) new_password string The new (single) password (required) Response Returns a status code to indicate password creation success or failure. If an error occurs, the response body may include a more specific error code and message.\nError codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description password_not_complex The given password is not complex enough (Only work when the password_complexity feature is enabled). new_password_same_as_current The given new password is identical to one of the already existing passwords. Status codes Code Description 200 OK Success, new password was added to the list of valid passwords. 400 Bad Request Bad or missing parameters. 401 Unauthorized The user is unauthorized. 404 Not Found Attempting to add a password to a non-existing user. Delete password DELETE /v1/users/password Delete a password from an internal user\u0026rsquo;s passwords list.\nRequest Example HTTP request DELETE /users/password Example JSON body { \u0026#34;username\u0026#34;: \u0026#34;johnsmith\u0026#34;, \u0026#34;old_password\u0026#34;: \u0026#34;an existing password\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body The request must contain a single JSON with the following fields:\nField Type Description username string Affected user (required) old_password string Existing password to be deleted (required) Response Error codes When errors are reported, the server may return a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description cannot_delete_last_password Cannot delete the last password of a user Status codes Code Description 200 OK Success, new password was deleted from the list of valid passwords. 400 Bad Request Bad or missing parameters. 401 Unauthorized The user is unauthorized. 404 Not Found Attempting to delete a password to a non-existing user. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/passwords/","uriRel":"/rs/references/rest-api/requests/bdbs/passwords/","title":"Database passwords requests","tags":[],"keywords":[],"description":"Database password requests","content":" Method Path Description PUT /v1/bdbs/{uid}/passwords Update database password POST /v1/bdbs/{uid}/passwords Add database password DELETE /v1/bdbs/{uid}/passwords Delete database password Update database password PUT /v1/bdbs/{int: uid}/passwords Set a single password for the bdb\u0026rsquo;s default user (i.e., for AUTH \u0026lt;password\u0026gt; authentications).\nRequired permissions Permission name update_bdb Request Example HTTP request PUT /bdbs/1/passwords Example JSON body { \u0026#34;password\u0026#34;: \u0026#34;new password\u0026#34; } The above request resets the password of the bdb to ‘new password’.\nRequest headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database to update the password. Request body Field Type Description password string New password Response Returns a status code that indicates password update success or failure.\nStatus codes Code Description 200 OK The password was changed. 404 Not Found A nonexistent database. 406 Not Acceptable Invalid configuration parameters provided. Add database password POST /v1/bdbs/{int: uid}/passwords Add a password to the bdb\u0026rsquo;s default user (i.e., for AUTH \u0026lt;password\u0026gt; authentications).\nRequired permissions Permission name update_bdb Request Example HTTP request POST /bdbs/1/passwords Example JSON body { \u0026#34;password\u0026#34;: \u0026#34;password to add\u0026#34; } The above request adds a password to the bdb.\nRequest headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database to add password. Request body Field Type Description password string Password to add Response Returns a status code that indicates password creation success or failure.\nStatus codes Code Description 200 OK The password was added. 404 Not Found A nonexistent database. 406 Not Acceptable Invalid configuration parameters provided. Delete database password DELETE /v1/bdbs/{int: uid}/passwords Delete a password from the bdb\u0026rsquo;s default user (i.e., for AUTH \u0026lt;password\u0026gt; authentications).\nRequired permissions Permission name update_bdb Request Example HTTP request DELETE /bdbs/1/passwords Example JSON body { \u0026#34;password\u0026#34;: \u0026#34;password to delete\u0026#34; } The above request deletes a password from the bdb.\nRequest headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database to delete password. Request body Field Type Description password string Password to delete Response Returns a status code that indicates password deletion success or failure.\nStatus codes Code Description 200 OK The password was deleted. 404 Not Found A nonexistent database. 406 Not Acceptable Invalid configuration parameters provided. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/paths/","uriRel":"/rs/references/rest-api/objects/bootstrap/paths/","title":"Paths object","tags":[],"keywords":[],"description":"Documents the paths object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description bigstore_path string Bigredis storage path ccs_persistent_path string Persistent storage path of CCS ephemeral_path string Ephemeral storage path persistent_path string Persistent storage path ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/pdns_server/","uriRel":"/rs/references/rest-api/objects/services_configuration/pdns_server/","title":"PDNS server object","tags":[],"keywords":[],"description":"Documents the pdns_server object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the PDNS server ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/peer_stats/","uriRel":"/rs/references/rest-api/requests/bdbs/peer_stats/","title":"CRDB peer stats requests","tags":[],"keywords":[],"description":"Active-Active peer instance statistics requests","content":" Method Path Description GET /v1/bdbs/{bdb_uid}/peer_stats Get stats for all CRDB peer instances GET /v1/bdbs/{bdb_uid}/peer_stats/{uid} Get stats for a specific CRDB peer instance Get all CRDB peer stats GET /v1/bdbs/{bdb_uid}/peer_stats Get statistics for all peer instances of a local CRDB instance.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1/peer_stats?interval=5min Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description bdb_uid integer The unique ID of the local CRDB instance. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for all CRDB peer instances.\nExample JSON body { \u0026#34;peer_stats\u0026#34;: [ { \u0026#34;intervals\u0026#34;: [ { \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;egress_bytes_decompressed\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18528, \u0026#34;ingress_bytes_decompressed\u0026#34;: 185992, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.244, \u0026#34;pending_local_writes_max\u0026#34;: 0.0, \u0026#34;pending_local_writes_min\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:25:00Z\u0026#34; }, { \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;egress_bytes_decompressed\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:35:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18, \u0026#34;ingress_bytes_decompressed\u0026#34;: 192, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.0, \u0026#34;pending_local_writes_max\u0026#34;: 0.0, \u0026#34;pending_local_writes_min\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34; } ], \u0026#34;uid\u0026#34;: \u0026#34;3\u0026#34; } ] } Status codes Code Description 200 OK No error 404 Not Found Database does not exist. 406 Not Acceptable Database is not a CRDB. Get CRDB peer stats GET /v1/bdbs/{bdb_uid}/peer_stats/{int: uid} Get statistics for a specific CRDB peer instance.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1/peer_stats/3?interval=5min Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description bdb_uid integer The unique ID of the local CRDB instance. uid integer The peer instance uid, as specified in the CRDB instance list. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for a specific CRDB peer instance.\nExample JSON body { \u0026#34;intervals\u0026#34;: [ { \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;egress_bytes_decompressed\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18528, \u0026#34;ingress_bytes_decompressed\u0026#34;: 185992, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.244, \u0026#34;pending_local_writes_max\u0026#34;: 0.0, \u0026#34;pending_local_writes_min\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:25:00Z\u0026#34; }, { \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;egress_bytes_decompressed\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:35:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18, \u0026#34;ingress_bytes_decompressed\u0026#34;: 192, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.0, \u0026#34;pending_local_writes_max\u0026#34;: 0.0, \u0026#34;pending_local_writes_min\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34; } ], \u0026#34;uid\u0026#34;: \u0026#34;3\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Database or peer does not exist. 406 Not Acceptable Database is not a CRDB. ","categories":["RS"]},{"uri":"/rs/installing-upgrading/persistent-ephemeral-storage/","uriRel":"/rs/installing-upgrading/persistent-ephemeral-storage/","title":"Node persistent and ephemeral storage","tags":[],"keywords":[],"description":"","content":"For each node in the cluster, you can configure both persistent storage and ephemeral storage paths.\nPersistent storage is mandatory. It is used by the cluster to store information that needs to persist even if a shard or a node fails, including server logs, configurations, files. For example, if you configure persistence for a database, then the persistence information is stored in this location.\nThe persistent volume must be a SAN (Storage Area Network) using an EXT4 or XFS file system and be connected as an external storage volume.\nWhen using AOF persistence, we recommend that you use flash-based storage for the persistent volume.\nEphemeral storage is optional. If configured, the cluster stores temporary information that does not need to persist in the ephemeral storage. This improves performance and helps to reduce the load on the persistent storage.\nEphemeral storage must be a locally attached volume on each node.\nNote: Persistent and ephemeral storage discussed here is not related to Redis persistence or AWS ephemeral drives. For disk size requirements, see:\nHardware requirements for general guidelines regarding the ideal disk size each type of storage Disk size requirements for extreme write scenarios for special considerations when dealing with a high rate of write commands ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/placement/","uriRel":"/rs/references/cli-utilities/rladmin/placement/","title":"rladmin placement","tags":[],"keywords":[],"description":"Configures the shard placement policy for a database.","content":"Configures the shard placement policy for a specified database.\nrladmin placement db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } { dense | sparse } Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Configures shard placement for the specified database dense Places new shards on the same node as long as it has resources sparse Places new shards on the maximum number of available nodes within the cluster Returns Returns the new shard placement policy if the policy was changed successfully. Otherwise, it returns an error.\nUse rladmin status databases to verify that the failover completed.\nExample $ rladmin status databases DATABASES: DB:ID NAME TYPE STATUS SHARDS PLACEMENT REPLICATION PERSISTENCE ENDPOINT db:5 tr01 redis active 1 dense enabled aof redis-12000.cluster.local:12000 $ rladmin placement db db:5 sparse Shards placement policy is now sparse $ rladmin status databases DATABASES: DB:ID NAME TYPE STATUS SHARDS PLACEMENT REPLICATION PERSISTENCE ENDPOINT db:5 tr01 redis active 1 sparse enabled aof redis-12000.cluster.local:12000 ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bootstrap/policy/","uriRel":"/rs/references/rest-api/objects/bootstrap/policy/","title":"Policy object","tags":[],"keywords":[],"description":"Documents the policy object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description default_fork_evict_ram boolean (default: false) If true, the databases should evict data from RAM to ensure successful replication or persistence default_non_sharded_proxy_policy \u0026lsquo;single\u0026rsquo; \u0026lsquo;all-master-shards\u0026rsquo;\n\u0026lsquo;all-nodes\u0026rsquo; Default proxy_policy for newly created non-sharded databases\u0026rsquo; endpoints default_sharded_proxy_policy \u0026lsquo;single\u0026rsquo;\n\u0026lsquo;all-master-shards\u0026rsquo; \u0026lsquo;all-nodes\u0026rsquo; Default proxy_policy for newly created sharded databases\u0026rsquo; endpoints default_shards_placement \u0026lsquo;dense\u0026rsquo;\n\u0026lsquo;sparse\u0026rsquo; Default shards_placement for newly created databases rack_aware boolean Cluster rack awareness shards_overbooking boolean (default: true) If true, all databases\u0026rsquo; memory_size settings are ignored during shards placement ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/policy/","uriRel":"/rs/references/rest-api/requests/cluster/policy/","title":"Cluster policy requests","tags":[],"keywords":[],"description":"Cluster policy requests","content":" Method Path Description GET /v1/cluster/policy Get cluster policy settings PUT /v1/cluster/policy Update cluster policy settings Get cluster policy GET /v1/cluster/policy Gets the cluster\u0026rsquo;s current policy settings.\nRequired permissions Permission name view_cluster_info Request Example HTTP request GET /cluster/policy Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a cluster settings object.\nExample JSON body { \u0026#34;db_conns_auditing\u0026#34;: false, \u0026#34;default_non_sharded_proxy_policy\u0026#34;: \u0026#34;single\u0026#34;, \u0026#34;default_provisioned_redis_version\u0026#34;: \u0026#34;6.0\u0026#34;, \u0026#34;default_sharded_proxy_policy\u0026#34;: \u0026#34;single\u0026#34;, \u0026#34;default_shards_placement\u0026#34;: \u0026#34;dense\u0026#34;, \u0026#34;redis_upgrade_policy\u0026#34;: \u0026#34;major\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK Success Update cluster policy PUT /v1/cluster/policy Update cluster policy settings.\nRequired permissions Permission name update_cluster Request Example HTTP request PUT /cluster/policy Example JSON body { \u0026#34;default_shards_placement\u0026#34;: \u0026#34;sparse\u0026#34;, \u0026#34;default_sharded_proxy_policy\u0026#34;: \u0026#34;all-nodes\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a cluster settings object with updated fields in the request body.\nResponse Returns a status code that indicates the success or failure of the cluster settings update.\nStatus codes Code Description 200 OK Success 400 Bad Request Failed to set parameters ","categories":["RS"]},{"uri":"/rc/cloud-integrations/prometheus-integration/","uriRel":"/rc/cloud-integrations/prometheus-integration/","title":"Prometheus integration with Redis Cloud","tags":[],"keywords":[],"description":"To collect and display metrics data from your databases, you can connect your Prometheus or Grafana server to your Redis Cloud databases.","content":"To collect and display metrics data from your databases, you can connect your Prometheus or Grafana server to your Redis Cloud subscription.\nPrometheus is an open source systems monitoring and alerting toolkit that can scrape metrics from different sources. Grafana is an open source metrics dashboard and graph editor that can process Prometheus data. Redis Cloud has an Prometheus compatible endpoint available in order to pull metrics. Prometheus needs to connect to the internal server on port 8070. This is only available on the internal network so VPC peering is required. VPC peering is only available with Flexible or Annual subscriptions. Because VPC peering is not available on Fixed or Free subscriptions, Prometheus and Grafana cannot connect to databases on Fixed or Free subscriptions.\nFor more information on how Prometheus communicates with Redis Enterprise clusters, see Prometheus integration with Redis Enterprise Software.\nQuick start You can quickly set up Prometheus and Grafana for testing using the Prometheus and Grafana Docker images.\nPrerequisites Create a Flexible or Annual subscription with a database.\nSet up VPC peering for your subscription.\nExtract the Prometheus endpoint from the private endpoint to your database. The private endpoint is in the Redis Cloud console under the Configuration tab of your database. The Prometheus endpoint is on port 8070 of the internal server.\nFor example, if your private endpoint is:\nredis-12345.internal.\u0026lt;cluster_address\u0026gt;:12345 The Prometheus endpoint is:\ninternal.\u0026lt;cluster_address\u0026gt;:8070 Create an instance to run Prometheus and Grafana on the same provider as your Redis Cloud subscription (Amazon Web Services or Google Cloud Project). This instance must:\nExist in the same region as your Redis Cloud subscription. Connect to the VPC subnet that is peered with your Redis Cloud subscription. Allow outbound connections to port 8070, so that Prometheus can scrape the Redis Cloud server for data. Allow inbound connections to port 9090 for Prometheus and 3000 for Grafana. Set up Prometheus To get started with custom monitoring with Prometheus on Docker:\nCreate a directory on the Prometheus instance called prometheus and create a prometheus.yml file in that directory.\nAdd the following contents to prometheus.yml. Replace \u0026lt;instance_ip\u0026gt; with the IP address of the instance and replace \u0026lt;prometheus_endpoint\u0026gt; with the Prometheus endpoint.\nglobal: scrape_interval: 15s evaluation_interval: 15s # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: \u0026#34;prometheus-stack-monitor\u0026#34; # Load and evaluate rules in this file every \u0026#39;evaluation_interval\u0026#39; seconds. #rule_files: # - \u0026#34;first.rules\u0026#34; # - \u0026#34;second.rules\u0026#34; scrape_configs: # scrape Prometheus itself - job_name: prometheus scrape_interval: 10s scrape_timeout: 5s static_configs: - targets: [\u0026#34;\u0026lt;instance_ip\u0026gt;:9090\u0026#34;] # scrape Redis Cloud - job_name: redis-cloud scrape_interval: 30s scrape_timeout: 30s metrics_path: / scheme: https static_configs: - targets: [\u0026#34;\u0026lt;prometheus_endpoint\u0026gt;:8070\u0026#34;] Create a docker-compose.yml file with instructions to set up the Prometheus and Grafana Docker images.\nversion: \u0026#39;3\u0026#39; services: prometheus-server: image: prom/prometheus ports: - 9090:9090 volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml grafana-ui: image: grafana/grafana ports: - 3000:3000 environment: - GF_SECURITY_ADMIN_PASSWORD=secret links: - prometheus-server:prometheus To start the containers, run:\n$ docker compose up -d To check that all the containers are up, run: docker ps\nIn your browser, sign in to Prometheus at http://\u0026lt;instance_ip\u0026gt;:9090 to make sure the server is running.\nSelect Status and then Targets to check that Prometheus is collecting data from the Redis Cloud cluster.\nIf Prometheus is connected to the cluster, you can type node_up in the Expression field on the Prometheus home page to see the cluster metrics.\nSee Prometheus Metrics for a list of metrics that Prometheus collects from Redis Enterprise clusters.\nSet up Grafana Once the Prometheus and Grafana Docker containers are running, and Prometheus is connected to your Redis Cloud subscription, you can set up your Grafana dashboards.\nSign in to Grafana. If you installed Grafana with Docker, go to http://\u0026lt;instance_ip\u0026gt;:3000 and sign in with:\nUsername: admin Password: secret In the Grafana configuration menu, select Data Sources.\nSelect Add data source.\nSelect Prometheus from the list of data source types.\nEnter the Prometheus information:\nName: redis-cloud URL: http://\u0026lt;your prometheus address\u0026gt;:9090 Access: Server Note: If the network port is not accessible to the Grafana server, select the Browser option from the Access menu. In a testing environment, you can select Skip TLS verification. Add dashboards for cluster, node, and database metrics. To add preconfigured dashboards:\nIn the Grafana dashboards menu, select Manage. Select Import. Copy and enter one of the following configuration files into the Paste JSON field. database.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;refresh\u0026#34;: false, \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 121, \u0026#34;panels\u0026#34;: [ { \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$status\u0026lt;b\u0026gt;\\n\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$newStatus\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;id\u0026#34;: 9, \u0026#34;links\u0026#34;: [], \u0026#34;mode\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;span\u0026#34;: 2, \u0026#34;style\u0026#34;: { \u0026#34;font-size\u0026#34;: \u0026#34;72pt\u0026#34; }, \u0026#34;title\u0026#34;: \u0026#34;Status for BDB:$bdb\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 4, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb used memory\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Keys\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 6, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;scalar(bdb_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Connections\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 7, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(listener_total_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Listeners\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 12, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_no_of_keys{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}) or count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb #Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 258, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_write_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_write_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_read_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_read_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_avg_other_latency{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_other_latency\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;max_over_time(bdb_avg_latency_max{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_avg_latency (max over $aggregation)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg latency for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 55, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;listener_acc_latency{bdb=~\\\u0026#34;$bdb.*\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / listener_total_started_res{bdb=~\\\u0026#34;$bdb.*\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg listener latency for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 44, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*limit.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;used_memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_memory_limit{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026lt; bdb_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} * 1.5\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;limit (when low, near used)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Memory for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 306, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 10, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;listener_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;endpoint {{proxy}} node: {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb Connections\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 14, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_other_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;other requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;read requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_req{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;write requests\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_total_req_max{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests (max)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;max_over_time(bdb_total_req_max{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[$aggregation])\\n\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total requests (max over $aggregation)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Requests[total,read,write,other] for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 16, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_egress_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ingress_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb Ingress/Egress\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 43, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_main_thread_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_main_thread_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;main threads (masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_shard_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_shard_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all threads (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_fork_cpu_system{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_fork_cpu_user{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all forks (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_main_thread_cpu_system_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_main_thread_cpu_user_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;main threads max (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_shard_cpu_system_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_shard_cpu_user_max{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;all threads max (masters)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum((delta(redis_process_main_thread_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_main_thread_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Slave main threads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum((delta(redis_process_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Slave all threads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB CPU utilization for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 13, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(delta(redis_process_cpu_system_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60*100 or redis_cpu_percent{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard: {{redis}} role: {{role}} node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU usage by shards for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 53, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*maxmemory.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, role:{{role}}, node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_maxmemory{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;maxmemory: redis:{{redis}}, bdb:{{bdb}}, role:{{role}}, node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Sizes of shards for bdb:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 196, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 17, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} or redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, role: {{role}}, node:{{node}}, slots:{{slots}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;# keys/shard\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 11, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;num of keys\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_db0_expires{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;num of volatile keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;ram keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;disk keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;(sum(max(redis_bigdb0_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_disk{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) + sum(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))) - sum(redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;overlapping keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_dirty{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_dirty{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dirty keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(max(redis_bigdb0_clean{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_clean{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;clean keys\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;#Keys for BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 20, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_evicted_objects{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB:$bdb\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Evicted objects for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 21, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_expired_objects{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB:$bdb\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Expired objects for BDB:$bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 202, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 48, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_rewrites{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}[$aggregation]) !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard AOF Rewrites / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 57, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_bgsave_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BGSAVE {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_rewrite_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;AOFRW {{redis}} node {{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;forks\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 49, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_misses{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Read Misses\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_misses{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Write Misses\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_read_hits{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Read Hits\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_write_hits{cluster=\\\u0026#34;$cluster\\\u0026#34;, bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Write Hits\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Read/Write Misses\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 23, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_fragmentation{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} or redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} ({{role}})\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard RSS fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 26, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(bdb_read_hits{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_write_hits{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) / (bdb_read_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_write_req{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;DB hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DB Hit Ratio (hits / requests)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 47, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_replicaof_syncer_local_ingress_lag_time{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB $bdb src_id {{src_id}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_crdt_syncer_local_ingress_lag_time{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB $bdb crdt src_id {{src_id}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Syncer lag\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 34, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / count(redis_mem_fragmentation_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis_mem_fragmentation_ratio\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Shard allocator fragmentation\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 2400 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_resident{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard allocator rss\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB:$bdb total fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 50, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_allocator_active\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard allocator fragmentation % (defraggable)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 30, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_active_defrag_running{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis active defrag running\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 22, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_delayed_fsync{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} ({{role}})\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_aof_delayed_fsync\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Delayed fsync - (events / $aggregation)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 58, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_allocator_active{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_allocated{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}} role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_allocator_active\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard allocator fragmentation bytes (defraggable)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: true, \u0026#34;height\u0026#34;: 285, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 51, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*limit.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb_used_ram\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_limit{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;limit (when low, near used)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB (ROF) used RAM for $bdb\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 25, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;read hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;write hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;del hit ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) / (bdb_big_fetch_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_fetch_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_fetch_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;read hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_write_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_write_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;write hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / (bdb_big_del_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} + bdb_big_del_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} ) \u0026gt; 0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;del hit ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DB ram hit ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Positive is RAM, Negative is Flash\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 52, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/User*/\u0026#34;, \u0026#34;dashes\u0026#34;: true } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]))/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;RAM Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]) * -1)/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Flash Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_user_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]))/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;User RAM Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(delta(redis_big_user_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}[1m]) * -1)/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;User Flash Ops for Redis\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB RAM:Flash access ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Positive is RAM, Negative is Flash\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 54, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_big_io_ratio_redis{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[1m])/60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;RAM Ops for Redis {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_big_io_ratio_flash{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}[1m]) / -60\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Flash Ops for Redis {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;RAM:Flash access ratio\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_read_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Reads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_write_bytes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Writes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Flash IO Banwidth\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 18, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_reads{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Reads\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_writes{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Writes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_io_dels{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Deletes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Flash IOPS\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 33, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_wait_busy_key{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis wait busy key / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 31, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_blocking_reads{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis blocking reads / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 37, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;frag ratio\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_bigstore{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;used\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_bigstore{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} * bdb_disk_frag_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;actual\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_disk_frag_ratio{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;frag ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB used flash\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 29, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_postponed_clients{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB postponed clients\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 27, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_blocked_clients{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Number of blocked clients\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_big_blocked_clients\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB blocked clients\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;ratiojj\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Ratio\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;actual (bytes)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 120 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} / bdb_ram_limit{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Ratio\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_ram_overhead{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;} / redis_max_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;,bdb=\\\u0026#34;$bdb\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis: {{redis}} role:{{role}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB ram overhead\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 32, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(redis_blocking_writes{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis blocking writes\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 36, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_big_inst_avg_read_io_queue{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis big inst avg read io queue\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 35, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_big_inst_avg_write_io_queue{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB sum redis big inst avg write io queue\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 40, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_flash{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;objects in flash (sum of all master shards)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;objects in ram (sum of all master shards)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB objects(values) in FLASH\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 46, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/BDB.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/average.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/lowest.*/\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / bdb_no_of_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB % values in RAM (masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average master shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average slave shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis) * -1) *-1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest master shard % values in RAM - redsi:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min((max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster)) / on (redis,bdb,cluster) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis) * -1) *-1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest slave shard % values in RAM - redsi:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} / redis_bigdb0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest master shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} / redis_bigdb0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest slave shard % values in RAM\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 10 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB % values in RAM\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 39, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*average.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*lowest.*/\u0026#34;, \u0026#34;linewidth\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_bigstore_objs_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}} (sum of masters)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average number of values in ram per slave shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;average number of values in ram per master shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per slave shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per master shard\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 10 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;}) by (redis,bdb,cluster) \u0026lt; on(cluster,bdb,redis) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;slave\\\u0026#34;} ) by (bdb, redis) * -1) * -1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per slave shard - redis:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(1, min(max(redis_bigdb0_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;} or redis_bigdb_ram{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (redis,bdb,cluster) \u0026lt; on(cluster,bdb,redis) redis_db0_keys{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;,role=\\\u0026#34;master\\\u0026#34;}) by (bdb, redis) * -1) * -1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;lowest number of values in ram per master shard - redis:{{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard objects(values) in RAM\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 42, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_ram_dirty_evictions{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB dirty RAM evictions\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 41, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_big_ram_clean_evictions{bdb=\\\u0026#34;$bdb\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{$bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;BDB clean RAM evictions\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 56, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: false, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/stalls/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;/slowdowns/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 4, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_comp_started{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;compactions\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_started{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;flushes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_writes_slowdown{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;slowdowns / $aggregation\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_rocks_flush_writes_stop{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;stalls / $aggregation\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;RocksDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;events\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;ROF Metrics\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: \u0026#34;BDB Id\u0026#34;, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;bdb\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;}, bdb)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_status{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}, status)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;newStatus\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(bdb_up{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}, status)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;auto\u0026#34;: true, \u0026#34;auto_count\u0026#34;: 100, \u0026#34;auto_min\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;current\u0026#34;: { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, \u0026#34;hide\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;aggregation interval\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;aggregation\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;6h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;6h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;12h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;12h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;7d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;14d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;14d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30d\u0026#34; } ], \u0026#34;query\u0026#34;: \u0026#34;1m,10m,30m,1h,6h,12h,1d,7d,14d,30d\u0026#34;, \u0026#34;refresh\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;interval\u0026#34; } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;BDB Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 176 } node.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 174, \u0026#34;panels\u0026#34;: [ { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}) by (bdb))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# BDBs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: false }, \u0026#34;id\u0026#34;: 26, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;, role=\\\u0026#34;master\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Master Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: false }, \u0026#34;id\u0026#34;: 7, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;, role=\\\u0026#34;slave\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Slave Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: 2, \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: true, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 9, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_system{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} + node_cpu_user{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 7200 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;70,85\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CPU Utilization\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;70%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 10, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 2, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Free RAM\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;b style=\\\u0026#34;font-size: 20pt\\\u0026#34;\u0026gt;$version\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;id\u0026#34;: 14, \u0026#34;links\u0026#34;: [], \u0026#34;mode\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;span\u0026#34;: 2, \u0026#34;title\u0026#34;: \u0026#34;Node version\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 312, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;decimals\u0026#34;: 2, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 3, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: false, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*dmcproxy.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;stack\u0026#34;: false }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*shards.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;stack\u0026#34;: false } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34; node_cpu_user{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} + node_cpu_system{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;user+system\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;1- node_cpu_idle{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} - node_cpu_system{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} - node_cpu_user{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} unless node_cpu_nice{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;other\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 300 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_steal{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;steal\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_nice{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;nice\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_cpu_irqs{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;irqs\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;dmcproxy_process_cpu_usage_percent{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}/100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dmcproxy (100% = 1 core)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34; node_cpu_iowait{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;iowait\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_cpu_usage_percent{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}/100)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shards (100% = 1 core)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;H\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU Usage on node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;percents\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;1.5\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 297, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;display the egress and ingress traffic on the node\u0026#39;s NIC\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 4, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: null, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_ingress_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_egress_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;egress for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node $node In/Out Traffic\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;max\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;display the egress and ingress traffic on the node\u0026#39;s NIC\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 32, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total requests for node {{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node $node req/sec\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;max\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Egress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Ingress bytes/sec \u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 12, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/Ingress.*/\u0026#34;, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_egress_bytes{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Egress listener {{proxy}} for {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_ingress_bytes{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Ingress listener {{proxy}} for {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 listeners Egress/Ingrees\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 27, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*max.*/\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;fill\u0026#34;: 0 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(5, listener_acc_latency{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;} / listener_total_started_res{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Avg listener latency for node:$node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 325, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, redis_used_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} or redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, bdb:{{bdb}}, role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;dmcproxy_process_resident_memory_bytes{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;dmcproxy\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 shards by used ram\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 13, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;redis:{{redis}}, bdb:{{bdb}}, role:{{role}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Size of shards on node (Ability for sorting like by size)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;BIGSTORE IOPS\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 322, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 11, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 active listeners by connections\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 17, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 active listeners by requests\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 243, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 6, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}) by (bdb)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;bdb:{{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;#Shards per BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 8, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;available memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Available Memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 29, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total shards memory\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_ram{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total shards ram\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;total shards memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 30, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;active / allocated (defraggable)\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;resident / active (purgable)\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;active defrag running\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;process rss / allocator rss\u0026#34;, \u0026#34;dashes\u0026#34;: true, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_allocated{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - 1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active / allocated (defraggable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} / redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - 1\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;resident / active (purgable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_allocated{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active - allocated (defraggable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;} - redis_allocator_active{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;resident - active (purgable)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;avg(redis_active_defrag_running{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})/100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;active defrag running\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_resident_memory_bytes{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) - sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;process rss - allocator rss\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_process_resident_memory_bytes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_process_resident_memory_bytes{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) / sum(redis_allocator_resident{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;process rss / allocator rss\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_process_resident_memory_bytes\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;fragmentation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 16, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;(delta(redis_process_cpu_system_seconds_total{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]) + delta(redis_process_cpu_user_seconds_total{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[60s]))/60*100\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard: {{redis}} role: {{role}} bdb: {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard\u0026#39;s CPU on Node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 2, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(dmcproxy_process_cpu_user_seconds_total{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;CPU User\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;rate(dmcproxy_process_cpu_system_seconds_total{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;CPU System\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;dmcproxy_process_cpu_system_seconds_total\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;DMC Proxy CPU Usage\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;percentunit\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 22, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;/.*cow.*/\u0026#34;, \u0026#34;fill\u0026#34;: 0 }, { \u0026#34;alias\u0026#34;: \u0026#34;/.*mem.*/\u0026#34;, \u0026#34;fill\u0026#34;: 8 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_last_cow_size{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard last RDB cow - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_last_cow_size{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard last AOF cow - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_mem_aof_buffer{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard aofrw buffers - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_mem_clients_slaves{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;shard salve buffers - redis: {{redis}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;fork memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 25, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_rdb_bgsave_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BGSAVE {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;redis_aof_rewrite_in_progress{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}!=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;AOFRW {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;forks\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 23, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;delta(redis_aof_rewrites{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}[$aggregation]) !=0\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Redis {{redis}} bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Shard AOF Rewrites / $aggregation\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 6, \u0026#34;id\u0026#34;: 24, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(redis_aof_delayed_fsync{node=\\\u0026#34;$node\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}[$aggregation]))\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;delayed fsync\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;delayed fsync - (events / $aggregation)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: true, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 20, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_throughput{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_throughput on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_read_throughput{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_throughput on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE throughput\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 18, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_read_iops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_iops on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;node_flash_read_iops\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_iops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_iops on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE IOPS\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 21, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_flash_write_time{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} / 1000\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_write_time on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34; node_flash_read_time {cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;} / 1000\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_flash_read_time on node:{{node}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE wait time\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;node_bigstore_kv_ops{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;node_bigstore_kv_ops on node:{{node}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node BIGSTORE KV\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 31, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 3, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_disk{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total used flash\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_disk_allocation{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;total flash allocation\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 240 }, { \u0026#34;expr\u0026#34;: \u0026#34;node_bigstore_free{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;hide\u0026#34;: true, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;free flash\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 240 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Node used flash\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;ROF metrics\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;}, node)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 3, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 2, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;version\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;,node=\\\u0026#34;$node\\\u0026#34;}, cnm_version)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 0, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;auto\u0026#34;: true, \u0026#34;auto_count\u0026#34;: 100, \u0026#34;auto_min\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;current\u0026#34;: { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, \u0026#34;hide\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;interval aggregation\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;aggregation\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;$__auto_interval\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30m\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;6h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;6h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;12h\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;12h\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;7d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;14d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;14d\u0026#34; }, { \u0026#34;selected\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;30d\u0026#34; } ], \u0026#34;query\u0026#34;: \u0026#34;1m,10m,30m,1h,6h,12h,1d,7d,14d,30d\u0026#34;, \u0026#34;refresh\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;interval\u0026#34; } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Node Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 81 } cluster.json { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS1\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;4.4.0-pre1\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Singlestat\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Table\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;editable\u0026#34;: true, \u0026#34;gnetId\u0026#34;: null, \u0026#34;graphTooltip\u0026#34;: 1, \u0026#34;hideControls\u0026#34;: false, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;targetBlank\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;dashboards\u0026#34; } ], \u0026#34;refresh\u0026#34;: false, \u0026#34;rows\u0026#34;: [ { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 137, \u0026#34;panels\u0026#34;: [ { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 1, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count (bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 1, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 7200 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;# BDBs\u0026#34;, \u0026#34;transparent\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 3, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(redis_connected_clients{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Shards\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: false, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 5, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: false }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;# Nodes\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; }, { \u0026#34;cacheTimeout\u0026#34;: null, \u0026#34;colorBackground\u0026#34;: false, \u0026#34;colorValue\u0026#34;: true, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(247, 43, 43, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;gauge\u0026#34;: { \u0026#34;maxValue\u0026#34;: 100, \u0026#34;minValue\u0026#34;: 0, \u0026#34;show\u0026#34;: false, \u0026#34;thresholdLabels\u0026#34;: false, \u0026#34;thresholdMarkers\u0026#34;: true }, \u0026#34;id\u0026#34;: 14, \u0026#34;interval\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;dashUri\u0026#34;: \u0026#34;db/alerts\u0026#34;, \u0026#34;dashboard\u0026#34;: \u0026#34;Alerts\u0026#34;, \u0026#34;includeVars\u0026#34;: true, \u0026#34;keepTime\u0026#34;: true, \u0026#34;targetBlank\u0026#34;: true, \u0026#34;title\u0026#34;: \u0026#34;Alerts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dashboard\u0026#34; } ], \u0026#34;mappingType\u0026#34;: 1, \u0026#34;mappingTypes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;value to text\u0026#34;, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;range to text\u0026#34;, \u0026#34;value\u0026#34;: 2 } ], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;nullText\u0026#34;: null, \u0026#34;postfix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postfixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prefixFontSize\u0026#34;: \u0026#34;50%\u0026#34;, \u0026#34;rangeMaps\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;null\u0026#34; } ], \u0026#34;span\u0026#34;: 3, \u0026#34;sparkline\u0026#34;: { \u0026#34;fillColor\u0026#34;: \u0026#34;rgba(31, 118, 189, 0.18)\u0026#34;, \u0026#34;full\u0026#34;: false, \u0026#34;lineColor\u0026#34;: \u0026#34;rgb(31, 120, 193)\u0026#34;, \u0026#34;show\u0026#34;: true }, \u0026#34;tableColumn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;count(ALERTS{cluster=\\\u0026#34;$cluster\\\u0026#34;, alertstate=\\\u0026#34;firing\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 14400 } ], \u0026#34;thresholds\u0026#34;: \u0026#34;0:1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Number of active Alerts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;singlestat\u0026#34;, \u0026#34;valueFontSize\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;null\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;=\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;valueName\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 277, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 3, \u0026#34;id\u0026#34;: 2, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;alias\u0026#34;: \u0026#34;Total Available\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Provisional (No Overbooking)\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;alias\u0026#34;: \u0026#34;Total Provisional\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(redis_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Used\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_available_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Available\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_provisional_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Provisional\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;sum(node_provisional_memory_no_overbooking{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Total Provisional (No Overbooking)\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Memory\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: true, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 10, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;hideEmpty\u0026#34;: true, \u0026#34;hideZero\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: false, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: false, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(delta(bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;}[1m])) by (bdb) \u0026gt; 100 * 1024*1024\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB #{{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Grow over 1min (Over 100MiB)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 1, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 280, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 4, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;hideEmpty\u0026#34;: true, \u0026#34;hideZero\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;rightSide\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ {} ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, sum(bdb_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (bdb) \u0026gt; 0)\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;#{{bdb}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Requests by BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;ops\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;error\u0026#34;: false, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 6, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: false, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ {} ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;100 - node_cpu_idle{cluster=\\\u0026#34;$cluster\\\u0026#34;, node=\\\u0026#34;$node\\\u0026#34;}*100\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;CPU Usage for node $node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;msResolution\u0026#34;: false, \u0026#34;shared\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [ \u0026#34;total\u0026#34; ] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 1, \u0026#34;id\u0026#34;: 15, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_total_req{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener {{proxy}} for bdb {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Requests Per Listener\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 2, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: false } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 19, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;repeat\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 12, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;redis_used_memory{node=\\\u0026#34;$node\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB: {{bdb}} Redis: {{redis}}\u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;redis_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 600 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Mem By redis for Node $node\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 246, \u0026#34;panels\u0026#34;: [ { \u0026#34;columns\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;Avg\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;avg\u0026#34; }, { \u0026#34;text\u0026#34;: \u0026#34;Current\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;current\u0026#34; } ], \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;filterNull\u0026#34;: false, \u0026#34;fontSize\u0026#34;: \u0026#34;100%\u0026#34;, \u0026#34;id\u0026#34;: 23, \u0026#34;links\u0026#34;: [], \u0026#34;pageSize\u0026#34;: null, \u0026#34;scroll\u0026#34;: true, \u0026#34;showHeader\u0026#34;: true, \u0026#34;sort\u0026#34;: { \u0026#34;col\u0026#34;: 2, \u0026#34;desc\u0026#34;: true }, \u0026#34;span\u0026#34;: 4, \u0026#34;styles\u0026#34;: [ { \u0026#34;dateFormat\u0026#34;: \u0026#34;YYYY-MM-DD HH:mm:ss\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;Time\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;colorMode\u0026#34;: null, \u0026#34;colors\u0026#34;: [ \u0026#34;rgba(245, 54, 54, 0.9)\u0026#34;, \u0026#34;rgba(237, 129, 40, 0.89)\u0026#34;, \u0026#34;rgba(50, 172, 45, 0.97)\u0026#34; ], \u0026#34;decimals\u0026#34;: 2, \u0026#34;pattern\u0026#34;: \u0026#34;/.*/\u0026#34;, \u0026#34;thresholds\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;decbytes\u0026#34; } ], \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB {{bdb}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 2400 } ], \u0026#34;title\u0026#34;: \u0026#34;List of BDBs\u0026#34;, \u0026#34;transform\u0026#34;: \u0026#34;timeseries_aggregations\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;table\u0026#34; }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 28, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 8, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;bdb_used_memory{cluster=\\\u0026#34;$cluster\\\u0026#34;} \u0026gt; 30 * 1024 * 1024\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_used_memory\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Used Mem By BDB over 30MB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; }, { \u0026#34;collapse\u0026#34;: false, \u0026#34;height\u0026#34;: 250, \u0026#34;panels\u0026#34;: [ { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 5, \u0026#34;id\u0026#34;: 35, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: true, \u0026#34;steppedLine\u0026#34;: true, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, bdb_conns{cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Conns for BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_conns\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 }, { \u0026#34;expr\u0026#34;: \u0026#34;topk(10, listener_conns{bdb=\\\u0026#34;$bdb\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;listener #{{listener}}\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top 10 Used conns By BDB\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;id\u0026#34;: 47, \u0026#34;legend\u0026#34;: { \u0026#34;alignAsTable\u0026#34;: true, \u0026#34;avg\u0026#34;: true, \u0026#34;current\u0026#34;: true, \u0026#34;max\u0026#34;: true, \u0026#34;min\u0026#34;: true, \u0026#34;show\u0026#34;: true, \u0026#34;sort\u0026#34;: \u0026#34;current\u0026#34;, \u0026#34;sortDesc\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: true }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;links\u0026#34;: [], \u0026#34;nullPointMode\u0026#34;: \u0026#34;null as zero\u0026#34;, \u0026#34;percentage\u0026#34;: false, \u0026#34;pointradius\u0026#34;: 5, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;span\u0026#34;: 6, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;topk(10,bdb_avg_latency{cluster=\\\u0026#34;$cluster\\\u0026#34;} )\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;intervalFactor\u0026#34;: 2, \u0026#34;legendFormat\u0026#34;: \u0026#34;Latency for BDB: {{bdb}} \u0026#34;, \u0026#34;metric\u0026#34;: \u0026#34;bdb_conns\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;step\u0026#34;: 1200 } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeFrom\u0026#34;: null, \u0026#34;timeShift\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Top bdb by latency in cluster\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;buckets\u0026#34;: null, \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: null, \u0026#34;logBase\u0026#34;: 1, \u0026#34;max\u0026#34;: null, \u0026#34;min\u0026#34;: null, \u0026#34;show\u0026#34;: true } ] } ], \u0026#34;repeat\u0026#34;: null, \u0026#34;repeatIteration\u0026#34;: null, \u0026#34;repeatRowId\u0026#34;: null, \u0026#34;showTitle\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Dashboard Row\u0026#34;, \u0026#34;titleSize\u0026#34;: \u0026#34;h6\u0026#34; } ], \u0026#34;schemaVersion\u0026#34;: 14, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;RLEC\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up,cluster)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 1, \u0026#34;tagValuesQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false }, { \u0026#34;allValue\u0026#34;: null, \u0026#34;current\u0026#34;: {}, \u0026#34;datasource\u0026#34;: \u0026#34;${DS_PROMETHEUS1}\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: true, \u0026#34;label\u0026#34;: null, \u0026#34;multi\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: \u0026#34;label_values(node_up{cluster=\\\u0026#34;$cluster\\\u0026#34;}, node)\u0026#34;, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sort\u0026#34;: 3, \u0026#34;tagValuesQuery\u0026#34;: null, \u0026#34;tags\u0026#34;: [], \u0026#34;tagsQuery\u0026#34;: null, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;useTags\u0026#34;: false } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-7d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;7d\u0026#34;, \u0026#34;30d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;utc\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Cluster Dashboard\u0026#34;, \u0026#34;version\u0026#34;: 38 } In the Import options, select the redis-cloud data source and select Import. These examples create sample dashboards. For more information about configuring dashboards, see the Grafana documentation.\n","categories":["RC"]},{"uri":"/rs/references/rest-api/objects/proxy/","uriRel":"/rs/references/rest-api/objects/proxy/","title":"Proxy object","tags":[],"keywords":[],"description":"An object that represents a proxy in the cluster","content":"An API object that represents a proxy in the cluster.\nName Type/Value Description uid integer Unique ID of the proxy (read-only) backlog integer TCP listen queue backlog client_keepcnt integer Client TCP keepalive count client_keepidle integer Client TCP keepalive idle client_keepintvl integer Client TCP keepalive interval conns integer Number of connections duration_usage_threshold integer, (range: 10-300) Max number of threads dynamic_threads_scaling boolean Automatically adjust the number of threads ignore_bdb_cconn_limit boolean Ignore client connection limits ignore_bdb_cconn_output_buff_limits boolean Ignore buffer limit max_listeners integer Max number of listeners max_servers integer Max number of Redis servers max_threads integer, (range: 1-256) Max number of threads max_worker_client_conns integer Max client connections per thread max_worker_server_conns integer Max server connections per thread max_worker_txns integer Max in-flight transactions per thread threads integer, (range: 1-256) Number of threads threads_usage_threshold integer, (range: 50-99) Max number of threads ","categories":["RS"]},{"uri":"/rs/networking/private-public-endpoints/","uriRel":"/rs/networking/private-public-endpoints/","title":"Enable private and public database endpoints","tags":[],"keywords":[],"description":"Describes how to enable public and private endpoints for databases on a cluster.","content":"By default, Redis Enterprise Software databases expose a single endpoint, e.g. cluster.com (FQDN).\nWhen you create a cluster via the UI, you can configure it to expose private and public endpoints. This is common for environments such as cloud platforms and enterprises.\nWhen doing so, the cluster creates an additional FQDN, e.g. internal.cluster.com for private network (e.g. VPC or an internal network), while the cluster.com FQDN can be used by a public network (e.g. the internet).\nThis configuration can be enabled at cluster creation only. Once the cluster is created, it is possible to add an additional FQDN in a differnet domain only, e.g. cluster.io.\nFollow these steps to enable private and public endpoints:\nVerify that the IP addresses are bound to the server/instance.\nWhen setting up the cluster via the UI, select the Enable private and public endpoint support setting. It appears in the Cluster configuration section of the Node configuration screen.\nIf this setting is not enabled when the cluster is created, databases on the cluster support only a single endpoint.\nConfigure the public IP of the machine to be used for external traffic in the node configuration.\nConfigure private IP to be used for both internal and external traffic in the node configuration so it can be used for private database endpoints.\nWhen you finish, both sets of endpoints are available for databases in the cluster.\nUse the Configuration tab of a database on the cluster to verify availability of private and public endpoints.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdbs/purge/","uriRel":"/rs/references/rest-api/requests/crdbs/purge/","title":"CRDB purge requests","tags":[],"keywords":[],"description":"Purge removed Active-Active database requests","content":" Method Path Description PUT /v1/crdbs/{crdb_guid}/purge Purge data from an instance that was forcibly removed from the Active-Active database Purge data from removed instance PUT /v1/crdbs/{crdb_guid}/purge Purge the data from an instance that was removed from the Active-Active database by force.\nWhen you force the removal of an instance from an Active-Active database, the removed instance keeps the data and configuration according to the last successful synchronization.\nTo delete the data and configuration from the forcefully removed instance you must use this API (Must be executed locally on the removed instance).\nRequest Example HTTP request PUT /crdbs/1/purge URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Request body Field Type Description instances array of integers Array of unique instance IDs Response Returns a CRDB task object.\nStatus codes Code Description 200 OK Action was successful. 400 Bad Request The request is invalid or malformed. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Configuration, instance, or Active-Active database not found. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/purge-instance/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/purge-instance/","title":"crdb-cli crdb purge-instance","tags":[],"keywords":[],"description":"Deletes data from a local instance and removes it from the Active-Active database.","content":"Deletes data from a local instance and removes the instance from the Active-Active database.\ncrdb-cli crdb purge-instance --crdb-guid \u0026lt;guid\u0026gt; --instance-id \u0026lt;instance-id\u0026gt; [ --no-wait ] Once this command finishes, the other replicas must remove this instance with crdb-cli crdb remove-instance --force.\nParameters Parameter Value Description crdb-guid string The GUID of the database (required) instance-id string The ID of the local instance (required) no-wait Does not wait for the task to complete Returns Returns the task ID of the task that is purging the local instance.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the instance to be purged and return finished.\nExample $ crdb-cli crdb purge-instance --crdb-guid db6365b5-8aca-4055-95d8-7eb0105c0b35 --instance-id 2 Task add0705c-87f1-4c28-ad6a-ab5d98e00c58 created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/recover/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/recover/","title":"rladmin cluster recover","tags":["non-configured"],"keywords":[],"description":"Recovers a cluster from a backup file.","content":"Recovers a cluster from a backup file. The default location of the configuration backup file is /var/opt/redislabs/persist/ccs/ccs-redis.rdb.\nrladmin cluster recover filename \u0026lt;recovery filename\u0026gt; [ ephemeral_path \u0026lt;path\u0026gt; ] [ persistent_path \u0026lt;path\u0026gt; ] [ ccs_persistent_path \u0026lt;path\u0026gt; ] [ rack_id \u0026lt;ID\u0026gt; ] [ override_rack_id ] [ node_uid \u0026lt;number\u0026gt; ] [ flash_enabled ] [ flash_path \u0026lt;path\u0026gt; ] [ addr \u0026lt;IP address\u0026gt; ] [ external_addr \u0026lt;IP address\u0026gt; ] Parameters Parameter Type/Value Description addr IP address Sets a node\u0026rsquo;s internal IP address. If not provided, the node sets the address automatically. (optional) ccs_persistent_path filepath Path to the location of CCS snapshots (default is the same as persistent_path) (optional) external_addr IP address Sets a node\u0026rsquo;s external IP address. If not provided, the node sets the address automatically. (optional) ephemeral_path filepath (default: /var/opt/redislabs) Path to an ephemeral storage location (optional) filename filepath Backup file to use for recovery flash_enabled Enables flash storage (optional) flash_path filepath (default: /var/opt/redislabs/flash) Path to the flash storage location in case the node does not support CAPI (required if flash_enabled) node_uid integer (default: 1) Specifies which node will recover first and become master (optional) override_rack_id Changes to a new rack, specified by rack_id (optional) persistent_path filepath Path to the persistent storage location (optional) rack_id string Switches to the specified rack (optional) Returns Returns ok if the cluster recovered successfully. Otherwise, it returns an error message.\nExample $ rladmin cluster recover filename /tmp/persist/ccs/ccs-redis.rdb node_uid 1 rack_id 5 Initiating cluster recovery... ok ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/recover/","uriRel":"/rs/references/cli-utilities/rladmin/recover/","title":"rladmin recover","tags":[],"keywords":[],"description":"Recovers databases in recovery mode.","content":"Recovers databases in recovery mode.\nrecover all Recovers all databases in recovery mode.\nrladmin recover all [ only_configuration ] Parameters Parameters Type/Value Description only_configuration Recover database configuration without data Returns Returns Completed successfully if the database was recovered. Otherwise, returns an error.\nrecover db Recovers a specific database in recovery mode.\nrladmin recover db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } [ only_configuration ] Parameters Parameters Type/Value Description db db:\u0026lt;id\u0026gt; name Database to recover only_configuration Recover database configuration without data Returns Returns Completed successfully if the database was recovered. Otherwise, returns an error.\nrecover list Shows a list of all databases that are currently in recovery mode.\nrladmin recover list Parameters None\nReturns Displays a list of all recoverable databases. If no databases are in recovery mode, returns No recoverable databases found.\nExample $ rladmin recover list DATABASES IN RECOVERY STATE: DB:ID NAME TYPE SHARDS REPLICATION PERSISTENCE STATUS db:5 tr01 redis 1 enabled aof missing-files db:6 tr02 redis 4 enabled snapshot ready recover s3_import Imports current database snapshot files from an AWS S3 bucket to a directory on the node.\n$ rladmin recover s3_import s3_bucket \u0026lt;bucket name\u0026gt; [ s3_prefix \u0026lt;prefix\u0026gt; ] s3_access_key_id \u0026lt;access key\u0026gt; s3_secret_access_key \u0026lt;secret access key\u0026gt; local_path \u0026lt;path\u0026gt; Parameters Parameters Type/Value Description s3_bucket string S3 bucket name s3_prefix string S3 object prefix s3_access_key_id string S3 access key ID s3_secret_access_key string S3 secret access key local_path filepath Local import path where all database snapshots will be imported Returns Returns Completed successfully if the database files were imported. Otherwise, returns an error.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/recovery-path/","uriRel":"/rs/references/cli-utilities/rladmin/node/recovery-path/","title":"rladmin node recovery_path set","tags":[],"keywords":[],"description":"Sets a node&#39;s local recovery path.","content":"Sets the node\u0026rsquo;s local recovery path, which specifies the directory where persistence files are stored. You can use these persistence files to recover a failed database.\nrladmin node \u0026lt;ID\u0026gt; recovery_path set \u0026lt;path\u0026gt; Parameters Parameter Type/Value Description node integer Sets the recovery path for the specified node path filepath Path to the folder where persistence files are stored Returns Returns Updated successfully if the recovery path was set. Otherwise, it returns an error.\nExample $ rladmin node 2 recovery_path set /var/opt/redislabs/persist/redis Updated successfully. ","categories":["RS"]},{"uri":"/","uriRel":"/","title":"Redis Documentation","tags":[],"keywords":[],"description":"","content":" Redis Enterprise Cloud\nRedis Enterprise Cloud delivers a fully managed Redis database offering hosted on major public cloud services. With Redis Enterprise Cloud, you get all of the features of Redis Enterprise, including: Redis and Redis Stack support Linear scalability Instant failover, backups, and recovery Predictable performance 24/7 monitoring and support Get started Use the Quick start to create a free subscription and create your first database. Connect with redis-cli Connect with Redis client Connect with RedisInsight Subscriptions Learn about the types of subscriptions.\nRedis Enterprise Software\nRedis Enterprise is a self-managed, enterprise-grade version of Redis. With Redis Enterprise, you get many enterprise-grade capabilities, including: Linear scalability High availability, backups, and recovery Predictable performance 24/7 support You can run Redis Enterprise Software in an on-premises data center or on your preferred cloud platform. Get started Build a small-scale cluster with the Redis Enterprise Software container image. Get started Docker Get started with Active-Active Install \u0026amp; setup Install \u0026amp; set up a Redis Enterprise Software cluster.\nRedis Enterprise for Kubernetes\nThe Redis Enterprise operators allows you to use Redis Enterprise for Kubernetes.\nRedis Stack and modules\nRedis develops several modules that extend the core Redis feature set. Some of the features these modules provide include querying, indexing and full-text search, JSON support, and probabalistic data structures. Redis Stack lets you install and leverage modules quickly; it enables access to multiple modules in a single Redis database. Redis Enterprise Software and Redis Enterprise Cloud support all capabilities of Redis Stack. Each module includes a quick start guide.\nRedisInsight\nRedisInsight is a free GUI for Redis that is available on all platforms (Windows, Mac, Linux, and Docker) and works with all variants of Redis. RedisInsight allows you to: View performance metrics for your Redis instance with the Overview tool View data structures visually with the Browser tool Manage basic properties of your Redis cluster such as cluster node timeout, IP, or port with the Cluster Management tool Run commands with a REPL (read-eval-print-loop) with the the CLI tool Analyze memory usage with the Memory Analysis tool Identify and troubleshoot bottlenecks with the Slowlog tool Edit the configuration of your Redis instance with the Configuration tool RedisInsight also has support for several Redis modules, including RedisGraph, RedisTimeSeries, and RediSearch.\nGlossary\nA, B access control list (ACL) Allows you to manage permissions based on key patterns. More info: redis.io/topics/acl; ACL wikipedia; Database access control; Update database ACLs; Passwords, users, and roles Active-Active database (CRDB) Geo-distributed databases that span multiple Redis Enterprise Software clusters. Active-Active databases, also known as conflict-free replicated databases (CRDB), depend on multi-master replication (MMR) and conflict-free replicated data types (CRDTs) to power a simple development experience for geo-distributed applications.\nEmbeds\n","categories":[]},{"uri":"/rs/clusters/optimize/oss-cluster-api/","uriRel":"/rs/clusters/optimize/oss-cluster-api/","title":"Redis OSS Cluster API Architecture","tags":[],"keywords":[],"description":"You can use Redis OSS Cluster API improve performance and let applications stay current with cluster topology changes.","content":" Redis OSS Cluster API reduces access times and latency with near-linear scalability. The Redis OSS Cluster API provides a simple mechanism for Redis clients to know the cluster topology.\nClients must first connect to the master node to get the cluster topology, and then they connect directly to the Redis proxy on each node that hosts a master shard.\nNote: You must use a client that supports the OSS cluster API to connect to a database that has the OSS cluster API enabled. You can use Redis OSS Cluster API along with other Redis Enterprise Software high availability to get high performance with low latency and let applications stay current with cluster topology changes, including add node, remove node, and node failover.\nFor more about working with the OSS Cluster API, see Using the OSS Cluster API.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/redis_acl/","uriRel":"/rs/references/rest-api/objects/redis_acl/","title":"Redis ACL object","tags":[],"keywords":[],"description":"An object that represents a Redis access control list (ACL)","content":"An API object that represents a Redis access control list (ACL)\nName Type/Value Description uid integer Object\u0026rsquo;s unique ID account_id integer SM account ID acl string Redis ACL\u0026rsquo;s string action_uid string Action UID. If it exists, progress can be tracked by the GET /actions/{uid} API (read-only) name string Redis ACL\u0026rsquo;s name ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/redis_acls/","uriRel":"/rs/references/rest-api/requests/redis_acls/","title":"Redis access control list (ACL) requests","tags":[],"keywords":[],"description":"Redis access control list (ACL) requests","content":" Method Path Description GET /v1/redis_acls Get all Redis ACLs GET /v1/redis_acls/{uid} Get a single Redis ACL PUT /v1/redis_acls/{uid} Update a Redis ACL POST /v1/redis_acls Create a new Redis ACL DELETE /v1/redis_acls/{uid} Delete a Redis ACL Get all Redis ACLs GET /v1/redis_acls Get all Redis ACL objects.\nPermissions Permission name Roles view_all_redis_acls_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /redis_acls Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON array of Redis ACL objects.\nExample JSON body [ { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Full Access\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;+@all ~*\u0026#34; }, { \u0026#34;uid\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Read Only\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;+@read ~*\u0026#34; }, { \u0026#34;uid\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;Not Dangerous\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;+@all -@dangerous ~*\u0026#34; }, { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; } ] Status codes Code Description 200 OK No error 501 Not Implemented Cluster doesn\u0026rsquo;t support redis_acl yet. Get Redis ACL GET /v1/redis_acls/{int: uid} Get a single Redis ACL object.\nPermissions Permission name Roles view_redis_acl_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /redis_acls/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The object\u0026rsquo;s unique ID. Response Returns a Redis ACL object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; } Status codes Code Description 200 OK Success. 403 Forbidden Operation is forbidden. 404 Not Found redis_acl does not exist. 501 Not Implemented Cluster doesn\u0026rsquo;t support redis_acl yet. Update Redis ACL PUT /v1/redis_acls/{int: uid} Update an existing Redis ACL object.\nPermissions Permission name Roles update_redis_acl admin Request Example HTTP request PUT /redis_acls/17 Example JSON body { \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo -@dangerous\u0026#34; } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a Redis ACL object with updated fields in the request body.\nResponse Returns the updated Redis ACL object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo -@dangerous\u0026#34; } Error codes Code Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists invalid_param A parameter has an illegal value Status codes Code Description 200 OK Success, redis_acl was updated. 400 Bad Request Bad or missing configuration parameters. 404 Not Found Attempting to change a non-existent redis_acl. 409 Conflict Cannot change a read-only object 501 Not Implemented Cluster doesn\u0026rsquo;t support redis_acl yet. Create Redis ACL POST /v1/redis_acls Create a new Redis ACL object.\nPermissions Permission name Roles create_redis_acl admin Request Example HTTP request POST /redis_acls Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a Redis ACL object in the request body.\nResponse Returns the newly created Redis ACL object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; } Error codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists missing_field A needed field is missing invalid_param A parameter has an illegal value Status codes Code Description 200 OK Success, redis_acl is created. 400 Bad Request Bad or missing configuration parameters. 501 Not Implemented Cluster doesn\u0026rsquo;t support redis_acl yet. Examples cURL curl -k -u \u0026#34;[username]:[password]\u0026#34; -X POST \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; }\u0026#39; \\ https://[host][:port]/v1/redis_acls Python import requests import json url = \u0026#34;https://[host][:port]/v1/redis_acls\u0026#34; headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } payload = json.dumps({ \u0026#34;name\u0026#34;: \u0026#34;Geo\u0026#34;, \u0026#34;acl\u0026#34;: \u0026#34;~* +@geo\u0026#34; }) auth=(\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;POST\u0026#34;, url, auth=auth, headers=headers, payload=payload, verify=False) print(response.text) Delete Redis ACL DELETE /v1/redis_acls/{int: uid} Delete a Redis ACL object.\nPermissions Permission name Roles delete_redis_acl admin Request Example HTTP request DELETE /redis_acls/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The redis_acl unique ID. Response Returns a status code that indicates the Redis ACL deletion success or failure.\nStatus codes Code Description 200 OK Success, the redis_acl is deleted. 406 Not Acceptable The request is not acceptable. 409 Conflict Cannot delete a read-only object 501 Not Implemented Cluster doesn\u0026rsquo;t support redis_acl yet. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/redis_cleanup_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/redis_cleanup_job_settings/","title":"Redis cleanup job settings object","tags":[],"keywords":[],"description":"Documents the redis_cleanup_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the Redis cleanup schedule ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/redis-cli/","uriRel":"/rs/references/cli-utilities/redis-cli/","title":"redis-cli","tags":[],"keywords":[],"description":"Run Redis commands.","content":"The redis-cli command-line utility lets you interact with a Redis database. With redis-cli, you can run Redis commands directly from the command-line terminal or with interactive mode.\nIf you want to run Redis commands without redis-cli, you can connect to a database with RedisInsight and use the built-in CLI prompt instead.\nInstall redis-cli When you install Redis Enterprise Software or open source Redis, it also installs the redis-cli command-line utility.\nTo learn how to install Redis and redis-cli, see the following installation guides:\nOpen source Redis\nRedis Enterprise Software\nRedis Enterprise Software with Docker\nConnect to a database To run Redis commands with redis-cli, you need to connect to your Redis database.\nConnect from a node If you have SSH access to a node in a Redis cluster, you can run redis-cli directly from the node:\nUse SSH to sign in to a node in the Redis Enterprise cluster.\nConnect to the database with redis-cli:\n$ redis-cli -p \u0026lt;port\u0026gt; Connect remotely If you have redis-cli installed on your local machine, you can use it to connect to a remote Redis database. You will need to provide the database\u0026rsquo;s connection details, such as the hostname or IP address, port, and password.\n$ redis-cli -h \u0026lt;endpoint\u0026gt; -p \u0026lt;port\u0026gt; -a \u0026lt;password\u0026gt; You can also provide the password with the REDISCLI_AUTH environment variable instead of the -a option:\n$ export REDISCLI_AUTH=\u0026lt;password\u0026gt; $ redis-cli -h \u0026lt;endpoint\u0026gt; -p \u0026lt;port\u0026gt; Connect with Docker If your Redis database runs in a Docker container, you can use docker exec to run redis-cli commands:\n$ docker exec -it \u0026lt;Redis container name\u0026gt; redis-cli -p \u0026lt;port\u0026gt; Basic use You can run redis-cli commands directly from the command-line terminal:\n$ redis-cli -p \u0026lt;port\u0026gt; \u0026lt;Redis command\u0026gt; For example, you can use redis-cli to test your database connection and store a new Redis string in the database:\n$ redis-cli -p 12000 PING PONG $ redis-cli -p 12000 SET mykey \u0026#34;Hello world\u0026#34; OK $ redis-cli -p 12000 GET mykey \u0026#34;Hello world\u0026#34; For more information, see Command line usage.\nInteractive mode In redis-cli interactive mode, you can:\nRun any redis-cli command without prefacing it with redis-cli. Enter ? for more information about how to use the HELP command and set redis-cli preferences. Enter HELP followed by the name of a command for more information about the command and its options. Press the Tab key for command completion. Enter exit or quit or press Control+D to exit interactive mode and return to the terminal prompt. This example shows how to start interactive mode and run Redis commands:\n$ redis-cli -p 12000 127.0.0.1:12000\u0026gt; PING PONG 127.0.0.1:12000\u0026gt; SET mykey \u0026#34;Hello world\u0026#34; OK 127.0.0.1:12000\u0026gt; GET mykey \u0026#34;Hello world\u0026#34; More info Redis CLI documentation Redis commands reference ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/users/refresh_jwt/","uriRel":"/rs/references/rest-api/requests/users/refresh_jwt/","title":"Refresh JWT requests","tags":[],"keywords":[],"description":"Refresh JW token requests","content":" Method Path Description POST /v1/users/refresh_jwt Get a new authentication token Get a new authentication token POST /v1/users/refresh_jwt Generate a new JSON Web Token (JWT) for authentication.\nTakes a valid token and returns the new token generated by the request.\nRequest Example HTTP request POST /users/refresh_jwt Request headers Key Value Description Host cnm.cluster.fqdn Domain name Authorization JWT eyJ5bGciOiKIUzI1NiIsInR5cCI6IkpXVCJ9.\neyJpYXViOjE0NjU0NzU0ODYsInVpZFI1IjEiLCJ\nleHAiOjE0NjU0Nz30OTZ9.2xYXumd1rDoE0e\ndFzcLElMOHsshaqQk2HUNgdsUKxMU Valid JSON Web Token (JWT) Request body Field Type Description ttl integer Time to live - The amount of time in seconds the token will be valid (optional) Response Returns a JSON object that contains the generated access token.\nExample JSON body { \u0026#34;access_token\u0026#34;: \u0026#34;eyJ5bGciOiKIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXViOjE0NjU0NzU0ODYsInVpZFI1IjEiLCJleHAiOjE0NjU0Nz30OTZ9.2xYXumd1rDoE0edFzcLElMOHsshaqQk2HUNgdsUKxMU\u0026#34; } Status codes Code Description 200 OK A new token is given. 401 Unauthorized The token is invalid or password has expired. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/remove/","uriRel":"/rs/references/cli-utilities/rladmin/node/remove/","title":"rladmin node remove","tags":[],"keywords":[],"description":"Removes a node from the cluster.","content":"Removes the specified node from the cluster.\nrladmin node \u0026lt;ID\u0026gt; remove Parameters Parameter Type/Value Description node integer The node to remove from the cluster Returns Returns OK if the node was removed successfully. Otherwise, it returns an error.\nUse rladmin status nodes to verify that the node was removed.\nExample $ rladmin status nodes CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.12 198.51.100.1 3d99db1fdf4b 5/100 6 14.26GB/19.54GB 10.67GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.13 198.51.100.2 fc7a3d332458 4/100 6 14.26GB/19.54GB 10.71GB/16.02GB 6.2.12-37 OK node:3 slave 192.0.2.14 b87cc06c830f 1/120 6 14.26GB/19.54GB 10.7GB/16.02GB 6.2.12-37 OK $ rladmin node 3 remove Performing remove action on node:3: 100% OK CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS *node:1 master 192.0.2.12 198.51.100.1 3d99db1fdf4b 5/100 6 14.34GB/19.54GB 10.74GB/16.02GB 6.2.12-37 OK node:2 slave 192.0.2.13 198.51.100.2 fc7a3d332458 5/100 6 14.34GB/19.54GB 10.74GB/16.02GB 6.2.12-37 OK ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/remove-instance/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/remove-instance/","title":"crdb-cli crdb remove-instance","tags":[],"keywords":[],"description":"Removes a peer replica from an Active-Active database.","content":"Removes a peer replica instance from the Active-Active database and deletes the instance and its data from the participating cluster.\ncrdb-cli crdb remove-instance --crdb-guid \u0026lt;guid\u0026gt; --instance-id \u0026lt;instance-id\u0026gt; [ --force ] [ --no-wait ] If the cluster cannot communicate with the instance that you want to remove, you can:\nUse the --force option to remove the instance from the Active-Active database without purging the data from the instance.\nRun crdb-cli crdb purge-instance from the removed instance to delete the Active-Active database and its data.\nParameters Parameter Value Description crdb-guid string The GUID of the database (required) instance-id string The ID of the local instance to remove (required) force Removes the instance without purging data from the instance. If \u0026ndash;force is specified, you must run crdb-cli crdb purge-instance from the removed instance. no-wait Does not wait for the task to complete Returns Returns the task ID of the task that is deleting the instance.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the instance to be removed and return finished.\nExample $ crdb-cli crdb remove-instance --crdb-guid db6365b5-8aca-4055-95d8-7eb0105c0b35 --instance-id 2 --force Task b1eba5ba-90de-49e9-8678-d66daa1afb51 created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/databases/import-export/replica-of/","uriRel":"/rs/databases/import-export/replica-of/","title":"Replica Of geo-distributed Redis","tags":[],"keywords":[],"description":"Replica Of","content":"In Redis Enterprise, the Replica Of feature provides active-passive geo-distribution to applications for read-only access to replicas of the data set from different geographical locations. The Redis Enterprise implementation of active-passive replication is called Replica Of.\nIn Replica Of, an administrator designates a database as a replica (destination) of one or more databases (sources). After the initial data load from source to destination is completed, all write commands are synchronized from the sources to the destination. Replica Of lets you distribute the read load of your application across multiple databases or synchronize the database, either within Redis Enterprise or external to Redis Enterprise, to another database.\nYou can create Active-Passive databases on Redis Enterprise Software or Redis Cloud.\nActive-Active Geo-Distribution (CRDB) provides these benefits and also provides write access to all of the database replicas.\nWarning - Configuring a database as a replica of the database that it replicates creates a cyclical replication and is not supported. The Replica Of is defined in the context of the destination database by specifying the source databases.\nA destination database can have a maximum of thirty-two (32) source databases.\nIf only one source is defined, then the command execution order in the source is kept in the destination. However, when multiple sources are defined, commands that are replicated from the source databases are executed in the order in which they reach the destination database. As a result, commands that were executed in a certain order when compared across source databases might be executed in a different order on the destination database.\nNote: The Replica Of feature should not be confused with the in-memory Database replication feature, which is used for creating a master / replica configuration that enables ensuring database high-availability. For a quick overview of Replica Of capabilities watch this quick video.\nReplication process When a database is defined as a replica of another database, all its existing data is deleted and replaced by data that is loaded from the source database.\nOnce the initial data load is completed, an ongoing synchronization process takes place to keep the destination always synchronized with its source. During the ongoing synchronization process, there is a certain delay between the time when a command was executed on the source and when it is executed on the destination. This delay is referred to as the Lag.\nWhen there is a synchronization error, the process might stop or it might continue running on the assumption that the error automatically resolves. The result depends on the error type. See more details below.\nIn addition, the user can manually stop the synchronization process.\nWhen the process is in the stopped state - whether stopped by the user or by the system - the user can restart the process. Restarting the process causes the synchronization process to flush the DB and restart the process from the beginning.\nReplica Of status The replication process can have the following statuses:\nSyncing - indicates that the synchronization process has started from scratch. Progress is indicated in percentages (%). Synced - indicates that the initial synchronization process was completed and the destination is synchronizing changes on an ongoing basis. The Lag delay in synchronization with the source is indicated as a time duration. Sync stopped - indicates that the synchronization process is currently not running and the user needs to restart it in order for it to continue running. This status happens if the user stops the process, or if certain errors arose that prevent synchronization from continuing without manual intervention. See more details below. The statuses above are shown for the source database. In addition, a timestamp is shown on the source indicating when the last command from the source was executed on the destination.\nThe system also displays the destination database status as an aggregate of the statuses of all the sources.\nNote: If you encounter issues with the Replica Of process, refer to the troubleshooting section Replica Of repeatedly fails. Synchronization errors Certain errors that occur during the synchronization process require user intervention for their resolution. When such errors occur, the synchronization process is automatically stopped.\nFor other errors, the synchronization process continues running on the assumption that the error automatically resolves.\nExamples of errors that require user intervention for their resolution and that stop the synchronization process include:\nError authenticating with the source database. Cross slot violation error while executing a command on a sharded destination database. Out-of-memory error on a source or on the destination database. Example of an error that does not cause the synchronization process to stop:\nConnection error with the source database. A connection error might occur occasionally, for example as result of temporary network issues that get resolved. Depending on the connection error and its duration the process might be able to start syncing from the last point it reached (partial sync) or require a complete resynchronization from scratch across all sources (full sync). Encryption Replica Of supports the ability to encrypt uni-directional replication communications between source and destination clusters utilizing TLS 1.2 based encryption.\nData compression for Replica Of When the Replica Of is defined across different Redis Enterprise Software clusters, it may be beneficial to compress the data that flows through the network (depending on where the clusters physically reside and the available network).\nCompressing the data reduces the traffic and can help:\nResolve throughput issues Reduce network traffic costs Compressing the data does have trade-offs, which is why it should not always be turned on by default. For example:\nIt uses CPU and disk resources to compress the data before sending it to the network and decompress it on the other side. It takes time to compress and decompress the data which can increase latency. Replication is disk-based and done gradually, shard by shard in the case of a multi-shard database. This may have an impact on replication times depending on the speed of the disks and load on the database. If traffic is too fast and the compression takes too much time it can cause the replication process to fail and be restarted. It is advised that you test compression out in a lower environment before enabling it in production.\nIn the Redis Enterprise Software management UI, when designating a Replica Of source from a different Redis Enterprise Software cluster, there is also an option to enable compression. When enabled, gzip compression with level -6 is utilized.\nDatabase clustering (sharding) implications If a source database is sharded, that entire database is treated as a single source for the destination database.\nIf the destination database is sharded, when the commands replicated from the source are executed on the destination database, the destination database\u0026rsquo;s hashing function is executed to determine to which shard/s the command refers.\nThe source and destination can have different shard counts and functions for placement of keys.\nSynchronization in Active-Passive Replication In Active-Passive databases, one cluster hosts the source database that receives read-write operations and the other clusters host destination databases that receive synchronization updates from the source database.\nWhen there is a significant difference between the source and destination databases, the destination database flushes all of the data from its memory and starts synchronizing the data again. This process is called a full sync.\nFor example, if the database updates for the destination databases that are stored by the destination database in a synchronization backlog exceed their allocated memory, the source database starts a full sync.\nWarning - When you failover to the destination database for write operations, make sure that you disable Replica Of before you direct clients to the destination database. This avoids a full sync that can overwrite your data. Active-Passive replication backlog In addition to the database replication backlog, active-passive databases maintain a replication backlog (per shard) to synchronize the database instances between clusters. By default, the replication backlog is set to one percent (1%) of the database size divided by the database number of shards and ranges between 1MB to 250MB per shard. Use the rladmin utility to control the size of the replication backlog. You can set it to auto or set a specific size.\nFor an Active-Passive database:\nrladmin tune db \u0026lt;db:id | name\u0026gt; repl_backlog \u0026lt;Backlog size in MB or \u0026#39;auto\u0026#39;\u0026gt; Note: On an Active-Passive database, the replication backlog configuration applies to both the replication backlog for shards synchronization and for synchronization of database instances between clusters. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/replica_sources_status/","uriRel":"/rs/references/rest-api/objects/bdb/replica_sources_status/","title":"BDB replica sources status field","tags":[],"keywords":[],"description":"Documents the bdb replica_sources status field used with Redis Enterprise Software REST API calls.","content":"The replica_sources status field relates to the Replica Of feature, which enables the creation of a Redis database (single- or multi-shard) that synchronizes data from another Redis database (single- or multi-shard).\nThe status field represents the Replica Of sync status for a specific sync source.\nPossible status values:\nStatus Description Possible next status \u0026lsquo;out-of-sync\u0026rsquo; Sync process is disconnected from source and trying to reconnect \u0026lsquo;syncing\u0026rsquo; \u0026lsquo;syncing\u0026rsquo; Sync process is in progress \u0026lsquo;in-sync\u0026rsquo; \u0026lsquo;out-of-sync\u0026rsquo; \u0026lsquo;in-sync\u0026rsquo; Sync process finished successfully, and new commands are syncing on a regular basis \u0026lsquo;syncing\u0026rsquo; \u0026lsquo;out-of-sync\u0026rsquo; ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/replica_sources-alerts/","uriRel":"/rs/references/rest-api/requests/bdbs/replica_sources-alerts/","title":"Database replica sources alerts requests","tags":[],"keywords":[],"description":"Replica source alert requests","content":" Method Path Description GET /v1/bdbs/replica_sources/alerts Get all replica sources alert states for all BDBs GET /v1/bdbs/replica_sources/alerts/{uid} Get all replica sources alert states for a BDB GET /v1/bdbs/replica_sources/alerts/{uid}/{replica_src_id} Get all alert states for a replica source GET /v1/bdbs/replica_sources/alerts/{uid}/{replica_src_id}/{alert} Get a replica source alert state Get all DBs replica sources alert states GET /v1/bdbs/replica_sources/alerts Get all alert states for all replica sources of all BDBs.\nRequired permissions Permission name view_all_bdbs_alerts Request Example HTTP request GET /bdbs/replica_sources/alerts Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a hash of alert UIDs and the alerts states for each BDB.\nSee [REST API alerts overview] for a description of the alert state object.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;replica_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error Get DB replica source alert states GET /v1/bdbs/replica_sources/alerts/{int: uid} Get all alert states for all replica sources of a specific bdb.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/replica_sources/alerts/1 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;replica_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified bdb does not exist Get replica source alert states GET /v1/bdbs/replica_sources/alerts/{int: uid}/{int: replica_src_id} Get all alert states for a specific replica source of a bdb.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/replica_sources/alerts/1/2 Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database replica_src_id integer The ID of the replica source in this BDB Response Returns a hash of alert objects and their states.\nExample JSON body { \u0026#34;replica_src_syncer_connection_error\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } }, \u0026#34;...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Specified bdb does not exist Get replica source alert state GET /v1/bdbs/replica_sources/alerts/{int: uid}/{int: replica_src_id}/{alert} Get a replica source alert state of a specific bdb.\nRequired permissions Permission name view_bdb_alerts Request Example HTTP request GET /bdbs/replica_sources/alerts/1/2/replica_src_syncer_connection_error Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the database replica_src_id integer The ID of the replica source in this BDB alert string The alert name Response Returns an alert state object.\nExample JSON body { \u0026#34;enabled\u0026#34;: true, \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;WARNING\u0026#34;, \u0026#34;change_time\u0026#34;: \u0026#34;2014-08-29T11:19:49Z\u0026#34;, \u0026#34;change_value\u0026#34;: { \u0026#34;state\u0026#34;: true, \u0026#34;threshold\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;memory_util\u0026#34;: 81.2 } } Status codes Code Description 200 OK No error 400 Bad Request Bad request 404 Not Found Specified alert or bdb does not exist ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/replica_sync/","uriRel":"/rs/references/rest-api/objects/bdb/replica_sync/","title":"BDB replica sync field","tags":[],"keywords":[],"description":"Documents the bdb replica_sync field used with Redis Enterprise Software REST API calls.","content":"The BDB replica_sync field relates to the Replica Of feature, which enables the creation of a Redis database (single- or multi-shard) that synchronizes data from another Redis database (single- or multi-shard).\nYou can use the replica_sync field to enable, disable, or pause the Replica Of sync process. The BDB crdt_sync field has a similar purpose for the Redis CRDB.\nPossible BDB sync values:\nStatus Description Possible next status \u0026lsquo;disabled\u0026rsquo; (default value) Disables the sync process and represents that no sync is currently configured or running. \u0026rsquo;enabled' \u0026rsquo;enabled' Enables the sync process and represents that the process is currently active. \u0026lsquo;stopped\u0026rsquo; \u0026lsquo;paused\u0026rsquo; \u0026lsquo;paused\u0026rsquo; Pauses the sync process. The process is configured but is not currently executing any sync commands. \u0026rsquo;enabled\u0026rsquo; \u0026lsquo;stopped\u0026rsquo; \u0026lsquo;stopped\u0026rsquo; An unrecoverable error occurred during the sync process, which caused the system to stop the sync. \u0026rsquo;enabled\u0026rsquo; When the sync is in the \u0026lsquo;stopped\u0026rsquo; or \u0026lsquo;paused\u0026rsquo; state, then the last_error field in the relevant source entry in the sync_sources \u0026ldquo;status\u0026rdquo; field contains the detailed error message.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/reset_password/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/reset_password/","title":"rladmin cluster reset_password","tags":["configured"],"keywords":[],"description":"Changes the password for a given email.","content":"Changes the password for the user associated with the specified email address.\nEnter a new password when prompted. Then enter the same password when prompted a second time to confirm the password change.\nrladmin cluster reset_password \u0026lt;user email\u0026gt; Parameters Parameter Type/Value Description user email email address The email address of the user that needs a password reset Returns Reports whether the password change succeeded or an error occurred.\nExample $ rladmin cluster reset_password user@example.com New password: New password (again): Password changed. ","categories":["RS"]},{"uri":"/rs/references/rest-api/","uriRel":"/rs/references/rest-api/","title":"REST API","tags":[],"keywords":[],"description":"Documents the REST API available to Redis Enterprise Software deployments.","content":"Redis Enterprise Software provides a REST API to help you automate common tasks.\nHere, you\u0026rsquo;ll find the details of the API and how to use it.\nFor more info, see:\nSupported request endpoints, organized by path Supported objects, both request and response Built-in roles and associated permissions Redis Enterprise Software REST API quick start with examples Authentication Authentication to the Redis Enterprise Software API occurs via Basic Auth. Provide your username and password as the basic auth credentials.\nIf the username and password is incorrect or missing, the request will fail with a 401 Unauthorized status code.\nExample request using cURL:\ncurl -u \u0026#34;demo@redislabs.com:password\u0026#34; \\ https://localhost:9443/v1/bdbs For more examples, see the Redis Enterprise Software REST API quick start\nPermissions By default, the admin user is authorized for access to all endpoints. Use role-based access controls and role permissions to manage access.\nIf a user attempts to access an endpoint that is not allowed in their role, the request will fail with a 403 Forbidden status code. For more details on which user roles can access certain endpoints, see Permissions.\nCertificates The Redis Enterprise Software REST API uses Self-signed certificates to ensure the product is secure. When you use the default self-signed certificates, the HTTPS requests will fail with SSL certificate problem: self signed certificate unless you turn off SSL certificate verification. The examples in this tutorial turn off SSL certificate verification.\nPorts All calls must be made over SSL to port 9443. For the API to work, port 9443 must be exposed to incoming traffic or mapped to a different port.\nIf you are using a Redis Enterprise Software Docker image, run the following command to start the Docker image with port 9443 exposed:\ndocker run -p 9443:9443 redislabs/redis Versions All API requests are versioned in order to minimize the impact of backwards-incompatible API changes and to coordinate between different versions operating in parallel.\nSpecify the version in the request URI, as shown in the following table:\nRequest path Description POST /v1/bdbs A version 1 request for the /bdbs endpoint. POST /v2/bdbs A version 2 request for the /bdbs endpoint. When an endpoint supports multiple versions, each version is documented on the corresponding endpoint. For example, the bdbs request page documents POST requests for version 1 and version 2.\nHeaders Requests Redis Enterprise REST API requests support the following HTTP headers:\nHeader Supported/Required Values Accept application/json Content-Length Length (in bytes) of request message Content-Type application/json (required for PUT or POST requests) If the client specifies an invalid header, the request will fail with a 400 Bad Request status code.\nResponses Redis Enterprise REST API responses support the following HTTP headers:\nHeader Supported/Required Values Content-Type application/json Content-Length Length (in bytes) of response message JSON requests and responses The Redis Enterprise Software REST API uses JavaScript Object Notation (JSON) for requests and responses. See the RFC 4627 technical specifications for additional information about JSON.\nSome responses may have an empty body but indicate the response with standard HTTP codes.\nBoth requests and responses may include zero or more objects.\nIf the request is for a single entity, the response returns a single JSON object or none. If the request is for a list of entities, the response returns a JSON array with zero or more elements.\nIf you omit certain JSON object fields from a request, they may be assigned default values, which often indicate that these fields are not in use.\nResponse types and error codes HTTP status codes indicate the result of an API request. This can be 200 OK if the server accepted the request, or it can be one of many error codes.\nThe most common responses for a Redis Enterprise API request are:\nResponse Condition/Required handling 200 OK Success 400 Bad Request The request failed, generally due to a typo or other mistake. 401 Unauthorized The request failed because the authentication information was missing or incorrect. 403 Forbidden The user cannot access the specified URI. 404 Not Found The URI does not exist. 503 Service Unavailable The node is not responding or is not a member of the cluster. 505 HTTP Version Not Supported An unsupported x-api-version was used. See versions. Some endpoints return different response codes. The request references for these endpoints document these special cases.\n","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/restart/","uriRel":"/rs/references/cli-utilities/rladmin/restart/","title":"rladmin restart","tags":[],"keywords":[],"description":"Restarts Redis Enterprise Software processes for a specific database.","content":"Schedules a restart of the Redis Enterprise Software processes on primary and replica instances of a specific database.\nrladmin restart db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } [preserve_roles] [discard_data] [force_discard] Parameters Parameter Type/Value Description db db:\u0026lt;id\u0026gt;\nname Restarts Redis Enterprise Software processes for the specified database discard_data Allows discarding data if there is no persistence or replication force_discard Forcibly discards data even if there is persistence or replication preserve_roles Performs an additional failover to maintain shard roles Returns Returns Done if the restart completed successfully. Otherwise, it returns an error.\nExample $ rladmin restart db db:5 preserve_roles Monitoring 1db07491-35da-4bb6-9bc1-56949f4c312a active - SMUpgradeBDB init active - SMUpgradeBDB stop_forwarding active - SMUpgradeBDB stop_active_expire active - SMUpgradeBDB check_slave oactive - SMUpgradeBDB stop_active_expire active - SMUpgradeBDB second_failover completed - SMUpgradeBDB Done ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/","uriRel":"/rs/references/cli-utilities/rladmin/","title":"rladmin","tags":[],"keywords":[],"description":"Manage Redis Enterprise clusters and databases.","content":"rladmin is a command-line utility that lets you perform administrative tasks such as failover, migration, and endpoint binding on a Redis Enterprise Software cluster. You can also use rladmin to edit cluster and database configurations.\nAlthough you can use the admin console for some of these tasks, others are unique to the rladmin command-line tool.\nrladmin commands Command Description bind Manages the proxy policy for a specified database endpoint. cluster Manage cluster. failover Fail over primary shards of a database to their replicas. help Shows available commands or specific command usage. info Shows the current configuration of a cluster, database, node, or proxy. migrate Moves Redis Enterprise Software shards or endpoints to a new node in the same cluster. node Manage nodes. placement Configures the shard placement policy for a database. recover Recovers databases in recovery mode. restart Restarts Redis Enterprise Software processes for a specific database. status Displays the current cluster status and topology information. suffix Manages the DNS suffixes in the cluster. tune Configures parameters for databases, proxies, nodes, and clusters. upgrade Upgrades the version of a module or Redis Enterprise Software for a database. verify Prints verification reports for the cluster. Use the rladmin shell To open the rladmin shell:\nSign in to a Redis Enterprise Software node with an account that is a member of the redislabs group.\nThe rladmin binary is located in /opt/redislabs/bin. If you don\u0026rsquo;t have this directory in your PATH, you may want to add it. Otherwise, you can use bash -l \u0026lt;username\u0026gt; to sign in as a user with permissions for that directory.\nRun: rladmin\nNote: If the CLI does not recognize the rladmin command, run this command to load the necessary configuration first: bash -l In the rladmin shell, you can:\nRun any rladmin command without prefacing it with rladmin. Enter ? to view the full list of available commands. Enter help followed by the name of a command for a detailed explanation of the command and its usage. Press the Tab key for command completion. Enter exit or press Control+D to exit the rladmin shell and return to the terminal prompt. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rlcheck/","uriRel":"/rs/references/cli-utilities/rlcheck/","title":"rlcheck","tags":[],"keywords":[],"description":"Verify nodes.","content":"The rlcheck utility runs various health checks on a Redis Enterprise Software node and reports any discovered issues. You can use this utility to confirm a successful installation or to verify that the node is functioning properly.\nYou can run rlcheck from the host\u0026rsquo;s command-line interface (CLI). The output of rlcheck shows information specific to the host that you run it on.\nTo open the rladmin CLI:\nSign in to the Redis Enterprise Software host with an account that is a member of the redislabs operating system group.\nRun: rlcheck\nThe utility runs and reports the result of each check.\nNote: To see the rlcheck optional flags, run: rlcheck --help Specifically, the --continue-on-error flag runs all tests to completion and shows all errors when complete. To resolve issues reported by rlcheck, contact Redis support.\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/role/","uriRel":"/rs/references/rest-api/objects/role/","title":"Role object","tags":[],"keywords":[],"description":"An object that represents a role","content":"An API object that represents a role.\nName Type/Value Description uid integer Role\u0026rsquo;s unique ID account_id integer SM account ID action_uid string Action UID. If it exists, progress can be tracked by the GET /actions/{uid} API (read-only) management \u0026lsquo;admin\u0026rsquo;\n\u0026lsquo;db_member\u0026rsquo;\n\u0026lsquo;db_viewer\u0026rsquo;\n\u0026lsquo;cluster_member\u0026rsquo;\n\u0026lsquo;cluster_viewer\u0026rsquo;\n\u0026lsquo;none\u0026rsquo; Management role name string Role\u0026rsquo;s name ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/roles/","uriRel":"/rs/references/rest-api/requests/roles/","title":"Roles requests","tags":[],"keywords":[],"description":"Roles requests","content":" Method Path Description GET /v1/roles Get all roles GET /v1/roles/{uid} Get a single role PUT /v1/roles/{uid} Update an existing role POST /v1/roles Create a new role DELETE /v1/roles/{uid} Delete a role Get all roles GET /v1/roles Get all roles\u0026rsquo; details.\nPermissions Permission name Roles view_all_roles_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /roles Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON array of role objects.\nExample JSON body [ { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; }, { \u0026#34;uid\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Cluster Member\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;cluster_member\u0026#34; }, { \u0026#34;uid\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;Cluster Viewer\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;cluster_viewer\u0026#34; }, { \u0026#34;uid\u0026#34;: 4, \u0026#34;name\u0026#34;: \u0026#34;DB Member\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;db_member\u0026#34; }, { \u0026#34;uid\u0026#34;: 5, \u0026#34;name\u0026#34;: \u0026#34;DB Viewer\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;db_viewer\u0026#34; }, { \u0026#34;uid\u0026#34;: 6, \u0026#34;name\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;none\u0026#34; }, { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; } ] Status codes Code Description 200 OK No error 501 Not Implemented Cluster doesn\u0026rsquo;t support roles yet. Get role GET /v1/roles/{int: uid} Get the details of a single role.\nPermissions Permission name Roles view_role_info admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /roles/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The role\u0026rsquo;s unique ID. Response Returns a role object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; } Status codes Code Description 200 OK Success. 403 Forbidden Operation is forbidden. 404 Not Found Role does not exist. 501 Not Implemented Cluster doesn\u0026rsquo;t support roles yet. Update role PUT /v1/roles/{int: uid} Update an existing role\u0026rsquo;s details.\nPermissions Permission name Roles update_role admin Request Example HTTP request PUT /roles/17 Example JSON body { \u0026#34;management\u0026#34;: \u0026#34;cluster_member\u0026#34; } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Body Include a role object with updated fields in the request body.\nResponse Returns a role object with the updated fields.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;cluster_member\u0026#34; } Error codes Possible error_code values:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists. change_last_admin_role_not_allowed At least one user with admin role should exist. Status codes Code Description 200 OK Success, role is created. 400 Bad Request Bad or missing configuration parameters. 404 Not Found Attempting to change a non-existant role. 501 Not Implemented Cluster doesn\u0026rsquo;t support roles yet. Create role POST /v1/roles Create a new role.\nPermissions Permission name Roles create_role admin Request Example HTTP request POST /roles Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Body Include a role object in the request body.\nResponse Returns the newly created role object.\nExample JSON body { \u0026#34;uid\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; } Error codes Possible error_codevalues:\nCode Description unsupported_resource The cluster is not yet able to handle this resource type. This could happen in a partially upgraded cluster, where some of the nodes are still on a previous version. name_already_exists An object of the same type and name exists missing_field A needed field is missing Status codes Code Description 200 OK Success, role is created. 400 Bad Request Bad or missing configuration parameters. 501 Not Implemented Cluster doesn\u0026rsquo;t support roles yet. Examples cURL curl -k -u \u0026#34;[username]:[password]\u0026#34; -X POST \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; }\u0026#39; \\ https://[host][:port]/v1/roles Python import requests import json url = \u0026#34;https://[host][:port]/v1/roles\u0026#34; headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } payload = json.dumps({ \u0026#34;name\u0026#34;: \u0026#34;DBA\u0026#34;, \u0026#34;management\u0026#34;: \u0026#34;admin\u0026#34; }) auth=(\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;POST\u0026#34;, url, auth=auth, headers=headers, payload=payload, verify=False) print(response.text) Delete role DELETE /v1/roles/{int: uid} Delete a role object.\nPermissions Permission name Roles delete_role admin Request Example HTTP request DELETE /roles/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The role unique ID. Response Returns a status code to indicate role deletion success or failure.\nStatus codes Code Description 200 OK Success, the role is deleted. 404 Not Found Role does not exist. 406 Not Acceptable The request is not acceptable. 501 Not Implemented Cluster doesn\u0026rsquo;t support roles yet. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/certificates/rotate/","uriRel":"/rs/references/rest-api/requests/cluster/certificates/rotate/","title":"Rotate cluster certificates requests","tags":[],"keywords":[],"description":"Rotate cluster certificates requests","content":" Method Path Description POST /v1/cluster/certificates/rotate Regenerate all internal cluster certificates Rotate cluster certificates POST /v1/cluster/certificates/rotate Regenerates all internal cluster certificates.\nThe certificate rotation will be performed on all nodes within the cluster. If \u0026ldquo;name\u0026rdquo; is provided, only rotate the specified certificate on all nodes within the cluster.\nRequest Example HTTP request POST /cluster/certificates/rotate Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Responds with a 200 OK status code if the internal certificates successfully rotate across the entire cluster.\nStatus codes Code Description 200 OK No error 400 Bad Request Failed, not all nodes have been updated. 403 Forbidden Unsupported internal certificate rotation. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/job_scheduler/rotate_ccs_job_settings/","uriRel":"/rs/references/rest-api/objects/job_scheduler/rotate_ccs_job_settings/","title":"Rotate CCS job settings object","tags":[],"keywords":[],"description":"Documents the rotate_ccs_job_settings object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description cron_expression string CRON expression that defines the CCS rotation schedule file_suffix string (default: 5min) String added to the end of the rotated RDB files rotate_max_num integer, (range: 1-100) (default: 24) The maximum number of saved RDB files ","categories":["RS"]},{"uri":"/rs/clusters/logging/rsyslog-logging/","uriRel":"/rs/clusters/logging/rsyslog-logging/","title":"rsyslog logging","tags":[],"keywords":[],"description":"This document explains the structure of Redis Enterprise Software log entries in `rsyslog` and how to use these log entries to identify events.","content":"This document explains the structure of Redis Enterprise Software log entries in rsyslog and how to use these log entries to identify events.\nNote: You can also secure your logs with a remote logging server and log rotation. Log concepts Redis Enterprise Software logs information from a variety of components in response to actions and events that occur within the cluster.\nIn some cases, a single action, such as removing a node from the cluster, may actually consist of several events. These actions may generate multiple log entries.\nAll log entries displayed in the admin console are also written to syslog. You can configure rsyslog to monitor syslog. Enabled alerts are logged to syslog and appear with other log entries.\nTypes of log entries Log entries are categorized into events and alerts. Both types of entries appear in the logs, but alert log entries also include a boolean \u0026quot;state\u0026quot; parameter that indicates whether the alert is enabled or disabled.\nLog entries include information about the specific event that occurred. See the log entry tables for clusters, databases, nodes, and users for more details.\nSeverity You can also configure rsyslog to add other information, such as the event severity.\nSince rsyslog entries do not include severity by default, you can follow these steps to enable it:\nAdd the following line to /etc/rsyslog.conf:\n$template TraditionalFormatWithPRI,\u0026#34;%pri-text%: %timegenerated% %HOSTNAME% %syslogtag%%msg:::drop-last-lf%\\n\u0026#34; Modify $ActionFileDefaultTemplate to use your new template $ActionFileDefaultTemplateTraditionalFormatWithPRI\nSave these changes and restart rsyslog to apply them\nYou can see the log entries for alerts and events in the /var/log/messages file.\nCommand components:\n%pri­text% ­adds the severity %timegenerated% ­adds the timestamp %HOSTNAME% ­adds the machine name %syslogtag% adds ­the Redis Enterprise Software message. See the log entry structure section for more details. %msg:::drop­last­lf%n ­removes duplicated log entries Log entry structure The log entries have the following basic structure:\nevent_log[\u0026lt;process id\u0026gt;]:{\u0026lt;list of key-value pairs in any order\u0026gt;} event_log:­ Plain static text is always shown at the beginning of the entry. process id­: The ID of the logging process list of key-value pairs in any order:­ A list of key-value pairs that describe the specific event. They can appear in any order. Some key­-value pairs are always shown, and some appear depending on the specific event. Key-­value pairs that always appear: \u0026quot;type\u0026quot;: A unique code­ name for the logged event. For the list of codenames, see the logged events and alerts tables for clusters, databases, nodes, and users. \u0026quot;object\u0026quot;: Defines the object type and ID (if relevant) of the object this event relates to, such as cluster, node with ID, BDB with ID, etc. Has the format of \u0026lt;object type\u0026gt;[:\u0026lt;id\u0026gt;]. \u0026quot;time\u0026quot;: Unix epoch time but can be ignored in this context. Key-­value pairs that might appear depending on the specific entry: \u0026quot;state\u0026quot;: A boolean where true means the alert is enabled, and false means the alert is disabled. This is only relevant for alert log entries. \u0026quot;global_threshold\u0026quot;: The value of a threshold for alerts related to cluster or node objects. \u0026quot;threshold\u0026quot;: The value of a threshold for alerts related to a BDB object. Log entry samples This section provides examples of log entries that include the rsyslog configuration to add the severity, timestamp, and machine name.\nEphemeral storage passed threshold \u0026ldquo;Alert on\u0026rdquo; log entry sample daemon.warning: Jun 14 14:49:20 node1 event_log[3464]: { \u0026#34;storage_util\u0026#34;: 90.061643120001, \u0026#34;global_threshold\u0026#34;: \u0026#34;70\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;node:1\u0026#34;, \u0026#34;state\u0026#34;: true, \u0026#34;time\u0026#34;: 1434282560, \u0026#34;type\u0026#34;: \u0026#34;ephemeral_storage\u0026#34; } In this example, the storage utilization on node 1 reached the value of ~90%, which triggered the alert for \u0026ldquo;Ephemeral storage has reached 70% of its capacity.\u0026rdquo;\nLog entry components:\ndaemon.warning -­ Severity of entry is warning Jun 14 14:49:20 -­ The timestamp of the event node1:­ Machine name event_log -­ Static text that always appears [3464]­ - Process ID \u0026quot;storage_util\u0026quot;:90.061643120001 - Current ephemeral storage utilization \u0026quot;global_threshold\u0026quot;:\u0026quot;70\u0026quot; - The user-configured threshold above which the alert is raised \u0026quot;object\u0026quot;:\u0026quot;node:1\u0026quot;­ - The object related to this alert \u0026quot;state\u0026quot;:true­ - Current state of the alert \u0026quot;time\u0026quot;:1434282560­ - Can be ignored \u0026quot;type\u0026quot;:\u0026quot;ephemeral_storage\u0026quot; - The code name of this specific event. See logged node alerts and events for more details. \u0026ldquo;Alert off\u0026rdquo; log entry sample daemon.info: Jun 14 14:51:35 node1 event_log[3464]: { \u0026#34;storage_util\u0026#34;:60.051723520008, \u0026#34;global_threshold\u0026#34;: \u0026#34;70\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;node:1\u0026#34;, \u0026#34;state\u0026#34;:false, \u0026#34;time\u0026#34;: 1434283480, \u0026#34;type\u0026#34;: \u0026#34;ephemeral_storage\u0026#34; } This log entry is an example of when the alert for the node with ID 1 \u0026ldquo;Ephemeral storage has reached 70% of its capacity\u0026rdquo; has been turned off as result of storage utilization reaching the value of ~60%.\nLog entry components:\ndaemon.info -­ Severity of entry is info Jun 14 14:51:35 -­ The timestamp of the event node1 -­ Machine name event_log -­ Static text that always appears [3464] -­ Process ID \u0026quot;storage_util\u0026quot;:60.051723520008­ - Current ephemeral storage utilization \u0026quot;global_threshold\u0026quot;:\u0026quot;70\u0026quot; - The user configured threshold above which the alert is raised (70% in this case) \u0026quot;object\u0026quot;:\u0026quot;node:1\u0026quot; -­ The object related to this alert \u0026quot;state\u0026quot;:false­ - Current state of the alert \u0026quot;time\u0026quot;:1434283480­ - Can be ignored \u0026quot;type\u0026quot;:\u0026quot;ephemeral_storage\u0026quot; -­ The code name identifier of this specific event. See logged node alerts and events for more details. Odd number of nodes with a minimum of three nodes alert \u0026ldquo;Alert on\u0026rdquo; log entry sample daemon.warning: Jun 14 15:25:00 node1 event_log[8310]: { \u0026#34;object\u0026#34;:\u0026#34;cluster\u0026#34;, \u0026#34;state\u0026#34;: true, \u0026#34;time\u0026#34;: 1434284700, \u0026#34;node_count\u0026#34;: 1, \u0026#34;type\u0026#34;:\u0026#34;even_node_count\u0026#34; } This log entry is an example of when the alert for \u0026ldquo;True high availability requires an odd number of nodes with a minimum of three nodes\u0026rdquo; has been turned on as result of the cluster having only one node.\nLog entry components:\ndaemon.warning­ - Severity of entry is warning Jun 14 15:25:00 - The timestamp of the event node1­ - Machine name event_log -­ Static text that always appears [8310]­ - Process ID \u0026quot;object\u0026quot;:\u0026quot;cluster\u0026quot;­ - The object related to this alert \u0026quot;state\u0026quot;:true -­ Current state of the alert \u0026quot;time\u0026quot;:1434284700­ - Can be ignored \u0026quot;node_count\u0026quot;:1­ - The number of nodes in the cluster \u0026quot;type\u0026quot;:\u0026quot;even_node_count\u0026quot;­ - The code name identifier of this specific event. See logged cluster alerts and events for more details. \u0026ldquo;Alert off\u0026rdquo; log entry sample daemon.warning: Jun 14 15:30:40 node1 event_log[8310]: { \u0026#34;object\u0026#34;:\u0026#34;cluster\u0026#34;, \u0026#34;state\u0026#34;: false, \u0026#34;time\u0026#34;: 1434285200, \u0026#34;node_count\u0026#34;: 3, \u0026#34;type\u0026#34;:\u0026#34;even_node_count\u0026#34; } This log entry is an example of when the alert for \u0026ldquo;True high availability requires an odd number of nodes with a minimum of three nodes\u0026rdquo; has been turned off as result of the cluster having 3 nodes.\nLog entry components:\ndaemon.warning - Severity of entry is warning Jun 14 15:30:40 -­ The timestamp of the event node1­ - Machine name event_log­ - Static text that always appears [8310] -­ Process ID \u0026quot;object\u0026quot;:\u0026quot;cluster\u0026quot; -­ The object related to this alert \u0026quot;state\u0026quot;:false­ - Current state of the alert \u0026quot;time\u0026quot;:1434285200­ - Can be ignored \u0026quot;node_count\u0026quot;:3­ - The number of nodes in the cluster \u0026quot;type\u0026quot;:\u0026quot;even_node_count\u0026quot; -­ The code name of this specific event. See logged cluster alerts and events for more details. Node has insufficient disk space for AOF rewrite \u0026ldquo;Alert on\u0026rdquo; log entry sample daemon.err: Jun 15 13:51:23 node1 event_log[34252]: { \u0026#34;used\u0026#34;: 23457188, \u0026#34;missing\u0026#34;: 604602126, \u0026#34;object\u0026#34;: \u0026#34;node:1\u0026#34;, \u0026#34;free\u0026#34;: 9867264, \u0026#34;needed\u0026#34;:637926578, \u0026#34;state\u0026#34;: true, \u0026#34;time\u0026#34;: 1434365483, \u0026#34;disk\u0026#34;: 705667072, \u0026#34;type\u0026#34;:\u0026#34;insufficient_disk_aofrw\u0026#34; } This log entry is an example of when the alert for \u0026ldquo;Node has insufficient disk space for AOF rewrite\u0026rdquo; has been turned on as result of not having enough persistent storage disk space for AOF rewrite purposes. It is missing 604602126 bytes.\nLog entry components:\ndaemon.err­ - Severity of entry is error Jun 15 13:51:23 - The timestamp of the event node1­ - Machine name event_log -­ Static text that always appears [34252] -­ Process ID \u0026quot;used\u0026quot;:23457188­ - The amount of disk space in bytes currently used for AOF files \u0026quot;missing\u0026quot;:604602126­ - The amount of disk space in bytes that is currently missing for AOF rewrite purposes \u0026quot;object\u0026quot;:\u0026quot;node:1″ -­ The object related to this alert \u0026quot;free\u0026quot;:9867264­ - The amount of disk space in bytes that is currently free \u0026quot;needed\u0026quot;:637926578­ - The amount of total disk space in bytes that is needed for AOF rewrite purposes \u0026quot;state\u0026quot;:true­ - Current state of the alert \u0026quot;time\u0026quot;:1434365483 -­ Can be ignored \u0026quot;disk\u0026quot;:705667072­ - The total size in bytes of the persistent storage \u0026quot;type\u0026quot;:\u0026quot;insufficient_disk_aofrw\u0026quot;­ - The code name of this specific event. See logged node alerts and events for more details. \u0026ldquo;Alert off\u0026rdquo; log entry sample daemon.info: Jun 15 13:51:11 node1 event_log[34252]: { \u0026#34;used\u0026#34;: 0, \u0026#34;missing\u0026#34;:-21614592, \u0026#34;object\u0026#34;: \u0026#34;node:1\u0026#34;, \u0026#34;free\u0026#34;: 21614592, \u0026#34;needed\u0026#34;: 0, \u0026#34;state\u0026#34;:false, \u0026#34;time\u0026#34;: 1434365471, \u0026#34;disk\u0026#34;: 705667072, \u0026#34;type\u0026#34;:\u0026#34;insufficient_disk_aofrw\u0026#34; } Log entry components:\ndaemon.info­ - Severity of entry is info Jun 15 13:51:11 - The timestamp of the event node1­ - Machine name event_log -­ Static text that always appears [34252]­ - Process ID \u0026quot;used\u0026quot;:0­ - The amount of disk space in bytes currently used for AOF files \u0026quot;missing\u0026quot;:‐21614592­ - The amount of disk space in bytes that is currently missing for AOF rewrite purposes. In this case, it is not missing because the number is negative. \u0026quot;object\u0026quot;:\u0026quot;node:1″ -­ The object related to this alert \u0026quot;free\u0026quot;:21614592 -­ The amount of disk space in bytes that is currently free \u0026quot;needed\u0026quot;:0­ - The amount of total disk space in bytes that is needed for AOF rewrite purposes. In this case, no space is needed. \u0026quot;state\u0026quot;:false­ - Current state of the alert \u0026quot;time\u0026quot;:1434365471­ - Can be ignored \u0026quot;disk\u0026quot;:705667072­ - The total size in bytes of the persistent storage \u0026quot;type\u0026quot;:\u0026quot;insufficient_disk_aofrw\u0026quot;­ - The code name of this specific event. See logged node alerts and events for more details. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/running_actions/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/running_actions/","title":"rladmin cluster running_actions","tags":["configured"],"keywords":[],"description":"Lists all active tasks.","content":"Lists all active tasks running on the cluster.\nrladmin cluster running_actions Parameters None\nReturns Returns details about any active tasks running on the cluster.\nExample $ rladmin cluster running_actions Got 1 tasks: 1) Task: maintenance_on (ce391d81-8d51-4ce2-8f63-729c7ac2589e) Node: 1 Status: running ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/saslauthd/","uriRel":"/rs/references/rest-api/objects/services_configuration/saslauthd/","title":"saslauthd object","tags":[],"keywords":[],"description":"Documents the saslauthd object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the saslauthd service ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/","uriRel":"/rs/references/rest-api/objects/services_configuration/","title":"Services configuration object","tags":[],"keywords":[],"description":"An object for optional cluster services settings","content":"Optional cluster services settings\nName Type/Value Description cm_server cm_server object Whether to enable/disable the CM server crdb_coordinator crdb_coordinator object Whether to enable/disable the CRDB coordinator process crdb_worker crdb_worker object Whether to enable/disable the CRDB worker processes mdns_server mdns_server object Whether to enable/disable the multicast DNS server pdns_server pdns_server object Whether to enable/disable the PDNS server saslauthd saslauthd object Whether to enable/disable the saslauthd service stats_archiver stats_archiver object Whether to enable/disable the stats archiver service ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/services_configuration/","uriRel":"/rs/references/rest-api/requests/cluster/services_configuration/","title":"Cluster services configuration requests","tags":[],"keywords":[],"description":"Cluster services configuration requests","content":" Method Path Description GET /v1/cluster/services_configuration Get cluster services settings PUT /v1/cluster/services_configuration Update cluster services settings Get cluster services configuration GET /v1/cluster/services_configuration Get cluster services settings.\nRequired permissions Permission name view_cluster_info Request Example HTTP request GET /cluster/services_configuration Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a services configuration object.\nExample JSON body { \u0026#34;cm_server\u0026#34;: { \u0026#34;operating_mode\u0026#34;: \u0026#34;disabled\u0026#34; }, \u0026#34;mdns_server\u0026#34;: { \u0026#34;operating_mode\u0026#34;: \u0026#34;enabled\u0026#34; }, \u0026#34;// additional services...\u0026#34; } Status codes Code Description 200 OK No error Update cluster services configuration PUT /v1/cluster/services_configuration Update the cluster services settings.\nRequired permissions Permission name update_cluster Request Example HTTP request PUT /cluster/services_configuration Example JSON body { \u0026#34;cm_server\u0026#34;: { \u0026#34;operating_mode\u0026#34;: \u0026#34;disabled\u0026#34; }, \u0026#34;// additional services...\u0026#34; } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Include a services configuration object with updated fields in the request body.\nResponse Returns the updated services configuration object.\nExample JSON body { \u0026#34;cm_server\u0026#34;: { \u0026#34;operating_mode\u0026#34;: \u0026#34;disabled\u0026#34; }, \u0026#34;mdns_server\u0026#34;: { \u0026#34;operating_mode\u0026#34;: \u0026#34;enabled\u0026#34; }, \u0026#34;// additional services...\u0026#34; } Status codes Code Description 200 OK No error ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/sets/","uriRel":"/rs/databases/active-active/develop/data-types/sets/","title":"Sets in Active-Active databases","tags":[],"keywords":[],"description":"Information about using sets with an Active-Active database.","content":"A Redis set is an unordered collection of strings. It is possible to add, remove, and test for the existence of members with Redis commands. A Redis set maintains a unique collection of elements. Sets can be great for maintaining a list of events (click streams), users (in a group conversation), products (in recommendation lists), engagements (likes, shares) and so on.\nSets in Active-Active databases behave the same and maintain additional metadata to achieve an \u0026ldquo;OR-Set\u0026rdquo; behavior to handle concurrent conflicting writes. With the OR-Set behavior, writes across multiple Active-Active database instances are typically unioned except in cases of conflicts. Conflicting instance writes can happen when a Active-Active database instance deletes an element while the other adds the same element. In this case and observed remove rule is followed. That is, remove can only remove instances it has already seen and in all other cases element add wins.\nHere is an example of an \u0026ldquo;add wins\u0026rdquo; case:\nTime CRDB Instance1 CRDB Instance2 t1 SADD key1 “a” t2 SADD key1 “b” t3 SMEMBERS key1 “a” SMEMBERS key1 “b” t4 — Sync — — Sync — t3 SMEMBERS key1 “a” “b” SMEMBERS key1 “a” “b” Here is an example of an \u0026ldquo;observed remove\u0026rdquo; case.\nTime CRDB Instance1 CRDB Instance2 t1 SMEMBERS key1 “a” “b” SMEMBERS key1 “a” “b” t2 SREM key1 “a” SADD key1 “c” t3 SREM key1 “c” t4 — Sync — — Sync — t3 SMEMBERS key1 “c” “b” SMEMBERS key1 “c” “b” ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/shard/","uriRel":"/rs/references/rest-api/objects/shard/","title":"Shard object","tags":[],"keywords":[],"description":"An object that represents a database shard","content":"An API object that represents a Redis shard in a database.\nName Type/Value Description uid string Cluster unique ID of shard assigned_slots string Shards hash slot range backup backup object Current status of scheduled periodic backup process bdb_uid integer The ID of the database this shard belongs to bigstore_ram_weight number Shards RAM distribution weight detailed_status \u0026lsquo;busy\u0026rsquo;\n\u0026lsquo;down\u0026rsquo;\n\u0026lsquo;importing\u0026rsquo;\n\u0026lsquo;loading\u0026rsquo;\n\u0026lsquo;ok\u0026rsquo;\n\u0026lsquo;timeout\u0026rsquo;\n\u0026lsquo;trimming\u0026rsquo;\n\u0026lsquo;unknown\u0026rsquo; A more detailed status of the shard loading loading object Current status of dump file loading node_uid string The ID of the node this shard is located on redis_info redis_info object A sub-dictionary of the Redis INFO command report_timestamp string The time in which the shard\u0026rsquo;s info was collected (read-only) role \u0026lsquo;master\u0026rsquo;\n\u0026lsquo;slave\u0026rsquo; Role of this shard status \u0026lsquo;active\u0026rsquo;\n\u0026lsquo;inactive\u0026rsquo;\n\u0026lsquo;trimming\u0026rsquo; The current status of the shard sync sync object Shard\u0026rsquo;s current sync status and progress ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/statistics/shard-metrics/","uriRel":"/rs/references/rest-api/objects/statistics/shard-metrics/","title":"Shard metrics","tags":[],"keywords":[],"description":"Documents the shard metrics used with Redis Enterprise Software REST API calls.","content":" Metric name Type Description aof_rewrite_inprog float The number of simultaneous AOF rewrites that are in progress avg_ttl float Estimated average time to live of a random key (msec) big_del_flash float Rate of key deletes for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_del_ram float Rate of key deletes for keys in RAM (BigRedis) (key access/sec); this includes write misses (new keys created). Only returned when BigRedis is enabled. big_fetch_flash float Rate of key reads/updates for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_fetch_ram float Rate of key reads/updates for keys in RAM (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_io_ratio_flash float Rate of key operations on flash. Can be used to compute the ratio of I/O operations (key access/sec). Only returned when BigRedis is enabled. big_io_ratio_redis float Rate of Redis operations on keys. Can be used to compute the ratio of I/O operations) (key access/sec). Only returned when BigRedis is enabled. big_write_flash float Rate of key writes for keys on flash (BigRedis) (key access/sec). Only returned when BigRedis is enabled. big_write_ram float Rate of key writes for keys in RAM (BigRedis) (key access/sec); this includes write misses (new keys created). Only returned when BigRedis is enabled. bigstore_io_dels float Rate of key deletions from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_io_read_bytes float Throughput of I/O read operations against backend flash for all shards of the DB (BigRedis) (bytes/sec). Only returned when BigRedis is enabled. bigstore_io_reads float Rate of key reads from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_io_write_bytes float Throughput of I/O write operations against backend flash for all shards of the DB (BigRedis) (bytes/sec). Only returned when BigRedis is enabled. bigstore_io_writes float Rate of key writes from flash (key access/sec). Only returned when BigRedis is enabled. bigstore_iops float Rate of I/O operations against backend flash for all shards of the DB (BigRedis) (ops/sec). Only returned when BigRedis is enabled. bigstore_kv_ops float Rate of value read/write/del operations against backend flash for all shards of the DB (BigRedis) (key access/sec). Only returned when BigRedis is enabled. bigstore_objs_flash float Key count on flash (BigRedis). Only returned when BigRedis is enabled. bigstore_objs_ram float Key count in RAM (BigRedis). Only returned when BigRedis is enabled. bigstore_throughput float Throughput of I/O operations against backend flash for all shards of the DB (BigRedis) (bytes/sec). Only returned when BigRedis is enabled. blocked_clients float Count the clients waiting on a blocking call connected_clients float Number of client connections to the specific shard disk_frag_ratio float Flash fragmentation ratio (used/required). Only returned when BigRedis is enabled. evicted_objects float Rate of key evictions from DB (evictions/sec) expired_objects float Rate keys expired in DB (expirations/sec) fork_cpu_system float % cores utilization in system mode for the Redis shard fork child process fork_cpu_user float % cores utilization in user mode for the Redis shard fork child process last_save_time float Time of the last RDB save main_thread_cpu_system float % cores utilization in system mode for the Redis shard main thread main_thread_cpu_user float % cores utilization in user mode for the Redis shard main thread mem_frag_ratio float RAM fragmentation ratio (RSS/allocated RAM) mem_not_counted_for_evict float Portion of used_memory (in bytes) not counted for eviction and OOM errors mem_size_lua float Redis Lua scripting heap size (bytes) no_of_expires float Number of volatile keys on the shard no_of_keys float Number of keys in DB pubsub_channels float Count the pub/sub channels with subscribed clients pubsub_patterns float Count the pub/sub patterns with subscribed clients rdb_changes_since_last_save float Count changes since last RDB save read_hits float Rate of read operations accessing an existing key (ops/sec) read_misses float Rate of read operations accessing a nonexistent key (ops/sec) shard_cpu_system float % cores utilization in system mode for the Redis shard process shard_cpu_user float % cores utilization in user mode for the Redis shard process total_req float Rate of operations on DB (ops/sec) used_memory float Memory used by shard (in BigRedis this includes flash) (bytes) used_memory_peak float The largest amount of memory used by this shard (bytes) used_memory_rss float Resident set size of this shard (bytes) write_hits float Rate of write operations accessing an existing key (ops/sec) write_misses float Rate of write operations accessing a nonexistent key (ops/sec) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/shards-stats/","uriRel":"/rs/references/rest-api/requests/shards-stats/","title":"Shards stats requests","tags":[],"keywords":[],"description":"Shard statistics requests","content":" Method Path Description GET /v1/shards/stats Get stats for all shards GET /v1/shards/stats/{uid} Get stats for a specific shard Get all shards stats GET /v1/shards/stats Get statistics for all shards.\nRequired permissions Permission name view_all_shard_stats Request Example HTTP request GET /shards/stats?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description parent_uid integer Only return shard from the given BDB ID (optional) interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) metrics list Comma-separated list of metric names for which we want statistics (default is all) (optional) Response Returns a JSON array of statistics for all shards.\nExample JSON body [ { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;assigned_slots\u0026#34;: \u0026#34;0-8191\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:35Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:27:40Z\u0026#34;, \u0026#34;used_memory_peak\u0026#34;: 5888264.0, \u0026#34;used_memory_rss\u0026#34;: 5888264.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;pubsub_patterns\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;last_save_time\u0026#34;: 1432541051.0, \u0026#34;sync_partial_ok\u0026#34;: 0.0, \u0026#34;connected_clients\u0026#34;: 9.0, \u0026#34;avg_ttl\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;used_memory\u0026#34;: 5651440.0, \u0026#34;sync_full\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;blocked_clients\u0026#34;: 0.0, \u0026#34;pubsub_channels\u0026#34;: 0.0, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;no_of_expires\u0026#34;: 0.0, \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;sync_partial_err\u0026#34;: 0.0, \u0026#34;rdb_changes_since_last_save\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:40Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:27:45Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] }, { \u0026#34;uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;assigned_slots\u0026#34;: \u0026#34;8192-16383\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:35Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:27:40Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:27:40Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:27:45Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] } ] Status codes Code Description 200 OK No error 404 Not Found No shards exist Get shard stats GET /v1/shards/stats/{int: uid} Get statistics for a specific shard.\nRequired permissions Permission name view_shard_stats Request Example HTTP request GET /shards/stats/1?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the shard requested. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for the specified shard.\nExample JSON body { \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;node_uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:24:13Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:24:18Z\u0026#34;, \u0026#34;avg_ttl\u0026#34;: 0.0, \u0026#34;blocked_clients\u0026#34;: 0.0, \u0026#34;connected_clients\u0026#34;: 9.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:24:18Z\u0026#34;, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;last_save_time\u0026#34;: 1432541051.0, \u0026#34;used_memory\u0026#34;: 5651440.0, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;used_memory_peak\u0026#34;: 5888264.0, \u0026#34;used_memory_rss\u0026#34;: 5888264.0, \u0026#34;no_of_expires\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;pubsub_channels\u0026#34;: 0.0, \u0026#34;pubsub_patterns\u0026#34;: 0.0, \u0026#34;rdb_changes_since_last_save\u0026#34;: 0.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:24:13Z\u0026#34;, \u0026#34;sync_full\u0026#34;: 0.0, \u0026#34;sync_partial_err\u0026#34;: 0.0, \u0026#34;sync_partial_ok\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:24:18Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:24:23Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] } Status codes Code Description 200 OK No error 404 Not Found Shard does not exist 406 Not Acceptable Shard isn\u0026rsquo;t currently active ","categories":["RS"]},{"uri":"/rs/clusters/logging/redis-slow-log/","uriRel":"/rs/clusters/logging/redis-slow-log/","title":"View Redis slow log","tags":[],"keywords":[],"description":"","content":"On the Database \u0026gt; Slow Log page, you can view Slow Log details for Redis Enterprise Software (RS) databases.\nRedis Slow Log is one of the best tools for debugging and tracing your Redis database, especially if you experience high latency and high CPU usage with Redis operations. Because Redis is based on a single threaded architecture, Redis Slow Log can be much more useful than slow log mechanisms of multi-threaded database systems such as MySQL Slow Query Log.\nUnlike tools that introduce lock overhead (which complicates the debugging process), Redis Slow Log is highly effective at showing the actual processing time of each command.\nRedis Enterprise Software includes enhancements to the standard Redis Slow Log capabilities that allow you to analyze the execution time complexity of each command. This enhancement can help you better analyze Redis operations, allowing you to compare the differences between execution times of the same command, observe spikes in CPU usage, and more.\nThis is especially useful with complex commands such as ZUNIONSTORE, ZINTERSTORE and ZRANGEBYSCORE.\nThe enhanced RS Slow Log adds the Complexity Info field to the output data.\nView the Complexity Info data by its respective Command in the table below:\nCommand Value of interest Complexity LINSERT N - list len O(N) LREM N - list len O(N) LTRIM N - number of removed elements O(N) PUBLISH N - number of channel subscribersM - number of subscribed patterns O(N+M) PSUBSCRIBE N - number of patterns client is subscribed toargc - number of arguments passed to the command O(argc*N) PUNSUBSCRIBE N - number of patterns client is subscribed toM - total number of subscribed patternsargc - number of arguments passed to the command O(argc*(N+M)) SDIFF N - total number of elements in all sets O(N) SDIFFSTORE N - total number of elements in all sets O(N) SINTER N - number of elements in smallest setargc - number of arguments passed to the command O(argc*N) SINTERSTORE N - number of elements in smallest setargc - number of arguments passed to the command O(argc*N) SMEMBERS N - number of elements in a set O(N) SORT N - number of elements in the when no sorting list/set/zsetM - number of elements in result O(N+M*log(M))O(N) SUNION N - number of elements in all sets O(N) SUNIONSTORE N - number of elements in all sets O(N) UNSUBSCRIBE N - total number of clients subscribed to all channels O(N) ZADD N - number of elements in the zset O(log(N)) ZCOUNT N - number of elements in the zsetM - number of elements between min and max O(log(N)+M) ZINCRBY N - number of elements in the zset O(log(N)) ZINTERSTORE N – number of elements in the smallest zsetK – number of zsetsM – number of elements in the results set O(N*K)+O(M*log(M)) ZRANGE N – number of elements in the zsetM – number of results O(log(N)+M) ZRANGEBYSCORE N – number of elements in the zsetM – number of results O(log(N)+M) ZRANK N – number of elements in the zset O(log(N)) ZREM N – number of elements in the zsetargc – number of arguments passed to the command O(argc*log(N)) ZREMRANGEBYRANK N – number of elements in the zsetargc – number of arguments passed to the command O(log(N)+M) ZREMRANGEBYSCORE N – number of elements in the zsetM – number of elements removed O(log(N)+M) ZREVRANGE N – number of elements in the zsetM – number of results O(log(N)+M) ZREVRANK N – number of elements in the zset O(log(N)) ZUNIONSTORE N – sum of element counts of all zsetsM – element count of result O(N)+O(M*log(M)) ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/node/snapshot/","uriRel":"/rs/references/cli-utilities/rladmin/node/snapshot/","title":"rladmin node snapshot","tags":[],"keywords":[],"description":"Manages snapshots of the state of a node&#39;s resources.","content":"Manages snapshots of the state of a node\u0026rsquo;s shards and endpoints.\nnode snapshot create Creates a snapshot of a node\u0026rsquo;s current state.\nrladmin node \u0026lt;ID\u0026gt; snapshot create \u0026lt;name\u0026gt; Parameters Parameter Type/Value Description node integer Creates a snapshot of the specified node name string Name of the created snapshot Returns Returns Done if the snapshot was created successfully. Otherwise, returns an error.\nExample $ rladmin node 1 snapshot create snap1 Creating node snapshot \u0026#39;snap1\u0026#39; for node:1 Done. node snapshot delete Deletes an existing snapshot of a node.\nrladmin node \u0026lt;ID\u0026gt; snapshot delete \u0026lt;name\u0026gt; Parameters Parameter Type/Value Description node integer Deletes a snapshot of the specified node name string Deletes the specified snapshot Returns Returns Done if the snapshot was deleted successfully. Otherwise, returns an error.\nExample $ rladmin node 1 snapshot delete snap1 Deleting node snapshot \u0026#39;snap1\u0026#39; for node:1 Done. node snapshot list Displays a list of created snapshots for the specified node.\nrladmin node \u0026lt;ID\u0026gt; snapshot list Parameters Parameter Type/Value Description node integer Displays snapshots of the specified node Returns Returns a list of snapshots of the specified node.\nExample $ rladmin node 2 snapshot list Name Node Time snap2 2 2022-05-12T19:27:51Z node snapshot restore Restores a node as close to the stored snapshot as possible.\nrladmin node \u0026lt;ID\u0026gt; snapshot restore \u0026lt;name\u0026gt; Parameters Parameter Type/Value Description node integer Restore the specified node from a snapshot. restore string Name of the snapshot used to restore the node. Returns Returns Snapshot restore completed successfully if the actions needed to restore the snapshot completed successfully. Otherwise, it returns an error.\nExample $ rladmin node 2 snapshot restore snap2 Reading node snapshot \u0026#39;snap2\u0026#39; for node:2 Planning restore Planned actions: * migrate redis:15 to node:2 * failover redis:14 * migrate redis:17 to node:2 * failover redis:16 * migrate redis:19 to node:2 * failover redis:18 * migrate redis:21 to node:2 * failover redis:20 Proceed?[Y]es/[N]o? Y 2022-05-12T19:43:31.486613 Scheduling 8 actions [2022-05-12T19:43:31.521422 Actions Status: 8 waiting ] * [migrate redis:21 to node:2] waiting =\u0026gt; executing * [migrate redis:19 to node:2] waiting =\u0026gt; executing * [migrate redis:17 to node:2] waiting =\u0026gt; executing * [migrate redis:15 to node:2] waiting =\u0026gt; executing [2022-05-12T19:43:32.586084 Actions Status: 4 executing | 4 waiting ] * [migrate redis:21 to node:2] executing =\u0026gt; finished * [migrate redis:19 to node:2] executing =\u0026gt; finished * [migrate redis:17 to node:2] executing =\u0026gt; finished * [migrate redis:15 to node:2] executing =\u0026gt; finished * [failover redis:20] waiting =\u0026gt; executing * [failover redis:18] waiting =\u0026gt; executing * [failover redis:16] waiting =\u0026gt; executing * [failover redis:14] waiting =\u0026gt; executing [2022-05-12T19:43:33.719496 Actions Status: 4 finished | 4 executing ] * [failover redis:20] executing =\u0026gt; finished * [failover redis:18] executing =\u0026gt; finished * [failover redis:16] executing =\u0026gt; finished * [failover redis:14] executing =\u0026gt; finished Snapshot restore completed successfully. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/snapshot_policy/","uriRel":"/rs/references/rest-api/objects/bdb/snapshot_policy/","title":"Snapshot policy object","tags":[],"keywords":[],"description":"Documents the snapshot_policy object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description secs integer Interval in seconds between snapshots writes integer Number of write changes required to trigger a snapshot ","categories":["RS"]},{"uri":"/rs/installing-upgrading/configuring/change-location-socket-files/","uriRel":"/rs/installing-upgrading/configuring/change-location-socket-files/","title":"Change socket file locations","tags":[],"keywords":[],"description":"","content":"There are two default locations for the socket files in Redis Enterprise Software (RS):\n/tmp - In clean installations of RS version lower than 5.2.2 /var/opt/redislabs/run - In clean installations of RS version 5.2.2 and higher We made this change because some customers have maintenance procedures that delete the /tmp directory.\nWhen you upgrade from a RS version lower than 5.2.2 to 5.2.2 and higher, the socket files are not moved to the new location by default. During installation you can specify a custom location for the socket files, but after installation you must use this procedure to move the socket files.\nTo change the location of the socket files:\nOn each node in the cluster, run:\nsudo rlutil create_socket_path socket_path=/var/opt/redislabs/run Identify the master node:\nOn any node in the cluster, run: rladmin status nodes Find the node that has the master role. On the master node, run:\nsudo rlutil set_socket_path socket_path=/var/opt/redislabs/run Now the master node points to the new socket file location. To update the location for all other nodes, you must restart RS on each node.\nTo restart RS, on each node in the cluster one at a time run:\nsudo service rlec_supervisor restart Now all nodes point to the new socket file location. To update the location for the databases in the cluster, you must restart each database.\nWarning - Database restart can cause interruptions in data traffic. To restart the databases, for each database in the cluster run:\nrladmin restart db \u0026lt;db name\u0026gt; ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/sorted-sets/","uriRel":"/rs/databases/active-active/develop/data-types/sorted-sets/","title":"Sorted sets in Active-Active databases","tags":[],"keywords":[],"description":"Information about using sorted sets with an Active-Active database.","content":" Note: Redis Geospatial (Geo) is based on Sorted Sets, so the same Active-Active database development instructions apply to Geo. Similar to Redis Sets, Redis Sorted Sets are non-repeating collections of Strings. The difference between the two is that every member of a Sorted Set is associated with a score used to order the Sorted Set from lowest to highest. While members are unique, they may have the same score.\nWith Sorted Sets, you can quickly add, remove or update elements as well as get ranges by score or by rank (position). Sorted Sets in Active-Active databases behave the same and maintain additional metadata to handle concurrent conflicting writes. Conflict resolution is done in two phases:\nFirst, the database resolves conflict at the set level using \u0026ldquo;OR Set\u0026rdquo; (Observed-Remove Set). With OR-Set behavior, writes across multiple Active-Active database instances are typically unioned except in cases of conflicts. Conflicting writes can happen when an Active-Active database instance deletes an element while the other adds or updates the same element. In this case, an observed Remove rule is followed, and only instances it has already seen are removed. In all other cases, the Add / Update element wins. Second, the database resolves conflict at the score level. In this case, the score is treated as a counter and applies the same conflict resolution as regular counters. See the following examples to get familiar with Sorted Sets' behavior in Active-Active database:\nExample of Simple Sorted Set with No Conflict:\nTime CRDB Instance 1 CRDB Instance 2 t1 ZADD Z 1.1 x t2 — Sync — — Sync — t3 ZADD Z 1.2 y t4 — Sync — — Sync — t5 ZRANGE Z 0 -1 =\u0026gt; x y ZRANGE Z 0 -1 =\u0026gt; x y Explanation: When adding two different elements to a Sorted Set from different replicas (in this example, x with score 1.1 was added by Instance 1 to Sorted Set Z, and y with score 1.2 was added by Instance 2 to Sorted Set Z) in a non-concurrent manner (i.e. each operation happened separately and after both instances were in sync), the end result is a Sorted Set including both elements in each Active-Active database instance. Example of Sorted Set and Concurrent Add:\nTime CRDB Instance 1 CRDB Instance 2 t1 ZADD Z 1.1 x t2 ZADD Z 2.1 x t3 ZSCORE Z x =\u0026gt; 1.1 ZSCORE Z x =\u0026gt; 2.1 t4 — Sync — — Sync — t5 ZSCORE Z x =\u0026gt; 2.1 ZSCORE Z x =\u0026gt; 2.1 Explanation: When concurrently adding an element x to a Sorted Set Z by two different Active-Active database instances (Instance 1 added score 1.1 and Instance 2 added score 2.1), the Active-Active database implements Last Write Win (LWW) to determine the score of x. In this scenario, Instance 2 performed the ZADD operation at time t2\u0026gt;t1 and therefore the Active-Active database sets the score 2.1 to x.\nExample of Sorted Set with Concurrent Add Happening at the Exact Same Time:\nTime CRDB Instance 1 CRDB Instance 2 t1 ZADD Z 1.1 x ZADD Z 2.1 x t2 ZSCORE Z x =\u0026gt; 1.1 ZSCORE Z x =\u0026gt; 2.1 t3 — Sync — — Sync — t4 ZSCORE Z x =\u0026gt; 1.1 ZSCORE Z x =\u0026gt; 1.1 Explanation: The example above shows a relatively rare situation, in which two Active-Active database instances concurrently added the same element x to a Sorted Set at the same exact time but with a different score, i.e. Instance 1 added x with a 1.1 score and Instance 2 added x with a 2.1 score. After syncing, the Active-Active database realized that both operations happened at the same time and resolved the conflict by arbitrarily (but consistently across all Active-Active database instances) giving precedence to Instance 1. Example of Sorted Set with Concurrent Counter Increment:\nTime CRDB Instance 1 CRDB Instance 2 t1 ZADD Z 1.1 x t2 — Sync — — Sync — t3 ZINCRBY Z 1.0 x ZINCRBY Z 1.0 x t4 — Sync — — Sync — t5 ZSCORE Z x =\u0026gt; 3.1 ZSCORE Z x =\u0026gt; 3.1 Explanation: The result is the sum of all ZINCRBY operations performed by all Active-Active database instances.\nExample of Removing an Element from a Sorted Set:\nTime CRDB Instance 1 CRDB Instance 2 t1 ZADD Z 4.1 x t2 — Sync — — Sync — t3 ZSCORE Z x =\u0026gt; 4.1 ZSCORE Z x =\u0026gt; 4.1 t4 ZREM Z x ZINCRBY Z 2.0 x t5 ZSCORE Z x =\u0026gt; nill ZSCORE Z x =\u0026gt; 6.1 t6 — Sync — — Sync — t7 ZSCORE Z x =\u0026gt; 2.0 ZSCORE Z x =\u0026gt; 2.0 Explanation: At t4 - t5, concurrent ZREM and ZINCRBY operations ran on Instance 1 and Instance 2 respectively. Before the instances were in sync, the ZREM operation could only delete what had been seen by Instance 1, so Instance 2 was not affected. Therefore, the ZSCORE operation shows the local effect on x. At t7, after both instances were in-sync, the Active-Active database resolved the conflict by subtracting 4.1 (the value of element x in Instance 1) from 6.1 (the value of element x in Instance 2).\n","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/state-machine/","uriRel":"/rs/references/rest-api/objects/state-machine/","title":"State machine object","tags":[],"keywords":[],"description":"An object that represents a state machine.","content":"A state machine object tracks the status of database actions.\nA state machine contains the following attributes:\nName Type/Value Description action_uid string A globally unique identifier of the action object_name string Name of the object being manipulated by the state machine status pending Requested state machine has not started active State machine is currently running completed Operation complete failed Operation or state machine failed name string Name of the running (or failed) state machine state string Current state within the state machine, when known error string A descriptive error string for failed state machine, when known ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/statistics/","uriRel":"/rs/references/rest-api/objects/statistics/","title":"Statistics","tags":[],"keywords":[],"description":"An object that contains metrics for clusters, databases, nodes, or shards","content":"Statistics overview Clusters, databases, nodes, and shards collect various statistics at regular time intervals. You can view the statistics for these objects via GET stats requests to their respective endpoints:\nCluster stats Database stats Node stats Shard stats Response object Statistics returned from API requests always contain the following fields:\ninterval: a string that represents the statistics time interval. Valid values include: 1sec 10sec 5min 15min 1hour 12hour 1week stime: a timestamp that represents the beginning of the interval, in the format \u0026ldquo;2015-05-27T12:00:00Z\u0026rdquo; etime: a timestamp that represents the end of the interval, in the format \u0026ldquo;2015-05-27T12:00:00Z\u0026rdquo; The statistics returned by the API also contain fields that represent the values of different metrics for an object during the specified time interval.\nMore details about the metrics relevant to each object:\nCluster metrics DB metrics Node metrics Shard metrics Note: Some statistics are for internal use only. They are not documented and should be ignored. Note: Certain statistics will only appear in API responses when they are relevant. Optional URL parameters There are several optional URL parameters you can pass to the various GET stats requests in order to filter the returned statistics.\nstime: limit the start of the time range of the returned statistics etime: limit the end of the time range of the returned statistics metrics: only return the statistics for the specified metrics (comma-separated list) Maximum number of samples per interval The system retains a maximum number of most recent samples for each interval.\nInterval Max samples 1sec 10 10sec 30 5min 12 15min 96 1hour 168 12hour 62 1week 53 The actual number of samples returned by a GET stats request depends on how many samples are available and any filters applied by the optional URL parameters. For example, newly created objects (clusters, nodes, databases, or shards) or a narrow time filter range will return fewer samples.\nNote: To reduce load generated by stats collection, relatively inactive databases or shards (less than 5 ops/sec) do not collect 1sec stats at one second intervals. Instead, they collect 1sec stats every 2-5 seconds but still retain the same maximum number of samples. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/stats/","uriRel":"/rs/references/rest-api/requests/bdbs/stats/","title":"Database stats requests","tags":[],"keywords":[],"description":"Database statistics requests","content":" Method Path Description GET /v1/bdbs/stats Get stats for all databases GET /v1/bdbs/stats/{uid} Get stats for a specific database Get all database stats GET /v1/bdbs/stats Get statistics for all databases.\nPermissions Permission name Roles view_all_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/stats?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for all databases.\nExample JSON body [ { \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T12:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T12:59:59Z\u0026#34;, \u0026#34;avg_latency\u0026#34;: 0.0, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T00:00:00Z\u0026#34;, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;instantaneous_ops_per_sec\u0026#34;: 0.00011973180076628352, \u0026#34;last_req_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;last_res_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;used_memory\u0026#34;: 5656299.362068966, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;monitor_sessions_count\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;other_req\u0026#34;: 0.0, \u0026#34;other_res\u0026#34;: 0.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;read_req\u0026#34;: 0.0, \u0026#34;read_res\u0026#34;: 0.0, \u0026#34;total_connections_received\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;total_res\u0026#34;: 0.0, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;write_req\u0026#34;: 0.0, \u0026#34;write_res\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T13:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T13:59:59Z\u0026#34;, \u0026#34;avg_latency\u0026#34;: 599.08, \u0026#34;// additional fields...\u0026#34; } ] }, { \u0026#34;uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T12:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T12:59:59Z\u0026#34;, \u0026#34;avg_latency\u0026#34;: 0.0, \u0026#34;// additional fields...\u0026#34; }, { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T13:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T13:59:59Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] } ] Status codes Code Description 200 OK No error 404 Not Found No bdbs exist Example requests cURL $ curl -k -u \u0026#34;[username]:[password]\u0026#34; -X GET https://[host][:port]/v1/bdbs/stats?interval=1hour Python import requests url = \u0026#34;https://[host][:port]/v1/bdbs/stats?interval=1hour\u0026#34; auth = (\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;GET\u0026#34;, url, auth=auth) print(response.text) Get database stats GET /v1/bdbs/stats/{int: uid} Get statistics for a specific database.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/stats/1?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the BDB requested. Query parameters Field Type Description interval string Time interval for for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for a specific database.\nExample JSON body { \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T12:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T12:59:59Z\u0026#34;, \u0026#34;avg_latency\u0026#34;: 0.0, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;evicted_objects\u0026#34;: 0.0, \u0026#34;pubsub_channels\u0026#34;: 0, \u0026#34;pubsub_patterns\u0026#34;: 0, \u0026#34;expired_objects\u0026#34;: 0.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;instantaneous_ops_per_sec\u0026#34;: 0.00011973180076628352, \u0026#34;last_req_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;last_res_time\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;used_memory\u0026#34;: 5656299.362068966, \u0026#34;mem_size_lua\u0026#34;: 35840.0, \u0026#34;monitor_sessions_count\u0026#34;: 0.0, \u0026#34;no_of_keys\u0026#34;: 0.0, \u0026#34;other_req\u0026#34;: 0.0, \u0026#34;other_res\u0026#34;: 0.0, \u0026#34;read_hits\u0026#34;: 0.0, \u0026#34;read_misses\u0026#34;: 0.0, \u0026#34;read_req\u0026#34;: 0.0, \u0026#34;read_res\u0026#34;: 0.0, \u0026#34;total_connections_received\u0026#34;: 0.0, \u0026#34;total_req\u0026#34;: 0.0, \u0026#34;total_res\u0026#34;: 0.0, \u0026#34;write_hits\u0026#34;: 0.0, \u0026#34;write_misses\u0026#34;: 0.0, \u0026#34;write_req\u0026#34;: 0.0, \u0026#34;write_res\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T13:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T13:59:59Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] } Status codes Code Description 200 OK No error 404 Not Found bdb does not exist 406 Not Acceptable bdb isn\u0026rsquo;t currently active 503 Service Unavailable bdb is in recovery state ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/stats/","uriRel":"/rs/references/rest-api/requests/cluster/stats/","title":"Cluster stats requests","tags":[],"keywords":[],"description":"Cluster statistics requests","content":" Method Path Description GET /v1/cluster/stats Get cluster stats Get cluster stats GET /v1/cluster/stats Get cluster statistics.\nPermissions Permission name Roles view_cluster_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /cluster/stats/1?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for the cluster.\nExample JSON body { \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T12:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T12:59:59Z\u0026#34;, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.8533959401503577, \u0026#34;cpu_system\u0026#34;: 0.01602159448549579, \u0026#34;cpu_user\u0026#34;: 0.08721123782294203, \u0026#34;egress_bytes\u0026#34;: 1111.2184745131947, \u0026#34;ephemeral_storage_avail\u0026#34;: 3406676307.1449075, \u0026#34;ephemeral_storage_free\u0026#34;: 4455091440.360014, \u0026#34;free_memory\u0026#34;: 2745470765.673594, \u0026#34;ingress_bytes\u0026#34;: 220.84083194769272, \u0026#34;interval\u0026#34;: \u0026#34;1week\u0026#34;, \u0026#34;persistent_storage_avail\u0026#34;: 3406676307.1533995, \u0026#34;persistent_storage_free\u0026#34;: 4455091440.088265, \u0026#34;total_req\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1hour\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-27T13:00:00Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T13:59:59Z\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] } Example requests cURL $ curl -k -u \u0026#34;[username]:[password]\u0026#34; -X GET https://[host][:port]/v1/cluster/stats?interval=1hour Python import requests url = \u0026#34;https://[host][:port]/v1/cluster/stats?interval=1hour\u0026#34; auth = (\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) response = requests.request(\u0026#34;GET\u0026#34;, url, auth=auth) print(response.text) Status codes Code Description 200 OK No error 500 Internal Server Error Internal server error ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/stats/","uriRel":"/rs/references/rest-api/requests/nodes/stats/","title":"Node stats requests","tags":[],"keywords":[],"description":"Node statistics requests","content":" Method Path Description GET /v1/nodes/stats Get stats for all nodes GET /v1/nodes/stats/{uid} Get stats for a single node Get all nodes stats GET /v1/nodes/stats Get statistics for all nodes.\nRequired permissions Permission name view_all_nodes_stats Request Example HTTP request GET /nodes/stats?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns a JSON array of statistics for all nodes.\nExample JSON body [ { \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:11Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.5499999999883585, \u0026#34;cpu_system\u0026#34;: 0.03499999999985448, \u0026#34;cpu_user\u0026#34;: 0.38000000000101863, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 2929315840.0, \u0026#34;ephemeral_storage_free\u0026#34;: 3977830400.0, \u0026#34;free_memory\u0026#34;: 893485056.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;persistent_storage_avail\u0026#34;: 2929315840.0, \u0026#34;persistent_storage_free\u0026#34;: 3977830400.0, \u0026#34;total_req\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:13Z\u0026#34;, \u0026#34;cpu_idle\u0026#34;: 1.2, \u0026#34;// additional fields...\u0026#34; } ] }, { \u0026#34;uid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:11Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.5499999999883585, \u0026#34;cpu_system\u0026#34;: 0.03499999999985448, \u0026#34;cpu_user\u0026#34;: 0.38000000000101863, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 2929315840.0, \u0026#34;ephemeral_storage_free\u0026#34;: 3977830400.0, \u0026#34;free_memory\u0026#34;: 893485056.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;persistent_storage_avail\u0026#34;: 2929315840.0, \u0026#34;persistent_storage_free\u0026#34;: 3977830400.0, \u0026#34;total_req\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:13Z\u0026#34;, \u0026#34;cpu_idle\u0026#34;: 1.2, \u0026#34;// additional fields...\u0026#34; } ] } ] Status codes Code Description 200 OK No error 404 Not Found No nodes exist Get node stats GET /v1/nodes/stats/{int: uid} Get statistics for a node.\nRequired permissions Permission name view_node_stats Request Example HTTP request GET /nodes/stats/1?interval=1hour\u0026amp;stime=2014-08-28T10:00:00Z Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The unique ID of the node requested. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 End time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for the specified node.\nExample JSON body { \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;intervals\u0026#34;: [ { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:11Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;conns\u0026#34;: 0.0, \u0026#34;cpu_idle\u0026#34;: 0.5499999999883585, \u0026#34;cpu_system\u0026#34;: 0.03499999999985448, \u0026#34;cpu_user\u0026#34;: 0.38000000000101863, \u0026#34;egress_bytes\u0026#34;: 0.0, \u0026#34;ephemeral_storage_avail\u0026#34;: 2929315840.0, \u0026#34;ephemeral_storage_free\u0026#34;: 3977830400.0, \u0026#34;free_memory\u0026#34;: 893485056.0, \u0026#34;ingress_bytes\u0026#34;: 0.0, \u0026#34;persistent_storage_avail\u0026#34;: 2929315840.0, \u0026#34;persistent_storage_free\u0026#34;: 3977830400.0, \u0026#34;total_req\u0026#34;: 0.0 }, { \u0026#34;interval\u0026#34;: \u0026#34;1sec\u0026#34;, \u0026#34;stime\u0026#34;: \u0026#34;2015-05-28T08:40:12Z\u0026#34;, \u0026#34;etime\u0026#34;: \u0026#34;2015-05-28T08:40:13Z\u0026#34;, \u0026#34;cpu_idle\u0026#34;: 1.2, \u0026#34;// additional fields...\u0026#34; } ] } Status codes Code Description 200 OK No error 404 Not Found Node does not exist 406 Not Acceptable Node isn\u0026rsquo;t currently active 503 Service Unavailable Node is in recovery state ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/cluster/stats_archiver/","uriRel":"/rs/references/cli-utilities/rladmin/cluster/stats_archiver/","title":"rladmin cluster stats_archiver","tags":["configured"],"keywords":[],"description":"Enables/deactivates the stats archiver.","content":"Enables or deactivates the stats archiver, which logs statistics in CSV (comma-separated values) format.\nrladmin cluster stats_archiver { enabled | disabled } Parameters Parameter Description enabled Turn on the stats archiver disabled Turn off the stats archiver Returns Returns the updated status of the stats archiver.\nExample $ rladmin cluster stats_archiver enabled Status: enabled ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/services_configuration/stats_archiver/","uriRel":"/rs/references/rest-api/objects/services_configuration/stats_archiver/","title":"Stats archiver object","tags":[],"keywords":[],"description":"Documents the stats_archiver object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description operating_mode \u0026lsquo;disabled\u0026rsquo;\n\u0026lsquo;enabled\u0026rsquo; Enable/disable the stats archiver service ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/task/status/","uriRel":"/rs/references/cli-utilities/crdb-cli/task/status/","title":"crdb-cli task status","tags":[],"keywords":[],"description":"Shows the status of a specified Active-Active database task.","content":"Shows the status of a specified Active-Active database task.\ncrdb-cli task status --task-id \u0026lt;task_id\u0026gt; Parameters Parameter Value Description task-id \u0026lt;task_id\u0026gt; string An Active-Active database task ID (required) verbose N/A Returns detailed information when specified no-verbose N/A Returns limited information when specified The --verbose and --no-verbose options are mutually incompatible; specify one or the other.\nThe 404 Not Found error indicates an invalid task ID. Use the task list command to determine available task IDs.\nReturns Returns the status of an Active-Active database task.\nExample $ crdb-cli task status --task-id e1c49470-ae0b-4df8-885b-9c755dd614d0 Task-ID: e1c49470-ae0b-4df8-885b-9c755dd614d0 CRDB-GUID: 1d7741cc-1110-4e2f-bc6c-935292783d24 Operation: create_crdb Status: finished Worker-Name: crdb_worker:1:0 Started: 2022-10-12T09:33:41Z Ended: 2022-10-12T09:33:55Z ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/status/","uriRel":"/rs/references/cli-utilities/rladmin/status/","title":"rladmin status","tags":[],"keywords":[],"description":"Displays the current cluster status and topology information.","content":"Displays the current cluster status and topology information.\nstatus Displays the current status of all nodes, databases, database endpoints, and shards on the cluster.\nrladmin status [ extra \u0026lt;parameter\u0026gt; ] [ issues_only] Parameters Parameter Description extra \u0026lt;parameter\u0026gt; Extra options that show more information issues_only Filters out all items that have an OK status Extra parameter Description extra all Shows all extra information extra backups Shows periodic backup status extra frag Shows fragmented memory available after the restart extra nodestats Shows shards per node extra rack_id Shows rack_id if customer is not rack_aware extra redis_version Shows Redis version of all databases in the cluster extra state_machine Shows execution of state machine information extra watchdog Shows watchdog status Returns Returns tables of the status of all nodes, databases, and database endpoints on the cluster.\nIf issues_only is specified, it only shows instances that do not have an OK status.\nExample $ rladmin status extra all CLUSTER: OK. Cluster master: 1 (198.51.100.2) Cluster health: OK, [1, 0.13333333333333333, 0.03333333333333333] failures/minute - avg1 1.00, avg15 0.13, avg60 0.03. CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME MASTERS SLAVES OVERBOOKING_DEPTH SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION SHA RACK-ID STATUS node:1 master 198.51.100.2 3d99db1fdf4b 4 0 10.91GB 4/100 6 14.91GB/19.54GB 10.91GB/16.02GB 6.2.12-37 5c2106 - OK node:2 slave 198.51.100.3 fc7a3d332458 0 0 11.4GB 0/100 6 14.91GB/19.54GB 11.4GB/16.02GB 6.2.12-37 5c2106 - OK *node:3 slave 198.51.100.4 b87cc06c830f 0 0 11.4GB 0/100 6 14.91GB/19.54GB 11.4GB/16.02GB 6.2.12-37 5c2106 - OK DATABASES: DB:ID NAME TYPE STATUS SHARDS PLACEMENT REPLICATION PERSISTENCE ENDPOINT EXEC_STATE EXEC_STATE_MACHINE BACKUP_PROGRESS MISSING_BACKUP_TIME REDIS_VERSION db:3 database3 redis active 4 dense disabled disabled redis-11103.cluster.local:11103 N/A N/A N/A N/A 6.0.16 ENDPOINTS: DB:ID NAME ID NODE ROLE SSL WATCHDOG_STATUS db:3 database3 endpoint:3:1 node:1 single No OK SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY BACKUP_PROGRESS RAM_FRAG WATCHDOG_STATUS STATUS db:3 database3 redis:4 node:1 master 0-4095 2.08MB N/A 4.73MB OK OK db:3 database3 redis:5 node:1 master 4096-8191 2.08MB N/A 4.62MB OK OK db:3 database3 redis:6 node:1 master 8192-12287 2.08MB N/A 4.59MB OK OK db:3 database3 redis:7 node:1 master 12288-16383 2.08MB N/A 4.66MB OK OK status databases Displays the current status of all databases on the cluster.\nrladmin status databases [ extra \u0026lt;parameters\u0026gt; ] [ sort \u0026lt;column_titles\u0026gt; ] [ issues_only ] Parameters Parameter Description extra \u0026lt;parameter\u0026gt; Extra options that show more information sort \u0026lt;column_titles\u0026gt; Sort results by specified column titles issues_only Filters out all items that have an OK status Extra parameter Description extra all Shows all extra information extra backups Shows periodic backup status extra frag Shows fragmented memory available after the restart extra nodestats Shows shards per node extra rack_id Shows rack_id if customer is not rack_aware extra redis_version Shows Redis version of all databases in the cluster extra state_machine Shows execution of state machine information extra watchdog Shows watchdog status Returns Returns a table of the status of all databases on the cluster.\nIf sort \u0026lt;column_titles\u0026gt; is specified, the result is sorted by the specified table columns.\nIf issues_only is specified, it only shows databases that do not have an OK status.\nExample $ rladmin status databases sort REPLICATION PERSISTENCE DB:ID NAME TYPE STATUS SHARDS PLACEMENT REPLICATION PERSISTENCE ENDPOINT db:1 database1 redis active 1 dense disabled disabled redis-10269.testdbd11169.localhost:10269 db:2 database2 redis active 1 dense disabled snapshot redis-13897.testdbd11169.localhost:13897 db:3 database3 redis active 1 dense enabled snapshot redis-19416.testdbd13186.localhost:19416 status endpoints Displays the current status of all endpoints on the cluster.\nrladmin status endpoints [ node \u0026lt;id\u0026gt; ] [ extra \u0026lt;parameters\u0026gt; ] [ sort \u0026lt;column_titles\u0026gt; ] [ issues_only ] Parameters Parameter Description node \u0026lt;id\u0026gt; Only show endpoints for the specified node ID extra \u0026lt;parameter\u0026gt; Extra options that show more information sort \u0026lt;column_titles\u0026gt; Sort results by specified column titles issues_only Filters out all items that have an OK status Extra parameter Description extra all Shows all extra information extra backups Shows periodic backup status extra frag Shows fragmented memory available after the restart extra nodestats Shows shards per node extra rack_id Shows rack_id if customer is not rack_aware extra redis_version Shows Redis version of all endpoints in the cluster extra state_machine Shows execution of state machine information extra watchdog Shows watchdog status Returns Returns a table of the status of all endpoints on the cluster.\nIf sort \u0026lt;column_titles\u0026gt; is specified, the result is sorted by the specified table columns.\nIf issues_only is specified, it only shows endpoints that do not have an OK status.\nExample $ rladmin status endpoints DB:ID NAME ID NODE ROLE SSL db:1 database1 endpoint:1:1 node:1 single No db:2 database2 endpoint:2:1 node:2 single No db:3 database3 endpoint:3:1 node:3 single No status nodes Displays the current status of all nodes on the cluster.\nrladmin status nodes [ extra \u0026lt;parameters\u0026gt; ] [ sort \u0026lt;column_titles\u0026gt; ] [ issues_only ] Parameters Parameter Description extra \u0026lt;parameter\u0026gt; Extra options that show more information sort \u0026lt;column_titles\u0026gt; Sort results by specified column titles issues_only Filters out all items that have an OK status Extra parameter Description extra all Shows all extra information extra backups Shows periodic backup status extra frag Shows fragmented memory available after the restart extra nodestats Shows shards per node extra rack_id Shows rack_id if customer is not rack_aware extra redis_version Shows Redis version of all nodes in the cluster extra state_machine Shows execution of state machine information extra watchdog Shows watchdog status Returns Returns a table of the status of all nodes on the cluster.\nIf sort \u0026lt;column_titles\u0026gt; is specified, the result is sorted by the specified table columns.\nIf issues_only is specified, it only shows nodes that do not have an OK status.\nExample $ rladmin status nodes sort PROVISIONAL_RAM HOSTNAME CLUSTER NODES: NODE:ID ROLE ADDRESS EXTERNAL_ADDRESS HOSTNAME SHARDS CORES FREE_RAM PROVISIONAL_RAM VERSION STATUS node:1 master 198.51.100.2 3d99db1fdf4b 4/100 6 14.74GB/19.54GB 10.73GB/16.02GB 6.2.12-37 OK *node:3 slave 198.51.100.4 b87cc06c830f 0/100 6 14.74GB/19.54GB 11.22GB/16.02GB 6.2.12-37 OK node:2 slave 198.51.100.3 fc7a3d332458 0/100 6 14.74GB/19.54GB 11.22GB/16.02GB 6.2.12-37 OK status shards Displays the current status of all shards on the cluster.\nrladmin status shards [ node \u0026lt;id\u0026gt; ] [ db {db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt;} ] [ extra \u0026lt;parameters\u0026gt; ] [ sort \u0026lt;column_titles\u0026gt; ] [ issues_only ] Parameters Parameter Description node \u0026lt;id\u0026gt; Only show shards for the specified node ID db db:\u0026lt;id\u0026gt; Only show shards for the specified database ID db \u0026lt;name\u0026gt; Only show shards for the specified database name extra \u0026lt;parameter\u0026gt; Extra options that show more information sort \u0026lt;column_titles\u0026gt; Sort results by specified column titles issues_only Filters out all items that have an OK status Extra parameter Description extra all Shows all extra information extra backups Shows periodic backup status extra frag Shows fragmented memory available after the restart extra shardstats Shows shards per node extra rack_id Shows rack_id if customer is not rack_aware extra redis_version Shows Redis version of all shards in the cluster extra state_machine Shows execution of state machine information extra watchdog Shows watchdog status Returns Returns a table of the status of all shards on the cluster.\nIf sort \u0026lt;column_titles\u0026gt; is specified, the result is sorted by the specified table columns.\nIf issues_only is specified, it only shows shards that do not have an OK status.\nExample $ rladmin status shards sort USED_MEMORY ID SHARDS: DB:ID NAME ID NODE ROLE SLOTS USED_MEMORY STATUS db:3 database3 redis:6 node:1 master 8192-12287 2.04MB OK db:3 database3 redis:4 node:1 master 0-4095 2.08MB OK db:3 database3 redis:5 node:1 master 4096-8191 2.08MB OK db:3 database3 redis:7 node:1 master 12288-16383 2.08MB OK ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/status/","uriRel":"/rs/references/rest-api/objects/bdb/status/","title":"BDB status field","tags":[],"keywords":[],"description":"Documents the bdb status field used with Redis Enterprise Software REST API calls.","content":"The BDB status field is a read-only field that represents the database status.\nPossible status values:\nStatus Description Possible next status \u0026lsquo;active\u0026rsquo; Database is active and no special action is in progress \u0026lsquo;active-change-pending\u0026rsquo; \u0026lsquo;import-pending\u0026rsquo; \u0026lsquo;delete-pending\u0026rsquo; \u0026lsquo;active-change-pending\u0026rsquo; \u0026lsquo;active\u0026rsquo; \u0026lsquo;creation-failed\u0026rsquo; Initial database creation failed \u0026lsquo;delete-pending\u0026rsquo; Database deletion is in progress \u0026lsquo;import-pending\u0026rsquo; Dataset import is in progress \u0026lsquo;active\u0026rsquo; \u0026lsquo;pending\u0026rsquo; Temporary status during database creation \u0026lsquo;active\u0026rsquo;\n\u0026lsquo;creation-failed\u0026rsquo; \u0026lsquo;recovery\u0026rsquo; Not currently relevant (intended for future use) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/nodes/status/","uriRel":"/rs/references/rest-api/requests/nodes/status/","title":"Node status requests","tags":[],"keywords":[],"description":"Requests that return a node&#39;s hostname and role.","content":" Method Path Description GET /v1/nodes/status Get the status of all nodes GET /v1/nodes/{uid}/status Get a node\u0026rsquo;s status Get all node statuses GET /v1/nodes/status Gets the status of all nodes. Includes each node\u0026rsquo;s hostname and role in the cluster:\nPrimary nodes return \u0026quot;role\u0026quot;: \u0026quot;master\u0026quot;\nReplica nodes return \u0026quot;role\u0026quot;: \u0026quot;slave\u0026quot;\nRequired permissions Permission name view_node_info Request Example HTTP request GET /nodes/status Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response For each node in the cluster, returns a JSON object that contains the node\u0026rsquo;s hostname and role.\nExample JSON body { \u0026#34;1\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;3d99db1fdf4b\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; }, \u0026#34;2\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;fc7a3d332458\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;slave\u0026#34; }, \u0026#34;3\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;b87cc06c830f\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;slave\u0026#34; } } Status codes Code Description 200 OK No error Get node status GET /v1/nodes/{int: uid}/status Gets the status of a node. Includes the node\u0026rsquo;s hostname and role in the cluster:\nPrimary nodes return \u0026quot;role\u0026quot;: \u0026quot;master\u0026quot;\nReplica nodes return \u0026quot;role\u0026quot;: \u0026quot;slave\u0026quot;\nRequired permissions Permission name view_node_info Request Example HTTP request GET /nodes/1/status Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The node\u0026rsquo;s unique ID. Response Returns a JSON object that contains the node\u0026rsquo;s hostname and role.\nExample JSON body { \u0026#34;hostname\u0026#34;: \u0026#34;3d99db1fdf4b\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;master\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Node UID does not exist ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/ocsp/status/","uriRel":"/rs/references/rest-api/requests/ocsp/status/","title":"OCSP status requests","tags":[],"keywords":[],"description":"OCSP status requests","content":" Method Path Description GET /v1/ocsp/status Get OCSP status Get OCSP status GET /v1/ocsp/status Gets the latest cached status of the proxy certificate’s OCSP response.\nRequired permissions Permission name view_ocsp_status Request Example HTTP request GET /ocsp/status Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns an OCSP status object.\nExample JSON body { \u0026#34;responder_url\u0026#34;: \u0026#34;http://responder.ocsp.url.com\u0026#34;, \u0026#34;cert_status\u0026#34;: \u0026#34;REVOKED\u0026#34;, \u0026#34;produced_at\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:11 GMT\u0026#34;, \u0026#34;this_update\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:11 GMT\u0026#34;, \u0026#34;next_update\u0026#34;: \u0026#34;Wed, 22 Dec 2021 14:50:00 GMT\u0026#34;, \u0026#34;revocation_time\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:04 GMT\u0026#34; } Error codes When errors occur, the server returns a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description ocsp_unsupported_by_capability Not all nodes support OCSP capability invalid_ocsp_response The server returned a response that is not OCSP-compatible Status codes Code Description 200 OK Success 406 Not Acceptable Feature not supported in all nodes ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/streams/","uriRel":"/rs/databases/active-active/develop/data-types/streams/","title":"Streams in Active-Active databases","tags":[],"keywords":[],"description":"Information about using streams with an Active-Active database.","content":"A Redis Stream is a data structure that acts like an append-only log. Each stream entry consists of:\nA unique, monotonically increasing ID A payload consisting of a series key-value pairs You add entries to a stream with the XADD command. You access stream entries using the XRANGE, XREADGROUP, and XREAD commands (however, see the caveat about XREAD below).\nStreams and Active-Active Active-Active databases allow you to write to the same logical stream from more than one region. Streams are synchronized across the regions of an Active-Active database.\nIn the example below, we write to a stream concurrently from two regions. Notice that after syncing, both regions have identical streams:\nTime Region 1 Region 2 t1 XADD messages * text hello XADD messages * text goodbye t2 XRANGE messages - + → [1589929244828-1] XRANGE messages - + → [1589929246795-2] t3 — Sync — — Sync — t4 XRANGE messages - + → [1589929244828-1, 1589929246795-2] XRANGE messages - + → [1589929244828-1, 1589929246795-2] Notice also that the synchronized streams contain no duplicate IDs. As long as you allow the database to generate your stream IDs, you\u0026rsquo;ll never have more than one stream entry with the same ID.\nNote: Open source Redis uses one radix tree (referred to as rax in the code base) to implement each stream. However, Active-Active databases implement a single logical stream using one rax per region. Each region adds entries only to its associated rax (but can remove entries from all rax trees). This means that XREAD and XREADGROUP iterate simultaneously over all rax trees and return the appropriate entry by comparing the entry IDs from each rax. Conflict resolution Active-Active databases use an \u0026ldquo;observed-remove\u0026rdquo; approach to automatically resolve potential conflicts.\nWith this approach, a delete only affects the locally observable data.\nIn the example below, a stream, x, is created at t1. At t3, the stream exists in two regions.\nTime Region 1 Region 2 t1 XADD messages * text hello t2 — Sync — — Sync — t3 XRANGE messages - + → [1589929244828-1] XRANGE messages - + → [1589929244828-1] t4 DEL messages XADD messages * text goodbye t5 — Sync — — Sync — t6 XRANGE messages - + → [1589929246795-2] XRANGE messages - + → [1589929246795-2] At t4, the stream is deleted from Region 1. At the same time, an entry with ID ending in 3700 is added to the same stream at Region 2. After the sync, at t6, the entry with ID ending in 3700 exists in both regions. This is because that entry was not visible when the local stream was deleted at t4.\nID generation modes Usually, you should allow Redis streams generate its own stream entry IDs. You do this by specifying * as the ID in calls to XADD. However, you can provide your own custom ID when adding entries to a stream.\nBecause Active-Active databases replicate asynchronously, providing your own IDs can create streams with duplicate IDs. This can occur when your write to the same stream from multiple regions.\nTime Region 1 Region 2 t1 XADD x 100-1 f1 v1 XADD x 100-1 f1 v1 t2 — Sync — — Sync — t3 XRANGE x - + → [100-1, 100-1] XRANGE x - + → [100-1, 100-1] In this scenario, two entries with the ID 100-1 are added at t1. After syncing, the stream x contains two entries with the same ID.\nNote: Stream IDs in open source Redis consist of two integers separated by a dash (\u0026rsquo;-\u0026rsquo;). When the server generates the ID, the first integer is the current time in milliseconds, and the second integer is a sequence number. So, the format for stream IDs is MS-SEQ. To prevent duplicate IDs and to comply with the original Redis streams design, Active-Active databases provide three ID modes for XADD:\nStrict: In strict mode, XADD allows server-generated IDs (using the \u0026lsquo;*\u0026rsquo; ID specifier) or IDs consisting only of the millisecond (MS) portion. When the millisecond portion of the ID is provided, the ID\u0026rsquo;s sequence number is calculated using the database\u0026rsquo;s region ID. This prevents duplicate IDs in the stream. Strict mode rejects full IDs (that is, IDs containing both milliseconds and a sequence number). Semi-strict: Semi-strict mode is just like strict mode except that it allows full IDs (MS-SEQ). Because it allows full IDs, duplicate IDs are possible in this mode. Liberal: XADD allows any monotonically ascending ID. When given the millisecond portion of the ID, the sequence number will be set to 0. This mode may also lead to duplicate IDs. The default and recommended mode is strict, which prevents duplicate IDs\nWarning - Why do you want to prevent duplicate IDs? First, XDEL, XCLAIM, and other commands can affect more than one entry when duplicate IDs are present in a stream. Second, duplicate entries may be removed if a database is exported or renamed. To change XADD\u0026rsquo;s ID generation mode, use the rladmin command-line utility:\nSet strict mode:\nrladmin\u0026gt; tune db crdb crdt_xadd_id_uniqueness_mode strict Set semi-strict mode:\nrladmin\u0026gt; tune db crdb crdt_xadd_id_uniqueness_mode semi-strict Set liberal mode:\nrladmin\u0026gt; tune db crdb crdt_xadd_id_uniqueness_mode liberal Iterating a stream with XREAD In open source Redis and in non-Active-Active databases, you can use XREAD to iterate over the entries in a Redis Stream. However, with an Active-Active database, XREAD may skip entries. This can happen when multiple regions write to the same stream.\nIn the example below, XREAD skips entry 115-2.\nTime Region 1 Region 2 t1 XADD x 110 f1 v1 XADD x 115 f1 v1 t2 XADD x 120 f1 v1 t3 XADD x 130 f1 v1 t4 XREAD COUNT 2 STREAMS x 0 → [110-1, 120-1] t5 — Sync — — Sync — t6 XREAD COUNT 2 STREAMS x 120-1 → [130-1] t7 XREAD STREAMS x 0 →[110-1, 115-2, 120-1, 130-1] XREAD STREAMS x 0 →[110-1, 115-2, 120-1, 130-1] You can use XREAD to reliably consume a stream only if all writes to the stream originate from a single region. Otherwise, you should use XREADGROUP, which always guarantees reliable stream consumption.\nConsumer groups Active-Active databases fully support consumer groups with Redis Streams. Here is an example of creating two consumer groups concurrently:\nTime Region 1 Region 2 t1 XGROUP CREATE x group1 0 XGROUP CREATE x group2 0 t2 XINFO GROUPS x → [group1] XINFO GROUPS x → [group2] t3 — Sync — — Sync — t4 XINFO GROUPS x → [group1, group2] XINFO GROUPS x → [group1, group2] Note: Open source Redis uses one radix tree (rax) to hold the global pending entries list and another rax for each consumer\u0026rsquo;s PEL. The global PEL is a unification of all consumer PELs, which are disjoint.\nAn Active-Active database stream maintains a global PEL and a per-consumer PEL for each region.\nWhen given an ID different from the special \u0026ldquo;\u0026gt;\u0026rdquo; ID, XREADGROUP iterates simultaneously over all of the PELs for all consumers. It returns the next entry by comparing entry IDs from the different PELs.\nConflict resolution The \u0026ldquo;delete wins\u0026rdquo; approach is a way to automatically resolve conflicts with consumer groups. In case of concurrent consumer group operations, a delete will \u0026ldquo;win\u0026rdquo; over other concurrent operations on the same group.\nIn this example, the DEL at t4 deletes both the observed group1 and the non-observed group2:\nTime Region 1 Region 2 t1 XGROUP CREATE x group1 0 t2 — Sync — — Sync — t3 XINFO GROUPS x → [group1] XINFO GROUPS x → [group1] t4 DEL x XGROUP CREATE x group2 0 t5 — Sync — — Sync — t6 EXISTS x → 0 EXISTS x → 0 In this example, the XGROUP DESTROY at t4 affects both the observed group1 created in Region 1 and the non-observed group1 created in Region 3:\ntime Region 1 Region 2 Region 3 t1 XGROUP CREATE x group1 0 t2 — Sync — — Sync — t3 XINFO GROUPS x → [group1] XINFO GROUPS x → [group1] XINFO GROUPS x → [] t4 XGROUP DESTROY x group1 XGROUP CREATE x group1 0 t5 — Sync — _— Sync — — Sync — t6 EXISTS x → 0 EXISTS x → 0 EXISTS x → 0 Group replication Calls to XREADGROUP and XACK change the state of a consumer group or consumer. However, it\u0026rsquo;s not efficient to replicate every change to a consumer or consumer group.\nTo maintain consumer groups in Active-Active databases with optimal performance:\nGroup existence (CREATE/DESTROY) is replicated. Most XACK operations are replicated. Other operations, such as XGROUP, SETID, DELCONSUMER, are not replicated. For example:\nTime Region 1 Region 2 t1 XADD messages 110 text hello t2 XGROUP CREATE messages group1 0 t3 XREADGROUP GROUP group1 Alice STREAMS messages \u0026gt; → [110-1] t4 — Sync — — Sync — t5 XRANGE messages - + → [110-1] XRANGE messages - + → [110-1] t6 XINFO GROUPS messages → [group1] XINFO GROUPS messages → [group1] t7 XINFO CONSUMERS messages group1 → [Alice] XINFO CONSUMERS messages group1 → [] t8 XPENDING messages group1 - + 1 → [110-1] XPENDING messages group1 - + 1→ [] Using XREADGROUP across regions can result in regions reading the same entries. This is due to the fact that Active-Active Streams is designed for at-least-once reads or a single consumer. As shown in the previous example, Region 2 is not aware of any consumer group activity, so redirecting the XREADGROUP traffic from Region 1 to Region 2 results in reading entries that have already been read.\nReplication performance optimizations Consumers acknowledge messages using the XACK command. Each ack effectively records the last consumed message. This can result in a lot of cross-region traffic. To reduce this traffic, we replicate XACK messages only when all of the read entries are acknowledged.\nTime Region 1 Region 2 Explanation t1 XADD x 110-0 f1 v1 t2 XADD x 120-0 f1 v1 t3 XADD x 130-0 f1 v1 t4 XGROUP CREATE x group1 0 t5 XREADGROUP GROUP group1 Alice STREAMS x \u0026gt; → [110-0, 120-0, 130-0] t6 XACK x group1 110-0 t7 — Sync — — Sync — 110-0 and its preceding entries (none) were acknowledged. We replicate an XACK effect for 110-0. t8 XACK x group1 130-0 t9 — Sync — — Sync — 130-0 was acknowledged, but not its preceding entries (120-0). We DO NOT replicate an XACK effect for 130-0 t10 XACK x group1 120-0 t11 — Sync — — Sync — 120-0 and its preceding entries (110-0 through 130-0) were acknowledged. We replicate an XACK effect for 130-0. In this scenario, if we redirect the XREADGROUP traffic from Region 1 to Region 2 we do not re-read entries 110-0, 120-0 and 130-0. This means that the XREADGROUP does not return already-acknowledged entries.\nGuarantees Unlike XREAD, XREADGOUP will never skip stream entries. In traffic redirection, XREADGROUP may return entries that have been read but not acknowledged. It may also even return entries that have already been acknowledged.\nSummary With Active-Active streams, you can write to the same logical stream from multiple regions. As a result, the behavior of Active-Active streams differs somewhat from the behavior you get with open source Redis. This is summarized below:\nStream commands When using the strict ID generation mode, XADD does not permit full stream entry IDs (that is, an ID containing both MS and SEQ). XREAD may skip entries when iterating a stream that is concurrently written to from more than one region. For reliable stream iteration, use XREADGROUP instead. XSETID fails when the new ID is less than current ID. Consumer group notes The following consumer group operations are replicated:\nConsecutive XACK operations Consumer group creation and deletion (that is, XGROUP CREATE and XGROUP DESTROY) All other consumer group metadata is not replicated.\nA few other notes:\nXGROUP SETID and DELCONSUMER are not replicated. Consumers exist locally (XREADGROUP creates a consumer implicitly). Renaming a stream (using RENAME) deletes all consumer group information. ","categories":["RS"]},{"uri":"/rs/databases/active-active/develop/data-types/strings/","uriRel":"/rs/databases/active-active/develop/data-types/strings/","title":"Strings and bitfields in Active-Active databases","tags":[],"keywords":[],"description":"Information about using strings and bitfields with an Active-Active database.","content":"Active-Active databases support both strings and bitfields.\nNote: Active-Active bitfield support was added in RS version 6.0.20. Changes to both of these data structures will be replicated across Active-Active member databases.\nReplication semantics Except in the case of string counters (see below), both strings and bitfields are replicated using a \u0026ldquo;last write wins\u0026rdquo; approach. The reason for this is that strings and bitfields are effectively binary objects. So, unlike with lists, sets, and hashes, the conflict resolution semantics of a given operation on a string or bitfield are undefined.\nHow \u0026ldquo;last write wins\u0026rdquo; works A wall-clock timestamp (OS time) is stored in the metadata of every string and bitfield operation. If the replication syncer cannot determine the order of operations, the value with the latest timestamp wins. This is the only case with Active-Active databases where OS time is used to resolve a conflict.\nHere\u0026rsquo;s an example where an update happening to the same key at a later time (t2) wins over the update at t1.\nTime Region 1 Region 2 t1 SET text “a” t2 SET text “b” t3 — Sync — — Sync — t4 SET text “c” t5 — Sync — — Sync — t6 SET text “d” String counter support When you\u0026rsquo;re using a string as counter (for instance, with the INCR or INCRBY commands), then conflicts will be resolved semantically.\nOn conflicting writes, counters accumulate the total counter operations across all member Active-Active databases in each sync.\nHere\u0026rsquo;s an example of how counter values works when synced between two member Active-Active databases. With each sync, the counter value accumulates the private increment and decrements of each site and maintain an accurate counter across concurrent writes.\nTime Region 1 Region 2 t1 INCRBY counter 7 t2 INCRBY counter 3 t3 GET counter7 GET counter3 t4 — Sync — — Sync — t5 GET counter10 GET counter10 t6 DECRBY counter 3 t7 INCRBY counter 6 t8 — Sync — — Sync — t9 GET counter13 GET counter13 Note: Active-Active databases support 59-bit counters. This limitation is to protect from overflowing a counter in a concurrent operation. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/suffix/","uriRel":"/rs/references/cli-utilities/rladmin/suffix/","title":"rladmin suffix","tags":[],"keywords":[],"description":"Manages the DNS suffixes in the cluster.","content":"Manages the DNS suffixes in the cluster.\nsuffix add Adds a DNS suffix to the cluster.\nrladmin suffix add name \u0026lt;name\u0026gt; [default] [internal] [mdns] [use_aaaa_ns] [slaves \u0026lt;ip\u0026gt;..] Parameters Parameter Type/Value Description name string DNS suffix to add to the cluster default Sets the given suffix as the default. If a default already exists, this overwrites it. internal Forces the given suffix to use private IPs mdns Activates multicast DNS support for the given suffix slaves list of IPv4 addresses The given suffix will notify the frontend DNS servers when a change in the frontend DNS has occurred use_aaaa_ns Activates IPv6 address support Returns Returns Added suffixes successfully if the suffix was added. Otherwise, it returns an error.\nExample $ rladmin suffix add name new.rediscluster.local Added suffixes successfully suffix delete Deletes an existing DNS suffix from the cluster.\nrladmin suffix delete name \u0026lt;name\u0026gt; Parameters Parameter Type/Value Description name string DNS suffix to delete from the cluster Returns Returns Suffix deleted successfully if the suffix was deleted. Otherwise, it returns an error.\nExample $ rladmin suffix delete name new.rediscluster.local Suffix deleted successfully suffix list Lists the DNS suffixes in the cluster.\nrladmin suffix list Parameters None\nReturns Returns a list of the DNS suffixes.\nExample $ rladmin suffix list List of all suffixes: cluster.local new.rediscluster.local ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/suffix/","uriRel":"/rs/references/rest-api/objects/suffix/","title":"Suffix object","tags":[],"keywords":[],"description":"An object that represents a DNS suffix","content":"An API object that represents a DNS suffix in the cluster.\nName Type/Value Description default boolean Suffix is the default suffix for the cluster (read-only) internal boolean Does the suffix point to internal IP addresses (read-only) mdns boolean Support for multicast DNS (read-only) name string Unique suffix name that represents its zone (read-only) slaves array of strings Frontend DNS servers to be updated by this suffix use_aaaa_ns boolean Suffix uses AAAA NS entries (read-only) ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/suffix/","uriRel":"/rs/references/rest-api/requests/suffix/","title":"Suffix requests","tags":[],"keywords":[],"description":"DNS suffix requests","content":" Method Path Description GET /v1/suffix/{name} Get a single DNS suffix Get suffix GET /v1/suffix/{string: name} Get a single DNS suffix.\nRequest Example HTTP request GET /suffix/cluster.fqdn Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description name string The unique name of the suffix requested. Response Returns a suffix object.\nExample JSON body { \u0026#34;name\u0026#34;: \u0026#34;cluster.fqdn\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Suffix name does not exist ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/suffixes/","uriRel":"/rs/references/rest-api/requests/suffixes/","title":"Suffixes requests","tags":[],"keywords":[],"description":"DNS suffixes requests","content":" Method Path Description GET /v1/suffixes Get all DNS suffixes Get all suffixes GET /v1/suffixes Get all DNS suffixes in the cluster.\nRequest Example HTTP request GET /suffixes Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response The response body contains a JSON array with all suffixes, represented as suffix objects.\nExample JSON body [ { \u0026#34;name\u0026#34;: \u0026#34;cluster.fqdn\u0026#34;, \u0026#34;// additional fields...\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;internal.cluster.fqdn\u0026#34;, \u0026#34;// additional fields...\u0026#34; } ] Status codes Code Description 200 OK No error ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/shard/sync/","uriRel":"/rs/references/rest-api/objects/shard/sync/","title":"Sync object","tags":[],"keywords":[],"description":"Documents the sync object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description progress integer Number of bytes remaining in current sync status \u0026lsquo;in_progress\u0026rsquo;\n\u0026lsquo;idle\u0026rsquo;\n\u0026lsquo;link_down\u0026rsquo; Indication of the shard\u0026rsquo;s current sync status ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/sync_source_stats/","uriRel":"/rs/references/rest-api/requests/bdbs/sync_source_stats/","title":"Database syncer source stats requests","tags":[],"keywords":[],"description":"Syncer source statistics requests","content":" Method Path Description GET /v1/bdbs/{bdb_uid}/sync_source_stats Get stats for all syncer sources GET /v1/bdbs/{bdb_uid}/sync_source_stats/{uid} Get stats for a specific syncer instance Get all syncer source stats GET /v1/bdbs/{bdb_uid}/sync_source_stats Get stats for all syncer sources of a local database.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1/sync_source_stats?interval=5min Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description bdb_uid integer The unique ID of the local database. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 Optional end time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for all syncer sources.\nExample JSON body { \u0026#34;sync_source_stats\u0026#34;: [ { \u0026#34;intervals\u0026#34;: [ { \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18528, \u0026#34;ingress_bytes_decompressed\u0026#34;: 185992, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.244, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:25:00Z\u0026#34; }, { \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:35:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18, \u0026#34;ingress_bytes_decompressed\u0026#34;: 192, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34; } ], \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34; } ] } Status codes Code Description 200 OK No error 404 Not Found Database does not exist. Get syncer instance stats GET /v1/bdbs/{bdb_uid}/sync_source_stats/{int: uid} Get stats for a specific syncer (Replica Of) instance.\nPermissions Permission name Roles view_bdb_stats admin\ncluster_member\ncluster_viewer\ndb_member\ndb_viewer Request Example HTTP request GET /bdbs/1/sync_source_stats/1?interval=5min Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description bdb_uid integer The unique ID of the local database. uid integer The sync_source uid. Query parameters Field Type Description interval string Time interval for which we want stats: 1sec/10sec/5min/15min/1hour/12hour/1week (optional) stime ISO_8601 Optional start time from which we want the stats. Should comply with the ISO_8601 format (optional) etime ISO_8601 Optional end time after which we don\u0026rsquo;t want the stats. Should comply with the ISO_8601 format (optional) Response Returns statistics for a specific syncer instance.\nExample JSON body { \u0026#34;intervals\u0026#34;: [ { \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18528, \u0026#34;ingress_bytes_decompressed\u0026#34;: 185992, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.244, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:25:00Z\u0026#34; }, { \u0026#34;etime\u0026#34;: \u0026#34;2017-10-22T19:35:00Z\u0026#34;, \u0026#34;ingress_bytes\u0026#34;: 18, \u0026#34;ingress_bytes_decompressed\u0026#34;: 192, \u0026#34;interval\u0026#34;: \u0026#34;5min\u0026#34;, \u0026#34;local_ingress_lag_time\u0026#34;: 0.0, \u0026#34;stime\u0026#34;: \u0026#34;2017-10-22T19:30:00Z\u0026#34; } ], \u0026#34;uid\u0026#34;: \u0026#34;1\u0026#34; } Status codes Code Description 200 OK No error 404 Not Found Database or sync_source do not exist. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/bdb/syncer_sources/","uriRel":"/rs/references/rest-api/objects/bdb/syncer_sources/","title":"Syncer sources object","tags":[],"keywords":[],"description":"Documents the syncer_sources object used with Redis Enterprise Software REST API calls.","content":" Name Type/Value Description uid integer Unique ID of this source client_cert string Client certificate to use if encryption is enabled client_key string Client key to use if encryption is enabled compression integer, (range: 0-6) Compression level for the replication link encryption boolean Encryption enabled/disabled lag integer Lag in milliseconds between source and destination (while synced) last_error string Last error encountered when syncing from the source last_update string Time when we last received an update from the source rdb_size integer The source\u0026rsquo;s RDB size to be transferred during the syncing phase rdb_transferred integer Number of bytes transferred from the source\u0026rsquo;s RDB during the syncing phase replication_tls_sni string Replication TLS server name indication server_cert string Server certificate to use if encryption is enabled status string Sync status of this source uri string Source Redis URI ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/task/","uriRel":"/rs/references/cli-utilities/crdb-cli/task/","title":"crdb-cli task commands","tags":[],"keywords":[],"description":"Manage Active-Active tasks.","content":"The crdb-cli task commands help investigate Active-Active database performance issues. They should not be used except as directed by Support.\ncrdb-cli task commands Command Description cancel Attempts to cancel a specified Active-Active database task. list Lists active and recent Active-Active database tasks. status Shows the status of a specified Active-Active database task. ","categories":["RS"]},{"uri":"/rs/references/terminology/","uriRel":"/rs/references/terminology/","title":"Terminology in Redis Enterprise Software","tags":[],"keywords":[],"description":"Explains terms used in Redis Enterprise Software and its docs.","content":"Here are explanations of some of the terms used in Redis Enterprise Software.\nNode A node is a physical machine, virtual machine, container or cloud instance on which the RS installation package was installed and the setup process was run in order to make the machine part of the cluster.\nEach node is a container for running multiple open source Redis instances, referred to as \u0026ldquo;shards\u0026rdquo;.\nThe recommended configuration for a production cluster is an uneven number of nodes, with a minimum of three. Note that in some configurations, certain functionalities might be blocked. For example, if a cluster has only one node you cannot enable database replication, which helps to achieve high availability.\nA node is made up of several components, as detailed below, and works together with the other cluster nodes.\nRedis instance (shard) As indicated above, each node serves as a container for hosting multiple database instances, referred to as \u0026ldquo;shards\u0026rdquo;.\nRedis Enterprise Software supports various database configurations:\nStandard Redis database - A single Redis shard with no replication or clustering. Highly available Redis database - Every database master shard has a replica shard, so that if the master shard fails the cluster can automatically fail over to the replica with minimal impact. Master and replica shards are always placed on separate nodes to ensure high availability. Clustered Redis database - The data stored in the database is split across several shards. The number of shards can be defined by the user. Various performance optimization algorithms define where shards are placed within the cluster. During the lifetime of the cluster, these algorithms might migrate a shard between nodes. Clustered and highly available Redis database - Each master shard in the clustered database has a replica shard, enabling failover if the master shard fails. Proxy Each node includes one zero-latency, multi-threaded proxy (written in low-level C) that masks the underlying system complexity. The proxy oversees forwarding Redis operations to the database shards on behalf of a Redis client.\nThe proxy simplifies the cluster operation, from the application or Redis client point of view, by enabling the use of a standard Redis client. The zero-latency proxy is built over a cut-through architecture and employs various optimization methods. For example, to help ensure high-throughput and low-latency performance, the proxy might use instruction pipelining even if not instructed to do so by the client.\nDatabase endpoint Each database is served by a database endpoint that is part of and managed by the proxies. The endpoint oversees forwarding Redis operations to specific database shards.\nIf the master shard fails and the replica shard is promoted to master, the master endpoint is updated to point to the new master shard.\nIf the master endpoint fails, the replica endpoint is promoted to be the new master endpoint and is updated to point to the master shard.\nSimilarly, if both the master shard and the master endpoint fail, then both the replica shard and the replica endpoint are promoted to be the new master shard and master endpoint.\nShards and their endpoints do not have to reside within the same node in the cluster.\nIn the case of a clustered database with multiple database shards, only one master endpoint acts as the master endpoint for all master shards, forwarding Redis operations to all shards as needed.\nCluster manager The cluster manager oversees all node management-related tasks, and the cluster manager in the master node looks after all the cluster related tasks.\nThe cluster manager is designed in a way that is totally decoupled from the Redis operation. This enables RS to react in a much faster and accurate manner to failure events, so that, for example, a node failure event triggers mass failover operations of all the master endpoints and master shards that are hosted on the failed node.\nIn addition, this architecture guarantees that each Redis shard is only dealing with processing Redis commands in a shared-nothing architecture, thus maintaining the inherent high-throughput and low-latency of each Redis process. Lastly, this architecture guarantees that any change in the cluster manager itself does not affect the Redis operation.\nSome of the primary functionalities of the cluster manager include:\nDeciding where shards are created Deciding when shards are migrated and to where Monitoring database size Monitoring databases and endpoints across all nodes Running the database resharding process Running the database provisioning and de-provisioning processes Gathering operational statistics Enforcing license and subscription limitations ","categories":["RS"]},{"uri":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/terraform/","uriRel":"/rc/cloud-integrations/aws-cloud-accounts/iam-resources/terraform/","title":"Create IAM resources using Terraform","tags":[],"keywords":[],"description":"","content":"You can use HashiCorp Terraform to create identity and access management (IAM) resources to support AWS cloud account access to Redis Enterprise Cloud subscriptions.\nThe following example uses the terraform-aws-Redislabs-Cloud-Account-IAM-Resources module, located in Amazon S3:\nCreate a main.tf as shown below (update the profile, region, and pgp_key values as appropriate).\nNote that a pgp_key is required. For details, see the Terraform docs.\nView terraformIAMTemplate.json \u0026lt;div class=\u0026quot;highlight\u0026quot;\u0026gt;\u0026lt;pre tabindex=\u0026quot;0\u0026quot; style=\u0026quot;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;language-js\u0026quot; data-lang=\u0026quot;js\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;provider\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;aws\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;profile\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;tobyhf-admin\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;region\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;us-east-1\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Redislabs-Cloud-Account-Resources\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;https://iam-resource-automation-do-not-delete.s3.amazonaws.com/terraform-aws-Redislabs-Cloud-Account-IAM-Resources.zip\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;pgp_key\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;keybase:toby_h_ferguson\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;accessKeyId\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;accessKeyId\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;accessSecretKey\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;accessSecretKey\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;sensitive\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#00a8c8\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;IAMRoleName\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;IAMRoleName\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;consoleUsername\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;consoleUsername\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;signInLoginUrl\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;description\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;Redis User\u0026amp;#39;s console login URL\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;signInLoginUrl\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;output\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#d88200\u0026quot;\u0026gt;\u0026amp;#34;consolePassword\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;module\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Redislabs\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Cloud\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Account\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;Resources\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;consolePassword\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt; \u0026lt;span style=\u0026quot;color:#75af00\u0026quot;\u0026gt;sensitive\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#00a8c8\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;display:flex;\u0026quot;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;span style=\u0026quot;color:#111\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt; Initialize Terraform with the module:\nNote: Terraform requires AWS credentials be supplied, but the source of the module is a public S3 bucket, so any valid credentials should work. Replace the XXXX fields below with your relevant values\nAWS_ACCESS_KEY_ID=XXXX AWS_SECRET_KEY=XXXX terraform init Build the resources:\nterraform apply This displays the required values. To access the sensitive data:\naccessSecretKey: echo $(terraform output -raw accessSecretKey)\nconsolePassword:\necho $(terraform output -raw consolePassword | base64 --decode | keybase pgp decrypt)\n","categories":["RC"]},{"uri":"/rs/references/rest-api/requests/ocsp/test/","uriRel":"/rs/references/rest-api/requests/ocsp/test/","title":"OCSP test requests","tags":[],"keywords":[],"description":"OCSP test requests","content":" Method Path Description POST /v1/ocsp/test Test OCSP Test OCSP POST /v1/ocsp/test Queries the OCSP server for the proxy certificate’s latest status and returns the response as JSON. It caches the response if the OCSP feature is enabled.\nRequired permissions Permission name test_ocsp_status Request Example HTTP request POST /ocsp/test Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns an OCSP status object.\nExample JSON body { \u0026#34;responder_url\u0026#34;: \u0026#34;http://responder.ocsp.url.com\u0026#34;, \u0026#34;cert_status\u0026#34;: \u0026#34;REVOKED\u0026#34;, \u0026#34;produced_at\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:11 GMT\u0026#34;, \u0026#34;this_update\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:11 GMT\u0026#34;, \u0026#34;next_update\u0026#34;: \u0026#34;Wed, 22 Dec 2021 14:50:00 GMT\u0026#34;, \u0026#34;revocation_time\u0026#34;: \u0026#34;Wed, 22 Dec 2021 12:50:04 GMT\u0026#34; } Error codes When errors occur, the server returns a JSON object with error_code and message fields that provide additional information. The following are possible error_code values:\nCode Description no_responder_url Tried to test OCSP status with no responder URL configured ocsp_unsupported_by_capability Not all nodes support OCSP capability task_queued_for_too_long OCSP polling task was in status “queued” for over 5 seconds invalid_ocsp_response The server returned a response that is not compatible with OCSP Status codes Code Description 200 OK Success querying the OCSP server 406 Not Acceptable Feature is not supported in all nodes 500 Internal Server Error responder_url is not configured or polling task failed ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/tune/","uriRel":"/rs/references/cli-utilities/rladmin/tune/","title":"rladmin tune","tags":[],"keywords":[],"description":"Configures parameters for databases, proxies, nodes, and clusters.","content":"Configures parameters for databases, proxies, nodes, and clusters.\ntune cluster Configures cluster parameters.\nrladmin tune cluster [ repl_diskless { enabled | disabled } ] [ redis_provision_node_threshold \u0026lt;size\u0026gt; ] [ redis_migrate_node_threshold \u0026lt;size\u0026gt; ] [ redis_provision_node_threshold_percent \u0026lt;percent\u0026gt; ] [ redis_migrate_node_threshold_percent \u0026lt;percent\u0026gt; ] [ max_simultaneous_backups \u0026lt;size\u0026gt; ] [ watchdog_profile { cloud | local-network } ] [ slave_ha { enabled | disabled } ] [ slave_ha_grace_period \u0026lt;seconds\u0026gt; ] [ slave_ha_cooldown_period \u0026lt;seconds\u0026gt; ] [ slave_ha_bdb_cooldown_period \u0026lt;seconds\u0026gt; ] [ max_saved_events_per_type \u0026lt;value\u0026gt; ] [ parallel_shards_upgrade \u0026lt;value\u0026gt; ] [ default_concurrent_restore_actions \u0026lt;value\u0026gt; ] [ show_internals { enabled | disabled } ] [ expose_hostnames_for_all_suffixes { enabled | disabled } ] [ redis_upgrade_policy { latest | major } ] [ default_redis_version \u0026lt;value\u0026gt; ] [ data_internode_encryption { enabled | disabled } ] [ db_conns_auditing { enabled | disabled } ] [ acl_pubsub_default { resetchannels | allchannels } ] Redis cluster watchdog supports two preconfigured profiles:\nThe cloud profile is suitable for common cloud environments. It has a higher tolerance for network jitter.\nThe local-network profile is suitable for dedicated LANs. It has better failure detection and failover times.\nParameters Parameters Type/Value Description acl_pubsub_default resetchannels\nallchannels Default pub/sub ACL rule for all databases in the cluster:\n•resetchannels blocks access to all channels (restrictive)\n•allchannels allows access to all channels (permissive) data_internode_encryption enabled\ndisabled Activates or deactivates internode encryption for new databases db_conns_auditing enabled\ndisabled Activates or deactivates connection auditing by default for new databases of a cluster default_concurrent_restore_actions integer\nall Default number of concurrent actions when restoring a node from a snapshot (positive integer or \u0026ldquo;all\u0026rdquo;) default_redis_version version number The default Redis database compatibility version used to create new databases. The value parameter should be a version number in the form of \u0026ldquo;x.y\u0026rdquo; where x represents the major version number and y represents the minor version number. The final value corresponds to the desired version of Redis.You cannot set default_redis_version to a value higher than that supported by the current redis_upgrade_policy value. expose_hostnames_for_all_suffixes enabled\ndisabled Exposes hostnames for all DNS suffixes login_lockout_counter_reset_after time in seconds Time after failed login attempt before the counter resets to 0 login_lockout_duration time in seconds Time a locked account remains locked ( \u0026ldquo;0\u0026rdquo; means only an admin can unlock the account) login_lockout_threshold integer Number of failed sign-in attempts to trigger locking a user account (\u0026ldquo;0\u0026rdquo; means never lock the account) max_saved_events_per_type integer Maximum number of events each type saved in CCS per object type max_simultaneous_backups integer Number of database backups allowed to run at the same time. Combines with max_redis_forks (set by tune node) to determine the number of shard backups allowed to run simultaneously. parallel_shards_upgrade integer\nall Number of shards upgraded in parallel during DB upgrade (positive integer or \u0026ldquo;all\u0026rdquo;) redis_migrate_node_threshold size in MB Memory (in MBs by default or can be specified) needed to migrate a database between nodes redis_migrate_node_threshold_percent percentage Memory (in percentage) needed to migrate a database between nodes redis_provision_node_threshold size in MB Memory (in MBs by default or can be specified) needed to provision a new database redis_provision_node_threshold_percent percentage Memory (in percentage) needed to provision a new database redis_upgrade_policy latest\nmajor When you upgrade or create a new Redis database, this policy determines which version of Redis database compatibility is used.\nSupported values are:latest, which applies the most recent Redis compatibility update (effective default prior to v6.2.4)major, which applies the most recent major release compatibility update (default as of v6.2.4). repl_diskless enabled\ndisabled Activates or deactivates diskless replication (can be overwritten per database) show_internals enabled\ndisabled Controls the visibility of internal databases that are only used for the cluster\u0026rsquo;s management slave_ha enabled disabled Activates or deactivates replica high availability slave_ha_bdb_cooldown_period time in seconds Time (in seconds) after shard relocation during which databases can\u0026rsquo;t be relocated to another node slave_ha_cooldown_period time in seconds Time (in seconds) after shard relocation during which the replica high-availability mechanism can\u0026rsquo;t relocate to another node slave_ha_grace_period time in seconds Time (in seconds) between when a node fails and when replica high availability starts relocating shards to another node watchdog_profile cloud local-network Activates preconfigured watchdog profiles Returns Returns Finished successfully if the cluster configuration was changed. Otherwise, it returns an error.\nUse rladmin info cluster to verify the cluster configuration was changed.\nExample $ rladmin tune cluster slave_ha enabled Finished successfully $ rladmin info cluster | grep slave_ha slave_ha: enabled tune db Configures database parameters.\nrladmin tune db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } [ slave_buffer \u0026lt;valueMG | hard:soft:time\u0026gt; ] [ client_buffer \u0026lt;value\u0026gt; ] [ repl_backlog \u0026lt;valueMB | auto\u0026gt; ] [ crdt_repl_backlog \u0026lt;valueMB | auto\u0026gt; ] [ repl_timeout \u0026lt;seconds\u0026gt; ] [ repl_diskless { enabled | disabled | default } ] [ master_persistence { enabled | disabled } ] [ maxclients \u0026lt;value\u0026gt; ] [ schedpolicy { cmp | mru | spread | mnp } ] [ max_shard_pipeline \u0026lt;value\u0026gt; ] [ conns \u0026lt;value\u0026gt; ] [ conns_type \u0026lt;value\u0026gt; ] [ max_client_pipeline \u0026lt;value\u0026gt; ] [ max_connections \u0026lt;value\u0026gt; ] [ max_aof_file_size \u0026lt;size\u0026gt; ] [ oss_cluster { enabled | disabled } ] [ oss_cluster_api_preferred_ip_type \u0026lt;value\u0026gt; ] [ slave_ha { enabled | disabled } ] [ slave_ha_priority \u0026lt;value\u0026gt; ] [ skip_import_analyze { enabled | disabled } ] [ mkms { enabled | disabled } ] [ continue_on_error ] [ gradual_src_mode { enabled | disabled } ] [ gradual_sync_mode { enabled | disabled | auto } ] [ gradual_sync_max_shards_per_source \u0026lt;value\u0026gt; ] [ module_name \u0026lt;value\u0026gt; ] [ module_config_params \u0026lt;value\u0026gt; ] [ crdt_xadd_id_uniqueness_mode { liberal | semi-strict | strict } ] [ metrics_export_all { enabled | disabled } ] [ syncer_mode { distributed | centralized }] [ syncer_monitoring { enabled | disabled } ] [ mtls_allow_weak_hashing { enabled | disabled } ] [ mtls_allow_outdated_cert { enabled | disabled } ] [ data_internode_encryption { enabled | disabled } ] [ db_conns_auditing { enabled | disabled } ] Parameters Parameter Type/Value Description db:id integer ID of the specified database name string Name of the specified database client_buffer value in MB hard:soft:time Redis client output buffer limits conns integer Size of internal connection pool, specified per-thread or per-shard depending on conns_type conns_type per-thread\nper-shard Specifies connection pool size as either per-thread or per-shard continue_on_error Flag that skips tuning shards that can\u0026rsquo;t be reached crdt_repl_backlog value in MB\nauto Size of the Active-Active replication buffer crdt_xadd_id_uniqueness_mode liberal\nsemi-strict\nstrict XADD\u0026rsquo;s behavior in an Active-Active database, defined as liberal, semi-strict, or strict (see descriptions below) data_internode_encryption enabled\ndisabled Activates or deactivates internode encryption for the database db_conns_auditing enabled\ndisabled Activates or deactivates database connection auditing for a database gradual_src_mode enabled\ndisabled Activates or deactivates gradual sync of sources gradual_sync_max_shards_per_source integer Number of shards per sync source that can be replicated in parallel (positive integer) gradual_sync_mode enabled\ndisabled\nauto Activates, deactivates, or automatically determines gradual sync of source shards master_persistence enabled\ndisabled Activates or deactivates persistence of the primary shard max_aof_file_size size in MB Maximum size (in MB, if not specified) of AoF file (minimum value is 10 GB) max_client_pipeline integer Maximum commands in the proxy\u0026rsquo;s pipeline per client connection (max value is 2047, default value is 200) max_connections integer Maximum client connections to the database\u0026rsquo;s endpoint (default value is 0, which is unlimited) max_shard_pipeline integer Maximum commands in the proxy\u0026rsquo;s pipeline per shard connection (default value is 200) maxclients integer Controls the maximum client connections between the proxy and shards (default value is 10000) metrics_export_all enabled\ndisabled Activates the exporter to expose all shard metrics mkms enabled\ndisabled Activates multi-key multi-slot commands module_name and module_config_params string Configures arguments of a module in runtime. The module_config_params should be inside \u0026lsquo;quotation marks\u0026rsquo;. mtls_allow_outdated_cert enabled\ndisabled Activates outdated certificates in mTLS connections mtls_allow_weak_hashing enabled\ndisabled Activates weak hashing (less than 2048 bits) in mTLS connections oss_cluster enabled\ndisabled Activates OSS cluster API oss_cluster_api_preferred_ip_type internal\nexternal IP type for the endpoint and database in the OSS cluster API (default is internal) repl_backlog size in MB\nauto Size of the replication buffer repl_diskless enabled\ndisabled\ndefault Activates or deactivates diskless replication (defaults to the cluster setting) repl_timeout time in seconds Replication timeout (in seconds) schedpolicy cmp\nmru\nspread\nmnp Controls how server-side connections are used when forwarding traffic to shards skip_import_analyze enabled\ndisabled Skips the analyzing step when importing a database slave_buffer value in MB\nhard:soft:time Redis replica output buffer limits slave_ha enabled\ndisabled Activates or deactivates replica high availability (defaults to the cluster setting) slave_ha_priority integer Priority of the database in the replica high-availability mechanism syncer_mode distributed\ncentralized Configures syncer to run in distributed or centralized mode. For distributed syncer, the DMC policy must be all-nodes or all-master-nodes syncer_monitoring enabled\ndisabled Activates syncer monitoring XADD behavior mode Description liberal XADD succeeds with any valid ID (not recommended, allows duplicate IDs) semi-strict Allows a full ID. Partial IDs are completed with the unique database instance ID (not recommended, allows duplicate IDs). strict XADD fails if a full ID is given. Partial IDs are completed using the unique database instance ID. Returns Returns Finished successfully if the database configuration was changed. Otherwise, it returns an error.\nUse rladmin info db to verify the database configuration was changed.\nExample $ rladmin tune db db:4 repl_timeout 300 Tuning database: o Finished successfully $ rladmin info db db:4 | grep repl_timeout repl_timeout: 300 seconds tune node Configures node parameters.\ntune node { \u0026lt;id\u0026gt; | all } [ max_listeners \u0026lt;value\u0026gt; ] [ max_redis_forks \u0026lt;value\u0026gt; ] [ max_redis_servers \u0026lt;value\u0026gt; ] [ max_slave_full_syncs \u0026lt;value\u0026gt; ] [ quorum_only { enabled | disabled } ] Parameters Parameter Type/Value Description id integer ID of the specified node all Configures settings for all nodes max_listeners integer Maximum number of endpoints that may be bound to the node max_redis_forks integer Maximum number of background processes forked from shards that may exist on the node at any given time max_redis_servers integer Maximum number of shards allowed to reside on the node max_slave_full_syncs integer Maximum number of simultaneous replica full-syncs that may be running at any given time (0: Unlimited, -1: Use cluster settings) quorum_only enabled\ndisabled If activated, configures the node as a quorum-only node Returns Returns Finished successfully if the node configuration was changed. Otherwise, it returns an error.\nUse rladmin info node to verify the node configuration was changed.\nExample $ rladmin tune node 3 max_redis_servers 120 Finished successfully $ rladmin info node 3 | grep \u0026#34;max redis servers\u0026#34; max redis servers: 120 tune proxy Configures proxy parameters.\nrladmin tune proxy { \u0026lt;id\u0026gt; | all } [ mode { static | dynamic } ] [ threads \u0026lt;value\u0026gt; ] [ max_threads \u0026lt;value\u0026gt; ] [ scale_threshold \u0026lt;value\u0026gt; ] [ scale_duration \u0026lt;seconds\u0026gt; ] Parameters Parameter Type/Value Description id integer ID of the specified proxy all Configures settings for all proxies max_threads integer Maximum number of threads allowed mode static\ndynamic Determines if the proxy automatically adjusts the number of threads based on load size scale_duration time in seconds Time of scale_threshold CPU utilization before the automatic proxy automatically scales scale_threshold percentage CPU utilization threshold that triggers spawning new threads threads integer Initial number of threads created at startup Returns Returns OK if the proxy configuration was changed. Otherwise, it returns an error.\nUse rladmin info proxy to verify the proxy configuration was changed.\nExample $ rladmin tune proxy 2 scale_threshold 75 Configuring proxies: - proxy:2: ok $ rladmin info proxy 2 | grep scale_threshold scale_threshold: 75 (%) ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/crdb-cli/crdb/update/","uriRel":"/rs/references/cli-utilities/crdb-cli/crdb/update/","title":"crdb-cli crdb update","tags":[],"keywords":[],"description":"Updates the configuration of an Active-Active database.","content":"Updates the configuration of an Active-Active database.\ncrdb-cli crdb update --crdb-guid \u0026lt;guid\u0026gt; [--no-wait] [--force] [--default-db-config \u0026lt;configuration\u0026gt; ] [--default-db-config-file \u0026lt;filename\u0026gt;] [--compression \u0026lt;0-6\u0026gt;] [--causal-consistency { true | false } ] [--credentials id=\u0026lt;id\u0026gt;,username=\u0026lt;username\u0026gt;,password=\u0026lt;password\u0026gt; ] [--encryption { true | false } ] [--oss-cluster { true | false } ] [--featureset-version { true | false } ] [--memory-size \u0026lt;maximum_memory\u0026gt;] [--bigstore-ram-size \u0026lt;maximum_memory\u0026gt;] [--update-module name=\u0026lt;name\u0026gt;,featureset_version=\u0026lt;version\u0026gt;] If you want to change the configuration of the local instance only, use rladmin instead.\nParameters Parameter Value Description crdb-guid \u0026lt;guid\u0026gt; string GUID of the Active-Active database (required) bigstore-ram-size \u0026lt;maximum_memory\u0026gt; size in bytes, kilobytes (KB), or gigabytes (GB) Maximum RAM limit for the Redis on Flash database, if activated memory-size \u0026lt;maximum_memory\u0026gt; size in bytes, kilobytes (KB), or gigabytes (GB) Maximum database memory (required) causal-consistency true false Causal consistency applies updates to all instances in the order they were received compression 0-6 The level of data compression: 0 = No compression 6 = High compression and resource load (Default: 3) credentials id=\u0026lt;id\u0026gt;,username=\u0026lt;username\u0026gt;,password=\u0026lt;password\u0026gt; strings Updates the credentials for access to the instance default-db-config \u0026lt;configuration\u0026gt; Default database configuration from stdin default-db-config-file \u0026lt;filename\u0026gt; filepath Default database configuration from file encryption true false Activates or deactivates encryption force Force an update even if there are no changes no-wait Do not wait for the command to finish oss-cluster true false Activates or deactivates OSS Cluster mode eviction-policy noevictionallkeys-lruallkeys-lfuallkeys-randomvolatile-lruvolatile-lfuvolatile-randomvolatile-ttl Updates eviction policy featureset-version truefalse Updates to latest FeatureSet version update-module name=\u0026lt;name\u0026gt;,featureset_version=\u0026lt;version\u0026gt; strings Update a module to the specified version Returns Returns the task ID of the task that is updating the database.\nIf --no-wait is specified, the command exits. Otherwise, it will wait for the database to be updated and then return \u0026ldquo;finished.\u0026rdquo;\nExample $ crdb-cli crdb update --crdb-guid 968d586c-e12d-4b8f-8473-42eb88d0a3a2 --memory-size 2GBTask 7e98efc1-8233-4578-9e0c-cdc854b8af9e created ---\u0026gt; Status changed: queued -\u0026gt; started ---\u0026gt; Status changed: started -\u0026gt; finished ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/cluster/update-cert/","uriRel":"/rs/references/rest-api/requests/cluster/update-cert/","title":"Update cluster certificate requests","tags":[],"keywords":[],"description":"Update cluster certificate requests","content":" Method Path Description PUT /v1/cluster/update_cert Update a cluster certificate Update cluster certificate PUT /v1/cluster/update_cert Replaces an existing certificate on all nodes within the cluster with a new certificate. The new certificate must pass validation before it can replace the old certificate.\nSee the certificates table for the list of cluster certificates and their descriptions.\nRequest Example HTTP request PUT /cluster/update_cert Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;certificate1\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\n[key_content]\\n-----END RSA PRIVATE KEY-----\u0026#34;, \u0026#34;certificate\u0026#34;: \u0026#34;-----BEGIN CERTIFICATE-----\\n[cert_content]\\n-----END CERTIFICATE-----\u0026#34;, } Replace [key_content] with the content of the private key and [cert_content] with the content of the certificate.\nResponse Responds with the 200 OK status code if the certificate replacement succeeds across the entire cluster.\nOtherwise, retry the certificate update in case the failure was due to a temporary issue in the cluster.\nStatus codes Code Description 200 OK No error 400 Bad Request Failed, invalid certificate. 403 Forbidden Failed, unknown certificate. 404 Not Found Failed, invalid certificate. 406 Not Acceptable Failed, expired certificate. 409 Conflict Failed, not all nodes have been updated. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/crdbs/updates/","uriRel":"/rs/references/rest-api/requests/crdbs/updates/","title":"CRDB updates requests","tags":[],"keywords":[],"description":"Update Active-Active configuration requests","content":" Method Path Description POST /v1/crdbs/{crdb_guid}/updates Modify Active-Active confgurarion Modify Active-Active configuration POST /v1/crdbs/{crdb_guid}/updates Modify Active-Active configuration.\nWarning - This is a very powerful API request and can cause damage if used incorrectly. In order to add or remove instances, you must use this API. For simple configuration updates, it is recommended to use PATCH on /crdbs/{crdb_guid} instead.\nUpdating default_db_config affects both existing and new instances that may be added.\nWhen you update db_config, it changes the configuration of the database that you specify. This field overrides corresponding fields (if any) in default_db_config.\nRequest Example HTTP request POST /crdbs/1/updates Request headers Key Value Description X-Task-ID string Specified task ID X-Result-TTL integer Time (in seconds) to keep task result URL parameters Field Type Description crdb_guid string Globally unique Active-Active database ID (GUID) Request body Include a CRDB modify_request object with updated fields in the request body.\nResponse Returns a CRDB task object.\nStatus codes Code Description 200 OK The request has been accepted. 400 Bad Request The posted Active-Active database contains invalid parameters. 401 Unauthorized Unauthorized request. Invalid credentials 404 Not Found Configuration, instance or Active-Active database not found. 406 Not Acceptable The posted Active-Active database cannot be accepted. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/upgrade/","uriRel":"/rs/references/cli-utilities/rladmin/upgrade/","title":"rladmin upgrade","tags":[],"keywords":[],"description":"Upgrades the version of a module or Redis Enterprise Software for a database.","content":"Upgrades the version of a module or Redis Enterprise Software for a database.\nupgrade db Schedules a restart of the primary and replica processes of a database and then upgrades the database to the latest version of Redis Enterprise Software.\nFor more information, see Upgrade an existing Redis Software Deployment.\nrladmin upgrade db { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } [ preserve_roles ] [ keep_redis_version ] [ discard_data ] [ force_discard ] [ parallel_shards_upgrade ] [ keep_crdt_protocol_version ] [ redis_version \u0026lt;version\u0026gt; ] [ force ] [ { latest_with_modules | and module module_name \u0026lt;module name\u0026gt; version \u0026lt;version\u0026gt; module_args \u0026lt;arguments string\u0026gt; } ] As of v6.2.4, the default behavior for upgrade db has changed. It is now controlled by a new parameter that sets the default upgrade policy used to create new databases and to upgrade ones already in the cluster. To learn more, see tune cluster default_redis_version.\nParameters Parameters Type/Value Description db db:\u0026lt;id\u0026gt; name Database to upgrade and module upgrade module command Clause that allows the upgrade of a database and a specified Redis module in a single step with only one restart (can be specified multiple times) discard_data Indicates that data will not be saved after the upgrade force Forces upgrade and skips warnings and confirmations force_discard Forces discard_data if replication or persistence is enabled keep_crdt_protocol_version Keeps the current CRDT protocol version keep_current_version Upgrades to a new patch release, not to the latest major.minor version latest_with_modules Upgrades the Redis Enterprise Software version and all modules in the database parallel_shards_upgrade integer \u0026lsquo;all\u0026rsquo; Maximum number of shards to upgrade all at once preserve_roles Performs an additional failover to guarantee the shards\u0026rsquo; roles are preserved redis_version Redis version Upgrades the database to the specified version instead of the latest version Returns Returns Done if the upgrade completed. Otherwise, it returns an error.\nExample $ rladmin upgrade db db:5 Monitoring e39c8e87-75f9-4891-8c86-78cf151b720b active - SMUpgradeBDB init active - SMUpgradeBDB check_slaves .active - SMUpgradeBDB prepare active - SMUpgradeBDB stop_forwarding oactive - SMUpgradeBDB start_wd active - SMUpgradeBDB wait_for_version .completed - SMUpgradeBDB Done upgrade module Upgrades Redis modules in use by a specific database.\nFor more information, see Upgrade modules.\nrladmin upgrade module db_name { db:\u0026lt;id\u0026gt; | \u0026lt;name\u0026gt; } module_name \u0026lt;mod_name\u0026gt; version \u0026lt;version\u0026gt; module_args \u0026lt;args_str\u0026gt; Parameters Parameters Type/Value Description db_name db:\u0026lt;id\u0026gt; name Upgrade a module for the specified database module_name \u0026lsquo;ReJSON\u0026rsquo;\n\u0026lsquo;graph\u0026rsquo;\n\u0026lsquo;search\u0026rsquo;\n\u0026lsquo;bf\u0026rsquo;\n\u0026lsquo;timeseries\u0026rsquo; Redis module to upgrade version module version number Upgrades the module to the specified version module_args \u0026lsquo;keep_args\u0026rsquo;\nstring Module configuration options For more information about module configuration options, see Module configuration options.\nReturns Returns Done if the upgrade completed. Otherwise, it returns an error.\nExample $ rladmin upgrade module db_name db:8 module_name graph version 20812 module_args \u0026#34;\u0026#34; Monitoring 21ac7659-e44c-4cc9-b243-a07922b2a6cc active - SMUpgradeBDB init active - SMUpgradeBDB wait_for_version Ocompleted - SMUpgradeBDB Done ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/modules/upgrade/","uriRel":"/rs/references/rest-api/requests/bdbs/modules/upgrade/","title":"Database upgrade modules requests","tags":[],"keywords":[],"description":"Upgrade Redis module requests","content":" Method Path Description POST /v1/bdbs/{uid}/modules/upgrade Upgrade module Upgrade module POST /v1/bdbs/{string: uid}/modules/upgrade Upgrades module version on a specific BDB.\nRequired permissions Permission name edit_bdb_module Request Example HTTP request POST /bdbs/1/modules/upgrade Example JSON body { \u0026#34;modules\u0026#34;: [ {\u0026#34;module_name\u0026#34;: \u0026#34;ReJson\u0026#34;, \u0026#34;current_semantic_version\u0026#34;: \u0026#34;2.2.1\u0026#34;, \u0026#34;new_module\u0026#34;: \u0026#34;aa3648d79bd4082d414587c42ea0b234\u0026#34;} ], \u0026#34;// Optional fields to fine-tune restart and failover behavior:\u0026#34;, \u0026#34;preserve_roles\u0026#34;: true, \u0026#34;may_discard_data\u0026#34;: false } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Field Type Description modules list List of dicts representing the modules that will be upgraded. Each dict must include: • current_module: uid of a module to upgrade • new_module: UID of the module we want to upgrade to • new_module_args: args list for the new module preserve_roles boolean Preserve shards’ master/replica roles (optional) may_discard_data boolean Discard data in a non-replicated non-persistent bdb (optional) Response Returns the upgraded module object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;name of database #1\u0026#34;, \u0026#34;module_id\u0026#34;: \u0026#34;aa3648d79bd4082d414587c42ea0b234\u0026#34;, \u0026#34;module_name\u0026#34;: \u0026#34;ReJson\u0026#34;, \u0026#34;semantic_version\u0026#34;: \u0026#34;2.2.2\u0026#34; \u0026#34;// additional fields...\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description missing_module Module is not present in cluster. module_downgrade_unsupported Module downgrade is not allowed. redis_incompatible_version Module min_redis_version is bigger than the current Redis version. redis_pack_incompatible_version Module min_redis_pack_version is bigger than the current Redis Enterprise version. unsupported_module_capabilities New version of module does support all the capabilities needed for the database configuration Status codes Code Description 200 OK Success, module updated on bdb. 404 Not Found bdb or node not found. 400 Bad Request Bad or missing configuration parameters. 406 Not Acceptable The requested configuration is invalid. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bdbs/upgrade/","uriRel":"/rs/references/rest-api/requests/bdbs/upgrade/","title":"Database upgrade requests","tags":[],"keywords":[],"description":"Database upgrade requests","content":" Method Path Description POST /v1/bdbs/{uid}/upgrade Upgrade database Upgrade database POST /v1/bdbs/{int: uid}/upgrade Upgrade a database.\nRequired permissions Permission name update_bdb_with_action Request Example HTTP request POST /bdbs/1/upgrade Example JSON body { \u0026#34;swap_roles\u0026#34;: true, \u0026#34;may_discard_data\u0026#34;: false } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Field Type Description force_restart boolean Restart shards even if no version change (default: false) keep_redis_version boolean Keep current Redis version (default: false) keep_crdt_protocol_version boolean Keep current crdt protocol version (default: false) may_discard_data boolean Discard data in a non-replicated, non-persistent bdb (default: false) force_discard boolean Discard data even if the bdb is replicated and/or persistent (default: false) preserve_roles boolean Preserve shards\u0026rsquo; master/replica roles (requires an extra failover) (default: false) parallel_shards_upgrade integer Max number of shards to upgrade in parallel (default: all) modules list of modules List of dicts representing the modules that will be upgraded.\nEach dict includes:\n• current_module: uid of a module to upgrade\n• new_module: uid of the module we want to upgrade to\n• new_module_args: args list for the new module (no defaults for the three module-related parameters). Response Returns the upgraded BDB object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;replication\u0026#34;: true, \u0026#34;data_persistence\u0026#34;: \u0026#34;aof\u0026#34;, \u0026#34;// additional fields...\u0026#34; } Status codes Code Description 200 OK Success, bdb upgrade initiated (action_uid can be used to track progress) 400 Bad Request Malformed or bad command 404 Not Found bdb not found 406 Not Acceptable New module version capabilities don\u0026rsquo;t comply with the database configuration 500 Internal Server Error Internal error ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/modules/upgrade/","uriRel":"/rs/references/rest-api/requests/modules/upgrade/","title":"Upgrade module requests","tags":[],"keywords":[],"description":"Upgrade module requests","content":" Method Path Description POST /v1/modules/upgrade/bdb/{uid} Upgrade module Upgrade module POST /v1/modules/upgrade/bdb/{string: uid} Upgrades the module version on a specific database.\nRequired permissions Permission name edit_bdb_module Request Example HTTP request POST /modules/upgrade/bdb/1 Example JSON body { \u0026#34;modules\u0026#34;: [ {\u0026#34;module_name\u0026#34;: \u0026#34;ReJson\u0026#34;, \u0026#34;current_semantic_version\u0026#34;: \u0026#34;2.2.1\u0026#34;, \u0026#34;new_module\u0026#34;: \u0026#34;aa3648d79bd4082d414587c42ea0b234\u0026#34;} ], \u0026#34;// Optional fields to fine-tune restart and failover behavior:\u0026#34;, \u0026#34;preserve_roles\u0026#34;: true, \u0026#34;may_discard_data\u0026#34;: false } Request headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Request body Field Type Description modules list List of dicts representing the modules that will be upgraded. Each dict must include: • current_module: UID of a module to upgrade • new_module: UID of the module we want to upgrade to • new_module_args: args list for the new module preserve_roles boolean Preserve shards’ master/replica roles (optional) may_discard_data boolean Discard data in a non-replicated non-persistent database (optional) Response Returns the upgraded module object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;name of database #1\u0026#34;, \u0026#34;module_id\u0026#34;: \u0026#34;aa3648d79bd4082d414587c42ea0b234\u0026#34;, \u0026#34;module_name\u0026#34;: \u0026#34;ReJson\u0026#34;, \u0026#34;semantic_version\u0026#34;: \u0026#34;2.2.2\u0026#34; \u0026#34;// additional fields...\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description missing_module Module is not present in cluster. module_downgrade_unsupported Module downgrade is not allowed. redis_incompatible_version Module min_redis_version is bigger than the current Redis version. redis_pack_incompatible_version Module min_redis_pack_version is bigger than the current Redis Enterprise version. unsupported_module_capabilities New version of module does support all the capabilities needed for the database configuration Status codes Code Description 200 OK Success, module updated on bdb. 404 Not Found bdb or node not found. 400 Bad Request Bad or missing configuration parameters. 406 Not Acceptable The requested configuration is invalid. ","categories":["RS"]},{"uri":"/rs/references/rest-api/objects/user/","uriRel":"/rs/references/rest-api/objects/user/","title":"User object","tags":[],"keywords":[],"description":"An object that represents a Redis Enterprise user","content":"An API object that represents a Redis Enterprise user.\nName Type/Value Description uid integer User\u0026rsquo;s unique ID account_id integer SM account ID action_uid string Action UID. If it exists, progress can be tracked by the GET /actions/{uid} API request (read-only) auth_method \u0026rsquo;external\u0026rsquo;\n\u0026lsquo;regular\u0026rsquo; User\u0026rsquo;s authentication method bdbs_email_alerts complex object UIDs of databases that user will receive alerts for cluster_email_alerts boolean Activate cluster email alerts for a user email string User\u0026rsquo;s email (pattern matching only ASCII characters) email_alerts boolean (default: true) Activate email alerts for a user name string User\u0026rsquo;s name (pattern does not allow non-ASCII and special characters \u0026amp;,\u0026lt;,\u0026gt;,\u0026quot;) password string User\u0026rsquo;s password. Note that it could also be an already hashed value, in which case the password_hash_method parameter is also provided. password_hash_method \u0026lsquo;1\u0026rsquo; Used when password is passed pre-hashed to specify the hashing method password_issue_date string The date in which the password was set (read-only) role \u0026lsquo;admin\u0026rsquo;\n\u0026lsquo;cluster_member\u0026rsquo;\n\u0026lsquo;cluster_viewer\u0026rsquo;\n\u0026lsquo;db_member\u0026rsquo;\n\u0026lsquo;db_viewer\u0026rsquo; \u0026lsquo;none\u0026rsquo; User\u0026rsquo;s role role_uids array of integers List of role UIDs associated with the LDAP group ","categories":["RS"]},{"uri":"/rc/accounts/user-profile/","uriRel":"/rc/accounts/user-profile/","title":"Manage user account and profile","tags":[],"keywords":[],"description":"Describes the how to manage your user account profile and how to switch between Redis Cloud accounts.","content":"When you sign in to the Redis Cloud admin console, you use a profile associated with one or more Redis Cloud accounts.\nThis account has a profile with settings that you can manage using the Profile control located near the top, right corner of the admin console:\nWhen you open the Profile control, you can:\nReview and manage your user account profile.\nSign out from the admin console.\nSwitch between Redis Cloud subscriptions administered by your user account.\nManage user profile To review your user profile settings, select User profile from the Profile control. This displays the User Profile screen:\nThis screen contains up to three sections, including:\nThe User details section includes basic information about your account, including First name, Last name, Job title, Email, and the date the account was created. The names and job title can be edited; other settings are read-only.\nThe Password section lets you change the password for accounts created and managed by Redis Cloud.\nIf you\u0026rsquo;re using single sign-on authentication, you cannot change the password using the User Profile screen. Such accounts are managed by an identity provider (IdP). For help changing (or recovering) the passwords for these accounts, consult your identity provider docs.\nThe Multi-factor authentication (MFA) section lets you manage MFA settings for the current user account.\nWhen you activate a mobile device, you can use SMS MFA as a second authentication factor.\nTo use an authentication app as the factor, you need to activate a mobile device and then use that device to enable the app.\nSign out To sign out from the admin console, select Logout from the profile control.\nSwitch Redis cloud accounts When your user account is authorized to manage multiple Redis Cloud accounts, each account is displayed in the Profile control.\nTo switch accounts, select the desired account from the list shown in the Profile control.\nSave or discard changes Use the Discard changes button to cancel user profile setting changes or the Save changes button to save changes.\n","categories":["RC"]},{"uri":"/rs/references/rest-api/requests/users/","uriRel":"/rs/references/rest-api/requests/users/","title":"Users requests","tags":[],"keywords":[],"description":"User requests","content":" Method Path Description GET /v1/users Get all users GET /v1/users/{uid} Get a single user PUT /v1/users/{uid} Update a user\u0026rsquo;s configuration POST /v1/users Create a new user DELETE /v1/users/{uid} Delete a user Get all users GET /v1/users Get a list of all users.\nPermissions Permission name Roles view_all_users_info admin Request Example HTTP request GET /users Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Response Returns a JSON array of user objects.\nExample JSON body [ { \u0026#34;uid\u0026#34;: 1, \u0026#34;password_issue_date\u0026#34;: \u0026#34;2017-03-02T09:43:34Z\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@redislabs.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; }, { \u0026#34;uid\u0026#34;: 2, \u0026#34;password_issue_date\u0026#34;: \u0026#34;2017-03-02T09:43:34Z\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user2@redislabs.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane Poe\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;role\u0026#34;: \u0026#34;db_viewer\u0026#34;, \u0026#34;auth_method\u0026#34;: \u0026#34;external\u0026#34; } ] Status codes Code Description 200 OK No error Get user GET /v1/users/{int: uid} Get a single user\u0026rsquo;s details.\nPermissions Permission name Roles view_user_info admin Request Example HTTP request GET /users/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The user\u0026rsquo;s unique ID Response Returns a user object that contains the details for the specified user ID.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;password_issue_date\u0026#34;: \u0026#34;2017-03-07T15:11:08Z\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;db_viewer\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;email\u0026#34;: \u0026#34;user@redislabs.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; } Status codes Code Description 200 OK Success. 403 Forbidden Operation is forbidden. 404 Not Found User does not exist. Update user PUT /v1/users/{int: uid} Update an existing user\u0026rsquo;s configuration.\nPermissions Permission name Roles update_user admin Any user can change their own name, password, or alert preferences.\nRequest Example HTTP request PUT /users/1 Example JSON body { \u0026#34;name\u0026#34;: \u0026#34;Jane Poe\u0026#34;, \u0026#34;email_alerts\u0026#34;: false } Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The user\u0026rsquo;s unique ID Request body Include a user object with updated fields in the request body.\nResponse Returns the updated user object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;password_issue_date\u0026#34;: \u0026#34;2017-03-07T15:11:08Z\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@redislabs.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane Poe\u0026#34;, \u0026#34;email_alerts\u0026#34;: false, \u0026#34;role\u0026#34;: \u0026#34;db_viewer\u0026#34;, \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; } Note: For RBAC-enabled clusters, the returned user details include role_uids instead of role. Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information. The following are possible error_code values:\nCode Description password_not_complex The given password is not complex enough (Only works when the password_complexity feature is enabled). new_password_same_as_current The given new password is identical to the old password. email_already_exists The given email is already taken. change_last_admin_role_not_allowed At least one user with admin role should exist. Status codes Code Description 200 OK Success, the user is updated. 400 Bad Request Bad or missing configuration parameters. 404 Not Found Attempting to change a non-existing user. 406 Not Acceptable The requested configuration is invalid. Create user POST /v1/users Create a new user.\nPermissions Permission name Roles create_new_user admin Request Example HTTP request POST /users Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type Body Include a single user object in the request body. The user object must have an email, password, and role.\nNote: For RBAC-enabled clusters, use role_uids instead of role in the request body. email_alerts can be configured either as:\ntrue - user will receive alerts for all databases configured in bdbs_email_alerts. The user will receive alerts for all databases by default if bdbs_email_alerts is not configured. bdbs_email_alerts can be a list of database UIDs or [‘all’] meaning all databases.\nfalse - user will not receive alerts for any databases\nExample JSON body { \u0026#34;email\u0026#34;: \u0026#34;newuser@redis.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my-password\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Pat Doe\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;role_uids\u0026#34;: [ 3, 4 ], \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; } Response Returns the newly created user object.\nExample JSON body { \u0026#34;uid\u0026#34;: 1, \u0026#34;password_issue_date\u0026#34;: \u0026#34;2017-03-07T15:11:08Z\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@redislabs.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane Poe\u0026#34;, \u0026#34;email_alerts\u0026#34;: true, \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \u0026#34;role\u0026#34;: \u0026#34;db_viewer\u0026#34;, \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; } Error codes When errors are reported, the server may return a JSON object with error_code and message field that provide additional information.\nThe following are possible error_code values:\nCode Description password_not_complex The given password is not complex enough (Only works when the password_complexity feature is enabled). email_already_exists The given email is already taken. name_already_exists The given name is already taken. Status codes Code Description 200 OK Success, user is created. 400 Bad Request Bad or missing configuration parameters. 409 Conflict User with the same email already exists. Examples cURL $ curl -k -X POST -u \u0026#39;[username]:[password]\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;newuser@redis.com\u0026#34;, \\ \u0026#34;password\u0026#34;: \u0026#34;my-password\u0026#34;, \\ \u0026#34;name\u0026#34;: \u0026#34;Pat Doe\u0026#34;, \\ \u0026#34;email_alerts\u0026#34;: true, \\ \u0026#34;bdbs_email_alerts\u0026#34;: [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;], \\ \u0026#34;role_uids\u0026#34;: [ 3, 4 ], \\ \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; }\u0026#39; \\ \u0026#39;https://[host][:port]/v1/users\u0026#39; Python import requests import json url = \u0026#34;https://[host][:port]/v1/users\u0026#34; auth = (\u0026#34;[username]\u0026#34;, \u0026#34;[password]\u0026#34;) payload = json.dumps({ \u0026#34;email\u0026#34;: \u0026#34;newuser@redis.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;my-password\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Pat Doe\u0026#34;, \u0026#34;email_alerts\u0026#34;: True, \u0026#34;bdbs_email_alerts\u0026#34;: [ \u0026#34;1\u0026#34;, \u0026#34;2\u0026#34; ], \u0026#34;role_uids\u0026#34;: [ 3, 4 ], \u0026#34;auth_method\u0026#34;: \u0026#34;regular\u0026#34; }) headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } response = requests.request(\u0026#34;POST\u0026#34;, url, auth=auth, headers=headers, data=payload, verify=False) print(response.text) Delete user DELETE /v1/users/{int: uid} Delete a user.\nPermissions Permission name Roles delete_user admin Request Example HTTP request DELETE /users/1 Headers Key Value Description Host cnm.cluster.fqdn Domain name Accept application/json Accepted media type URL parameters Field Type Description uid integer The user\u0026rsquo;s unique ID Response Returns a status code to indicate the success or failure of the user deletion.\nStatus codes Code Description 200 OK Success, the user is deleted. 406 Not Acceptable The request is not acceptable. ","categories":["RS"]},{"uri":"/rs/references/rest-api/requests/bootstrap/validate/","uriRel":"/rs/references/rest-api/requests/bootstrap/validate/","title":"Bootstrap validation requests","tags":[],"keywords":[],"description":"Boostrap validation requests","content":" Method Path Description POST /v1/bootstrap/validate/{action} Perform bootstrap validation Bootstrap validation POST /v1/bootstrap/validate/{action} Perform bootstrap validation.\nUnlike actual bootstrapping, this request blocks and immediately returns with a response.\nRequest Example HTTP request POST /bootstrap/validate/join_cluster Request body The request must contain a bootstrap configuration object, similar to the one used for actual bootstrapping.\nResponse If an error occurs, the call returns a bootstrap_status JSON object that contains the following fields:\nField Description state Current bootstrap state.\nidle: No bootstrapping started.\ninitiated: Bootstrap request received.\ncreating_cluster: In the process of creating a new cluster.\njoining_cluster: In the process of joining an existing cluster.\nerror: The last bootstrap action failed.\ncompleted: The last bootstrap action completed successfully. start_time Bootstrap process start time end_time Bootstrap process end time error_code If state is error, this error code describes the type of error encountered. error_details An error-specific object that may contain additional information about the error. A common field in use is message which provides a more verbose error message. Status codes Code Description 200 OK No error, validation was successful. 406 Not Acceptable Validation failed, bootstrap status is returned as body. ","categories":["RS"]},{"uri":"/rs/references/cli-utilities/rladmin/verify/","uriRel":"/rs/references/cli-utilities/rladmin/verify/","title":"rladmin verify","tags":[],"keywords":[],"description":"Prints verification reports for the cluster.","content":"Prints verification reports for the cluster.\nverify balance Prints a balance report that displays all of the unbalanced endpoints or nodes in the cluster.\nrladmin verify balance [ node \u0026lt;ID\u0026gt; ] The proxy policy determines which nodes or endpoints to report as unbalanced.\nA node is unbalanced if:\nall-nodes proxy policy and the node has no endpoint An endpoint is unbalanced in the following cases:\nsingle proxy policy and one of the following is true: Shard placement is sparse and none of the master shards are on the node Shard placement is dense and some master shards are on a different node from the endpoint all-master-shards proxy policy and one of the following is true: None of the master shards are on the node Some master shards are on a different node from the endpoint Parameters Parameter Type/Value Description node integer Specify a node ID to return a balance table for that node only (optional) Returns Returns a table of unbalanced endpoints and nodes in the cluster.\nExamples Verify all nodes:\n$ rladmin verify balance The table presents all of the unbalanced endpoints/nodes in the cluster BALANCE: NODE:ID DB:ID NAME ENDPOINT:ID PROXY_POLICY LOCAL SHARDS TOTAL SHARDS Verify a specific node:\n$ rladmin verify balance node 1 The table presents all of the unbalanced endpoints/nodes in the cluster BALANCE: NODE:ID DB:ID NAME ENDPOINT:ID PROXY_POLICY LOCAL SHARDS TOTAL SHARDS verify rack_aware Verifies that the cluster complies with the rack awareness policy and reports any discovered rack collisions, if rack-zone awareness is enabled.\nrladmin verify rack_aware Parameters None\nReturns Returns whether the cluster is rack aware. If rack awareness is enabled, it returns any rack collisions.\nExample $ rladmin verify rack_aware Cluster policy is not configured for rack awareness. ","categories":["RS"]},{"uri":"/rs/clusters/optimize/wait/","uriRel":"/rs/clusters/optimize/wait/","title":"Use the WAIT command for strong consistency","tags":[],"keywords":[],"description":"Use the wait command to take full advantage of Redis Enterprise Software&#39;s replication based durability.","content":"Redis Enterprise Software comes with the ability to replicate data to another replica for high availability and persist in-memory data on disk permanently for durability. With the WAIT command, you can control the consistency and durability guarantees for the replicated and persisted database.\nAny updates that are issued to the database are typically performed with the following flow shown below;\nApplication issues a write, Proxy communicates with the correct master \u0026ldquo;shard\u0026rdquo; in the system that contains the given key, The acknowledgment is sent to proxy once the write operation completes The proxy sends the acknowledgment back to the application. Independently, the write is communicated from master to replica and replication acknowledges the write back to the master. These are steps 5 and 6.\nIndependently, the write to a replica is also persisted to disk and acknowledged within the replica. These are steps 7 and 8.\nWith the WAIT command, applications can ask to wait for acknowledgments only after replication or persistence is confirmed on the replica. The flow of a write operation with the WAIT command is shown below:\nApplication issues a write, Proxy communicates with the correct master \u0026ldquo;shard\u0026rdquo; in the system that contains the given key, Replication communicated the update to the replica shard. Replica persists the update to disk (assuming AOF every write setting is selected). The acknowledgment is sent back from the replica all the way to the proxy with steps 5 to 8. With this flow, the application only gets the acknowledgment from the write after durability is achieved with replication to the replica and to the persistent storage.\nWith the WAIT command, applications can have a guarantee that even under a node failure or node restart, an acknowledged write is recorded.\nSee the WAIT command for details on the new durability and consistency options.\n","categories":["RS"]}]